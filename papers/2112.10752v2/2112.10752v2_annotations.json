{
    "annotations": [
        {
            "text": "# High-Resolution Image Synthesis with Latent Diffusion Models",
            "page": 1,
            "x": 99,
            "y": 66,
            "width": 395,
            "height": 20,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "0-title",
            "chunk_id": "80acbeaa-8136-4372-9118-a7e24685bbad",
            "group_text": "# High-Resolution Image Synthesis with Latent Diffusion Models\n\nRobin Rombach\\textsuperscript{1 *} \\hspace{1cm} Andreas Blattmann\\textsuperscript{1 *} \\hspace{1cm} Dominik Lorenz\\textsuperscript{1} \\hspace{1cm} Patrick Esser\\textsuperscript{R} \\hspace{1cm} Bj\u00f6rn Ommer\\textsuperscript{1}\n\n\\textsuperscript{1}Ludwig Maximilian University of Munich & IWR, Heidelberg University, Germany \\hspace{0.5cm} \\textsuperscript{R}Runway ML\n\nhttps://github.com/CompVis/latent-diffusion"
        },
        {
            "text": "Robin Rombach\\textsuperscript{1 *} \\hspace{1cm} Andreas Blattmann\\textsuperscript{1 *} \\hspace{1cm} Dominik Lorenz\\textsuperscript{1} \\hspace{1cm} Patrick Esser\\textsuperscript{R} \\hspace{1cm} Bj\u00f6rn Ommer\\textsuperscript{1}\n\n\\textsuperscript{1}Ludwig Maximilian University of Munich & IWR, Heidelberg University, Germany \\hspace{0.5cm} \\textsuperscript{R}Runway ML\n\nhttps://github.com/CompVis/latent-diffusion",
            "page": 1,
            "x": 52,
            "y": 105,
            "width": 526,
            "height": 46,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "0-title",
            "chunk_id": "f4f74b8f-820a-4b8e-9590-0d557ef83fc3",
            "group_text": "# High-Resolution Image Synthesis with Latent Diffusion Models\n\nRobin Rombach\\textsuperscript{1 *} \\hspace{1cm} Andreas Blattmann\\textsuperscript{1 *} \\hspace{1cm} Dominik Lorenz\\textsuperscript{1} \\hspace{1cm} Patrick Esser\\textsuperscript{R} \\hspace{1cm} Bj\u00f6rn Ommer\\textsuperscript{1}\n\n\\textsuperscript{1}Ludwig Maximilian University of Munich & IWR, Heidelberg University, Germany \\hspace{0.5cm} \\textsuperscript{R}Runway ML\n\nhttps://github.com/CompVis/latent-diffusion"
        },
        {
            "text": "# Abstract\n\n*By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state-of-the-art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including text-to-image synthesis, unconditional image generation and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.*",
            "page": 1,
            "x": 46,
            "y": 176,
            "width": 244,
            "height": 351,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "1-abstract",
            "chunk_id": "9b659559-4f42-49d0-8c40-948c1ea6daed",
            "group_text": "# Abstract\n\n*By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state-of-the-art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including text-to-image synthesis, unconditional image generation and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.*"
        },
        {
            "text": "1. Introduction\n\n    Image synthesis is one of the computer vision fields with the most spectacular recent development, but also among those with the greatest computational demands.    Especially high-resolution synthesis of complex, natural scenes is presently dominated by scaling up likelihood-based models, potentially containing billions of parameters in autoregressive (AR) transformers [66, 67]. In contrast, the promising results of GANs [3, 27, 40] have been revealed to be mostly confined to data with comparably limited variability as their adversarial learning procedure does not easily scale to modeling complex, multi-modal distributions. Recently, diffusion models [82], which are built from a hierarchy of denoising autoencoders, have shown to achieve impressive",
            "page": 1,
            "x": 46,
            "y": 534,
            "width": 243,
            "height": 174,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "036afae0-573f-47eb-b949-bac65253ccdc",
            "group_text": "1. Introduction\n\n    Image synthesis is one of the computer vision fields with the most spectacular recent development, but also among those with the greatest computational demands.    Especially high-resolution synthesis of complex, natural scenes is presently dominated by scaling up likelihood-based models, potentially containing billions of parameters in autoregressive (AR) transformers [66, 67]. In contrast, the promising results of GANs [3, 27, 40] have been revealed to be mostly confined to data with comparably limited variability as their adversarial learning procedure does not easily scale to modeling complex, multi-modal distributions. Recently, diffusion models [82], which are built from a hierarchy of denoising autoencoders, have shown to achieve impressive\n\nFigure 1.  Boosting the upper bound on achievable quality with less agressive downsampling. Since diffusion models offer excellent inductive biases for spatial data, we do not need the heavy spatial downsampling of related generative models in latent space, but can still greatly reduce the dimensionality of the data via suitable autoencoding models, see Sec. 3. Images are from the DIV2K [1] validation set, evaluated at $512^2$ px. We denote the spatial downsampling factor by $f$. Reconstruction FIDs [29] and PSNR are calculated on ImageNet-val. [12]; see also Tab. 8.\n\nresults in image synthesis [30, 85] and beyond [7, 45, 48, 57],\nand define the state-of-the-art in class-conditional image\nsynthesis [15, 31] and super-resolution [72]. Moreover, even\nunconditional DMs can readily be applied to tasks such\nas inpainting and colorization [85] or stroke-based syn-\nthesis [53], in contrast to other types of generative mod-\nels [19, 46, 69]. Being likelihood-based models, they do not\nexhibit mode-collapse and training instabilities as GANs\nand, by heavily exploiting parameter sharing, they can\nmodel highly complex distributions of natural images with-\nout involving billions of parameters as in AR models [67].\n**Democratizing High-Resolution Image Synthesis** DMs\nbelong to the class of likelihood-based models, whose\nmode-covering behavior makes them prone to spend ex-\ncessive amounts of capacity (and thus compute resources)\non modeling imperceptible details of the data [16, 73]. Al-\nthough the reweighted variational objective [30] aims to ad-\ndress this by undersampling the initial denoising steps, DMs\nare still computationally demanding, since training and\nevaluating such a model requires repeated function evalu-\nations (and gradient computations) in the high-dimensional\nspace of RGB images. As an example, training the most\npowerful DMs often takes hundreds of GPU days (e.g. 150 -\n1000 V100 days in [15]) and repeated evaluations on a noisy\nversion of the input space render also inference expensive,\n\nso that producing 50k samples takes approximately 5 days\n[15] on a single A100 GPU. This has two consequences for\nthe research community and users in general: Firstly, train-\ning such a model requires massive computational resources\nonly available to a small fraction of the field, and leaves a\nhuge carbon footprint [65, 86]. Secondly, evaluating an al-\nready trained model is also expensive in time and memory,\nsince the same model architecture must run sequentially for\na large number of steps (e.g. 25 - 1000 steps in [15]).\n\nTo increase the accessibility of this powerful model class and at the same time reduce its significant resource consumption, a method is needed that reduces the computational complexity for both training and sampling. Reducing the computational demands of DMs without impairing their performance is, therefore, key to enhance their accessibility.\n\n**Departure to Latent Space** Our approach starts with the analysis of already trained diffusion models in pixel space: Fig. 2 shows the rate-distortion trade-off of a trained model. As with any likelihood-based model, learning can be roughly divided into two stages: First is a *perceptual compression* stage which removes high-frequency details but still learns little semantic variation. In the second stage, the actual generative model learns the semantic and conceptual composition of the data (*semantic compression*). We thus aim to first find a *perceptually equivalent, but computationally more suitable space*, in which we will train diffusion models for high-resolution image synthesis.\n\nFollowing common practice [11, 23, 66, 67, 96], we separate training into two distinct phases: First, we train an autoencoder which provides a lower-dimensional (and thereby efficient) representational space which is perceptually equivalent to the data space. Importantly, and in contrast to previous work [23, 66], we do not need to rely on excessive spatial compression, as we train DMs in the learned latent space, which exhibits better scaling properties with respect to the spatial dimensionality. The reduced complexity also provides efficient image generation from the latent space with a single network pass. We dub the resulting model class *Latent Diffusion Models* (LDMs).\n\nA notable advantage of this approach is that we need to train the universal autoencoding stage only once and can therefore reuse it for multiple DM trainings or to explore possibly completely different tasks [81]. This enables efficient exploration of a large number of diffusion models for various image-to-image and text-to-image tasks. For the latter, we design an architecture that connects transformers to the DM\u2019s UNet backbone [71] and enables arbitrary types of token-based conditioning mechanisms, see Sec. 3.3.\n\nIn sum, our work makes the following **contributions**:\n(i) In contrast to purely transformer-based approaches [23, 66], our method scales more graceful to higher dimensional data and can thus (a) work on a compression level which provides more faithful and detailed reconstructions than previous work (see Fig. 1) and (b) can be efficiently\n\napplied to high-resolution synthesis of megapixel images.\n    (ii) We achieve competitive performance on multiple\ntasks (unconditional image synthesis, inpainting, stochastic\nsuper-resolution) and datasets while significantly lowering\ncomputational costs. Compared to pixel-based diffusion ap-\nproaches, we also significantly decrease inference costs.\n\n(iii) We show that, in contrast to previous work [93] which learns both an encoder/decoder architecture and a score-based prior simultaneously, our approach does not require a delicate weighting of reconstruction and generative abilities. This ensures extremely faithful reconstructions and requires very little regularization of the latent space.\n\n(iv) We find that for densely conditioned tasks such as super-resolution, inpainting and semantic synthesis, our model can be applied in a convolutional fashion and render large, consistent images of $\\sim 1024^2$ px.\n\n(v) Moreover, we design a general-purpose conditioning mechanism based on cross-attention, enabling multi-modal training. We use it to train class-conditional, text-to-image and layout-to-image models.\n\n(vi) Finally, we release pretrained latent diffusion and autoencoding models at https://github.com/CompVis/latent-diffusion which might be reusable for a various tasks besides training of DMs [81]."
        },
        {
            "text": "Figure 1.  Boosting the upper bound on achievable quality with less agressive downsampling. Since diffusion models offer excellent inductive biases for spatial data, we do not need the heavy spatial downsampling of related generative models in latent space, but can still greatly reduce the dimensionality of the data via suitable autoencoding models, see Sec. 3. Images are from the DIV2K [1] validation set, evaluated at $512^2$ px. We denote the spatial downsampling factor by $f$. Reconstruction FIDs [29] and PSNR are calculated on ImageNet-val. [12]; see also Tab. 8.",
            "page": 1,
            "x": 306,
            "y": 318,
            "width": 242,
            "height": 101,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "eeb790ec-7aeb-4b4d-8f71-9604faa66ffa",
            "group_text": "1. Introduction\n\n    Image synthesis is one of the computer vision fields with the most spectacular recent development, but also among those with the greatest computational demands.    Especially high-resolution synthesis of complex, natural scenes is presently dominated by scaling up likelihood-based models, potentially containing billions of parameters in autoregressive (AR) transformers [66, 67]. In contrast, the promising results of GANs [3, 27, 40] have been revealed to be mostly confined to data with comparably limited variability as their adversarial learning procedure does not easily scale to modeling complex, multi-modal distributions. Recently, diffusion models [82], which are built from a hierarchy of denoising autoencoders, have shown to achieve impressive\n\nFigure 1.  Boosting the upper bound on achievable quality with less agressive downsampling. Since diffusion models offer excellent inductive biases for spatial data, we do not need the heavy spatial downsampling of related generative models in latent space, but can still greatly reduce the dimensionality of the data via suitable autoencoding models, see Sec. 3. Images are from the DIV2K [1] validation set, evaluated at $512^2$ px. We denote the spatial downsampling factor by $f$. Reconstruction FIDs [29] and PSNR are calculated on ImageNet-val. [12]; see also Tab. 8.\n\nresults in image synthesis [30, 85] and beyond [7, 45, 48, 57],\nand define the state-of-the-art in class-conditional image\nsynthesis [15, 31] and super-resolution [72]. Moreover, even\nunconditional DMs can readily be applied to tasks such\nas inpainting and colorization [85] or stroke-based syn-\nthesis [53], in contrast to other types of generative mod-\nels [19, 46, 69]. Being likelihood-based models, they do not\nexhibit mode-collapse and training instabilities as GANs\nand, by heavily exploiting parameter sharing, they can\nmodel highly complex distributions of natural images with-\nout involving billions of parameters as in AR models [67].\n**Democratizing High-Resolution Image Synthesis** DMs\nbelong to the class of likelihood-based models, whose\nmode-covering behavior makes them prone to spend ex-\ncessive amounts of capacity (and thus compute resources)\non modeling imperceptible details of the data [16, 73]. Al-\nthough the reweighted variational objective [30] aims to ad-\ndress this by undersampling the initial denoising steps, DMs\nare still computationally demanding, since training and\nevaluating such a model requires repeated function evalu-\nations (and gradient computations) in the high-dimensional\nspace of RGB images. As an example, training the most\npowerful DMs often takes hundreds of GPU days (e.g. 150 -\n1000 V100 days in [15]) and repeated evaluations on a noisy\nversion of the input space render also inference expensive,\n\nso that producing 50k samples takes approximately 5 days\n[15] on a single A100 GPU. This has two consequences for\nthe research community and users in general: Firstly, train-\ning such a model requires massive computational resources\nonly available to a small fraction of the field, and leaves a\nhuge carbon footprint [65, 86]. Secondly, evaluating an al-\nready trained model is also expensive in time and memory,\nsince the same model architecture must run sequentially for\na large number of steps (e.g. 25 - 1000 steps in [15]).\n\nTo increase the accessibility of this powerful model class and at the same time reduce its significant resource consumption, a method is needed that reduces the computational complexity for both training and sampling. Reducing the computational demands of DMs without impairing their performance is, therefore, key to enhance their accessibility.\n\n**Departure to Latent Space** Our approach starts with the analysis of already trained diffusion models in pixel space: Fig. 2 shows the rate-distortion trade-off of a trained model. As with any likelihood-based model, learning can be roughly divided into two stages: First is a *perceptual compression* stage which removes high-frequency details but still learns little semantic variation. In the second stage, the actual generative model learns the semantic and conceptual composition of the data (*semantic compression*). We thus aim to first find a *perceptually equivalent, but computationally more suitable space*, in which we will train diffusion models for high-resolution image synthesis.\n\nFollowing common practice [11, 23, 66, 67, 96], we separate training into two distinct phases: First, we train an autoencoder which provides a lower-dimensional (and thereby efficient) representational space which is perceptually equivalent to the data space. Importantly, and in contrast to previous work [23, 66], we do not need to rely on excessive spatial compression, as we train DMs in the learned latent space, which exhibits better scaling properties with respect to the spatial dimensionality. The reduced complexity also provides efficient image generation from the latent space with a single network pass. We dub the resulting model class *Latent Diffusion Models* (LDMs).\n\nA notable advantage of this approach is that we need to train the universal autoencoding stage only once and can therefore reuse it for multiple DM trainings or to explore possibly completely different tasks [81]. This enables efficient exploration of a large number of diffusion models for various image-to-image and text-to-image tasks. For the latter, we design an architecture that connects transformers to the DM\u2019s UNet backbone [71] and enables arbitrary types of token-based conditioning mechanisms, see Sec. 3.3.\n\nIn sum, our work makes the following **contributions**:\n(i) In contrast to purely transformer-based approaches [23, 66], our method scales more graceful to higher dimensional data and can thus (a) work on a compression level which provides more faithful and detailed reconstructions than previous work (see Fig. 1) and (b) can be efficiently\n\napplied to high-resolution synthesis of megapixel images.\n    (ii) We achieve competitive performance on multiple\ntasks (unconditional image synthesis, inpainting, stochastic\nsuper-resolution) and datasets while significantly lowering\ncomputational costs. Compared to pixel-based diffusion ap-\nproaches, we also significantly decrease inference costs.\n\n(iii) We show that, in contrast to previous work [93] which learns both an encoder/decoder architecture and a score-based prior simultaneously, our approach does not require a delicate weighting of reconstruction and generative abilities. This ensures extremely faithful reconstructions and requires very little regularization of the latent space.\n\n(iv) We find that for densely conditioned tasks such as super-resolution, inpainting and semantic synthesis, our model can be applied in a convolutional fashion and render large, consistent images of $\\sim 1024^2$ px.\n\n(v) Moreover, we design a general-purpose conditioning mechanism based on cross-attention, enabling multi-modal training. We use it to train class-conditional, text-to-image and layout-to-image models.\n\n(vi) Finally, we release pretrained latent diffusion and autoencoding models at https://github.com/CompVis/latent-diffusion which might be reusable for a various tasks besides training of DMs [81]."
        },
        {
            "text": "results in image synthesis [30, 85] and beyond [7, 45, 48, 57],\nand define the state-of-the-art in class-conditional image\nsynthesis [15, 31] and super-resolution [72]. Moreover, even\nunconditional DMs can readily be applied to tasks such\nas inpainting and colorization [85] or stroke-based syn-\nthesis [53], in contrast to other types of generative mod-\nels [19, 46, 69]. Being likelihood-based models, they do not\nexhibit mode-collapse and training instabilities as GANs\nand, by heavily exploiting parameter sharing, they can\nmodel highly complex distributions of natural images with-\nout involving billions of parameters as in AR models [67].\n**Democratizing High-Resolution Image Synthesis** DMs\nbelong to the class of likelihood-based models, whose\nmode-covering behavior makes them prone to spend ex-\ncessive amounts of capacity (and thus compute resources)\non modeling imperceptible details of the data [16, 73]. Al-\nthough the reweighted variational objective [30] aims to ad-\ndress this by undersampling the initial denoising steps, DMs\nare still computationally demanding, since training and\nevaluating such a model requires repeated function evalu-\nations (and gradient computations) in the high-dimensional\nspace of RGB images. As an example, training the most\npowerful DMs often takes hundreds of GPU days (e.g. 150 -\n1000 V100 days in [15]) and repeated evaluations on a noisy\nversion of the input space render also inference expensive,",
            "page": 1,
            "x": 305,
            "y": 426,
            "width": 242,
            "height": 300,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "3c24182c-75f9-4d28-81f4-bb6f34f3e528",
            "group_text": "1. Introduction\n\n    Image synthesis is one of the computer vision fields with the most spectacular recent development, but also among those with the greatest computational demands.    Especially high-resolution synthesis of complex, natural scenes is presently dominated by scaling up likelihood-based models, potentially containing billions of parameters in autoregressive (AR) transformers [66, 67]. In contrast, the promising results of GANs [3, 27, 40] have been revealed to be mostly confined to data with comparably limited variability as their adversarial learning procedure does not easily scale to modeling complex, multi-modal distributions. Recently, diffusion models [82], which are built from a hierarchy of denoising autoencoders, have shown to achieve impressive\n\nFigure 1.  Boosting the upper bound on achievable quality with less agressive downsampling. Since diffusion models offer excellent inductive biases for spatial data, we do not need the heavy spatial downsampling of related generative models in latent space, but can still greatly reduce the dimensionality of the data via suitable autoencoding models, see Sec. 3. Images are from the DIV2K [1] validation set, evaluated at $512^2$ px. We denote the spatial downsampling factor by $f$. Reconstruction FIDs [29] and PSNR are calculated on ImageNet-val. [12]; see also Tab. 8.\n\nresults in image synthesis [30, 85] and beyond [7, 45, 48, 57],\nand define the state-of-the-art in class-conditional image\nsynthesis [15, 31] and super-resolution [72]. Moreover, even\nunconditional DMs can readily be applied to tasks such\nas inpainting and colorization [85] or stroke-based syn-\nthesis [53], in contrast to other types of generative mod-\nels [19, 46, 69]. Being likelihood-based models, they do not\nexhibit mode-collapse and training instabilities as GANs\nand, by heavily exploiting parameter sharing, they can\nmodel highly complex distributions of natural images with-\nout involving billions of parameters as in AR models [67].\n**Democratizing High-Resolution Image Synthesis** DMs\nbelong to the class of likelihood-based models, whose\nmode-covering behavior makes them prone to spend ex-\ncessive amounts of capacity (and thus compute resources)\non modeling imperceptible details of the data [16, 73]. Al-\nthough the reweighted variational objective [30] aims to ad-\ndress this by undersampling the initial denoising steps, DMs\nare still computationally demanding, since training and\nevaluating such a model requires repeated function evalu-\nations (and gradient computations) in the high-dimensional\nspace of RGB images. As an example, training the most\npowerful DMs often takes hundreds of GPU days (e.g. 150 -\n1000 V100 days in [15]) and repeated evaluations on a noisy\nversion of the input space render also inference expensive,\n\nso that producing 50k samples takes approximately 5 days\n[15] on a single A100 GPU. This has two consequences for\nthe research community and users in general: Firstly, train-\ning such a model requires massive computational resources\nonly available to a small fraction of the field, and leaves a\nhuge carbon footprint [65, 86]. Secondly, evaluating an al-\nready trained model is also expensive in time and memory,\nsince the same model architecture must run sequentially for\na large number of steps (e.g. 25 - 1000 steps in [15]).\n\nTo increase the accessibility of this powerful model class and at the same time reduce its significant resource consumption, a method is needed that reduces the computational complexity for both training and sampling. Reducing the computational demands of DMs without impairing their performance is, therefore, key to enhance their accessibility.\n\n**Departure to Latent Space** Our approach starts with the analysis of already trained diffusion models in pixel space: Fig. 2 shows the rate-distortion trade-off of a trained model. As with any likelihood-based model, learning can be roughly divided into two stages: First is a *perceptual compression* stage which removes high-frequency details but still learns little semantic variation. In the second stage, the actual generative model learns the semantic and conceptual composition of the data (*semantic compression*). We thus aim to first find a *perceptually equivalent, but computationally more suitable space*, in which we will train diffusion models for high-resolution image synthesis.\n\nFollowing common practice [11, 23, 66, 67, 96], we separate training into two distinct phases: First, we train an autoencoder which provides a lower-dimensional (and thereby efficient) representational space which is perceptually equivalent to the data space. Importantly, and in contrast to previous work [23, 66], we do not need to rely on excessive spatial compression, as we train DMs in the learned latent space, which exhibits better scaling properties with respect to the spatial dimensionality. The reduced complexity also provides efficient image generation from the latent space with a single network pass. We dub the resulting model class *Latent Diffusion Models* (LDMs).\n\nA notable advantage of this approach is that we need to train the universal autoencoding stage only once and can therefore reuse it for multiple DM trainings or to explore possibly completely different tasks [81]. This enables efficient exploration of a large number of diffusion models for various image-to-image and text-to-image tasks. For the latter, we design an architecture that connects transformers to the DM\u2019s UNet backbone [71] and enables arbitrary types of token-based conditioning mechanisms, see Sec. 3.3.\n\nIn sum, our work makes the following **contributions**:\n(i) In contrast to purely transformer-based approaches [23, 66], our method scales more graceful to higher dimensional data and can thus (a) work on a compression level which provides more faithful and detailed reconstructions than previous work (see Fig. 1) and (b) can be efficiently\n\napplied to high-resolution synthesis of megapixel images.\n    (ii) We achieve competitive performance on multiple\ntasks (unconditional image synthesis, inpainting, stochastic\nsuper-resolution) and datasets while significantly lowering\ncomputational costs. Compared to pixel-based diffusion ap-\nproaches, we also significantly decrease inference costs.\n\n(iii) We show that, in contrast to previous work [93] which learns both an encoder/decoder architecture and a score-based prior simultaneously, our approach does not require a delicate weighting of reconstruction and generative abilities. This ensures extremely faithful reconstructions and requires very little regularization of the latent space.\n\n(iv) We find that for densely conditioned tasks such as super-resolution, inpainting and semantic synthesis, our model can be applied in a convolutional fashion and render large, consistent images of $\\sim 1024^2$ px.\n\n(v) Moreover, we design a general-purpose conditioning mechanism based on cross-attention, enabling multi-modal training. We use it to train class-conditional, text-to-image and layout-to-image models.\n\n(vi) Finally, we release pretrained latent diffusion and autoencoding models at https://github.com/CompVis/latent-diffusion which might be reusable for a various tasks besides training of DMs [81]."
        },
        {
            "text": "so that producing 50k samples takes approximately 5 days\n[15] on a single A100 GPU. This has two consequences for\nthe research community and users in general: Firstly, train-\ning such a model requires massive computational resources\nonly available to a small fraction of the field, and leaves a\nhuge carbon footprint [65, 86]. Secondly, evaluating an al-\nready trained model is also expensive in time and memory,\nsince the same model architecture must run sequentially for\na large number of steps (e.g. 25 - 1000 steps in [15]).",
            "page": 2,
            "x": 46,
            "y": 72,
            "width": 243,
            "height": 109,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "f96deb89-8899-498d-b7ca-7167d3b6830e",
            "group_text": "1. Introduction\n\n    Image synthesis is one of the computer vision fields with the most spectacular recent development, but also among those with the greatest computational demands.    Especially high-resolution synthesis of complex, natural scenes is presently dominated by scaling up likelihood-based models, potentially containing billions of parameters in autoregressive (AR) transformers [66, 67]. In contrast, the promising results of GANs [3, 27, 40] have been revealed to be mostly confined to data with comparably limited variability as their adversarial learning procedure does not easily scale to modeling complex, multi-modal distributions. Recently, diffusion models [82], which are built from a hierarchy of denoising autoencoders, have shown to achieve impressive\n\nFigure 1.  Boosting the upper bound on achievable quality with less agressive downsampling. Since diffusion models offer excellent inductive biases for spatial data, we do not need the heavy spatial downsampling of related generative models in latent space, but can still greatly reduce the dimensionality of the data via suitable autoencoding models, see Sec. 3. Images are from the DIV2K [1] validation set, evaluated at $512^2$ px. We denote the spatial downsampling factor by $f$. Reconstruction FIDs [29] and PSNR are calculated on ImageNet-val. [12]; see also Tab. 8.\n\nresults in image synthesis [30, 85] and beyond [7, 45, 48, 57],\nand define the state-of-the-art in class-conditional image\nsynthesis [15, 31] and super-resolution [72]. Moreover, even\nunconditional DMs can readily be applied to tasks such\nas inpainting and colorization [85] or stroke-based syn-\nthesis [53], in contrast to other types of generative mod-\nels [19, 46, 69]. Being likelihood-based models, they do not\nexhibit mode-collapse and training instabilities as GANs\nand, by heavily exploiting parameter sharing, they can\nmodel highly complex distributions of natural images with-\nout involving billions of parameters as in AR models [67].\n**Democratizing High-Resolution Image Synthesis** DMs\nbelong to the class of likelihood-based models, whose\nmode-covering behavior makes them prone to spend ex-\ncessive amounts of capacity (and thus compute resources)\non modeling imperceptible details of the data [16, 73]. Al-\nthough the reweighted variational objective [30] aims to ad-\ndress this by undersampling the initial denoising steps, DMs\nare still computationally demanding, since training and\nevaluating such a model requires repeated function evalu-\nations (and gradient computations) in the high-dimensional\nspace of RGB images. As an example, training the most\npowerful DMs often takes hundreds of GPU days (e.g. 150 -\n1000 V100 days in [15]) and repeated evaluations on a noisy\nversion of the input space render also inference expensive,\n\nso that producing 50k samples takes approximately 5 days\n[15] on a single A100 GPU. This has two consequences for\nthe research community and users in general: Firstly, train-\ning such a model requires massive computational resources\nonly available to a small fraction of the field, and leaves a\nhuge carbon footprint [65, 86]. Secondly, evaluating an al-\nready trained model is also expensive in time and memory,\nsince the same model architecture must run sequentially for\na large number of steps (e.g. 25 - 1000 steps in [15]).\n\nTo increase the accessibility of this powerful model class and at the same time reduce its significant resource consumption, a method is needed that reduces the computational complexity for both training and sampling. Reducing the computational demands of DMs without impairing their performance is, therefore, key to enhance their accessibility.\n\n**Departure to Latent Space** Our approach starts with the analysis of already trained diffusion models in pixel space: Fig. 2 shows the rate-distortion trade-off of a trained model. As with any likelihood-based model, learning can be roughly divided into two stages: First is a *perceptual compression* stage which removes high-frequency details but still learns little semantic variation. In the second stage, the actual generative model learns the semantic and conceptual composition of the data (*semantic compression*). We thus aim to first find a *perceptually equivalent, but computationally more suitable space*, in which we will train diffusion models for high-resolution image synthesis.\n\nFollowing common practice [11, 23, 66, 67, 96], we separate training into two distinct phases: First, we train an autoencoder which provides a lower-dimensional (and thereby efficient) representational space which is perceptually equivalent to the data space. Importantly, and in contrast to previous work [23, 66], we do not need to rely on excessive spatial compression, as we train DMs in the learned latent space, which exhibits better scaling properties with respect to the spatial dimensionality. The reduced complexity also provides efficient image generation from the latent space with a single network pass. We dub the resulting model class *Latent Diffusion Models* (LDMs).\n\nA notable advantage of this approach is that we need to train the universal autoencoding stage only once and can therefore reuse it for multiple DM trainings or to explore possibly completely different tasks [81]. This enables efficient exploration of a large number of diffusion models for various image-to-image and text-to-image tasks. For the latter, we design an architecture that connects transformers to the DM\u2019s UNet backbone [71] and enables arbitrary types of token-based conditioning mechanisms, see Sec. 3.3.\n\nIn sum, our work makes the following **contributions**:\n(i) In contrast to purely transformer-based approaches [23, 66], our method scales more graceful to higher dimensional data and can thus (a) work on a compression level which provides more faithful and detailed reconstructions than previous work (see Fig. 1) and (b) can be efficiently\n\napplied to high-resolution synthesis of megapixel images.\n    (ii) We achieve competitive performance on multiple\ntasks (unconditional image synthesis, inpainting, stochastic\nsuper-resolution) and datasets while significantly lowering\ncomputational costs. Compared to pixel-based diffusion ap-\nproaches, we also significantly decrease inference costs.\n\n(iii) We show that, in contrast to previous work [93] which learns both an encoder/decoder architecture and a score-based prior simultaneously, our approach does not require a delicate weighting of reconstruction and generative abilities. This ensures extremely faithful reconstructions and requires very little regularization of the latent space.\n\n(iv) We find that for densely conditioned tasks such as super-resolution, inpainting and semantic synthesis, our model can be applied in a convolutional fashion and render large, consistent images of $\\sim 1024^2$ px.\n\n(v) Moreover, we design a general-purpose conditioning mechanism based on cross-attention, enabling multi-modal training. We use it to train class-conditional, text-to-image and layout-to-image models.\n\n(vi) Finally, we release pretrained latent diffusion and autoencoding models at https://github.com/CompVis/latent-diffusion which might be reusable for a various tasks besides training of DMs [81]."
        },
        {
            "text": "To increase the accessibility of this powerful model class and at the same time reduce its significant resource consumption, a method is needed that reduces the computational complexity for both training and sampling. Reducing the computational demands of DMs without impairing their performance is, therefore, key to enhance their accessibility.",
            "page": 2,
            "x": 46,
            "y": 181,
            "width": 243,
            "height": 73,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "46136785-e07e-4f09-b701-782128d92942",
            "group_text": "1. Introduction\n\n    Image synthesis is one of the computer vision fields with the most spectacular recent development, but also among those with the greatest computational demands.    Especially high-resolution synthesis of complex, natural scenes is presently dominated by scaling up likelihood-based models, potentially containing billions of parameters in autoregressive (AR) transformers [66, 67]. In contrast, the promising results of GANs [3, 27, 40] have been revealed to be mostly confined to data with comparably limited variability as their adversarial learning procedure does not easily scale to modeling complex, multi-modal distributions. Recently, diffusion models [82], which are built from a hierarchy of denoising autoencoders, have shown to achieve impressive\n\nFigure 1.  Boosting the upper bound on achievable quality with less agressive downsampling. Since diffusion models offer excellent inductive biases for spatial data, we do not need the heavy spatial downsampling of related generative models in latent space, but can still greatly reduce the dimensionality of the data via suitable autoencoding models, see Sec. 3. Images are from the DIV2K [1] validation set, evaluated at $512^2$ px. We denote the spatial downsampling factor by $f$. Reconstruction FIDs [29] and PSNR are calculated on ImageNet-val. [12]; see also Tab. 8.\n\nresults in image synthesis [30, 85] and beyond [7, 45, 48, 57],\nand define the state-of-the-art in class-conditional image\nsynthesis [15, 31] and super-resolution [72]. Moreover, even\nunconditional DMs can readily be applied to tasks such\nas inpainting and colorization [85] or stroke-based syn-\nthesis [53], in contrast to other types of generative mod-\nels [19, 46, 69]. Being likelihood-based models, they do not\nexhibit mode-collapse and training instabilities as GANs\nand, by heavily exploiting parameter sharing, they can\nmodel highly complex distributions of natural images with-\nout involving billions of parameters as in AR models [67].\n**Democratizing High-Resolution Image Synthesis** DMs\nbelong to the class of likelihood-based models, whose\nmode-covering behavior makes them prone to spend ex-\ncessive amounts of capacity (and thus compute resources)\non modeling imperceptible details of the data [16, 73]. Al-\nthough the reweighted variational objective [30] aims to ad-\ndress this by undersampling the initial denoising steps, DMs\nare still computationally demanding, since training and\nevaluating such a model requires repeated function evalu-\nations (and gradient computations) in the high-dimensional\nspace of RGB images. As an example, training the most\npowerful DMs often takes hundreds of GPU days (e.g. 150 -\n1000 V100 days in [15]) and repeated evaluations on a noisy\nversion of the input space render also inference expensive,\n\nso that producing 50k samples takes approximately 5 days\n[15] on a single A100 GPU. This has two consequences for\nthe research community and users in general: Firstly, train-\ning such a model requires massive computational resources\nonly available to a small fraction of the field, and leaves a\nhuge carbon footprint [65, 86]. Secondly, evaluating an al-\nready trained model is also expensive in time and memory,\nsince the same model architecture must run sequentially for\na large number of steps (e.g. 25 - 1000 steps in [15]).\n\nTo increase the accessibility of this powerful model class and at the same time reduce its significant resource consumption, a method is needed that reduces the computational complexity for both training and sampling. Reducing the computational demands of DMs without impairing their performance is, therefore, key to enhance their accessibility.\n\n**Departure to Latent Space** Our approach starts with the analysis of already trained diffusion models in pixel space: Fig. 2 shows the rate-distortion trade-off of a trained model. As with any likelihood-based model, learning can be roughly divided into two stages: First is a *perceptual compression* stage which removes high-frequency details but still learns little semantic variation. In the second stage, the actual generative model learns the semantic and conceptual composition of the data (*semantic compression*). We thus aim to first find a *perceptually equivalent, but computationally more suitable space*, in which we will train diffusion models for high-resolution image synthesis.\n\nFollowing common practice [11, 23, 66, 67, 96], we separate training into two distinct phases: First, we train an autoencoder which provides a lower-dimensional (and thereby efficient) representational space which is perceptually equivalent to the data space. Importantly, and in contrast to previous work [23, 66], we do not need to rely on excessive spatial compression, as we train DMs in the learned latent space, which exhibits better scaling properties with respect to the spatial dimensionality. The reduced complexity also provides efficient image generation from the latent space with a single network pass. We dub the resulting model class *Latent Diffusion Models* (LDMs).\n\nA notable advantage of this approach is that we need to train the universal autoencoding stage only once and can therefore reuse it for multiple DM trainings or to explore possibly completely different tasks [81]. This enables efficient exploration of a large number of diffusion models for various image-to-image and text-to-image tasks. For the latter, we design an architecture that connects transformers to the DM\u2019s UNet backbone [71] and enables arbitrary types of token-based conditioning mechanisms, see Sec. 3.3.\n\nIn sum, our work makes the following **contributions**:\n(i) In contrast to purely transformer-based approaches [23, 66], our method scales more graceful to higher dimensional data and can thus (a) work on a compression level which provides more faithful and detailed reconstructions than previous work (see Fig. 1) and (b) can be efficiently\n\napplied to high-resolution synthesis of megapixel images.\n    (ii) We achieve competitive performance on multiple\ntasks (unconditional image synthesis, inpainting, stochastic\nsuper-resolution) and datasets while significantly lowering\ncomputational costs. Compared to pixel-based diffusion ap-\nproaches, we also significantly decrease inference costs.\n\n(iii) We show that, in contrast to previous work [93] which learns both an encoder/decoder architecture and a score-based prior simultaneously, our approach does not require a delicate weighting of reconstruction and generative abilities. This ensures extremely faithful reconstructions and requires very little regularization of the latent space.\n\n(iv) We find that for densely conditioned tasks such as super-resolution, inpainting and semantic synthesis, our model can be applied in a convolutional fashion and render large, consistent images of $\\sim 1024^2$ px.\n\n(v) Moreover, we design a general-purpose conditioning mechanism based on cross-attention, enabling multi-modal training. We use it to train class-conditional, text-to-image and layout-to-image models.\n\n(vi) Finally, we release pretrained latent diffusion and autoencoding models at https://github.com/CompVis/latent-diffusion which might be reusable for a various tasks besides training of DMs [81]."
        },
        {
            "text": "**Departure to Latent Space** Our approach starts with the analysis of already trained diffusion models in pixel space: Fig. 2 shows the rate-distortion trade-off of a trained model. As with any likelihood-based model, learning can be roughly divided into two stages: First is a *perceptual compression* stage which removes high-frequency details but still learns little semantic variation. In the second stage, the actual generative model learns the semantic and conceptual composition of the data (*semantic compression*). We thus aim to first find a *perceptually equivalent, but computationally more suitable space*, in which we will train diffusion models for high-resolution image synthesis.",
            "page": 2,
            "x": 46,
            "y": 256,
            "width": 242,
            "height": 144,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "66f3312d-1d18-498e-ac9b-acb30a7c36f9",
            "group_text": "1. Introduction\n\n    Image synthesis is one of the computer vision fields with the most spectacular recent development, but also among those with the greatest computational demands.    Especially high-resolution synthesis of complex, natural scenes is presently dominated by scaling up likelihood-based models, potentially containing billions of parameters in autoregressive (AR) transformers [66, 67]. In contrast, the promising results of GANs [3, 27, 40] have been revealed to be mostly confined to data with comparably limited variability as their adversarial learning procedure does not easily scale to modeling complex, multi-modal distributions. Recently, diffusion models [82], which are built from a hierarchy of denoising autoencoders, have shown to achieve impressive\n\nFigure 1.  Boosting the upper bound on achievable quality with less agressive downsampling. Since diffusion models offer excellent inductive biases for spatial data, we do not need the heavy spatial downsampling of related generative models in latent space, but can still greatly reduce the dimensionality of the data via suitable autoencoding models, see Sec. 3. Images are from the DIV2K [1] validation set, evaluated at $512^2$ px. We denote the spatial downsampling factor by $f$. Reconstruction FIDs [29] and PSNR are calculated on ImageNet-val. [12]; see also Tab. 8.\n\nresults in image synthesis [30, 85] and beyond [7, 45, 48, 57],\nand define the state-of-the-art in class-conditional image\nsynthesis [15, 31] and super-resolution [72]. Moreover, even\nunconditional DMs can readily be applied to tasks such\nas inpainting and colorization [85] or stroke-based syn-\nthesis [53], in contrast to other types of generative mod-\nels [19, 46, 69]. Being likelihood-based models, they do not\nexhibit mode-collapse and training instabilities as GANs\nand, by heavily exploiting parameter sharing, they can\nmodel highly complex distributions of natural images with-\nout involving billions of parameters as in AR models [67].\n**Democratizing High-Resolution Image Synthesis** DMs\nbelong to the class of likelihood-based models, whose\nmode-covering behavior makes them prone to spend ex-\ncessive amounts of capacity (and thus compute resources)\non modeling imperceptible details of the data [16, 73]. Al-\nthough the reweighted variational objective [30] aims to ad-\ndress this by undersampling the initial denoising steps, DMs\nare still computationally demanding, since training and\nevaluating such a model requires repeated function evalu-\nations (and gradient computations) in the high-dimensional\nspace of RGB images. As an example, training the most\npowerful DMs often takes hundreds of GPU days (e.g. 150 -\n1000 V100 days in [15]) and repeated evaluations on a noisy\nversion of the input space render also inference expensive,\n\nso that producing 50k samples takes approximately 5 days\n[15] on a single A100 GPU. This has two consequences for\nthe research community and users in general: Firstly, train-\ning such a model requires massive computational resources\nonly available to a small fraction of the field, and leaves a\nhuge carbon footprint [65, 86]. Secondly, evaluating an al-\nready trained model is also expensive in time and memory,\nsince the same model architecture must run sequentially for\na large number of steps (e.g. 25 - 1000 steps in [15]).\n\nTo increase the accessibility of this powerful model class and at the same time reduce its significant resource consumption, a method is needed that reduces the computational complexity for both training and sampling. Reducing the computational demands of DMs without impairing their performance is, therefore, key to enhance their accessibility.\n\n**Departure to Latent Space** Our approach starts with the analysis of already trained diffusion models in pixel space: Fig. 2 shows the rate-distortion trade-off of a trained model. As with any likelihood-based model, learning can be roughly divided into two stages: First is a *perceptual compression* stage which removes high-frequency details but still learns little semantic variation. In the second stage, the actual generative model learns the semantic and conceptual composition of the data (*semantic compression*). We thus aim to first find a *perceptually equivalent, but computationally more suitable space*, in which we will train diffusion models for high-resolution image synthesis.\n\nFollowing common practice [11, 23, 66, 67, 96], we separate training into two distinct phases: First, we train an autoencoder which provides a lower-dimensional (and thereby efficient) representational space which is perceptually equivalent to the data space. Importantly, and in contrast to previous work [23, 66], we do not need to rely on excessive spatial compression, as we train DMs in the learned latent space, which exhibits better scaling properties with respect to the spatial dimensionality. The reduced complexity also provides efficient image generation from the latent space with a single network pass. We dub the resulting model class *Latent Diffusion Models* (LDMs).\n\nA notable advantage of this approach is that we need to train the universal autoencoding stage only once and can therefore reuse it for multiple DM trainings or to explore possibly completely different tasks [81]. This enables efficient exploration of a large number of diffusion models for various image-to-image and text-to-image tasks. For the latter, we design an architecture that connects transformers to the DM\u2019s UNet backbone [71] and enables arbitrary types of token-based conditioning mechanisms, see Sec. 3.3.\n\nIn sum, our work makes the following **contributions**:\n(i) In contrast to purely transformer-based approaches [23, 66], our method scales more graceful to higher dimensional data and can thus (a) work on a compression level which provides more faithful and detailed reconstructions than previous work (see Fig. 1) and (b) can be efficiently\n\napplied to high-resolution synthesis of megapixel images.\n    (ii) We achieve competitive performance on multiple\ntasks (unconditional image synthesis, inpainting, stochastic\nsuper-resolution) and datasets while significantly lowering\ncomputational costs. Compared to pixel-based diffusion ap-\nproaches, we also significantly decrease inference costs.\n\n(iii) We show that, in contrast to previous work [93] which learns both an encoder/decoder architecture and a score-based prior simultaneously, our approach does not require a delicate weighting of reconstruction and generative abilities. This ensures extremely faithful reconstructions and requires very little regularization of the latent space.\n\n(iv) We find that for densely conditioned tasks such as super-resolution, inpainting and semantic synthesis, our model can be applied in a convolutional fashion and render large, consistent images of $\\sim 1024^2$ px.\n\n(v) Moreover, we design a general-purpose conditioning mechanism based on cross-attention, enabling multi-modal training. We use it to train class-conditional, text-to-image and layout-to-image models.\n\n(vi) Finally, we release pretrained latent diffusion and autoencoding models at https://github.com/CompVis/latent-diffusion which might be reusable for a various tasks besides training of DMs [81]."
        },
        {
            "text": "Following common practice [11, 23, 66, 67, 96], we separate training into two distinct phases: First, we train an autoencoder which provides a lower-dimensional (and thereby efficient) representational space which is perceptually equivalent to the data space. Importantly, and in contrast to previous work [23, 66], we do not need to rely on excessive spatial compression, as we train DMs in the learned latent space, which exhibits better scaling properties with respect to the spatial dimensionality. The reduced complexity also provides efficient image generation from the latent space with a single network pass. We dub the resulting model class *Latent Diffusion Models* (LDMs).",
            "page": 2,
            "x": 46,
            "y": 401,
            "width": 243,
            "height": 143,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "e9c99d61-a28c-4a81-9d2f-f52122ff8307",
            "group_text": "1. Introduction\n\n    Image synthesis is one of the computer vision fields with the most spectacular recent development, but also among those with the greatest computational demands.    Especially high-resolution synthesis of complex, natural scenes is presently dominated by scaling up likelihood-based models, potentially containing billions of parameters in autoregressive (AR) transformers [66, 67]. In contrast, the promising results of GANs [3, 27, 40] have been revealed to be mostly confined to data with comparably limited variability as their adversarial learning procedure does not easily scale to modeling complex, multi-modal distributions. Recently, diffusion models [82], which are built from a hierarchy of denoising autoencoders, have shown to achieve impressive\n\nFigure 1.  Boosting the upper bound on achievable quality with less agressive downsampling. Since diffusion models offer excellent inductive biases for spatial data, we do not need the heavy spatial downsampling of related generative models in latent space, but can still greatly reduce the dimensionality of the data via suitable autoencoding models, see Sec. 3. Images are from the DIV2K [1] validation set, evaluated at $512^2$ px. We denote the spatial downsampling factor by $f$. Reconstruction FIDs [29] and PSNR are calculated on ImageNet-val. [12]; see also Tab. 8.\n\nresults in image synthesis [30, 85] and beyond [7, 45, 48, 57],\nand define the state-of-the-art in class-conditional image\nsynthesis [15, 31] and super-resolution [72]. Moreover, even\nunconditional DMs can readily be applied to tasks such\nas inpainting and colorization [85] or stroke-based syn-\nthesis [53], in contrast to other types of generative mod-\nels [19, 46, 69]. Being likelihood-based models, they do not\nexhibit mode-collapse and training instabilities as GANs\nand, by heavily exploiting parameter sharing, they can\nmodel highly complex distributions of natural images with-\nout involving billions of parameters as in AR models [67].\n**Democratizing High-Resolution Image Synthesis** DMs\nbelong to the class of likelihood-based models, whose\nmode-covering behavior makes them prone to spend ex-\ncessive amounts of capacity (and thus compute resources)\non modeling imperceptible details of the data [16, 73]. Al-\nthough the reweighted variational objective [30] aims to ad-\ndress this by undersampling the initial denoising steps, DMs\nare still computationally demanding, since training and\nevaluating such a model requires repeated function evalu-\nations (and gradient computations) in the high-dimensional\nspace of RGB images. As an example, training the most\npowerful DMs often takes hundreds of GPU days (e.g. 150 -\n1000 V100 days in [15]) and repeated evaluations on a noisy\nversion of the input space render also inference expensive,\n\nso that producing 50k samples takes approximately 5 days\n[15] on a single A100 GPU. This has two consequences for\nthe research community and users in general: Firstly, train-\ning such a model requires massive computational resources\nonly available to a small fraction of the field, and leaves a\nhuge carbon footprint [65, 86]. Secondly, evaluating an al-\nready trained model is also expensive in time and memory,\nsince the same model architecture must run sequentially for\na large number of steps (e.g. 25 - 1000 steps in [15]).\n\nTo increase the accessibility of this powerful model class and at the same time reduce its significant resource consumption, a method is needed that reduces the computational complexity for both training and sampling. Reducing the computational demands of DMs without impairing their performance is, therefore, key to enhance their accessibility.\n\n**Departure to Latent Space** Our approach starts with the analysis of already trained diffusion models in pixel space: Fig. 2 shows the rate-distortion trade-off of a trained model. As with any likelihood-based model, learning can be roughly divided into two stages: First is a *perceptual compression* stage which removes high-frequency details but still learns little semantic variation. In the second stage, the actual generative model learns the semantic and conceptual composition of the data (*semantic compression*). We thus aim to first find a *perceptually equivalent, but computationally more suitable space*, in which we will train diffusion models for high-resolution image synthesis.\n\nFollowing common practice [11, 23, 66, 67, 96], we separate training into two distinct phases: First, we train an autoencoder which provides a lower-dimensional (and thereby efficient) representational space which is perceptually equivalent to the data space. Importantly, and in contrast to previous work [23, 66], we do not need to rely on excessive spatial compression, as we train DMs in the learned latent space, which exhibits better scaling properties with respect to the spatial dimensionality. The reduced complexity also provides efficient image generation from the latent space with a single network pass. We dub the resulting model class *Latent Diffusion Models* (LDMs).\n\nA notable advantage of this approach is that we need to train the universal autoencoding stage only once and can therefore reuse it for multiple DM trainings or to explore possibly completely different tasks [81]. This enables efficient exploration of a large number of diffusion models for various image-to-image and text-to-image tasks. For the latter, we design an architecture that connects transformers to the DM\u2019s UNet backbone [71] and enables arbitrary types of token-based conditioning mechanisms, see Sec. 3.3.\n\nIn sum, our work makes the following **contributions**:\n(i) In contrast to purely transformer-based approaches [23, 66], our method scales more graceful to higher dimensional data and can thus (a) work on a compression level which provides more faithful and detailed reconstructions than previous work (see Fig. 1) and (b) can be efficiently\n\napplied to high-resolution synthesis of megapixel images.\n    (ii) We achieve competitive performance on multiple\ntasks (unconditional image synthesis, inpainting, stochastic\nsuper-resolution) and datasets while significantly lowering\ncomputational costs. Compared to pixel-based diffusion ap-\nproaches, we also significantly decrease inference costs.\n\n(iii) We show that, in contrast to previous work [93] which learns both an encoder/decoder architecture and a score-based prior simultaneously, our approach does not require a delicate weighting of reconstruction and generative abilities. This ensures extremely faithful reconstructions and requires very little regularization of the latent space.\n\n(iv) We find that for densely conditioned tasks such as super-resolution, inpainting and semantic synthesis, our model can be applied in a convolutional fashion and render large, consistent images of $\\sim 1024^2$ px.\n\n(v) Moreover, we design a general-purpose conditioning mechanism based on cross-attention, enabling multi-modal training. We use it to train class-conditional, text-to-image and layout-to-image models.\n\n(vi) Finally, we release pretrained latent diffusion and autoencoding models at https://github.com/CompVis/latent-diffusion which might be reusable for a various tasks besides training of DMs [81]."
        },
        {
            "text": "A notable advantage of this approach is that we need to train the universal autoencoding stage only once and can therefore reuse it for multiple DM trainings or to explore possibly completely different tasks [81]. This enables efficient exploration of a large number of diffusion models for various image-to-image and text-to-image tasks. For the latter, we design an architecture that connects transformers to the DM\u2019s UNet backbone [71] and enables arbitrary types of token-based conditioning mechanisms, see Sec. 3.3.",
            "page": 2,
            "x": 46,
            "y": 545,
            "width": 243,
            "height": 108,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "3c8393ca-ed61-476b-b531-a6762e5ba23c",
            "group_text": "1. Introduction\n\n    Image synthesis is one of the computer vision fields with the most spectacular recent development, but also among those with the greatest computational demands.    Especially high-resolution synthesis of complex, natural scenes is presently dominated by scaling up likelihood-based models, potentially containing billions of parameters in autoregressive (AR) transformers [66, 67]. In contrast, the promising results of GANs [3, 27, 40] have been revealed to be mostly confined to data with comparably limited variability as their adversarial learning procedure does not easily scale to modeling complex, multi-modal distributions. Recently, diffusion models [82], which are built from a hierarchy of denoising autoencoders, have shown to achieve impressive\n\nFigure 1.  Boosting the upper bound on achievable quality with less agressive downsampling. Since diffusion models offer excellent inductive biases for spatial data, we do not need the heavy spatial downsampling of related generative models in latent space, but can still greatly reduce the dimensionality of the data via suitable autoencoding models, see Sec. 3. Images are from the DIV2K [1] validation set, evaluated at $512^2$ px. We denote the spatial downsampling factor by $f$. Reconstruction FIDs [29] and PSNR are calculated on ImageNet-val. [12]; see also Tab. 8.\n\nresults in image synthesis [30, 85] and beyond [7, 45, 48, 57],\nand define the state-of-the-art in class-conditional image\nsynthesis [15, 31] and super-resolution [72]. Moreover, even\nunconditional DMs can readily be applied to tasks such\nas inpainting and colorization [85] or stroke-based syn-\nthesis [53], in contrast to other types of generative mod-\nels [19, 46, 69]. Being likelihood-based models, they do not\nexhibit mode-collapse and training instabilities as GANs\nand, by heavily exploiting parameter sharing, they can\nmodel highly complex distributions of natural images with-\nout involving billions of parameters as in AR models [67].\n**Democratizing High-Resolution Image Synthesis** DMs\nbelong to the class of likelihood-based models, whose\nmode-covering behavior makes them prone to spend ex-\ncessive amounts of capacity (and thus compute resources)\non modeling imperceptible details of the data [16, 73]. Al-\nthough the reweighted variational objective [30] aims to ad-\ndress this by undersampling the initial denoising steps, DMs\nare still computationally demanding, since training and\nevaluating such a model requires repeated function evalu-\nations (and gradient computations) in the high-dimensional\nspace of RGB images. As an example, training the most\npowerful DMs often takes hundreds of GPU days (e.g. 150 -\n1000 V100 days in [15]) and repeated evaluations on a noisy\nversion of the input space render also inference expensive,\n\nso that producing 50k samples takes approximately 5 days\n[15] on a single A100 GPU. This has two consequences for\nthe research community and users in general: Firstly, train-\ning such a model requires massive computational resources\nonly available to a small fraction of the field, and leaves a\nhuge carbon footprint [65, 86]. Secondly, evaluating an al-\nready trained model is also expensive in time and memory,\nsince the same model architecture must run sequentially for\na large number of steps (e.g. 25 - 1000 steps in [15]).\n\nTo increase the accessibility of this powerful model class and at the same time reduce its significant resource consumption, a method is needed that reduces the computational complexity for both training and sampling. Reducing the computational demands of DMs without impairing their performance is, therefore, key to enhance their accessibility.\n\n**Departure to Latent Space** Our approach starts with the analysis of already trained diffusion models in pixel space: Fig. 2 shows the rate-distortion trade-off of a trained model. As with any likelihood-based model, learning can be roughly divided into two stages: First is a *perceptual compression* stage which removes high-frequency details but still learns little semantic variation. In the second stage, the actual generative model learns the semantic and conceptual composition of the data (*semantic compression*). We thus aim to first find a *perceptually equivalent, but computationally more suitable space*, in which we will train diffusion models for high-resolution image synthesis.\n\nFollowing common practice [11, 23, 66, 67, 96], we separate training into two distinct phases: First, we train an autoencoder which provides a lower-dimensional (and thereby efficient) representational space which is perceptually equivalent to the data space. Importantly, and in contrast to previous work [23, 66], we do not need to rely on excessive spatial compression, as we train DMs in the learned latent space, which exhibits better scaling properties with respect to the spatial dimensionality. The reduced complexity also provides efficient image generation from the latent space with a single network pass. We dub the resulting model class *Latent Diffusion Models* (LDMs).\n\nA notable advantage of this approach is that we need to train the universal autoencoding stage only once and can therefore reuse it for multiple DM trainings or to explore possibly completely different tasks [81]. This enables efficient exploration of a large number of diffusion models for various image-to-image and text-to-image tasks. For the latter, we design an architecture that connects transformers to the DM\u2019s UNet backbone [71] and enables arbitrary types of token-based conditioning mechanisms, see Sec. 3.3.\n\nIn sum, our work makes the following **contributions**:\n(i) In contrast to purely transformer-based approaches [23, 66], our method scales more graceful to higher dimensional data and can thus (a) work on a compression level which provides more faithful and detailed reconstructions than previous work (see Fig. 1) and (b) can be efficiently\n\napplied to high-resolution synthesis of megapixel images.\n    (ii) We achieve competitive performance on multiple\ntasks (unconditional image synthesis, inpainting, stochastic\nsuper-resolution) and datasets while significantly lowering\ncomputational costs. Compared to pixel-based diffusion ap-\nproaches, we also significantly decrease inference costs.\n\n(iii) We show that, in contrast to previous work [93] which learns both an encoder/decoder architecture and a score-based prior simultaneously, our approach does not require a delicate weighting of reconstruction and generative abilities. This ensures extremely faithful reconstructions and requires very little regularization of the latent space.\n\n(iv) We find that for densely conditioned tasks such as super-resolution, inpainting and semantic synthesis, our model can be applied in a convolutional fashion and render large, consistent images of $\\sim 1024^2$ px.\n\n(v) Moreover, we design a general-purpose conditioning mechanism based on cross-attention, enabling multi-modal training. We use it to train class-conditional, text-to-image and layout-to-image models.\n\n(vi) Finally, we release pretrained latent diffusion and autoencoding models at https://github.com/CompVis/latent-diffusion which might be reusable for a various tasks besides training of DMs [81]."
        },
        {
            "text": "In sum, our work makes the following **contributions**:\n(i) In contrast to purely transformer-based approaches [23, 66], our method scales more graceful to higher dimensional data and can thus (a) work on a compression level which provides more faithful and detailed reconstructions than previous work (see Fig. 1) and (b) can be efficiently",
            "page": 2,
            "x": 47,
            "y": 653,
            "width": 242,
            "height": 74,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "be07f892-4c26-48ee-a6a0-23ef4c918501",
            "group_text": "1. Introduction\n\n    Image synthesis is one of the computer vision fields with the most spectacular recent development, but also among those with the greatest computational demands.    Especially high-resolution synthesis of complex, natural scenes is presently dominated by scaling up likelihood-based models, potentially containing billions of parameters in autoregressive (AR) transformers [66, 67]. In contrast, the promising results of GANs [3, 27, 40] have been revealed to be mostly confined to data with comparably limited variability as their adversarial learning procedure does not easily scale to modeling complex, multi-modal distributions. Recently, diffusion models [82], which are built from a hierarchy of denoising autoencoders, have shown to achieve impressive\n\nFigure 1.  Boosting the upper bound on achievable quality with less agressive downsampling. Since diffusion models offer excellent inductive biases for spatial data, we do not need the heavy spatial downsampling of related generative models in latent space, but can still greatly reduce the dimensionality of the data via suitable autoencoding models, see Sec. 3. Images are from the DIV2K [1] validation set, evaluated at $512^2$ px. We denote the spatial downsampling factor by $f$. Reconstruction FIDs [29] and PSNR are calculated on ImageNet-val. [12]; see also Tab. 8.\n\nresults in image synthesis [30, 85] and beyond [7, 45, 48, 57],\nand define the state-of-the-art in class-conditional image\nsynthesis [15, 31] and super-resolution [72]. Moreover, even\nunconditional DMs can readily be applied to tasks such\nas inpainting and colorization [85] or stroke-based syn-\nthesis [53], in contrast to other types of generative mod-\nels [19, 46, 69]. Being likelihood-based models, they do not\nexhibit mode-collapse and training instabilities as GANs\nand, by heavily exploiting parameter sharing, they can\nmodel highly complex distributions of natural images with-\nout involving billions of parameters as in AR models [67].\n**Democratizing High-Resolution Image Synthesis** DMs\nbelong to the class of likelihood-based models, whose\nmode-covering behavior makes them prone to spend ex-\ncessive amounts of capacity (and thus compute resources)\non modeling imperceptible details of the data [16, 73]. Al-\nthough the reweighted variational objective [30] aims to ad-\ndress this by undersampling the initial denoising steps, DMs\nare still computationally demanding, since training and\nevaluating such a model requires repeated function evalu-\nations (and gradient computations) in the high-dimensional\nspace of RGB images. As an example, training the most\npowerful DMs often takes hundreds of GPU days (e.g. 150 -\n1000 V100 days in [15]) and repeated evaluations on a noisy\nversion of the input space render also inference expensive,\n\nso that producing 50k samples takes approximately 5 days\n[15] on a single A100 GPU. This has two consequences for\nthe research community and users in general: Firstly, train-\ning such a model requires massive computational resources\nonly available to a small fraction of the field, and leaves a\nhuge carbon footprint [65, 86]. Secondly, evaluating an al-\nready trained model is also expensive in time and memory,\nsince the same model architecture must run sequentially for\na large number of steps (e.g. 25 - 1000 steps in [15]).\n\nTo increase the accessibility of this powerful model class and at the same time reduce its significant resource consumption, a method is needed that reduces the computational complexity for both training and sampling. Reducing the computational demands of DMs without impairing their performance is, therefore, key to enhance their accessibility.\n\n**Departure to Latent Space** Our approach starts with the analysis of already trained diffusion models in pixel space: Fig. 2 shows the rate-distortion trade-off of a trained model. As with any likelihood-based model, learning can be roughly divided into two stages: First is a *perceptual compression* stage which removes high-frequency details but still learns little semantic variation. In the second stage, the actual generative model learns the semantic and conceptual composition of the data (*semantic compression*). We thus aim to first find a *perceptually equivalent, but computationally more suitable space*, in which we will train diffusion models for high-resolution image synthesis.\n\nFollowing common practice [11, 23, 66, 67, 96], we separate training into two distinct phases: First, we train an autoencoder which provides a lower-dimensional (and thereby efficient) representational space which is perceptually equivalent to the data space. Importantly, and in contrast to previous work [23, 66], we do not need to rely on excessive spatial compression, as we train DMs in the learned latent space, which exhibits better scaling properties with respect to the spatial dimensionality. The reduced complexity also provides efficient image generation from the latent space with a single network pass. We dub the resulting model class *Latent Diffusion Models* (LDMs).\n\nA notable advantage of this approach is that we need to train the universal autoencoding stage only once and can therefore reuse it for multiple DM trainings or to explore possibly completely different tasks [81]. This enables efficient exploration of a large number of diffusion models for various image-to-image and text-to-image tasks. For the latter, we design an architecture that connects transformers to the DM\u2019s UNet backbone [71] and enables arbitrary types of token-based conditioning mechanisms, see Sec. 3.3.\n\nIn sum, our work makes the following **contributions**:\n(i) In contrast to purely transformer-based approaches [23, 66], our method scales more graceful to higher dimensional data and can thus (a) work on a compression level which provides more faithful and detailed reconstructions than previous work (see Fig. 1) and (b) can be efficiently\n\napplied to high-resolution synthesis of megapixel images.\n    (ii) We achieve competitive performance on multiple\ntasks (unconditional image synthesis, inpainting, stochastic\nsuper-resolution) and datasets while significantly lowering\ncomputational costs. Compared to pixel-based diffusion ap-\nproaches, we also significantly decrease inference costs.\n\n(iii) We show that, in contrast to previous work [93] which learns both an encoder/decoder architecture and a score-based prior simultaneously, our approach does not require a delicate weighting of reconstruction and generative abilities. This ensures extremely faithful reconstructions and requires very little regularization of the latent space.\n\n(iv) We find that for densely conditioned tasks such as super-resolution, inpainting and semantic synthesis, our model can be applied in a convolutional fashion and render large, consistent images of $\\sim 1024^2$ px.\n\n(v) Moreover, we design a general-purpose conditioning mechanism based on cross-attention, enabling multi-modal training. We use it to train class-conditional, text-to-image and layout-to-image models.\n\n(vi) Finally, we release pretrained latent diffusion and autoencoding models at https://github.com/CompVis/latent-diffusion which might be reusable for a various tasks besides training of DMs [81]."
        },
        {
            "text": "applied to high-resolution synthesis of megapixel images.\n    (ii) We achieve competitive performance on multiple\ntasks (unconditional image synthesis, inpainting, stochastic\nsuper-resolution) and datasets while significantly lowering\ncomputational costs. Compared to pixel-based diffusion ap-\nproaches, we also significantly decrease inference costs.",
            "page": 2,
            "x": 305,
            "y": 349,
            "width": 242,
            "height": 72,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "c92505a5-f986-401f-b0f8-815a03fdc6df",
            "group_text": "1. Introduction\n\n    Image synthesis is one of the computer vision fields with the most spectacular recent development, but also among those with the greatest computational demands.    Especially high-resolution synthesis of complex, natural scenes is presently dominated by scaling up likelihood-based models, potentially containing billions of parameters in autoregressive (AR) transformers [66, 67]. In contrast, the promising results of GANs [3, 27, 40] have been revealed to be mostly confined to data with comparably limited variability as their adversarial learning procedure does not easily scale to modeling complex, multi-modal distributions. Recently, diffusion models [82], which are built from a hierarchy of denoising autoencoders, have shown to achieve impressive\n\nFigure 1.  Boosting the upper bound on achievable quality with less agressive downsampling. Since diffusion models offer excellent inductive biases for spatial data, we do not need the heavy spatial downsampling of related generative models in latent space, but can still greatly reduce the dimensionality of the data via suitable autoencoding models, see Sec. 3. Images are from the DIV2K [1] validation set, evaluated at $512^2$ px. We denote the spatial downsampling factor by $f$. Reconstruction FIDs [29] and PSNR are calculated on ImageNet-val. [12]; see also Tab. 8.\n\nresults in image synthesis [30, 85] and beyond [7, 45, 48, 57],\nand define the state-of-the-art in class-conditional image\nsynthesis [15, 31] and super-resolution [72]. Moreover, even\nunconditional DMs can readily be applied to tasks such\nas inpainting and colorization [85] or stroke-based syn-\nthesis [53], in contrast to other types of generative mod-\nels [19, 46, 69]. Being likelihood-based models, they do not\nexhibit mode-collapse and training instabilities as GANs\nand, by heavily exploiting parameter sharing, they can\nmodel highly complex distributions of natural images with-\nout involving billions of parameters as in AR models [67].\n**Democratizing High-Resolution Image Synthesis** DMs\nbelong to the class of likelihood-based models, whose\nmode-covering behavior makes them prone to spend ex-\ncessive amounts of capacity (and thus compute resources)\non modeling imperceptible details of the data [16, 73]. Al-\nthough the reweighted variational objective [30] aims to ad-\ndress this by undersampling the initial denoising steps, DMs\nare still computationally demanding, since training and\nevaluating such a model requires repeated function evalu-\nations (and gradient computations) in the high-dimensional\nspace of RGB images. As an example, training the most\npowerful DMs often takes hundreds of GPU days (e.g. 150 -\n1000 V100 days in [15]) and repeated evaluations on a noisy\nversion of the input space render also inference expensive,\n\nso that producing 50k samples takes approximately 5 days\n[15] on a single A100 GPU. This has two consequences for\nthe research community and users in general: Firstly, train-\ning such a model requires massive computational resources\nonly available to a small fraction of the field, and leaves a\nhuge carbon footprint [65, 86]. Secondly, evaluating an al-\nready trained model is also expensive in time and memory,\nsince the same model architecture must run sequentially for\na large number of steps (e.g. 25 - 1000 steps in [15]).\n\nTo increase the accessibility of this powerful model class and at the same time reduce its significant resource consumption, a method is needed that reduces the computational complexity for both training and sampling. Reducing the computational demands of DMs without impairing their performance is, therefore, key to enhance their accessibility.\n\n**Departure to Latent Space** Our approach starts with the analysis of already trained diffusion models in pixel space: Fig. 2 shows the rate-distortion trade-off of a trained model. As with any likelihood-based model, learning can be roughly divided into two stages: First is a *perceptual compression* stage which removes high-frequency details but still learns little semantic variation. In the second stage, the actual generative model learns the semantic and conceptual composition of the data (*semantic compression*). We thus aim to first find a *perceptually equivalent, but computationally more suitable space*, in which we will train diffusion models for high-resolution image synthesis.\n\nFollowing common practice [11, 23, 66, 67, 96], we separate training into two distinct phases: First, we train an autoencoder which provides a lower-dimensional (and thereby efficient) representational space which is perceptually equivalent to the data space. Importantly, and in contrast to previous work [23, 66], we do not need to rely on excessive spatial compression, as we train DMs in the learned latent space, which exhibits better scaling properties with respect to the spatial dimensionality. The reduced complexity also provides efficient image generation from the latent space with a single network pass. We dub the resulting model class *Latent Diffusion Models* (LDMs).\n\nA notable advantage of this approach is that we need to train the universal autoencoding stage only once and can therefore reuse it for multiple DM trainings or to explore possibly completely different tasks [81]. This enables efficient exploration of a large number of diffusion models for various image-to-image and text-to-image tasks. For the latter, we design an architecture that connects transformers to the DM\u2019s UNet backbone [71] and enables arbitrary types of token-based conditioning mechanisms, see Sec. 3.3.\n\nIn sum, our work makes the following **contributions**:\n(i) In contrast to purely transformer-based approaches [23, 66], our method scales more graceful to higher dimensional data and can thus (a) work on a compression level which provides more faithful and detailed reconstructions than previous work (see Fig. 1) and (b) can be efficiently\n\napplied to high-resolution synthesis of megapixel images.\n    (ii) We achieve competitive performance on multiple\ntasks (unconditional image synthesis, inpainting, stochastic\nsuper-resolution) and datasets while significantly lowering\ncomputational costs. Compared to pixel-based diffusion ap-\nproaches, we also significantly decrease inference costs.\n\n(iii) We show that, in contrast to previous work [93] which learns both an encoder/decoder architecture and a score-based prior simultaneously, our approach does not require a delicate weighting of reconstruction and generative abilities. This ensures extremely faithful reconstructions and requires very little regularization of the latent space.\n\n(iv) We find that for densely conditioned tasks such as super-resolution, inpainting and semantic synthesis, our model can be applied in a convolutional fashion and render large, consistent images of $\\sim 1024^2$ px.\n\n(v) Moreover, we design a general-purpose conditioning mechanism based on cross-attention, enabling multi-modal training. We use it to train class-conditional, text-to-image and layout-to-image models.\n\n(vi) Finally, we release pretrained latent diffusion and autoencoding models at https://github.com/CompVis/latent-diffusion which might be reusable for a various tasks besides training of DMs [81]."
        },
        {
            "text": "(iii) We show that, in contrast to previous work [93] which learns both an encoder/decoder architecture and a score-based prior simultaneously, our approach does not require a delicate weighting of reconstruction and generative abilities. This ensures extremely faithful reconstructions and requires very little regularization of the latent space.",
            "page": 2,
            "x": 305,
            "y": 422,
            "width": 242,
            "height": 72,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "63aeddf2-ce40-4523-b8ec-af755b55131e",
            "group_text": "1. Introduction\n\n    Image synthesis is one of the computer vision fields with the most spectacular recent development, but also among those with the greatest computational demands.    Especially high-resolution synthesis of complex, natural scenes is presently dominated by scaling up likelihood-based models, potentially containing billions of parameters in autoregressive (AR) transformers [66, 67]. In contrast, the promising results of GANs [3, 27, 40] have been revealed to be mostly confined to data with comparably limited variability as their adversarial learning procedure does not easily scale to modeling complex, multi-modal distributions. Recently, diffusion models [82], which are built from a hierarchy of denoising autoencoders, have shown to achieve impressive\n\nFigure 1.  Boosting the upper bound on achievable quality with less agressive downsampling. Since diffusion models offer excellent inductive biases for spatial data, we do not need the heavy spatial downsampling of related generative models in latent space, but can still greatly reduce the dimensionality of the data via suitable autoencoding models, see Sec. 3. Images are from the DIV2K [1] validation set, evaluated at $512^2$ px. We denote the spatial downsampling factor by $f$. Reconstruction FIDs [29] and PSNR are calculated on ImageNet-val. [12]; see also Tab. 8.\n\nresults in image synthesis [30, 85] and beyond [7, 45, 48, 57],\nand define the state-of-the-art in class-conditional image\nsynthesis [15, 31] and super-resolution [72]. Moreover, even\nunconditional DMs can readily be applied to tasks such\nas inpainting and colorization [85] or stroke-based syn-\nthesis [53], in contrast to other types of generative mod-\nels [19, 46, 69]. Being likelihood-based models, they do not\nexhibit mode-collapse and training instabilities as GANs\nand, by heavily exploiting parameter sharing, they can\nmodel highly complex distributions of natural images with-\nout involving billions of parameters as in AR models [67].\n**Democratizing High-Resolution Image Synthesis** DMs\nbelong to the class of likelihood-based models, whose\nmode-covering behavior makes them prone to spend ex-\ncessive amounts of capacity (and thus compute resources)\non modeling imperceptible details of the data [16, 73]. Al-\nthough the reweighted variational objective [30] aims to ad-\ndress this by undersampling the initial denoising steps, DMs\nare still computationally demanding, since training and\nevaluating such a model requires repeated function evalu-\nations (and gradient computations) in the high-dimensional\nspace of RGB images. As an example, training the most\npowerful DMs often takes hundreds of GPU days (e.g. 150 -\n1000 V100 days in [15]) and repeated evaluations on a noisy\nversion of the input space render also inference expensive,\n\nso that producing 50k samples takes approximately 5 days\n[15] on a single A100 GPU. This has two consequences for\nthe research community and users in general: Firstly, train-\ning such a model requires massive computational resources\nonly available to a small fraction of the field, and leaves a\nhuge carbon footprint [65, 86]. Secondly, evaluating an al-\nready trained model is also expensive in time and memory,\nsince the same model architecture must run sequentially for\na large number of steps (e.g. 25 - 1000 steps in [15]).\n\nTo increase the accessibility of this powerful model class and at the same time reduce its significant resource consumption, a method is needed that reduces the computational complexity for both training and sampling. Reducing the computational demands of DMs without impairing their performance is, therefore, key to enhance their accessibility.\n\n**Departure to Latent Space** Our approach starts with the analysis of already trained diffusion models in pixel space: Fig. 2 shows the rate-distortion trade-off of a trained model. As with any likelihood-based model, learning can be roughly divided into two stages: First is a *perceptual compression* stage which removes high-frequency details but still learns little semantic variation. In the second stage, the actual generative model learns the semantic and conceptual composition of the data (*semantic compression*). We thus aim to first find a *perceptually equivalent, but computationally more suitable space*, in which we will train diffusion models for high-resolution image synthesis.\n\nFollowing common practice [11, 23, 66, 67, 96], we separate training into two distinct phases: First, we train an autoencoder which provides a lower-dimensional (and thereby efficient) representational space which is perceptually equivalent to the data space. Importantly, and in contrast to previous work [23, 66], we do not need to rely on excessive spatial compression, as we train DMs in the learned latent space, which exhibits better scaling properties with respect to the spatial dimensionality. The reduced complexity also provides efficient image generation from the latent space with a single network pass. We dub the resulting model class *Latent Diffusion Models* (LDMs).\n\nA notable advantage of this approach is that we need to train the universal autoencoding stage only once and can therefore reuse it for multiple DM trainings or to explore possibly completely different tasks [81]. This enables efficient exploration of a large number of diffusion models for various image-to-image and text-to-image tasks. For the latter, we design an architecture that connects transformers to the DM\u2019s UNet backbone [71] and enables arbitrary types of token-based conditioning mechanisms, see Sec. 3.3.\n\nIn sum, our work makes the following **contributions**:\n(i) In contrast to purely transformer-based approaches [23, 66], our method scales more graceful to higher dimensional data and can thus (a) work on a compression level which provides more faithful and detailed reconstructions than previous work (see Fig. 1) and (b) can be efficiently\n\napplied to high-resolution synthesis of megapixel images.\n    (ii) We achieve competitive performance on multiple\ntasks (unconditional image synthesis, inpainting, stochastic\nsuper-resolution) and datasets while significantly lowering\ncomputational costs. Compared to pixel-based diffusion ap-\nproaches, we also significantly decrease inference costs.\n\n(iii) We show that, in contrast to previous work [93] which learns both an encoder/decoder architecture and a score-based prior simultaneously, our approach does not require a delicate weighting of reconstruction and generative abilities. This ensures extremely faithful reconstructions and requires very little regularization of the latent space.\n\n(iv) We find that for densely conditioned tasks such as super-resolution, inpainting and semantic synthesis, our model can be applied in a convolutional fashion and render large, consistent images of $\\sim 1024^2$ px.\n\n(v) Moreover, we design a general-purpose conditioning mechanism based on cross-attention, enabling multi-modal training. We use it to train class-conditional, text-to-image and layout-to-image models.\n\n(vi) Finally, we release pretrained latent diffusion and autoencoding models at https://github.com/CompVis/latent-diffusion which might be reusable for a various tasks besides training of DMs [81]."
        },
        {
            "text": "(iv) We find that for densely conditioned tasks such as super-resolution, inpainting and semantic synthesis, our model can be applied in a convolutional fashion and render large, consistent images of $\\sim 1024^2$ px.",
            "page": 2,
            "x": 305,
            "y": 495,
            "width": 242,
            "height": 48,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "9e471038-6360-4ffc-91fe-814187475cf7",
            "group_text": "1. Introduction\n\n    Image synthesis is one of the computer vision fields with the most spectacular recent development, but also among those with the greatest computational demands.    Especially high-resolution synthesis of complex, natural scenes is presently dominated by scaling up likelihood-based models, potentially containing billions of parameters in autoregressive (AR) transformers [66, 67]. In contrast, the promising results of GANs [3, 27, 40] have been revealed to be mostly confined to data with comparably limited variability as their adversarial learning procedure does not easily scale to modeling complex, multi-modal distributions. Recently, diffusion models [82], which are built from a hierarchy of denoising autoencoders, have shown to achieve impressive\n\nFigure 1.  Boosting the upper bound on achievable quality with less agressive downsampling. Since diffusion models offer excellent inductive biases for spatial data, we do not need the heavy spatial downsampling of related generative models in latent space, but can still greatly reduce the dimensionality of the data via suitable autoencoding models, see Sec. 3. Images are from the DIV2K [1] validation set, evaluated at $512^2$ px. We denote the spatial downsampling factor by $f$. Reconstruction FIDs [29] and PSNR are calculated on ImageNet-val. [12]; see also Tab. 8.\n\nresults in image synthesis [30, 85] and beyond [7, 45, 48, 57],\nand define the state-of-the-art in class-conditional image\nsynthesis [15, 31] and super-resolution [72]. Moreover, even\nunconditional DMs can readily be applied to tasks such\nas inpainting and colorization [85] or stroke-based syn-\nthesis [53], in contrast to other types of generative mod-\nels [19, 46, 69]. Being likelihood-based models, they do not\nexhibit mode-collapse and training instabilities as GANs\nand, by heavily exploiting parameter sharing, they can\nmodel highly complex distributions of natural images with-\nout involving billions of parameters as in AR models [67].\n**Democratizing High-Resolution Image Synthesis** DMs\nbelong to the class of likelihood-based models, whose\nmode-covering behavior makes them prone to spend ex-\ncessive amounts of capacity (and thus compute resources)\non modeling imperceptible details of the data [16, 73]. Al-\nthough the reweighted variational objective [30] aims to ad-\ndress this by undersampling the initial denoising steps, DMs\nare still computationally demanding, since training and\nevaluating such a model requires repeated function evalu-\nations (and gradient computations) in the high-dimensional\nspace of RGB images. As an example, training the most\npowerful DMs often takes hundreds of GPU days (e.g. 150 -\n1000 V100 days in [15]) and repeated evaluations on a noisy\nversion of the input space render also inference expensive,\n\nso that producing 50k samples takes approximately 5 days\n[15] on a single A100 GPU. This has two consequences for\nthe research community and users in general: Firstly, train-\ning such a model requires massive computational resources\nonly available to a small fraction of the field, and leaves a\nhuge carbon footprint [65, 86]. Secondly, evaluating an al-\nready trained model is also expensive in time and memory,\nsince the same model architecture must run sequentially for\na large number of steps (e.g. 25 - 1000 steps in [15]).\n\nTo increase the accessibility of this powerful model class and at the same time reduce its significant resource consumption, a method is needed that reduces the computational complexity for both training and sampling. Reducing the computational demands of DMs without impairing their performance is, therefore, key to enhance their accessibility.\n\n**Departure to Latent Space** Our approach starts with the analysis of already trained diffusion models in pixel space: Fig. 2 shows the rate-distortion trade-off of a trained model. As with any likelihood-based model, learning can be roughly divided into two stages: First is a *perceptual compression* stage which removes high-frequency details but still learns little semantic variation. In the second stage, the actual generative model learns the semantic and conceptual composition of the data (*semantic compression*). We thus aim to first find a *perceptually equivalent, but computationally more suitable space*, in which we will train diffusion models for high-resolution image synthesis.\n\nFollowing common practice [11, 23, 66, 67, 96], we separate training into two distinct phases: First, we train an autoencoder which provides a lower-dimensional (and thereby efficient) representational space which is perceptually equivalent to the data space. Importantly, and in contrast to previous work [23, 66], we do not need to rely on excessive spatial compression, as we train DMs in the learned latent space, which exhibits better scaling properties with respect to the spatial dimensionality. The reduced complexity also provides efficient image generation from the latent space with a single network pass. We dub the resulting model class *Latent Diffusion Models* (LDMs).\n\nA notable advantage of this approach is that we need to train the universal autoencoding stage only once and can therefore reuse it for multiple DM trainings or to explore possibly completely different tasks [81]. This enables efficient exploration of a large number of diffusion models for various image-to-image and text-to-image tasks. For the latter, we design an architecture that connects transformers to the DM\u2019s UNet backbone [71] and enables arbitrary types of token-based conditioning mechanisms, see Sec. 3.3.\n\nIn sum, our work makes the following **contributions**:\n(i) In contrast to purely transformer-based approaches [23, 66], our method scales more graceful to higher dimensional data and can thus (a) work on a compression level which provides more faithful and detailed reconstructions than previous work (see Fig. 1) and (b) can be efficiently\n\napplied to high-resolution synthesis of megapixel images.\n    (ii) We achieve competitive performance on multiple\ntasks (unconditional image synthesis, inpainting, stochastic\nsuper-resolution) and datasets while significantly lowering\ncomputational costs. Compared to pixel-based diffusion ap-\nproaches, we also significantly decrease inference costs.\n\n(iii) We show that, in contrast to previous work [93] which learns both an encoder/decoder architecture and a score-based prior simultaneously, our approach does not require a delicate weighting of reconstruction and generative abilities. This ensures extremely faithful reconstructions and requires very little regularization of the latent space.\n\n(iv) We find that for densely conditioned tasks such as super-resolution, inpainting and semantic synthesis, our model can be applied in a convolutional fashion and render large, consistent images of $\\sim 1024^2$ px.\n\n(v) Moreover, we design a general-purpose conditioning mechanism based on cross-attention, enabling multi-modal training. We use it to train class-conditional, text-to-image and layout-to-image models.\n\n(vi) Finally, we release pretrained latent diffusion and autoencoding models at https://github.com/CompVis/latent-diffusion which might be reusable for a various tasks besides training of DMs [81]."
        },
        {
            "text": "(v) Moreover, we design a general-purpose conditioning mechanism based on cross-attention, enabling multi-modal training. We use it to train class-conditional, text-to-image and layout-to-image models.",
            "page": 2,
            "x": 306,
            "y": 543,
            "width": 242,
            "height": 47,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "605d9ae7-7b92-4e76-9279-d116564cdcde",
            "group_text": "1. Introduction\n\n    Image synthesis is one of the computer vision fields with the most spectacular recent development, but also among those with the greatest computational demands.    Especially high-resolution synthesis of complex, natural scenes is presently dominated by scaling up likelihood-based models, potentially containing billions of parameters in autoregressive (AR) transformers [66, 67]. In contrast, the promising results of GANs [3, 27, 40] have been revealed to be mostly confined to data with comparably limited variability as their adversarial learning procedure does not easily scale to modeling complex, multi-modal distributions. Recently, diffusion models [82], which are built from a hierarchy of denoising autoencoders, have shown to achieve impressive\n\nFigure 1.  Boosting the upper bound on achievable quality with less agressive downsampling. Since diffusion models offer excellent inductive biases for spatial data, we do not need the heavy spatial downsampling of related generative models in latent space, but can still greatly reduce the dimensionality of the data via suitable autoencoding models, see Sec. 3. Images are from the DIV2K [1] validation set, evaluated at $512^2$ px. We denote the spatial downsampling factor by $f$. Reconstruction FIDs [29] and PSNR are calculated on ImageNet-val. [12]; see also Tab. 8.\n\nresults in image synthesis [30, 85] and beyond [7, 45, 48, 57],\nand define the state-of-the-art in class-conditional image\nsynthesis [15, 31] and super-resolution [72]. Moreover, even\nunconditional DMs can readily be applied to tasks such\nas inpainting and colorization [85] or stroke-based syn-\nthesis [53], in contrast to other types of generative mod-\nels [19, 46, 69]. Being likelihood-based models, they do not\nexhibit mode-collapse and training instabilities as GANs\nand, by heavily exploiting parameter sharing, they can\nmodel highly complex distributions of natural images with-\nout involving billions of parameters as in AR models [67].\n**Democratizing High-Resolution Image Synthesis** DMs\nbelong to the class of likelihood-based models, whose\nmode-covering behavior makes them prone to spend ex-\ncessive amounts of capacity (and thus compute resources)\non modeling imperceptible details of the data [16, 73]. Al-\nthough the reweighted variational objective [30] aims to ad-\ndress this by undersampling the initial denoising steps, DMs\nare still computationally demanding, since training and\nevaluating such a model requires repeated function evalu-\nations (and gradient computations) in the high-dimensional\nspace of RGB images. As an example, training the most\npowerful DMs often takes hundreds of GPU days (e.g. 150 -\n1000 V100 days in [15]) and repeated evaluations on a noisy\nversion of the input space render also inference expensive,\n\nso that producing 50k samples takes approximately 5 days\n[15] on a single A100 GPU. This has two consequences for\nthe research community and users in general: Firstly, train-\ning such a model requires massive computational resources\nonly available to a small fraction of the field, and leaves a\nhuge carbon footprint [65, 86]. Secondly, evaluating an al-\nready trained model is also expensive in time and memory,\nsince the same model architecture must run sequentially for\na large number of steps (e.g. 25 - 1000 steps in [15]).\n\nTo increase the accessibility of this powerful model class and at the same time reduce its significant resource consumption, a method is needed that reduces the computational complexity for both training and sampling. Reducing the computational demands of DMs without impairing their performance is, therefore, key to enhance their accessibility.\n\n**Departure to Latent Space** Our approach starts with the analysis of already trained diffusion models in pixel space: Fig. 2 shows the rate-distortion trade-off of a trained model. As with any likelihood-based model, learning can be roughly divided into two stages: First is a *perceptual compression* stage which removes high-frequency details but still learns little semantic variation. In the second stage, the actual generative model learns the semantic and conceptual composition of the data (*semantic compression*). We thus aim to first find a *perceptually equivalent, but computationally more suitable space*, in which we will train diffusion models for high-resolution image synthesis.\n\nFollowing common practice [11, 23, 66, 67, 96], we separate training into two distinct phases: First, we train an autoencoder which provides a lower-dimensional (and thereby efficient) representational space which is perceptually equivalent to the data space. Importantly, and in contrast to previous work [23, 66], we do not need to rely on excessive spatial compression, as we train DMs in the learned latent space, which exhibits better scaling properties with respect to the spatial dimensionality. The reduced complexity also provides efficient image generation from the latent space with a single network pass. We dub the resulting model class *Latent Diffusion Models* (LDMs).\n\nA notable advantage of this approach is that we need to train the universal autoencoding stage only once and can therefore reuse it for multiple DM trainings or to explore possibly completely different tasks [81]. This enables efficient exploration of a large number of diffusion models for various image-to-image and text-to-image tasks. For the latter, we design an architecture that connects transformers to the DM\u2019s UNet backbone [71] and enables arbitrary types of token-based conditioning mechanisms, see Sec. 3.3.\n\nIn sum, our work makes the following **contributions**:\n(i) In contrast to purely transformer-based approaches [23, 66], our method scales more graceful to higher dimensional data and can thus (a) work on a compression level which provides more faithful and detailed reconstructions than previous work (see Fig. 1) and (b) can be efficiently\n\napplied to high-resolution synthesis of megapixel images.\n    (ii) We achieve competitive performance on multiple\ntasks (unconditional image synthesis, inpainting, stochastic\nsuper-resolution) and datasets while significantly lowering\ncomputational costs. Compared to pixel-based diffusion ap-\nproaches, we also significantly decrease inference costs.\n\n(iii) We show that, in contrast to previous work [93] which learns both an encoder/decoder architecture and a score-based prior simultaneously, our approach does not require a delicate weighting of reconstruction and generative abilities. This ensures extremely faithful reconstructions and requires very little regularization of the latent space.\n\n(iv) We find that for densely conditioned tasks such as super-resolution, inpainting and semantic synthesis, our model can be applied in a convolutional fashion and render large, consistent images of $\\sim 1024^2$ px.\n\n(v) Moreover, we design a general-purpose conditioning mechanism based on cross-attention, enabling multi-modal training. We use it to train class-conditional, text-to-image and layout-to-image models.\n\n(vi) Finally, we release pretrained latent diffusion and autoencoding models at https://github.com/CompVis/latent-diffusion which might be reusable for a various tasks besides training of DMs [81]."
        },
        {
            "text": "(vi) Finally, we release pretrained latent diffusion and autoencoding models at https://github.com/CompVis/latent-diffusion which might be reusable for a various tasks besides training of DMs [81].",
            "page": 2,
            "x": 306,
            "y": 591,
            "width": 242,
            "height": 49,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "430814a5-58eb-441e-b511-ebe0a273ae55",
            "group_text": "1. Introduction\n\n    Image synthesis is one of the computer vision fields with the most spectacular recent development, but also among those with the greatest computational demands.    Especially high-resolution synthesis of complex, natural scenes is presently dominated by scaling up likelihood-based models, potentially containing billions of parameters in autoregressive (AR) transformers [66, 67]. In contrast, the promising results of GANs [3, 27, 40] have been revealed to be mostly confined to data with comparably limited variability as their adversarial learning procedure does not easily scale to modeling complex, multi-modal distributions. Recently, diffusion models [82], which are built from a hierarchy of denoising autoencoders, have shown to achieve impressive\n\nFigure 1.  Boosting the upper bound on achievable quality with less agressive downsampling. Since diffusion models offer excellent inductive biases for spatial data, we do not need the heavy spatial downsampling of related generative models in latent space, but can still greatly reduce the dimensionality of the data via suitable autoencoding models, see Sec. 3. Images are from the DIV2K [1] validation set, evaluated at $512^2$ px. We denote the spatial downsampling factor by $f$. Reconstruction FIDs [29] and PSNR are calculated on ImageNet-val. [12]; see also Tab. 8.\n\nresults in image synthesis [30, 85] and beyond [7, 45, 48, 57],\nand define the state-of-the-art in class-conditional image\nsynthesis [15, 31] and super-resolution [72]. Moreover, even\nunconditional DMs can readily be applied to tasks such\nas inpainting and colorization [85] or stroke-based syn-\nthesis [53], in contrast to other types of generative mod-\nels [19, 46, 69]. Being likelihood-based models, they do not\nexhibit mode-collapse and training instabilities as GANs\nand, by heavily exploiting parameter sharing, they can\nmodel highly complex distributions of natural images with-\nout involving billions of parameters as in AR models [67].\n**Democratizing High-Resolution Image Synthesis** DMs\nbelong to the class of likelihood-based models, whose\nmode-covering behavior makes them prone to spend ex-\ncessive amounts of capacity (and thus compute resources)\non modeling imperceptible details of the data [16, 73]. Al-\nthough the reweighted variational objective [30] aims to ad-\ndress this by undersampling the initial denoising steps, DMs\nare still computationally demanding, since training and\nevaluating such a model requires repeated function evalu-\nations (and gradient computations) in the high-dimensional\nspace of RGB images. As an example, training the most\npowerful DMs often takes hundreds of GPU days (e.g. 150 -\n1000 V100 days in [15]) and repeated evaluations on a noisy\nversion of the input space render also inference expensive,\n\nso that producing 50k samples takes approximately 5 days\n[15] on a single A100 GPU. This has two consequences for\nthe research community and users in general: Firstly, train-\ning such a model requires massive computational resources\nonly available to a small fraction of the field, and leaves a\nhuge carbon footprint [65, 86]. Secondly, evaluating an al-\nready trained model is also expensive in time and memory,\nsince the same model architecture must run sequentially for\na large number of steps (e.g. 25 - 1000 steps in [15]).\n\nTo increase the accessibility of this powerful model class and at the same time reduce its significant resource consumption, a method is needed that reduces the computational complexity for both training and sampling. Reducing the computational demands of DMs without impairing their performance is, therefore, key to enhance their accessibility.\n\n**Departure to Latent Space** Our approach starts with the analysis of already trained diffusion models in pixel space: Fig. 2 shows the rate-distortion trade-off of a trained model. As with any likelihood-based model, learning can be roughly divided into two stages: First is a *perceptual compression* stage which removes high-frequency details but still learns little semantic variation. In the second stage, the actual generative model learns the semantic and conceptual composition of the data (*semantic compression*). We thus aim to first find a *perceptually equivalent, but computationally more suitable space*, in which we will train diffusion models for high-resolution image synthesis.\n\nFollowing common practice [11, 23, 66, 67, 96], we separate training into two distinct phases: First, we train an autoencoder which provides a lower-dimensional (and thereby efficient) representational space which is perceptually equivalent to the data space. Importantly, and in contrast to previous work [23, 66], we do not need to rely on excessive spatial compression, as we train DMs in the learned latent space, which exhibits better scaling properties with respect to the spatial dimensionality. The reduced complexity also provides efficient image generation from the latent space with a single network pass. We dub the resulting model class *Latent Diffusion Models* (LDMs).\n\nA notable advantage of this approach is that we need to train the universal autoencoding stage only once and can therefore reuse it for multiple DM trainings or to explore possibly completely different tasks [81]. This enables efficient exploration of a large number of diffusion models for various image-to-image and text-to-image tasks. For the latter, we design an architecture that connects transformers to the DM\u2019s UNet backbone [71] and enables arbitrary types of token-based conditioning mechanisms, see Sec. 3.3.\n\nIn sum, our work makes the following **contributions**:\n(i) In contrast to purely transformer-based approaches [23, 66], our method scales more graceful to higher dimensional data and can thus (a) work on a compression level which provides more faithful and detailed reconstructions than previous work (see Fig. 1) and (b) can be efficiently\n\napplied to high-resolution synthesis of megapixel images.\n    (ii) We achieve competitive performance on multiple\ntasks (unconditional image synthesis, inpainting, stochastic\nsuper-resolution) and datasets while significantly lowering\ncomputational costs. Compared to pixel-based diffusion ap-\nproaches, we also significantly decrease inference costs.\n\n(iii) We show that, in contrast to previous work [93] which learns both an encoder/decoder architecture and a score-based prior simultaneously, our approach does not require a delicate weighting of reconstruction and generative abilities. This ensures extremely faithful reconstructions and requires very little regularization of the latent space.\n\n(iv) We find that for densely conditioned tasks such as super-resolution, inpainting and semantic synthesis, our model can be applied in a convolutional fashion and render large, consistent images of $\\sim 1024^2$ px.\n\n(v) Moreover, we design a general-purpose conditioning mechanism based on cross-attention, enabling multi-modal training. We use it to train class-conditional, text-to-image and layout-to-image models.\n\n(vi) Finally, we release pretrained latent diffusion and autoencoding models at https://github.com/CompVis/latent-diffusion which might be reusable for a various tasks besides training of DMs [81]."
        },
        {
            "text": "## 2. Related Work\n\n**Generative Models for Image Synthesis** The high dimensional nature of images presents distinct challenges to generative modeling. Generative Adversarial Networks (GAN) [27] allow for efficient sampling of high resolution images with good perceptual quality [3, 42], but are diffi-",
            "page": 2,
            "x": 305,
            "y": 651,
            "width": 242,
            "height": 75,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "3-related",
            "chunk_id": "2e028ca6-0da9-458b-88a3-ce0e296b6e26",
            "group_text": "## 2. Related Work\n\n**Generative Models for Image Synthesis** The high dimensional nature of images presents distinct challenges to generative modeling. Generative Adversarial Networks (GAN) [27] allow for efficient sampling of high resolution images with good perceptual quality [3, 42], but are diffi-\n\ncult to optimize [2, 28, 54] and struggle to capture the full\ndata distribution [55]. In contrast, likelihood-based meth-\nods emphasize good density estimation which renders op-\ntimization more well-behaved. Variational autoencoders\n(VAE) [46] and flow-based models [18, 19] enable efficient\nsynthesis of high resolution images [9, 44, 92], but sam-\nple quality is not on par with GANs. While autoregressive\nmodels (ARM) [6, 10, 94, 95] achieve strong performance\nin density estimation, computationally demanding architec-\ntures [97] and a sequential sampling process limit them to\nlow resolution images. Because pixel based representations\nof images contain barely perceptible, high-frequency de-\ntails [16,73], maximum-likelihood training spends a dispro-\nportionate amount of capacity on modeling them, resulting\nin long training times. To scale to higher resolutions, several\ntwo-stage approaches [23,67,101,103] use ARMs to model\na compressed latent image space instead of raw pixels.\n\nRecently, **Diffusion Probabilistic Models** (DM) [82], have achieved state-of-the-art results in density estimation [45] as well as in sample quality [15]. The generative power of these models stems from a natural fit to the inductive biases of image-like data when their underlying neural backbone is implemented as a UNet [15, 30, 71, 85]. The best synthesis quality is usually achieved when a reweighted objective [30] is used for training. In this case, the DM corresponds to a lossy compressor and allow to trade image quality for compression capabilities. Evaluating and optimizing these models in pixel space, however, has the downside of low inference speed and very high training costs. While the former can be partially adressed by advanced sampling strategies [47, 75, 84] and hierarchical approaches [31, 93], training on high-resolution image data always requires to calculate expensive gradients. We adress both drawbacks with our proposed _LDMs_, which work on a compressed latent space of lower dimensionality. This renders training computationally cheaper and speeds up inference with almost no reduction in synthesis quality (see Fig. 1).\n\n**Two-Stage Image Synthesis** To mitigate the shortcomings of individual generative approaches, a lot of research [11, 23, 67, 70, 101, 103] has gone into combining the strengths of different methods into more efficient and performant models via a two stage approach. VQ-VAEs [67, 101] use autoregressive models to learn an expressive prior over a discretized latent space. [66] extend this approach to text-to-image generation by learning a joint distributation over discretized image and text representations. More generally, [70] uses conditionally invertible networks to provide a generic transfer between latent spaces of diverse domains. Different from VQ-VAEs, VQGANs [23, 103] employ a first stage with an adversarial and perceptual objective to scale autoregressive transformers to larger images. However, the high compression rates required for feasible ARM training, which introduces billions of trainable parameters [23, 66], limit the overall performance of such ap-\n\nproaches and less compression comes at the price of high computational cost [23, 66]. Our work prevents such trade-offs, as our proposed *LDMs* scale more gently to higher dimensional latent spaces due to their convolutional backbone. Thus, we are free to choose the level of compression which optimally mediates between learning a powerful first stage, without leaving too much perceptual compression up to the generative diffusion model while guaranteeing high-fidelity reconstructions (see Fig. 1).\n\nWhile approaches to jointly [93] or separately [80] learn an encoding/decoding model together with a score-based prior exist, the former still require a difficult weighting between reconstruction and generative capabilities [11] and are outperformed by our approach (Sec. 4), and the latter focus on highly structured images such as human faces."
        },
        {
            "text": "cult to optimize [2, 28, 54] and struggle to capture the full\ndata distribution [55]. In contrast, likelihood-based meth-\nods emphasize good density estimation which renders op-\ntimization more well-behaved. Variational autoencoders\n(VAE) [46] and flow-based models [18, 19] enable efficient\nsynthesis of high resolution images [9, 44, 92], but sam-\nple quality is not on par with GANs. While autoregressive\nmodels (ARM) [6, 10, 94, 95] achieve strong performance\nin density estimation, computationally demanding architec-\ntures [97] and a sequential sampling process limit them to\nlow resolution images. Because pixel based representations\nof images contain barely perceptible, high-frequency de-\ntails [16,73], maximum-likelihood training spends a dispro-\nportionate amount of capacity on modeling them, resulting\nin long training times. To scale to higher resolutions, several\ntwo-stage approaches [23,67,101,103] use ARMs to model\na compressed latent image space instead of raw pixels.",
            "page": 3,
            "x": 45,
            "y": 71,
            "width": 244,
            "height": 206,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "3-related",
            "chunk_id": "1cde1c64-7fd6-4b55-986b-842fd7467faf",
            "group_text": "## 2. Related Work\n\n**Generative Models for Image Synthesis** The high dimensional nature of images presents distinct challenges to generative modeling. Generative Adversarial Networks (GAN) [27] allow for efficient sampling of high resolution images with good perceptual quality [3, 42], but are diffi-\n\ncult to optimize [2, 28, 54] and struggle to capture the full\ndata distribution [55]. In contrast, likelihood-based meth-\nods emphasize good density estimation which renders op-\ntimization more well-behaved. Variational autoencoders\n(VAE) [46] and flow-based models [18, 19] enable efficient\nsynthesis of high resolution images [9, 44, 92], but sam-\nple quality is not on par with GANs. While autoregressive\nmodels (ARM) [6, 10, 94, 95] achieve strong performance\nin density estimation, computationally demanding architec-\ntures [97] and a sequential sampling process limit them to\nlow resolution images. Because pixel based representations\nof images contain barely perceptible, high-frequency de-\ntails [16,73], maximum-likelihood training spends a dispro-\nportionate amount of capacity on modeling them, resulting\nin long training times. To scale to higher resolutions, several\ntwo-stage approaches [23,67,101,103] use ARMs to model\na compressed latent image space instead of raw pixels.\n\nRecently, **Diffusion Probabilistic Models** (DM) [82], have achieved state-of-the-art results in density estimation [45] as well as in sample quality [15]. The generative power of these models stems from a natural fit to the inductive biases of image-like data when their underlying neural backbone is implemented as a UNet [15, 30, 71, 85]. The best synthesis quality is usually achieved when a reweighted objective [30] is used for training. In this case, the DM corresponds to a lossy compressor and allow to trade image quality for compression capabilities. Evaluating and optimizing these models in pixel space, however, has the downside of low inference speed and very high training costs. While the former can be partially adressed by advanced sampling strategies [47, 75, 84] and hierarchical approaches [31, 93], training on high-resolution image data always requires to calculate expensive gradients. We adress both drawbacks with our proposed _LDMs_, which work on a compressed latent space of lower dimensionality. This renders training computationally cheaper and speeds up inference with almost no reduction in synthesis quality (see Fig. 1).\n\n**Two-Stage Image Synthesis** To mitigate the shortcomings of individual generative approaches, a lot of research [11, 23, 67, 70, 101, 103] has gone into combining the strengths of different methods into more efficient and performant models via a two stage approach. VQ-VAEs [67, 101] use autoregressive models to learn an expressive prior over a discretized latent space. [66] extend this approach to text-to-image generation by learning a joint distributation over discretized image and text representations. More generally, [70] uses conditionally invertible networks to provide a generic transfer between latent spaces of diverse domains. Different from VQ-VAEs, VQGANs [23, 103] employ a first stage with an adversarial and perceptual objective to scale autoregressive transformers to larger images. However, the high compression rates required for feasible ARM training, which introduces billions of trainable parameters [23, 66], limit the overall performance of such ap-\n\nproaches and less compression comes at the price of high computational cost [23, 66]. Our work prevents such trade-offs, as our proposed *LDMs* scale more gently to higher dimensional latent spaces due to their convolutional backbone. Thus, we are free to choose the level of compression which optimally mediates between learning a powerful first stage, without leaving too much perceptual compression up to the generative diffusion model while guaranteeing high-fidelity reconstructions (see Fig. 1).\n\nWhile approaches to jointly [93] or separately [80] learn an encoding/decoding model together with a score-based prior exist, the former still require a difficult weighting between reconstruction and generative capabilities [11] and are outperformed by our approach (Sec. 4), and the latter focus on highly structured images such as human faces."
        },
        {
            "text": "Recently, **Diffusion Probabilistic Models** (DM) [82], have achieved state-of-the-art results in density estimation [45] as well as in sample quality [15]. The generative power of these models stems from a natural fit to the inductive biases of image-like data when their underlying neural backbone is implemented as a UNet [15, 30, 71, 85]. The best synthesis quality is usually achieved when a reweighted objective [30] is used for training. In this case, the DM corresponds to a lossy compressor and allow to trade image quality for compression capabilities. Evaluating and optimizing these models in pixel space, however, has the downside of low inference speed and very high training costs. While the former can be partially adressed by advanced sampling strategies [47, 75, 84] and hierarchical approaches [31, 93], training on high-resolution image data always requires to calculate expensive gradients. We adress both drawbacks with our proposed _LDMs_, which work on a compressed latent space of lower dimensionality. This renders training computationally cheaper and speeds up inference with almost no reduction in synthesis quality (see Fig. 1).",
            "page": 3,
            "x": 45,
            "y": 278,
            "width": 245,
            "height": 241,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "3-related",
            "chunk_id": "b3d66ab8-3cc4-4ba4-a309-974800961e37",
            "group_text": "## 2. Related Work\n\n**Generative Models for Image Synthesis** The high dimensional nature of images presents distinct challenges to generative modeling. Generative Adversarial Networks (GAN) [27] allow for efficient sampling of high resolution images with good perceptual quality [3, 42], but are diffi-\n\ncult to optimize [2, 28, 54] and struggle to capture the full\ndata distribution [55]. In contrast, likelihood-based meth-\nods emphasize good density estimation which renders op-\ntimization more well-behaved. Variational autoencoders\n(VAE) [46] and flow-based models [18, 19] enable efficient\nsynthesis of high resolution images [9, 44, 92], but sam-\nple quality is not on par with GANs. While autoregressive\nmodels (ARM) [6, 10, 94, 95] achieve strong performance\nin density estimation, computationally demanding architec-\ntures [97] and a sequential sampling process limit them to\nlow resolution images. Because pixel based representations\nof images contain barely perceptible, high-frequency de-\ntails [16,73], maximum-likelihood training spends a dispro-\nportionate amount of capacity on modeling them, resulting\nin long training times. To scale to higher resolutions, several\ntwo-stage approaches [23,67,101,103] use ARMs to model\na compressed latent image space instead of raw pixels.\n\nRecently, **Diffusion Probabilistic Models** (DM) [82], have achieved state-of-the-art results in density estimation [45] as well as in sample quality [15]. The generative power of these models stems from a natural fit to the inductive biases of image-like data when their underlying neural backbone is implemented as a UNet [15, 30, 71, 85]. The best synthesis quality is usually achieved when a reweighted objective [30] is used for training. In this case, the DM corresponds to a lossy compressor and allow to trade image quality for compression capabilities. Evaluating and optimizing these models in pixel space, however, has the downside of low inference speed and very high training costs. While the former can be partially adressed by advanced sampling strategies [47, 75, 84] and hierarchical approaches [31, 93], training on high-resolution image data always requires to calculate expensive gradients. We adress both drawbacks with our proposed _LDMs_, which work on a compressed latent space of lower dimensionality. This renders training computationally cheaper and speeds up inference with almost no reduction in synthesis quality (see Fig. 1).\n\n**Two-Stage Image Synthesis** To mitigate the shortcomings of individual generative approaches, a lot of research [11, 23, 67, 70, 101, 103] has gone into combining the strengths of different methods into more efficient and performant models via a two stage approach. VQ-VAEs [67, 101] use autoregressive models to learn an expressive prior over a discretized latent space. [66] extend this approach to text-to-image generation by learning a joint distributation over discretized image and text representations. More generally, [70] uses conditionally invertible networks to provide a generic transfer between latent spaces of diverse domains. Different from VQ-VAEs, VQGANs [23, 103] employ a first stage with an adversarial and perceptual objective to scale autoregressive transformers to larger images. However, the high compression rates required for feasible ARM training, which introduces billions of trainable parameters [23, 66], limit the overall performance of such ap-\n\nproaches and less compression comes at the price of high computational cost [23, 66]. Our work prevents such trade-offs, as our proposed *LDMs* scale more gently to higher dimensional latent spaces due to their convolutional backbone. Thus, we are free to choose the level of compression which optimally mediates between learning a powerful first stage, without leaving too much perceptual compression up to the generative diffusion model while guaranteeing high-fidelity reconstructions (see Fig. 1).\n\nWhile approaches to jointly [93] or separately [80] learn an encoding/decoding model together with a score-based prior exist, the former still require a difficult weighting between reconstruction and generative capabilities [11] and are outperformed by our approach (Sec. 4), and the latter focus on highly structured images such as human faces."
        },
        {
            "text": "**Two-Stage Image Synthesis** To mitigate the shortcomings of individual generative approaches, a lot of research [11, 23, 67, 70, 101, 103] has gone into combining the strengths of different methods into more efficient and performant models via a two stage approach. VQ-VAEs [67, 101] use autoregressive models to learn an expressive prior over a discretized latent space. [66] extend this approach to text-to-image generation by learning a joint distributation over discretized image and text representations. More generally, [70] uses conditionally invertible networks to provide a generic transfer between latent spaces of diverse domains. Different from VQ-VAEs, VQGANs [23, 103] employ a first stage with an adversarial and perceptual objective to scale autoregressive transformers to larger images. However, the high compression rates required for feasible ARM training, which introduces billions of trainable parameters [23, 66], limit the overall performance of such ap-",
            "page": 3,
            "x": 45,
            "y": 521,
            "width": 245,
            "height": 206,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "3-related",
            "chunk_id": "86c30478-ee4d-44ab-a399-3d31008750a3",
            "group_text": "## 2. Related Work\n\n**Generative Models for Image Synthesis** The high dimensional nature of images presents distinct challenges to generative modeling. Generative Adversarial Networks (GAN) [27] allow for efficient sampling of high resolution images with good perceptual quality [3, 42], but are diffi-\n\ncult to optimize [2, 28, 54] and struggle to capture the full\ndata distribution [55]. In contrast, likelihood-based meth-\nods emphasize good density estimation which renders op-\ntimization more well-behaved. Variational autoencoders\n(VAE) [46] and flow-based models [18, 19] enable efficient\nsynthesis of high resolution images [9, 44, 92], but sam-\nple quality is not on par with GANs. While autoregressive\nmodels (ARM) [6, 10, 94, 95] achieve strong performance\nin density estimation, computationally demanding architec-\ntures [97] and a sequential sampling process limit them to\nlow resolution images. Because pixel based representations\nof images contain barely perceptible, high-frequency de-\ntails [16,73], maximum-likelihood training spends a dispro-\nportionate amount of capacity on modeling them, resulting\nin long training times. To scale to higher resolutions, several\ntwo-stage approaches [23,67,101,103] use ARMs to model\na compressed latent image space instead of raw pixels.\n\nRecently, **Diffusion Probabilistic Models** (DM) [82], have achieved state-of-the-art results in density estimation [45] as well as in sample quality [15]. The generative power of these models stems from a natural fit to the inductive biases of image-like data when their underlying neural backbone is implemented as a UNet [15, 30, 71, 85]. The best synthesis quality is usually achieved when a reweighted objective [30] is used for training. In this case, the DM corresponds to a lossy compressor and allow to trade image quality for compression capabilities. Evaluating and optimizing these models in pixel space, however, has the downside of low inference speed and very high training costs. While the former can be partially adressed by advanced sampling strategies [47, 75, 84] and hierarchical approaches [31, 93], training on high-resolution image data always requires to calculate expensive gradients. We adress both drawbacks with our proposed _LDMs_, which work on a compressed latent space of lower dimensionality. This renders training computationally cheaper and speeds up inference with almost no reduction in synthesis quality (see Fig. 1).\n\n**Two-Stage Image Synthesis** To mitigate the shortcomings of individual generative approaches, a lot of research [11, 23, 67, 70, 101, 103] has gone into combining the strengths of different methods into more efficient and performant models via a two stage approach. VQ-VAEs [67, 101] use autoregressive models to learn an expressive prior over a discretized latent space. [66] extend this approach to text-to-image generation by learning a joint distributation over discretized image and text representations. More generally, [70] uses conditionally invertible networks to provide a generic transfer between latent spaces of diverse domains. Different from VQ-VAEs, VQGANs [23, 103] employ a first stage with an adversarial and perceptual objective to scale autoregressive transformers to larger images. However, the high compression rates required for feasible ARM training, which introduces billions of trainable parameters [23, 66], limit the overall performance of such ap-\n\nproaches and less compression comes at the price of high computational cost [23, 66]. Our work prevents such trade-offs, as our proposed *LDMs* scale more gently to higher dimensional latent spaces due to their convolutional backbone. Thus, we are free to choose the level of compression which optimally mediates between learning a powerful first stage, without leaving too much perceptual compression up to the generative diffusion model while guaranteeing high-fidelity reconstructions (see Fig. 1).\n\nWhile approaches to jointly [93] or separately [80] learn an encoding/decoding model together with a score-based prior exist, the former still require a difficult weighting between reconstruction and generative capabilities [11] and are outperformed by our approach (Sec. 4), and the latter focus on highly structured images such as human faces."
        },
        {
            "text": "proaches and less compression comes at the price of high computational cost [23, 66]. Our work prevents such trade-offs, as our proposed *LDMs* scale more gently to higher dimensional latent spaces due to their convolutional backbone. Thus, we are free to choose the level of compression which optimally mediates between learning a powerful first stage, without leaving too much perceptual compression up to the generative diffusion model while guaranteeing high-fidelity reconstructions (see Fig. 1).",
            "page": 3,
            "x": 305,
            "y": 72,
            "width": 244,
            "height": 108,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "3-related",
            "chunk_id": "ea05f42a-66b4-4065-ade0-0ba54cb853fe",
            "group_text": "## 2. Related Work\n\n**Generative Models for Image Synthesis** The high dimensional nature of images presents distinct challenges to generative modeling. Generative Adversarial Networks (GAN) [27] allow for efficient sampling of high resolution images with good perceptual quality [3, 42], but are diffi-\n\ncult to optimize [2, 28, 54] and struggle to capture the full\ndata distribution [55]. In contrast, likelihood-based meth-\nods emphasize good density estimation which renders op-\ntimization more well-behaved. Variational autoencoders\n(VAE) [46] and flow-based models [18, 19] enable efficient\nsynthesis of high resolution images [9, 44, 92], but sam-\nple quality is not on par with GANs. While autoregressive\nmodels (ARM) [6, 10, 94, 95] achieve strong performance\nin density estimation, computationally demanding architec-\ntures [97] and a sequential sampling process limit them to\nlow resolution images. Because pixel based representations\nof images contain barely perceptible, high-frequency de-\ntails [16,73], maximum-likelihood training spends a dispro-\nportionate amount of capacity on modeling them, resulting\nin long training times. To scale to higher resolutions, several\ntwo-stage approaches [23,67,101,103] use ARMs to model\na compressed latent image space instead of raw pixels.\n\nRecently, **Diffusion Probabilistic Models** (DM) [82], have achieved state-of-the-art results in density estimation [45] as well as in sample quality [15]. The generative power of these models stems from a natural fit to the inductive biases of image-like data when their underlying neural backbone is implemented as a UNet [15, 30, 71, 85]. The best synthesis quality is usually achieved when a reweighted objective [30] is used for training. In this case, the DM corresponds to a lossy compressor and allow to trade image quality for compression capabilities. Evaluating and optimizing these models in pixel space, however, has the downside of low inference speed and very high training costs. While the former can be partially adressed by advanced sampling strategies [47, 75, 84] and hierarchical approaches [31, 93], training on high-resolution image data always requires to calculate expensive gradients. We adress both drawbacks with our proposed _LDMs_, which work on a compressed latent space of lower dimensionality. This renders training computationally cheaper and speeds up inference with almost no reduction in synthesis quality (see Fig. 1).\n\n**Two-Stage Image Synthesis** To mitigate the shortcomings of individual generative approaches, a lot of research [11, 23, 67, 70, 101, 103] has gone into combining the strengths of different methods into more efficient and performant models via a two stage approach. VQ-VAEs [67, 101] use autoregressive models to learn an expressive prior over a discretized latent space. [66] extend this approach to text-to-image generation by learning a joint distributation over discretized image and text representations. More generally, [70] uses conditionally invertible networks to provide a generic transfer between latent spaces of diverse domains. Different from VQ-VAEs, VQGANs [23, 103] employ a first stage with an adversarial and perceptual objective to scale autoregressive transformers to larger images. However, the high compression rates required for feasible ARM training, which introduces billions of trainable parameters [23, 66], limit the overall performance of such ap-\n\nproaches and less compression comes at the price of high computational cost [23, 66]. Our work prevents such trade-offs, as our proposed *LDMs* scale more gently to higher dimensional latent spaces due to their convolutional backbone. Thus, we are free to choose the level of compression which optimally mediates between learning a powerful first stage, without leaving too much perceptual compression up to the generative diffusion model while guaranteeing high-fidelity reconstructions (see Fig. 1).\n\nWhile approaches to jointly [93] or separately [80] learn an encoding/decoding model together with a score-based prior exist, the former still require a difficult weighting between reconstruction and generative capabilities [11] and are outperformed by our approach (Sec. 4), and the latter focus on highly structured images such as human faces."
        },
        {
            "text": "While approaches to jointly [93] or separately [80] learn an encoding/decoding model together with a score-based prior exist, the former still require a difficult weighting between reconstruction and generative capabilities [11] and are outperformed by our approach (Sec. 4), and the latter focus on highly structured images such as human faces.",
            "page": 3,
            "x": 305,
            "y": 181,
            "width": 244,
            "height": 75,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "3-related",
            "chunk_id": "ec1bbd81-247c-4866-bc78-d9bd076d9e39",
            "group_text": "## 2. Related Work\n\n**Generative Models for Image Synthesis** The high dimensional nature of images presents distinct challenges to generative modeling. Generative Adversarial Networks (GAN) [27] allow for efficient sampling of high resolution images with good perceptual quality [3, 42], but are diffi-\n\ncult to optimize [2, 28, 54] and struggle to capture the full\ndata distribution [55]. In contrast, likelihood-based meth-\nods emphasize good density estimation which renders op-\ntimization more well-behaved. Variational autoencoders\n(VAE) [46] and flow-based models [18, 19] enable efficient\nsynthesis of high resolution images [9, 44, 92], but sam-\nple quality is not on par with GANs. While autoregressive\nmodels (ARM) [6, 10, 94, 95] achieve strong performance\nin density estimation, computationally demanding architec-\ntures [97] and a sequential sampling process limit them to\nlow resolution images. Because pixel based representations\nof images contain barely perceptible, high-frequency de-\ntails [16,73], maximum-likelihood training spends a dispro-\nportionate amount of capacity on modeling them, resulting\nin long training times. To scale to higher resolutions, several\ntwo-stage approaches [23,67,101,103] use ARMs to model\na compressed latent image space instead of raw pixels.\n\nRecently, **Diffusion Probabilistic Models** (DM) [82], have achieved state-of-the-art results in density estimation [45] as well as in sample quality [15]. The generative power of these models stems from a natural fit to the inductive biases of image-like data when their underlying neural backbone is implemented as a UNet [15, 30, 71, 85]. The best synthesis quality is usually achieved when a reweighted objective [30] is used for training. In this case, the DM corresponds to a lossy compressor and allow to trade image quality for compression capabilities. Evaluating and optimizing these models in pixel space, however, has the downside of low inference speed and very high training costs. While the former can be partially adressed by advanced sampling strategies [47, 75, 84] and hierarchical approaches [31, 93], training on high-resolution image data always requires to calculate expensive gradients. We adress both drawbacks with our proposed _LDMs_, which work on a compressed latent space of lower dimensionality. This renders training computationally cheaper and speeds up inference with almost no reduction in synthesis quality (see Fig. 1).\n\n**Two-Stage Image Synthesis** To mitigate the shortcomings of individual generative approaches, a lot of research [11, 23, 67, 70, 101, 103] has gone into combining the strengths of different methods into more efficient and performant models via a two stage approach. VQ-VAEs [67, 101] use autoregressive models to learn an expressive prior over a discretized latent space. [66] extend this approach to text-to-image generation by learning a joint distributation over discretized image and text representations. More generally, [70] uses conditionally invertible networks to provide a generic transfer between latent spaces of diverse domains. Different from VQ-VAEs, VQGANs [23, 103] employ a first stage with an adversarial and perceptual objective to scale autoregressive transformers to larger images. However, the high compression rates required for feasible ARM training, which introduces billions of trainable parameters [23, 66], limit the overall performance of such ap-\n\nproaches and less compression comes at the price of high computational cost [23, 66]. Our work prevents such trade-offs, as our proposed *LDMs* scale more gently to higher dimensional latent spaces due to their convolutional backbone. Thus, we are free to choose the level of compression which optimally mediates between learning a powerful first stage, without leaving too much perceptual compression up to the generative diffusion model while guaranteeing high-fidelity reconstructions (see Fig. 1).\n\nWhile approaches to jointly [93] or separately [80] learn an encoding/decoding model together with a score-based prior exist, the former still require a difficult weighting between reconstruction and generative capabilities [11] and are outperformed by our approach (Sec. 4), and the latter focus on highly structured images such as human faces."
        },
        {
            "text": "## 3. Method\n\n    To lower the computational demands of training diffusion models towards high-resolution image synthesis, we observe that although diffusion models allow to ignore perceptually irrelevant details by undersampling the corresponding loss terms [30], they still require costly function evaluations in pixel space, which causes huge demands in computation time and energy resources.",
            "page": 3,
            "x": 304,
            "y": 263,
            "width": 244,
            "height": 101,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "4-method",
            "chunk_id": "fa848746-8243-475c-9583-e34bcc96eed5",
            "group_text": "## 3. Method\n\n    To lower the computational demands of training diffusion models towards high-resolution image synthesis, we observe that although diffusion models allow to ignore perceptually irrelevant details by undersampling the corresponding loss terms [30], they still require costly function evaluations in pixel space, which causes huge demands in computation time and energy resources.\n\nWe propose to circumvent this drawback by introducing\nan explicit separation of the compressive from the generative learning phase (see Fig. 2). To achieve this, we utilize\nan autoencoding model which learns a space that is perceptually equivalent to the image space, but offers significantly\nreduced computational complexity.\n\nSuch an approach offers several advantages: (i) By leav-\ning the high-dimensional image space, we obtain DMs\nwhich are computationally much more efficient because\nsampling is performed on a low-dimensional space. (ii) We\nexploit the inductive bias of DMs inherited from their UNet\narchitecture [71], which makes them particularly effective\nfor data with spatial structure and therefore alleviates the\nneed for aggressive, quality-reducing compression levels as\nrequired by previous approaches [23, 66]. (iii) Finally, we\nobtain general-purpose compression models whose latent\nspace can be used to train multiple generative models and\nwhich can also be utilized for other downstream applica-\ntions such as single-image CLIP-guided synthesis [25]."
        },
        {
            "text": "We propose to circumvent this drawback by introducing\nan explicit separation of the compressive from the generative learning phase (see Fig. 2). To achieve this, we utilize\nan autoencoding model which learns a space that is perceptually equivalent to the image space, but offers significantly\nreduced computational complexity.",
            "page": 3,
            "x": 305,
            "y": 365,
            "width": 244,
            "height": 72,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "4-method",
            "chunk_id": "9f07b744-f98d-422c-96d2-c054770702a5",
            "group_text": "## 3. Method\n\n    To lower the computational demands of training diffusion models towards high-resolution image synthesis, we observe that although diffusion models allow to ignore perceptually irrelevant details by undersampling the corresponding loss terms [30], they still require costly function evaluations in pixel space, which causes huge demands in computation time and energy resources.\n\nWe propose to circumvent this drawback by introducing\nan explicit separation of the compressive from the generative learning phase (see Fig. 2). To achieve this, we utilize\nan autoencoding model which learns a space that is perceptually equivalent to the image space, but offers significantly\nreduced computational complexity.\n\nSuch an approach offers several advantages: (i) By leav-\ning the high-dimensional image space, we obtain DMs\nwhich are computationally much more efficient because\nsampling is performed on a low-dimensional space. (ii) We\nexploit the inductive bias of DMs inherited from their UNet\narchitecture [71], which makes them particularly effective\nfor data with spatial structure and therefore alleviates the\nneed for aggressive, quality-reducing compression levels as\nrequired by previous approaches [23, 66]. (iii) Finally, we\nobtain general-purpose compression models whose latent\nspace can be used to train multiple generative models and\nwhich can also be utilized for other downstream applica-\ntions such as single-image CLIP-guided synthesis [25]."
        },
        {
            "text": "Such an approach offers several advantages: (i) By leav-\ning the high-dimensional image space, we obtain DMs\nwhich are computationally much more efficient because\nsampling is performed on a low-dimensional space. (ii) We\nexploit the inductive bias of DMs inherited from their UNet\narchitecture [71], which makes them particularly effective\nfor data with spatial structure and therefore alleviates the\nneed for aggressive, quality-reducing compression levels as\nrequired by previous approaches [23, 66]. (iii) Finally, we\nobtain general-purpose compression models whose latent\nspace can be used to train multiple generative models and\nwhich can also be utilized for other downstream applica-\ntions such as single-image CLIP-guided synthesis [25].",
            "page": 3,
            "x": 305,
            "y": 438,
            "width": 244,
            "height": 158,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "4-method",
            "chunk_id": "6adc2a0c-3361-401c-ab07-10c9e4901caf",
            "group_text": "## 3. Method\n\n    To lower the computational demands of training diffusion models towards high-resolution image synthesis, we observe that although diffusion models allow to ignore perceptually irrelevant details by undersampling the corresponding loss terms [30], they still require costly function evaluations in pixel space, which causes huge demands in computation time and energy resources.\n\nWe propose to circumvent this drawback by introducing\nan explicit separation of the compressive from the generative learning phase (see Fig. 2). To achieve this, we utilize\nan autoencoding model which learns a space that is perceptually equivalent to the image space, but offers significantly\nreduced computational complexity.\n\nSuch an approach offers several advantages: (i) By leav-\ning the high-dimensional image space, we obtain DMs\nwhich are computationally much more efficient because\nsampling is performed on a low-dimensional space. (ii) We\nexploit the inductive bias of DMs inherited from their UNet\narchitecture [71], which makes them particularly effective\nfor data with spatial structure and therefore alleviates the\nneed for aggressive, quality-reducing compression levels as\nrequired by previous approaches [23, 66]. (iii) Finally, we\nobtain general-purpose compression models whose latent\nspace can be used to train multiple generative models and\nwhich can also be utilized for other downstream applica-\ntions such as single-image CLIP-guided synthesis [25]."
        },
        {
            "text": "### 3.1. Perceptual Image Compression\n\nOur perceptual compression model is based on previous work [23] and consists of an autoencoder trained by combination of a perceptual loss [106] and a patch-based [33] adversarial objective [20, 23, 103]. This ensures that the reconstructions are confined to the image manifold by enforcing local realism and avoids blurriness introduced by relying solely on pixel-space losses such as $L_2$ or $L_1$ objectives.\n\nMore precisely, given an image $x \\in \\mathbb{R}^{H \\times W \\times 3}$ in RGB space, the encoder $\\mathcal{E}$ encodes $x$ into a latent representa-",
            "page": 3,
            "x": 305,
            "y": 602,
            "width": 244,
            "height": 124,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-perceptual",
            "chunk_id": "412f84d0-ddf7-484f-abce-3a2016459d2c",
            "group_text": "### 3.1. Perceptual Image Compression\n\nOur perceptual compression model is based on previous work [23] and consists of an autoencoder trained by combination of a perceptual loss [106] and a patch-based [33] adversarial objective [20, 23, 103]. This ensures that the reconstructions are confined to the image manifold by enforcing local realism and avoids blurriness introduced by relying solely on pixel-space losses such as $L_2$ or $L_1$ objectives.\n\nMore precisely, given an image $x \\in \\mathbb{R}^{H \\times W \\times 3}$ in RGB space, the encoder $\\mathcal{E}$ encodes $x$ into a latent representa-\n\ntion $z = \\mathcal{E}(x)$, and the decoder $\\mathcal{D}$ reconstructs the image from the latent, giving $\\tilde{x} = \\mathcal{D}(z) = \\mathcal{D}(\\mathcal{E}(x))$, where $z \\in \\mathbb{R}^{h \\times w \\times c}$. Importantly, the encoder *downsamples* the image by a factor $f = H/h = W/w$, and we investigate different downsampling factors $f = 2^m$, with $m \\in \\mathbb{N}$.\n\nIn order to avoid arbitrarily high-variance latent spaces,\nwe experiment with two different kinds of regularizations.\nThe first variant, *KL-reg.*, imposes a slight KL-penalty towards a standard normal on the learned latent, similar to a VAE [46, 69], whereas *VQ-reg.* uses a vector quantization layer [96] within the decoder. This model can be interpreted as a VQGAN [23] but with the quantization layer absorbed by the decoder. Because our subsequent DM is designed to work with the two-dimensional structure of our learned latent space $z = \\mathcal{E}(x)$, we can use relatively mild compression rates and achieve very good reconstructions. This is in contrast to previous works [23, 66], which relied on an arbitrary 1D ordering of the learned space $z$ to model its distribution autoregressively and thereby ignored much of the inherent structure of $z$. Hence, our compression model preserves details of $x$ better (see Tab. 8). The full objective and training details can be found in the supplement."
        },
        {
            "text": "tion $z = \\mathcal{E}(x)$, and the decoder $\\mathcal{D}$ reconstructs the image from the latent, giving $\\tilde{x} = \\mathcal{D}(z) = \\mathcal{D}(\\mathcal{E}(x))$, where $z \\in \\mathbb{R}^{h \\times w \\times c}$. Importantly, the encoder *downsamples* the image by a factor $f = H/h = W/w$, and we investigate different downsampling factors $f = 2^m$, with $m \\in \\mathbb{N}$.",
            "page": 4,
            "x": 46,
            "y": 71,
            "width": 244,
            "height": 62,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-perceptual",
            "chunk_id": "fbfe3cc7-2b84-4776-9afd-0f3e1a4bf1d5",
            "group_text": "### 3.1. Perceptual Image Compression\n\nOur perceptual compression model is based on previous work [23] and consists of an autoencoder trained by combination of a perceptual loss [106] and a patch-based [33] adversarial objective [20, 23, 103]. This ensures that the reconstructions are confined to the image manifold by enforcing local realism and avoids blurriness introduced by relying solely on pixel-space losses such as $L_2$ or $L_1$ objectives.\n\nMore precisely, given an image $x \\in \\mathbb{R}^{H \\times W \\times 3}$ in RGB space, the encoder $\\mathcal{E}$ encodes $x$ into a latent representa-\n\ntion $z = \\mathcal{E}(x)$, and the decoder $\\mathcal{D}$ reconstructs the image from the latent, giving $\\tilde{x} = \\mathcal{D}(z) = \\mathcal{D}(\\mathcal{E}(x))$, where $z \\in \\mathbb{R}^{h \\times w \\times c}$. Importantly, the encoder *downsamples* the image by a factor $f = H/h = W/w$, and we investigate different downsampling factors $f = 2^m$, with $m \\in \\mathbb{N}$.\n\nIn order to avoid arbitrarily high-variance latent spaces,\nwe experiment with two different kinds of regularizations.\nThe first variant, *KL-reg.*, imposes a slight KL-penalty towards a standard normal on the learned latent, similar to a VAE [46, 69], whereas *VQ-reg.* uses a vector quantization layer [96] within the decoder. This model can be interpreted as a VQGAN [23] but with the quantization layer absorbed by the decoder. Because our subsequent DM is designed to work with the two-dimensional structure of our learned latent space $z = \\mathcal{E}(x)$, we can use relatively mild compression rates and achieve very good reconstructions. This is in contrast to previous works [23, 66], which relied on an arbitrary 1D ordering of the learned space $z$ to model its distribution autoregressively and thereby ignored much of the inherent structure of $z$. Hence, our compression model preserves details of $x$ better (see Tab. 8). The full objective and training details can be found in the supplement."
        },
        {
            "text": "In order to avoid arbitrarily high-variance latent spaces,\nwe experiment with two different kinds of regularizations.\nThe first variant, *KL-reg.*, imposes a slight KL-penalty towards a standard normal on the learned latent, similar to a VAE [46, 69], whereas *VQ-reg.* uses a vector quantization layer [96] within the decoder. This model can be interpreted as a VQGAN [23] but with the quantization layer absorbed by the decoder. Because our subsequent DM is designed to work with the two-dimensional structure of our learned latent space $z = \\mathcal{E}(x)$, we can use relatively mild compression rates and achieve very good reconstructions. This is in contrast to previous works [23, 66], which relied on an arbitrary 1D ordering of the learned space $z$ to model its distribution autoregressively and thereby ignored much of the inherent structure of $z$. Hence, our compression model preserves details of $x$ better (see Tab. 8). The full objective and training details can be found in the supplement.",
            "page": 4,
            "x": 46,
            "y": 133,
            "width": 243,
            "height": 205,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-perceptual",
            "chunk_id": "cef8b774-1e78-437f-b45c-911a06202ba2",
            "group_text": "### 3.1. Perceptual Image Compression\n\nOur perceptual compression model is based on previous work [23] and consists of an autoencoder trained by combination of a perceptual loss [106] and a patch-based [33] adversarial objective [20, 23, 103]. This ensures that the reconstructions are confined to the image manifold by enforcing local realism and avoids blurriness introduced by relying solely on pixel-space losses such as $L_2$ or $L_1$ objectives.\n\nMore precisely, given an image $x \\in \\mathbb{R}^{H \\times W \\times 3}$ in RGB space, the encoder $\\mathcal{E}$ encodes $x$ into a latent representa-\n\ntion $z = \\mathcal{E}(x)$, and the decoder $\\mathcal{D}$ reconstructs the image from the latent, giving $\\tilde{x} = \\mathcal{D}(z) = \\mathcal{D}(\\mathcal{E}(x))$, where $z \\in \\mathbb{R}^{h \\times w \\times c}$. Importantly, the encoder *downsamples* the image by a factor $f = H/h = W/w$, and we investigate different downsampling factors $f = 2^m$, with $m \\in \\mathbb{N}$.\n\nIn order to avoid arbitrarily high-variance latent spaces,\nwe experiment with two different kinds of regularizations.\nThe first variant, *KL-reg.*, imposes a slight KL-penalty towards a standard normal on the learned latent, similar to a VAE [46, 69], whereas *VQ-reg.* uses a vector quantization layer [96] within the decoder. This model can be interpreted as a VQGAN [23] but with the quantization layer absorbed by the decoder. Because our subsequent DM is designed to work with the two-dimensional structure of our learned latent space $z = \\mathcal{E}(x)$, we can use relatively mild compression rates and achieve very good reconstructions. This is in contrast to previous works [23, 66], which relied on an arbitrary 1D ordering of the learned space $z$ to model its distribution autoregressively and thereby ignored much of the inherent structure of $z$. Hence, our compression model preserves details of $x$ better (see Tab. 8). The full objective and training details can be found in the supplement."
        },
        {
            "text": "### 3.2. Latent Diffusion Models\n\n**Diffusion Models** [82] are probabilistic models designed to learn a data distribution $p(x)$ by gradually denoising a normally distributed variable, which corresponds to learning the reverse process of a fixed Markov Chain of length $T$. For image synthesis, the most successful models [15,30,72] rely on a reweighted variant of the variational lower bound on $p(x)$, which mirrors denoising score-matching [85]. These models can be interpreted as an equally weighted sequence of denoising autoencoders $\\epsilon_\\theta(x_t, t); t = 1 \\ldots T$, which are trained to predict a denoised variant of their input $x_t$, where $x_t$ is a noisy version of the input $x$. The corresponding objective can be simplified to (Sec. B)",
            "page": 4,
            "x": 46,
            "y": 342,
            "width": 243,
            "height": 166,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "6-latent",
            "chunk_id": "f8a4c287-c544-414b-b4e6-de06c4279f23",
            "group_text": "### 3.2. Latent Diffusion Models\n\n**Diffusion Models** [82] are probabilistic models designed to learn a data distribution $p(x)$ by gradually denoising a normally distributed variable, which corresponds to learning the reverse process of a fixed Markov Chain of length $T$. For image synthesis, the most successful models [15,30,72] rely on a reweighted variant of the variational lower bound on $p(x)$, which mirrors denoising score-matching [85]. These models can be interpreted as an equally weighted sequence of denoising autoencoders $\\epsilon_\\theta(x_t, t); t = 1 \\ldots T$, which are trained to predict a denoised variant of their input $x_t$, where $x_t$ is a noisy version of the input $x$. The corresponding objective can be simplified to (Sec. B)\n\n$L_{DM} = \\mathbb{E}_{x, \\epsilon \\sim \\mathcal{N}(0,1), t} \\left[ \\left\\| \\epsilon - \\epsilon_\\theta(x_t, t) \\right\\|_2^2 \\right], \\qquad (1)$\n\nwith $t$ uniformly sampled from $\\{1, \\ldots, T\\}$.\n**Generative Modeling of Latent Representations** With our trained perceptual compression models consisting of $\\mathcal{E}$ and $\\mathcal{D}$, we now have access to an efficient, low-dimensional latent space in which high-frequency, imperceptible details are abstracted away. Compared to the high-dimensional pixel space, this space is more suitable for likelihood-based generative models, as they can now (i) focus on the important, semantic bits of the data and (ii) train in a lower dimensional, computationally much more efficient space.\n\nUnlike previous work that relied on autoregressive,\nattention-based transformer models in a highly compressed,\ndiscrete latent space [23, 66, 103], we can take advantage of\nimage-specific inductive biases that our model offers. This\n\nincludes the ability to build the underlying UNet primarily from 2D convolutional layers, and further focusing the objective on the perceptually most relevant bits using the reweighted bound, which now reads\n\n$L_{LDM} := \\mathbb{E}_{x(x), \\epsilon \\sim \\mathcal{N}(0,1), t} \\left[ \\left\\| \\epsilon - \\epsilon_\\theta(z_t, t) \\right\\|_2^2 \\right]. \\qquad (2)$\n\nThe neural backbone $\\epsilon_\\theta(o, t)$ of our model is realized as a time-conditional UNet [71]. Since the forward process is fixed, $z_t$ can be efficiently obtained from $\\mathcal{E}$ during training, and samples from $p(z)$ can be decoded to image space with a single pass through $\\mathcal{D}$."
        },
        {
            "text": "$L_{DM} = \\mathbb{E}_{x, \\epsilon \\sim \\mathcal{N}(0,1), t} \\left[ \\left\\| \\epsilon - \\epsilon_\\theta(x_t, t) \\right\\|_2^2 \\right], \\qquad (1)$",
            "page": 4,
            "x": 81,
            "y": 514,
            "width": 209,
            "height": 25,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "6-latent",
            "chunk_id": "33f8ba9c-4735-4e4e-8ca6-f1887ed5afaa",
            "group_text": "### 3.2. Latent Diffusion Models\n\n**Diffusion Models** [82] are probabilistic models designed to learn a data distribution $p(x)$ by gradually denoising a normally distributed variable, which corresponds to learning the reverse process of a fixed Markov Chain of length $T$. For image synthesis, the most successful models [15,30,72] rely on a reweighted variant of the variational lower bound on $p(x)$, which mirrors denoising score-matching [85]. These models can be interpreted as an equally weighted sequence of denoising autoencoders $\\epsilon_\\theta(x_t, t); t = 1 \\ldots T$, which are trained to predict a denoised variant of their input $x_t$, where $x_t$ is a noisy version of the input $x$. The corresponding objective can be simplified to (Sec. B)\n\n$L_{DM} = \\mathbb{E}_{x, \\epsilon \\sim \\mathcal{N}(0,1), t} \\left[ \\left\\| \\epsilon - \\epsilon_\\theta(x_t, t) \\right\\|_2^2 \\right], \\qquad (1)$\n\nwith $t$ uniformly sampled from $\\{1, \\ldots, T\\}$.\n**Generative Modeling of Latent Representations** With our trained perceptual compression models consisting of $\\mathcal{E}$ and $\\mathcal{D}$, we now have access to an efficient, low-dimensional latent space in which high-frequency, imperceptible details are abstracted away. Compared to the high-dimensional pixel space, this space is more suitable for likelihood-based generative models, as they can now (i) focus on the important, semantic bits of the data and (ii) train in a lower dimensional, computationally much more efficient space.\n\nUnlike previous work that relied on autoregressive,\nattention-based transformer models in a highly compressed,\ndiscrete latent space [23, 66, 103], we can take advantage of\nimage-specific inductive biases that our model offers. This\n\nincludes the ability to build the underlying UNet primarily from 2D convolutional layers, and further focusing the objective on the perceptually most relevant bits using the reweighted bound, which now reads\n\n$L_{LDM} := \\mathbb{E}_{x(x), \\epsilon \\sim \\mathcal{N}(0,1), t} \\left[ \\left\\| \\epsilon - \\epsilon_\\theta(z_t, t) \\right\\|_2^2 \\right]. \\qquad (2)$\n\nThe neural backbone $\\epsilon_\\theta(o, t)$ of our model is realized as a time-conditional UNet [71]. Since the forward process is fixed, $z_t$ can be efficiently obtained from $\\mathcal{E}$ during training, and samples from $p(z)$ can be decoded to image space with a single pass through $\\mathcal{D}$."
        },
        {
            "text": "with $t$ uniformly sampled from $\\{1, \\ldots, T\\}$.\n**Generative Modeling of Latent Representations** With our trained perceptual compression models consisting of $\\mathcal{E}$ and $\\mathcal{D}$, we now have access to an efficient, low-dimensional latent space in which high-frequency, imperceptible details are abstracted away. Compared to the high-dimensional pixel space, this space is more suitable for likelihood-based generative models, as they can now (i) focus on the important, semantic bits of the data and (ii) train in a lower dimensional, computationally much more efficient space.",
            "page": 4,
            "x": 46,
            "y": 545,
            "width": 242,
            "height": 120,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "6-latent",
            "chunk_id": "1bdbb44a-5cd0-45e8-ad6e-9b4411bb6d6f",
            "group_text": "### 3.2. Latent Diffusion Models\n\n**Diffusion Models** [82] are probabilistic models designed to learn a data distribution $p(x)$ by gradually denoising a normally distributed variable, which corresponds to learning the reverse process of a fixed Markov Chain of length $T$. For image synthesis, the most successful models [15,30,72] rely on a reweighted variant of the variational lower bound on $p(x)$, which mirrors denoising score-matching [85]. These models can be interpreted as an equally weighted sequence of denoising autoencoders $\\epsilon_\\theta(x_t, t); t = 1 \\ldots T$, which are trained to predict a denoised variant of their input $x_t$, where $x_t$ is a noisy version of the input $x$. The corresponding objective can be simplified to (Sec. B)\n\n$L_{DM} = \\mathbb{E}_{x, \\epsilon \\sim \\mathcal{N}(0,1), t} \\left[ \\left\\| \\epsilon - \\epsilon_\\theta(x_t, t) \\right\\|_2^2 \\right], \\qquad (1)$\n\nwith $t$ uniformly sampled from $\\{1, \\ldots, T\\}$.\n**Generative Modeling of Latent Representations** With our trained perceptual compression models consisting of $\\mathcal{E}$ and $\\mathcal{D}$, we now have access to an efficient, low-dimensional latent space in which high-frequency, imperceptible details are abstracted away. Compared to the high-dimensional pixel space, this space is more suitable for likelihood-based generative models, as they can now (i) focus on the important, semantic bits of the data and (ii) train in a lower dimensional, computationally much more efficient space.\n\nUnlike previous work that relied on autoregressive,\nattention-based transformer models in a highly compressed,\ndiscrete latent space [23, 66, 103], we can take advantage of\nimage-specific inductive biases that our model offers. This\n\nincludes the ability to build the underlying UNet primarily from 2D convolutional layers, and further focusing the objective on the perceptually most relevant bits using the reweighted bound, which now reads\n\n$L_{LDM} := \\mathbb{E}_{x(x), \\epsilon \\sim \\mathcal{N}(0,1), t} \\left[ \\left\\| \\epsilon - \\epsilon_\\theta(z_t, t) \\right\\|_2^2 \\right]. \\qquad (2)$\n\nThe neural backbone $\\epsilon_\\theta(o, t)$ of our model is realized as a time-conditional UNet [71]. Since the forward process is fixed, $z_t$ can be efficiently obtained from $\\mathcal{E}$ during training, and samples from $p(z)$ can be decoded to image space with a single pass through $\\mathcal{D}$."
        },
        {
            "text": "Unlike previous work that relied on autoregressive,\nattention-based transformer models in a highly compressed,\ndiscrete latent space [23, 66, 103], we can take advantage of\nimage-specific inductive biases that our model offers. This",
            "page": 4,
            "x": 46,
            "y": 666,
            "width": 243,
            "height": 50,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "6-latent",
            "chunk_id": "4c4c587c-bf6a-4f0c-b326-ea0f6306de23",
            "group_text": "### 3.2. Latent Diffusion Models\n\n**Diffusion Models** [82] are probabilistic models designed to learn a data distribution $p(x)$ by gradually denoising a normally distributed variable, which corresponds to learning the reverse process of a fixed Markov Chain of length $T$. For image synthesis, the most successful models [15,30,72] rely on a reweighted variant of the variational lower bound on $p(x)$, which mirrors denoising score-matching [85]. These models can be interpreted as an equally weighted sequence of denoising autoencoders $\\epsilon_\\theta(x_t, t); t = 1 \\ldots T$, which are trained to predict a denoised variant of their input $x_t$, where $x_t$ is a noisy version of the input $x$. The corresponding objective can be simplified to (Sec. B)\n\n$L_{DM} = \\mathbb{E}_{x, \\epsilon \\sim \\mathcal{N}(0,1), t} \\left[ \\left\\| \\epsilon - \\epsilon_\\theta(x_t, t) \\right\\|_2^2 \\right], \\qquad (1)$\n\nwith $t$ uniformly sampled from $\\{1, \\ldots, T\\}$.\n**Generative Modeling of Latent Representations** With our trained perceptual compression models consisting of $\\mathcal{E}$ and $\\mathcal{D}$, we now have access to an efficient, low-dimensional latent space in which high-frequency, imperceptible details are abstracted away. Compared to the high-dimensional pixel space, this space is more suitable for likelihood-based generative models, as they can now (i) focus on the important, semantic bits of the data and (ii) train in a lower dimensional, computationally much more efficient space.\n\nUnlike previous work that relied on autoregressive,\nattention-based transformer models in a highly compressed,\ndiscrete latent space [23, 66, 103], we can take advantage of\nimage-specific inductive biases that our model offers. This\n\nincludes the ability to build the underlying UNet primarily from 2D convolutional layers, and further focusing the objective on the perceptually most relevant bits using the reweighted bound, which now reads\n\n$L_{LDM} := \\mathbb{E}_{x(x), \\epsilon \\sim \\mathcal{N}(0,1), t} \\left[ \\left\\| \\epsilon - \\epsilon_\\theta(z_t, t) \\right\\|_2^2 \\right]. \\qquad (2)$\n\nThe neural backbone $\\epsilon_\\theta(o, t)$ of our model is realized as a time-conditional UNet [71]. Since the forward process is fixed, $z_t$ can be efficiently obtained from $\\mathcal{E}$ during training, and samples from $p(z)$ can be decoded to image space with a single pass through $\\mathcal{D}$."
        },
        {
            "text": "includes the ability to build the underlying UNet primarily from 2D convolutional layers, and further focusing the objective on the perceptually most relevant bits using the reweighted bound, which now reads",
            "page": 4,
            "x": 306,
            "y": 225,
            "width": 242,
            "height": 49,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "6-latent",
            "chunk_id": "83b31fe6-a4a1-4114-acc6-f9762b3a0a40",
            "group_text": "### 3.2. Latent Diffusion Models\n\n**Diffusion Models** [82] are probabilistic models designed to learn a data distribution $p(x)$ by gradually denoising a normally distributed variable, which corresponds to learning the reverse process of a fixed Markov Chain of length $T$. For image synthesis, the most successful models [15,30,72] rely on a reweighted variant of the variational lower bound on $p(x)$, which mirrors denoising score-matching [85]. These models can be interpreted as an equally weighted sequence of denoising autoencoders $\\epsilon_\\theta(x_t, t); t = 1 \\ldots T$, which are trained to predict a denoised variant of their input $x_t$, where $x_t$ is a noisy version of the input $x$. The corresponding objective can be simplified to (Sec. B)\n\n$L_{DM} = \\mathbb{E}_{x, \\epsilon \\sim \\mathcal{N}(0,1), t} \\left[ \\left\\| \\epsilon - \\epsilon_\\theta(x_t, t) \\right\\|_2^2 \\right], \\qquad (1)$\n\nwith $t$ uniformly sampled from $\\{1, \\ldots, T\\}$.\n**Generative Modeling of Latent Representations** With our trained perceptual compression models consisting of $\\mathcal{E}$ and $\\mathcal{D}$, we now have access to an efficient, low-dimensional latent space in which high-frequency, imperceptible details are abstracted away. Compared to the high-dimensional pixel space, this space is more suitable for likelihood-based generative models, as they can now (i) focus on the important, semantic bits of the data and (ii) train in a lower dimensional, computationally much more efficient space.\n\nUnlike previous work that relied on autoregressive,\nattention-based transformer models in a highly compressed,\ndiscrete latent space [23, 66, 103], we can take advantage of\nimage-specific inductive biases that our model offers. This\n\nincludes the ability to build the underlying UNet primarily from 2D convolutional layers, and further focusing the objective on the perceptually most relevant bits using the reweighted bound, which now reads\n\n$L_{LDM} := \\mathbb{E}_{x(x), \\epsilon \\sim \\mathcal{N}(0,1), t} \\left[ \\left\\| \\epsilon - \\epsilon_\\theta(z_t, t) \\right\\|_2^2 \\right]. \\qquad (2)$\n\nThe neural backbone $\\epsilon_\\theta(o, t)$ of our model is realized as a time-conditional UNet [71]. Since the forward process is fixed, $z_t$ can be efficiently obtained from $\\mathcal{E}$ during training, and samples from $p(z)$ can be decoded to image space with a single pass through $\\mathcal{D}$."
        },
        {
            "text": "$L_{LDM} := \\mathbb{E}_{x(x), \\epsilon \\sim \\mathcal{N}(0,1), t} \\left[ \\left\\| \\epsilon - \\epsilon_\\theta(z_t, t) \\right\\|_2^2 \\right]. \\qquad (2)$",
            "page": 4,
            "x": 331,
            "y": 280,
            "width": 216,
            "height": 25,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "6-latent",
            "chunk_id": "49133434-6d99-4926-9d90-8cee31bf9933",
            "group_text": "### 3.2. Latent Diffusion Models\n\n**Diffusion Models** [82] are probabilistic models designed to learn a data distribution $p(x)$ by gradually denoising a normally distributed variable, which corresponds to learning the reverse process of a fixed Markov Chain of length $T$. For image synthesis, the most successful models [15,30,72] rely on a reweighted variant of the variational lower bound on $p(x)$, which mirrors denoising score-matching [85]. These models can be interpreted as an equally weighted sequence of denoising autoencoders $\\epsilon_\\theta(x_t, t); t = 1 \\ldots T$, which are trained to predict a denoised variant of their input $x_t$, where $x_t$ is a noisy version of the input $x$. The corresponding objective can be simplified to (Sec. B)\n\n$L_{DM} = \\mathbb{E}_{x, \\epsilon \\sim \\mathcal{N}(0,1), t} \\left[ \\left\\| \\epsilon - \\epsilon_\\theta(x_t, t) \\right\\|_2^2 \\right], \\qquad (1)$\n\nwith $t$ uniformly sampled from $\\{1, \\ldots, T\\}$.\n**Generative Modeling of Latent Representations** With our trained perceptual compression models consisting of $\\mathcal{E}$ and $\\mathcal{D}$, we now have access to an efficient, low-dimensional latent space in which high-frequency, imperceptible details are abstracted away. Compared to the high-dimensional pixel space, this space is more suitable for likelihood-based generative models, as they can now (i) focus on the important, semantic bits of the data and (ii) train in a lower dimensional, computationally much more efficient space.\n\nUnlike previous work that relied on autoregressive,\nattention-based transformer models in a highly compressed,\ndiscrete latent space [23, 66, 103], we can take advantage of\nimage-specific inductive biases that our model offers. This\n\nincludes the ability to build the underlying UNet primarily from 2D convolutional layers, and further focusing the objective on the perceptually most relevant bits using the reweighted bound, which now reads\n\n$L_{LDM} := \\mathbb{E}_{x(x), \\epsilon \\sim \\mathcal{N}(0,1), t} \\left[ \\left\\| \\epsilon - \\epsilon_\\theta(z_t, t) \\right\\|_2^2 \\right]. \\qquad (2)$\n\nThe neural backbone $\\epsilon_\\theta(o, t)$ of our model is realized as a time-conditional UNet [71]. Since the forward process is fixed, $z_t$ can be efficiently obtained from $\\mathcal{E}$ during training, and samples from $p(z)$ can be decoded to image space with a single pass through $\\mathcal{D}$."
        },
        {
            "text": "The neural backbone $\\epsilon_\\theta(o, t)$ of our model is realized as a time-conditional UNet [71]. Since the forward process is fixed, $z_t$ can be efficiently obtained from $\\mathcal{E}$ during training, and samples from $p(z)$ can be decoded to image space with a single pass through $\\mathcal{D}$.",
            "page": 4,
            "x": 305,
            "y": 311,
            "width": 243,
            "height": 61,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "6-latent",
            "chunk_id": "d9824c5e-bf40-4317-8063-4909da7d5b6d",
            "group_text": "### 3.2. Latent Diffusion Models\n\n**Diffusion Models** [82] are probabilistic models designed to learn a data distribution $p(x)$ by gradually denoising a normally distributed variable, which corresponds to learning the reverse process of a fixed Markov Chain of length $T$. For image synthesis, the most successful models [15,30,72] rely on a reweighted variant of the variational lower bound on $p(x)$, which mirrors denoising score-matching [85]. These models can be interpreted as an equally weighted sequence of denoising autoencoders $\\epsilon_\\theta(x_t, t); t = 1 \\ldots T$, which are trained to predict a denoised variant of their input $x_t$, where $x_t$ is a noisy version of the input $x$. The corresponding objective can be simplified to (Sec. B)\n\n$L_{DM} = \\mathbb{E}_{x, \\epsilon \\sim \\mathcal{N}(0,1), t} \\left[ \\left\\| \\epsilon - \\epsilon_\\theta(x_t, t) \\right\\|_2^2 \\right], \\qquad (1)$\n\nwith $t$ uniformly sampled from $\\{1, \\ldots, T\\}$.\n**Generative Modeling of Latent Representations** With our trained perceptual compression models consisting of $\\mathcal{E}$ and $\\mathcal{D}$, we now have access to an efficient, low-dimensional latent space in which high-frequency, imperceptible details are abstracted away. Compared to the high-dimensional pixel space, this space is more suitable for likelihood-based generative models, as they can now (i) focus on the important, semantic bits of the data and (ii) train in a lower dimensional, computationally much more efficient space.\n\nUnlike previous work that relied on autoregressive,\nattention-based transformer models in a highly compressed,\ndiscrete latent space [23, 66, 103], we can take advantage of\nimage-specific inductive biases that our model offers. This\n\nincludes the ability to build the underlying UNet primarily from 2D convolutional layers, and further focusing the objective on the perceptually most relevant bits using the reweighted bound, which now reads\n\n$L_{LDM} := \\mathbb{E}_{x(x), \\epsilon \\sim \\mathcal{N}(0,1), t} \\left[ \\left\\| \\epsilon - \\epsilon_\\theta(z_t, t) \\right\\|_2^2 \\right]. \\qquad (2)$\n\nThe neural backbone $\\epsilon_\\theta(o, t)$ of our model is realized as a time-conditional UNet [71]. Since the forward process is fixed, $z_t$ can be efficiently obtained from $\\mathcal{E}$ during training, and samples from $p(z)$ can be decoded to image space with a single pass through $\\mathcal{D}$."
        },
        {
            "text": "## 3.3. Conditioning Mechanisms\nSimilar to other types of generative models [56, 83], diffusion models are in principle capable of modeling conditional distributions of the form $p(z|y)$. This can be implemented with a conditional denoising autoencoder $\\epsilon_\\theta(z_t, t, y)$ and paves the way to controlling the synthesis process through inputs $y$ such as text [68], semantic maps [33, 61] or other image-to-image translation tasks [34].",
            "page": 4,
            "x": 305,
            "y": 379,
            "width": 242,
            "height": 98,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-conditioning",
            "chunk_id": "fc3a7735-f5bc-4f08-a898-9772cb2ddd7f",
            "group_text": "## 3.3. Conditioning Mechanisms\nSimilar to other types of generative models [56, 83], diffusion models are in principle capable of modeling conditional distributions of the form $p(z|y)$. This can be implemented with a conditional denoising autoencoder $\\epsilon_\\theta(z_t, t, y)$ and paves the way to controlling the synthesis process through inputs $y$ such as text [68], semantic maps [33, 61] or other image-to-image translation tasks [34].\n\nIn the context of image synthesis, however, combining\nthe generative power of DMs with other types of condition-\nings beyond class-labels [15] or blurred variants of the input\nimage [72] is so far an under-explored area of research.\n\nWe turn DMs into more flexible conditional image generators by augmenting their underlying UNet backbone with the cross-attention mechanism [97], which is effective for learning attention-based models of various input modalities [35,36]. To pre-process $y$ from various modalities (such as language prompts) we introduce a domain specific encoder $\\tau_\\theta$ that projects $y$ to an intermediate representation $\\tau_\\theta(y) \\in \\mathbb{R}^{M \\times d_r}$, which is then mapped to the intermediate layers of the UNet via a cross-attention layer implementing\n\n$Attention(Q, K, V) = \\mathrm{softmax} \\left( \\frac{QK^T}{\\sqrt{d}} \\right) \\cdot V, \\text{ with}$\n\n$Q = W_Q^{(i)} \\cdot \\varphi_i(z_t), \\;\\; K = W_K^{(i)} \\cdot \\tau_\\theta(y), \\;\\; V = W_V^{(i)} \\cdot \\tau_\\theta(y).$\n\nHere, $\\varphi_i(z_t) \\in \\mathbb{R}^{N \\times d_c^l}$ denotes a (flattened) intermediate\n\nrepresentation of the UNet implementing $\\epsilon_\\theta$ and $W_V^{(i)} \\in$\n\nFigure 4.   Samples from *LDMs* trained on CelebAHQ [39], FFHQ [41], LSUN-Churches [102], LSUN-Bedrooms [102] and class-conditional ImageNet [12], each with a resolution of 256 \u00d7 256. Best viewed when zoomed in. For more samples *cf.* the supplement.\n\n$\\mathbb{R}^{d \\times d_t^i}$, $W_Q^{(i)} \\in \\mathbb{R}^{d \\times d_{\\tau}}$ & $W_K^{(i)} \\in \\mathbb{R}^{d \\times d_{\\tau}}$ are learnable projection matrices [36, 97]. See Fig. 3 for a visual depiction.\n    Based on image-conditioning pairs, we then learn the conditional LDM via\n\n$L_{LDM} := \\mathbb{E}_{\\mathcal{E}(x),y,\\epsilon \\sim \\mathcal{N}(0,1),t} \\left[ \\left\\| \\epsilon - \\epsilon_\\theta(z_t, t, \\tau_\\theta(y)) \\right\\|_2^2 \\right], \\quad (3)$\n\nwhere both $\\tau_\\theta$ and $\\epsilon_\\theta$ are jointly optimized via Eq. 3. This conditioning mechanism is flexible as $\\tau_\\theta$ can be parameterized with domain-specific experts, _e.g._ (unmasked) transformers [97] when $y$ are text prompts (see Sec. 4.3.1)"
        },
        {
            "text": "In the context of image synthesis, however, combining\nthe generative power of DMs with other types of condition-\nings beyond class-labels [15] or blurred variants of the input\nimage [72] is so far an under-explored area of research.",
            "page": 4,
            "x": 306,
            "y": 477,
            "width": 242,
            "height": 47,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-conditioning",
            "chunk_id": "4f76f1ee-6c03-4a4e-8254-c9d84d172269",
            "group_text": "## 3.3. Conditioning Mechanisms\nSimilar to other types of generative models [56, 83], diffusion models are in principle capable of modeling conditional distributions of the form $p(z|y)$. This can be implemented with a conditional denoising autoencoder $\\epsilon_\\theta(z_t, t, y)$ and paves the way to controlling the synthesis process through inputs $y$ such as text [68], semantic maps [33, 61] or other image-to-image translation tasks [34].\n\nIn the context of image synthesis, however, combining\nthe generative power of DMs with other types of condition-\nings beyond class-labels [15] or blurred variants of the input\nimage [72] is so far an under-explored area of research.\n\nWe turn DMs into more flexible conditional image generators by augmenting their underlying UNet backbone with the cross-attention mechanism [97], which is effective for learning attention-based models of various input modalities [35,36]. To pre-process $y$ from various modalities (such as language prompts) we introduce a domain specific encoder $\\tau_\\theta$ that projects $y$ to an intermediate representation $\\tau_\\theta(y) \\in \\mathbb{R}^{M \\times d_r}$, which is then mapped to the intermediate layers of the UNet via a cross-attention layer implementing\n\n$Attention(Q, K, V) = \\mathrm{softmax} \\left( \\frac{QK^T}{\\sqrt{d}} \\right) \\cdot V, \\text{ with}$\n\n$Q = W_Q^{(i)} \\cdot \\varphi_i(z_t), \\;\\; K = W_K^{(i)} \\cdot \\tau_\\theta(y), \\;\\; V = W_V^{(i)} \\cdot \\tau_\\theta(y).$\n\nHere, $\\varphi_i(z_t) \\in \\mathbb{R}^{N \\times d_c^l}$ denotes a (flattened) intermediate\n\nrepresentation of the UNet implementing $\\epsilon_\\theta$ and $W_V^{(i)} \\in$\n\nFigure 4.   Samples from *LDMs* trained on CelebAHQ [39], FFHQ [41], LSUN-Churches [102], LSUN-Bedrooms [102] and class-conditional ImageNet [12], each with a resolution of 256 \u00d7 256. Best viewed when zoomed in. For more samples *cf.* the supplement.\n\n$\\mathbb{R}^{d \\times d_t^i}$, $W_Q^{(i)} \\in \\mathbb{R}^{d \\times d_{\\tau}}$ & $W_K^{(i)} \\in \\mathbb{R}^{d \\times d_{\\tau}}$ are learnable projection matrices [36, 97]. See Fig. 3 for a visual depiction.\n    Based on image-conditioning pairs, we then learn the conditional LDM via\n\n$L_{LDM} := \\mathbb{E}_{\\mathcal{E}(x),y,\\epsilon \\sim \\mathcal{N}(0,1),t} \\left[ \\left\\| \\epsilon - \\epsilon_\\theta(z_t, t, \\tau_\\theta(y)) \\right\\|_2^2 \\right], \\quad (3)$\n\nwhere both $\\tau_\\theta$ and $\\epsilon_\\theta$ are jointly optimized via Eq. 3. This conditioning mechanism is flexible as $\\tau_\\theta$ can be parameterized with domain-specific experts, _e.g._ (unmasked) transformers [97] when $y$ are text prompts (see Sec. 4.3.1)"
        },
        {
            "text": "We turn DMs into more flexible conditional image generators by augmenting their underlying UNet backbone with the cross-attention mechanism [97], which is effective for learning attention-based models of various input modalities [35,36]. To pre-process $y$ from various modalities (such as language prompts) we introduce a domain specific encoder $\\tau_\\theta$ that projects $y$ to an intermediate representation $\\tau_\\theta(y) \\in \\mathbb{R}^{M \\times d_r}$, which is then mapped to the intermediate layers of the UNet via a cross-attention layer implementing",
            "page": 4,
            "x": 305,
            "y": 525,
            "width": 242,
            "height": 108,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-conditioning",
            "chunk_id": "9409c887-7234-45ef-9632-735a75d1035b",
            "group_text": "## 3.3. Conditioning Mechanisms\nSimilar to other types of generative models [56, 83], diffusion models are in principle capable of modeling conditional distributions of the form $p(z|y)$. This can be implemented with a conditional denoising autoencoder $\\epsilon_\\theta(z_t, t, y)$ and paves the way to controlling the synthesis process through inputs $y$ such as text [68], semantic maps [33, 61] or other image-to-image translation tasks [34].\n\nIn the context of image synthesis, however, combining\nthe generative power of DMs with other types of condition-\nings beyond class-labels [15] or blurred variants of the input\nimage [72] is so far an under-explored area of research.\n\nWe turn DMs into more flexible conditional image generators by augmenting their underlying UNet backbone with the cross-attention mechanism [97], which is effective for learning attention-based models of various input modalities [35,36]. To pre-process $y$ from various modalities (such as language prompts) we introduce a domain specific encoder $\\tau_\\theta$ that projects $y$ to an intermediate representation $\\tau_\\theta(y) \\in \\mathbb{R}^{M \\times d_r}$, which is then mapped to the intermediate layers of the UNet via a cross-attention layer implementing\n\n$Attention(Q, K, V) = \\mathrm{softmax} \\left( \\frac{QK^T}{\\sqrt{d}} \\right) \\cdot V, \\text{ with}$\n\n$Q = W_Q^{(i)} \\cdot \\varphi_i(z_t), \\;\\; K = W_K^{(i)} \\cdot \\tau_\\theta(y), \\;\\; V = W_V^{(i)} \\cdot \\tau_\\theta(y).$\n\nHere, $\\varphi_i(z_t) \\in \\mathbb{R}^{N \\times d_c^l}$ denotes a (flattened) intermediate\n\nrepresentation of the UNet implementing $\\epsilon_\\theta$ and $W_V^{(i)} \\in$\n\nFigure 4.   Samples from *LDMs* trained on CelebAHQ [39], FFHQ [41], LSUN-Churches [102], LSUN-Bedrooms [102] and class-conditional ImageNet [12], each with a resolution of 256 \u00d7 256. Best viewed when zoomed in. For more samples *cf.* the supplement.\n\n$\\mathbb{R}^{d \\times d_t^i}$, $W_Q^{(i)} \\in \\mathbb{R}^{d \\times d_{\\tau}}$ & $W_K^{(i)} \\in \\mathbb{R}^{d \\times d_{\\tau}}$ are learnable projection matrices [36, 97]. See Fig. 3 for a visual depiction.\n    Based on image-conditioning pairs, we then learn the conditional LDM via\n\n$L_{LDM} := \\mathbb{E}_{\\mathcal{E}(x),y,\\epsilon \\sim \\mathcal{N}(0,1),t} \\left[ \\left\\| \\epsilon - \\epsilon_\\theta(z_t, t, \\tau_\\theta(y)) \\right\\|_2^2 \\right], \\quad (3)$\n\nwhere both $\\tau_\\theta$ and $\\epsilon_\\theta$ are jointly optimized via Eq. 3. This conditioning mechanism is flexible as $\\tau_\\theta$ can be parameterized with domain-specific experts, _e.g._ (unmasked) transformers [97] when $y$ are text prompts (see Sec. 4.3.1)"
        },
        {
            "text": "$Attention(Q, K, V) = \\mathrm{softmax} \\left( \\frac{QK^T}{\\sqrt{d}} \\right) \\cdot V, \\text{ with}$",
            "page": 4,
            "x": 306,
            "y": 633,
            "width": 204,
            "height": 21,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-conditioning",
            "chunk_id": "e2593710-dae7-4ade-85ac-1657b0df9b15",
            "group_text": "## 3.3. Conditioning Mechanisms\nSimilar to other types of generative models [56, 83], diffusion models are in principle capable of modeling conditional distributions of the form $p(z|y)$. This can be implemented with a conditional denoising autoencoder $\\epsilon_\\theta(z_t, t, y)$ and paves the way to controlling the synthesis process through inputs $y$ such as text [68], semantic maps [33, 61] or other image-to-image translation tasks [34].\n\nIn the context of image synthesis, however, combining\nthe generative power of DMs with other types of condition-\nings beyond class-labels [15] or blurred variants of the input\nimage [72] is so far an under-explored area of research.\n\nWe turn DMs into more flexible conditional image generators by augmenting their underlying UNet backbone with the cross-attention mechanism [97], which is effective for learning attention-based models of various input modalities [35,36]. To pre-process $y$ from various modalities (such as language prompts) we introduce a domain specific encoder $\\tau_\\theta$ that projects $y$ to an intermediate representation $\\tau_\\theta(y) \\in \\mathbb{R}^{M \\times d_r}$, which is then mapped to the intermediate layers of the UNet via a cross-attention layer implementing\n\n$Attention(Q, K, V) = \\mathrm{softmax} \\left( \\frac{QK^T}{\\sqrt{d}} \\right) \\cdot V, \\text{ with}$\n\n$Q = W_Q^{(i)} \\cdot \\varphi_i(z_t), \\;\\; K = W_K^{(i)} \\cdot \\tau_\\theta(y), \\;\\; V = W_V^{(i)} \\cdot \\tau_\\theta(y).$\n\nHere, $\\varphi_i(z_t) \\in \\mathbb{R}^{N \\times d_c^l}$ denotes a (flattened) intermediate\n\nrepresentation of the UNet implementing $\\epsilon_\\theta$ and $W_V^{(i)} \\in$\n\nFigure 4.   Samples from *LDMs* trained on CelebAHQ [39], FFHQ [41], LSUN-Churches [102], LSUN-Bedrooms [102] and class-conditional ImageNet [12], each with a resolution of 256 \u00d7 256. Best viewed when zoomed in. For more samples *cf.* the supplement.\n\n$\\mathbb{R}^{d \\times d_t^i}$, $W_Q^{(i)} \\in \\mathbb{R}^{d \\times d_{\\tau}}$ & $W_K^{(i)} \\in \\mathbb{R}^{d \\times d_{\\tau}}$ are learnable projection matrices [36, 97]. See Fig. 3 for a visual depiction.\n    Based on image-conditioning pairs, we then learn the conditional LDM via\n\n$L_{LDM} := \\mathbb{E}_{\\mathcal{E}(x),y,\\epsilon \\sim \\mathcal{N}(0,1),t} \\left[ \\left\\| \\epsilon - \\epsilon_\\theta(z_t, t, \\tau_\\theta(y)) \\right\\|_2^2 \\right], \\quad (3)$\n\nwhere both $\\tau_\\theta$ and $\\epsilon_\\theta$ are jointly optimized via Eq. 3. This conditioning mechanism is flexible as $\\tau_\\theta$ can be parameterized with domain-specific experts, _e.g._ (unmasked) transformers [97] when $y$ are text prompts (see Sec. 4.3.1)"
        },
        {
            "text": "$Q = W_Q^{(i)} \\cdot \\varphi_i(z_t), \\;\\; K = W_K^{(i)} \\cdot \\tau_\\theta(y), \\;\\; V = W_V^{(i)} \\cdot \\tau_\\theta(y).$\n\nHere, $\\varphi_i(z_t) \\in \\mathbb{R}^{N \\times d_c^l}$ denotes a (flattened) intermediate",
            "page": 4,
            "x": 306,
            "y": 658,
            "width": 240,
            "height": 43,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-conditioning",
            "chunk_id": "dbed580f-6202-4519-b9c7-7acc8af4f5a7",
            "group_text": "## 3.3. Conditioning Mechanisms\nSimilar to other types of generative models [56, 83], diffusion models are in principle capable of modeling conditional distributions of the form $p(z|y)$. This can be implemented with a conditional denoising autoencoder $\\epsilon_\\theta(z_t, t, y)$ and paves the way to controlling the synthesis process through inputs $y$ such as text [68], semantic maps [33, 61] or other image-to-image translation tasks [34].\n\nIn the context of image synthesis, however, combining\nthe generative power of DMs with other types of condition-\nings beyond class-labels [15] or blurred variants of the input\nimage [72] is so far an under-explored area of research.\n\nWe turn DMs into more flexible conditional image generators by augmenting their underlying UNet backbone with the cross-attention mechanism [97], which is effective for learning attention-based models of various input modalities [35,36]. To pre-process $y$ from various modalities (such as language prompts) we introduce a domain specific encoder $\\tau_\\theta$ that projects $y$ to an intermediate representation $\\tau_\\theta(y) \\in \\mathbb{R}^{M \\times d_r}$, which is then mapped to the intermediate layers of the UNet via a cross-attention layer implementing\n\n$Attention(Q, K, V) = \\mathrm{softmax} \\left( \\frac{QK^T}{\\sqrt{d}} \\right) \\cdot V, \\text{ with}$\n\n$Q = W_Q^{(i)} \\cdot \\varphi_i(z_t), \\;\\; K = W_K^{(i)} \\cdot \\tau_\\theta(y), \\;\\; V = W_V^{(i)} \\cdot \\tau_\\theta(y).$\n\nHere, $\\varphi_i(z_t) \\in \\mathbb{R}^{N \\times d_c^l}$ denotes a (flattened) intermediate\n\nrepresentation of the UNet implementing $\\epsilon_\\theta$ and $W_V^{(i)} \\in$\n\nFigure 4.   Samples from *LDMs* trained on CelebAHQ [39], FFHQ [41], LSUN-Churches [102], LSUN-Bedrooms [102] and class-conditional ImageNet [12], each with a resolution of 256 \u00d7 256. Best viewed when zoomed in. For more samples *cf.* the supplement.\n\n$\\mathbb{R}^{d \\times d_t^i}$, $W_Q^{(i)} \\in \\mathbb{R}^{d \\times d_{\\tau}}$ & $W_K^{(i)} \\in \\mathbb{R}^{d \\times d_{\\tau}}$ are learnable projection matrices [36, 97]. See Fig. 3 for a visual depiction.\n    Based on image-conditioning pairs, we then learn the conditional LDM via\n\n$L_{LDM} := \\mathbb{E}_{\\mathcal{E}(x),y,\\epsilon \\sim \\mathcal{N}(0,1),t} \\left[ \\left\\| \\epsilon - \\epsilon_\\theta(z_t, t, \\tau_\\theta(y)) \\right\\|_2^2 \\right], \\quad (3)$\n\nwhere both $\\tau_\\theta$ and $\\epsilon_\\theta$ are jointly optimized via Eq. 3. This conditioning mechanism is flexible as $\\tau_\\theta$ can be parameterized with domain-specific experts, _e.g._ (unmasked) transformers [97] when $y$ are text prompts (see Sec. 4.3.1)"
        },
        {
            "text": "representation of the UNet implementing $\\epsilon_\\theta$ and $W_V^{(i)} \\in$",
            "page": 4,
            "x": 306,
            "y": 701,
            "width": 240,
            "height": 14,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-conditioning",
            "chunk_id": "9d470653-05e2-4e36-a03f-10938e47d8b7",
            "group_text": "## 3.3. Conditioning Mechanisms\nSimilar to other types of generative models [56, 83], diffusion models are in principle capable of modeling conditional distributions of the form $p(z|y)$. This can be implemented with a conditional denoising autoencoder $\\epsilon_\\theta(z_t, t, y)$ and paves the way to controlling the synthesis process through inputs $y$ such as text [68], semantic maps [33, 61] or other image-to-image translation tasks [34].\n\nIn the context of image synthesis, however, combining\nthe generative power of DMs with other types of condition-\nings beyond class-labels [15] or blurred variants of the input\nimage [72] is so far an under-explored area of research.\n\nWe turn DMs into more flexible conditional image generators by augmenting their underlying UNet backbone with the cross-attention mechanism [97], which is effective for learning attention-based models of various input modalities [35,36]. To pre-process $y$ from various modalities (such as language prompts) we introduce a domain specific encoder $\\tau_\\theta$ that projects $y$ to an intermediate representation $\\tau_\\theta(y) \\in \\mathbb{R}^{M \\times d_r}$, which is then mapped to the intermediate layers of the UNet via a cross-attention layer implementing\n\n$Attention(Q, K, V) = \\mathrm{softmax} \\left( \\frac{QK^T}{\\sqrt{d}} \\right) \\cdot V, \\text{ with}$\n\n$Q = W_Q^{(i)} \\cdot \\varphi_i(z_t), \\;\\; K = W_K^{(i)} \\cdot \\tau_\\theta(y), \\;\\; V = W_V^{(i)} \\cdot \\tau_\\theta(y).$\n\nHere, $\\varphi_i(z_t) \\in \\mathbb{R}^{N \\times d_c^l}$ denotes a (flattened) intermediate\n\nrepresentation of the UNet implementing $\\epsilon_\\theta$ and $W_V^{(i)} \\in$\n\nFigure 4.   Samples from *LDMs* trained on CelebAHQ [39], FFHQ [41], LSUN-Churches [102], LSUN-Bedrooms [102] and class-conditional ImageNet [12], each with a resolution of 256 \u00d7 256. Best viewed when zoomed in. For more samples *cf.* the supplement.\n\n$\\mathbb{R}^{d \\times d_t^i}$, $W_Q^{(i)} \\in \\mathbb{R}^{d \\times d_{\\tau}}$ & $W_K^{(i)} \\in \\mathbb{R}^{d \\times d_{\\tau}}$ are learnable projection matrices [36, 97]. See Fig. 3 for a visual depiction.\n    Based on image-conditioning pairs, we then learn the conditional LDM via\n\n$L_{LDM} := \\mathbb{E}_{\\mathcal{E}(x),y,\\epsilon \\sim \\mathcal{N}(0,1),t} \\left[ \\left\\| \\epsilon - \\epsilon_\\theta(z_t, t, \\tau_\\theta(y)) \\right\\|_2^2 \\right], \\quad (3)$\n\nwhere both $\\tau_\\theta$ and $\\epsilon_\\theta$ are jointly optimized via Eq. 3. This conditioning mechanism is flexible as $\\tau_\\theta$ can be parameterized with domain-specific experts, _e.g._ (unmasked) transformers [97] when $y$ are text prompts (see Sec. 4.3.1)"
        },
        {
            "text": "Figure 4.   Samples from *LDMs* trained on CelebAHQ [39], FFHQ [41], LSUN-Churches [102], LSUN-Bedrooms [102] and class-conditional ImageNet [12], each with a resolution of 256 \u00d7 256. Best viewed when zoomed in. For more samples *cf.* the supplement.",
            "page": 5,
            "x": 47,
            "y": 196,
            "width": 501,
            "height": 29,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-conditioning",
            "chunk_id": "4f5f3a7d-29a4-4d71-ba61-5d004379a62a",
            "group_text": "## 3.3. Conditioning Mechanisms\nSimilar to other types of generative models [56, 83], diffusion models are in principle capable of modeling conditional distributions of the form $p(z|y)$. This can be implemented with a conditional denoising autoencoder $\\epsilon_\\theta(z_t, t, y)$ and paves the way to controlling the synthesis process through inputs $y$ such as text [68], semantic maps [33, 61] or other image-to-image translation tasks [34].\n\nIn the context of image synthesis, however, combining\nthe generative power of DMs with other types of condition-\nings beyond class-labels [15] or blurred variants of the input\nimage [72] is so far an under-explored area of research.\n\nWe turn DMs into more flexible conditional image generators by augmenting their underlying UNet backbone with the cross-attention mechanism [97], which is effective for learning attention-based models of various input modalities [35,36]. To pre-process $y$ from various modalities (such as language prompts) we introduce a domain specific encoder $\\tau_\\theta$ that projects $y$ to an intermediate representation $\\tau_\\theta(y) \\in \\mathbb{R}^{M \\times d_r}$, which is then mapped to the intermediate layers of the UNet via a cross-attention layer implementing\n\n$Attention(Q, K, V) = \\mathrm{softmax} \\left( \\frac{QK^T}{\\sqrt{d}} \\right) \\cdot V, \\text{ with}$\n\n$Q = W_Q^{(i)} \\cdot \\varphi_i(z_t), \\;\\; K = W_K^{(i)} \\cdot \\tau_\\theta(y), \\;\\; V = W_V^{(i)} \\cdot \\tau_\\theta(y).$\n\nHere, $\\varphi_i(z_t) \\in \\mathbb{R}^{N \\times d_c^l}$ denotes a (flattened) intermediate\n\nrepresentation of the UNet implementing $\\epsilon_\\theta$ and $W_V^{(i)} \\in$\n\nFigure 4.   Samples from *LDMs* trained on CelebAHQ [39], FFHQ [41], LSUN-Churches [102], LSUN-Bedrooms [102] and class-conditional ImageNet [12], each with a resolution of 256 \u00d7 256. Best viewed when zoomed in. For more samples *cf.* the supplement.\n\n$\\mathbb{R}^{d \\times d_t^i}$, $W_Q^{(i)} \\in \\mathbb{R}^{d \\times d_{\\tau}}$ & $W_K^{(i)} \\in \\mathbb{R}^{d \\times d_{\\tau}}$ are learnable projection matrices [36, 97]. See Fig. 3 for a visual depiction.\n    Based on image-conditioning pairs, we then learn the conditional LDM via\n\n$L_{LDM} := \\mathbb{E}_{\\mathcal{E}(x),y,\\epsilon \\sim \\mathcal{N}(0,1),t} \\left[ \\left\\| \\epsilon - \\epsilon_\\theta(z_t, t, \\tau_\\theta(y)) \\right\\|_2^2 \\right], \\quad (3)$\n\nwhere both $\\tau_\\theta$ and $\\epsilon_\\theta$ are jointly optimized via Eq. 3. This conditioning mechanism is flexible as $\\tau_\\theta$ can be parameterized with domain-specific experts, _e.g._ (unmasked) transformers [97] when $y$ are text prompts (see Sec. 4.3.1)"
        },
        {
            "text": "$\\mathbb{R}^{d \\times d_t^i}$, $W_Q^{(i)} \\in \\mathbb{R}^{d \\times d_{\\tau}}$ & $W_K^{(i)} \\in \\mathbb{R}^{d \\times d_{\\tau}}$ are learnable projection matrices [36, 97]. See Fig. 3 for a visual depiction.\n    Based on image-conditioning pairs, we then learn the conditional LDM via",
            "page": 5,
            "x": 46,
            "y": 236,
            "width": 243,
            "height": 52,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-conditioning",
            "chunk_id": "8fcc480d-44fd-49f6-bb3c-6d4cca5b2ae3",
            "group_text": "## 3.3. Conditioning Mechanisms\nSimilar to other types of generative models [56, 83], diffusion models are in principle capable of modeling conditional distributions of the form $p(z|y)$. This can be implemented with a conditional denoising autoencoder $\\epsilon_\\theta(z_t, t, y)$ and paves the way to controlling the synthesis process through inputs $y$ such as text [68], semantic maps [33, 61] or other image-to-image translation tasks [34].\n\nIn the context of image synthesis, however, combining\nthe generative power of DMs with other types of condition-\nings beyond class-labels [15] or blurred variants of the input\nimage [72] is so far an under-explored area of research.\n\nWe turn DMs into more flexible conditional image generators by augmenting their underlying UNet backbone with the cross-attention mechanism [97], which is effective for learning attention-based models of various input modalities [35,36]. To pre-process $y$ from various modalities (such as language prompts) we introduce a domain specific encoder $\\tau_\\theta$ that projects $y$ to an intermediate representation $\\tau_\\theta(y) \\in \\mathbb{R}^{M \\times d_r}$, which is then mapped to the intermediate layers of the UNet via a cross-attention layer implementing\n\n$Attention(Q, K, V) = \\mathrm{softmax} \\left( \\frac{QK^T}{\\sqrt{d}} \\right) \\cdot V, \\text{ with}$\n\n$Q = W_Q^{(i)} \\cdot \\varphi_i(z_t), \\;\\; K = W_K^{(i)} \\cdot \\tau_\\theta(y), \\;\\; V = W_V^{(i)} \\cdot \\tau_\\theta(y).$\n\nHere, $\\varphi_i(z_t) \\in \\mathbb{R}^{N \\times d_c^l}$ denotes a (flattened) intermediate\n\nrepresentation of the UNet implementing $\\epsilon_\\theta$ and $W_V^{(i)} \\in$\n\nFigure 4.   Samples from *LDMs* trained on CelebAHQ [39], FFHQ [41], LSUN-Churches [102], LSUN-Bedrooms [102] and class-conditional ImageNet [12], each with a resolution of 256 \u00d7 256. Best viewed when zoomed in. For more samples *cf.* the supplement.\n\n$\\mathbb{R}^{d \\times d_t^i}$, $W_Q^{(i)} \\in \\mathbb{R}^{d \\times d_{\\tau}}$ & $W_K^{(i)} \\in \\mathbb{R}^{d \\times d_{\\tau}}$ are learnable projection matrices [36, 97]. See Fig. 3 for a visual depiction.\n    Based on image-conditioning pairs, we then learn the conditional LDM via\n\n$L_{LDM} := \\mathbb{E}_{\\mathcal{E}(x),y,\\epsilon \\sim \\mathcal{N}(0,1),t} \\left[ \\left\\| \\epsilon - \\epsilon_\\theta(z_t, t, \\tau_\\theta(y)) \\right\\|_2^2 \\right], \\quad (3)$\n\nwhere both $\\tau_\\theta$ and $\\epsilon_\\theta$ are jointly optimized via Eq. 3. This conditioning mechanism is flexible as $\\tau_\\theta$ can be parameterized with domain-specific experts, _e.g._ (unmasked) transformers [97] when $y$ are text prompts (see Sec. 4.3.1)"
        },
        {
            "text": "$L_{LDM} := \\mathbb{E}_{\\mathcal{E}(x),y,\\epsilon \\sim \\mathcal{N}(0,1),t} \\left[ \\left\\| \\epsilon - \\epsilon_\\theta(z_t, t, \\tau_\\theta(y)) \\right\\|_2^2 \\right], \\quad (3)$",
            "page": 5,
            "x": 51,
            "y": 294,
            "width": 238,
            "height": 25,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-conditioning",
            "chunk_id": "30d02fab-cd81-4c95-91ff-86d2b233902c",
            "group_text": "## 3.3. Conditioning Mechanisms\nSimilar to other types of generative models [56, 83], diffusion models are in principle capable of modeling conditional distributions of the form $p(z|y)$. This can be implemented with a conditional denoising autoencoder $\\epsilon_\\theta(z_t, t, y)$ and paves the way to controlling the synthesis process through inputs $y$ such as text [68], semantic maps [33, 61] or other image-to-image translation tasks [34].\n\nIn the context of image synthesis, however, combining\nthe generative power of DMs with other types of condition-\nings beyond class-labels [15] or blurred variants of the input\nimage [72] is so far an under-explored area of research.\n\nWe turn DMs into more flexible conditional image generators by augmenting their underlying UNet backbone with the cross-attention mechanism [97], which is effective for learning attention-based models of various input modalities [35,36]. To pre-process $y$ from various modalities (such as language prompts) we introduce a domain specific encoder $\\tau_\\theta$ that projects $y$ to an intermediate representation $\\tau_\\theta(y) \\in \\mathbb{R}^{M \\times d_r}$, which is then mapped to the intermediate layers of the UNet via a cross-attention layer implementing\n\n$Attention(Q, K, V) = \\mathrm{softmax} \\left( \\frac{QK^T}{\\sqrt{d}} \\right) \\cdot V, \\text{ with}$\n\n$Q = W_Q^{(i)} \\cdot \\varphi_i(z_t), \\;\\; K = W_K^{(i)} \\cdot \\tau_\\theta(y), \\;\\; V = W_V^{(i)} \\cdot \\tau_\\theta(y).$\n\nHere, $\\varphi_i(z_t) \\in \\mathbb{R}^{N \\times d_c^l}$ denotes a (flattened) intermediate\n\nrepresentation of the UNet implementing $\\epsilon_\\theta$ and $W_V^{(i)} \\in$\n\nFigure 4.   Samples from *LDMs* trained on CelebAHQ [39], FFHQ [41], LSUN-Churches [102], LSUN-Bedrooms [102] and class-conditional ImageNet [12], each with a resolution of 256 \u00d7 256. Best viewed when zoomed in. For more samples *cf.* the supplement.\n\n$\\mathbb{R}^{d \\times d_t^i}$, $W_Q^{(i)} \\in \\mathbb{R}^{d \\times d_{\\tau}}$ & $W_K^{(i)} \\in \\mathbb{R}^{d \\times d_{\\tau}}$ are learnable projection matrices [36, 97]. See Fig. 3 for a visual depiction.\n    Based on image-conditioning pairs, we then learn the conditional LDM via\n\n$L_{LDM} := \\mathbb{E}_{\\mathcal{E}(x),y,\\epsilon \\sim \\mathcal{N}(0,1),t} \\left[ \\left\\| \\epsilon - \\epsilon_\\theta(z_t, t, \\tau_\\theta(y)) \\right\\|_2^2 \\right], \\quad (3)$\n\nwhere both $\\tau_\\theta$ and $\\epsilon_\\theta$ are jointly optimized via Eq. 3. This conditioning mechanism is flexible as $\\tau_\\theta$ can be parameterized with domain-specific experts, _e.g._ (unmasked) transformers [97] when $y$ are text prompts (see Sec. 4.3.1)"
        },
        {
            "text": "where both $\\tau_\\theta$ and $\\epsilon_\\theta$ are jointly optimized via Eq. 3. This conditioning mechanism is flexible as $\\tau_\\theta$ can be parameterized with domain-specific experts, _e.g._ (unmasked) transformers [97] when $y$ are text prompts (see Sec. 4.3.1)",
            "page": 5,
            "x": 47,
            "y": 323,
            "width": 241,
            "height": 50,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-conditioning",
            "chunk_id": "005a3cfe-56ab-4df6-ab2a-271ffb16ee4b",
            "group_text": "## 3.3. Conditioning Mechanisms\nSimilar to other types of generative models [56, 83], diffusion models are in principle capable of modeling conditional distributions of the form $p(z|y)$. This can be implemented with a conditional denoising autoencoder $\\epsilon_\\theta(z_t, t, y)$ and paves the way to controlling the synthesis process through inputs $y$ such as text [68], semantic maps [33, 61] or other image-to-image translation tasks [34].\n\nIn the context of image synthesis, however, combining\nthe generative power of DMs with other types of condition-\nings beyond class-labels [15] or blurred variants of the input\nimage [72] is so far an under-explored area of research.\n\nWe turn DMs into more flexible conditional image generators by augmenting their underlying UNet backbone with the cross-attention mechanism [97], which is effective for learning attention-based models of various input modalities [35,36]. To pre-process $y$ from various modalities (such as language prompts) we introduce a domain specific encoder $\\tau_\\theta$ that projects $y$ to an intermediate representation $\\tau_\\theta(y) \\in \\mathbb{R}^{M \\times d_r}$, which is then mapped to the intermediate layers of the UNet via a cross-attention layer implementing\n\n$Attention(Q, K, V) = \\mathrm{softmax} \\left( \\frac{QK^T}{\\sqrt{d}} \\right) \\cdot V, \\text{ with}$\n\n$Q = W_Q^{(i)} \\cdot \\varphi_i(z_t), \\;\\; K = W_K^{(i)} \\cdot \\tau_\\theta(y), \\;\\; V = W_V^{(i)} \\cdot \\tau_\\theta(y).$\n\nHere, $\\varphi_i(z_t) \\in \\mathbb{R}^{N \\times d_c^l}$ denotes a (flattened) intermediate\n\nrepresentation of the UNet implementing $\\epsilon_\\theta$ and $W_V^{(i)} \\in$\n\nFigure 4.   Samples from *LDMs* trained on CelebAHQ [39], FFHQ [41], LSUN-Churches [102], LSUN-Bedrooms [102] and class-conditional ImageNet [12], each with a resolution of 256 \u00d7 256. Best viewed when zoomed in. For more samples *cf.* the supplement.\n\n$\\mathbb{R}^{d \\times d_t^i}$, $W_Q^{(i)} \\in \\mathbb{R}^{d \\times d_{\\tau}}$ & $W_K^{(i)} \\in \\mathbb{R}^{d \\times d_{\\tau}}$ are learnable projection matrices [36, 97]. See Fig. 3 for a visual depiction.\n    Based on image-conditioning pairs, we then learn the conditional LDM via\n\n$L_{LDM} := \\mathbb{E}_{\\mathcal{E}(x),y,\\epsilon \\sim \\mathcal{N}(0,1),t} \\left[ \\left\\| \\epsilon - \\epsilon_\\theta(z_t, t, \\tau_\\theta(y)) \\right\\|_2^2 \\right], \\quad (3)$\n\nwhere both $\\tau_\\theta$ and $\\epsilon_\\theta$ are jointly optimized via Eq. 3. This conditioning mechanism is flexible as $\\tau_\\theta$ can be parameterized with domain-specific experts, _e.g._ (unmasked) transformers [97] when $y$ are text prompts (see Sec. 4.3.1)"
        },
        {
            "text": "## 4. Experiments\n\n_LDMs_ provide means to flexible and computationally tractable diffusion based image synthesis of various image modalities, which we empirically show in the following. Firstly, however, we analyze the gains of our models compared to pixel-based diffusion models in both training and inference. Interestingly, we find that _LDMs_ trained in _VQ_-regularized latent spaces sometimes achieve better sample quality, even though the reconstruction capabilities of _VQ_-regularized first stage models slightly fall behind those of their continuous counterparts, _cf._ Tab. 8. A visual comparison between the effects of first stage regularization schemes on _LDM_ training and their generalization abilities to resolutions $>256^2$ can be found in Appendix D.1. In E.2 we list details on architecture, implementation, training and evaluation for all results presented in this section.",
            "page": 5,
            "x": 46,
            "y": 382,
            "width": 243,
            "height": 197,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "8-experiments",
            "chunk_id": "2d68eb1f-c373-4452-ab59-7ea79eac8489",
            "group_text": "## 4. Experiments\n\n_LDMs_ provide means to flexible and computationally tractable diffusion based image synthesis of various image modalities, which we empirically show in the following. Firstly, however, we analyze the gains of our models compared to pixel-based diffusion models in both training and inference. Interestingly, we find that _LDMs_ trained in _VQ_-regularized latent spaces sometimes achieve better sample quality, even though the reconstruction capabilities of _VQ_-regularized first stage models slightly fall behind those of their continuous counterparts, _cf._ Tab. 8. A visual comparison between the effects of first stage regularization schemes on _LDM_ training and their generalization abilities to resolutions $>256^2$ can be found in Appendix D.1. In E.2 we list details on architecture, implementation, training and evaluation for all results presented in this section."
        },
        {
            "text": "## 4.1. On Perceptual Compression Tradeoffs\n\nThis section analyzes the behavior of our LDMs with different downsampling factors $f \\in \\{1, 2, 4, 8, 16, 32\\}$ (abbreviated as *LDM-f*, where *LDM-1* corresponds to pixel-based DMs). To obtain a comparable test-field, we fix the computational resources to a single NVIDIA A100 for all experiments in this section and train all models for the same number of steps and with the same number of parameters.\n\nTab. 8 shows hyperparameters and reconstruction performance of the first stage models used for the *LDMs* com-",
            "page": 5,
            "x": 46,
            "y": 585,
            "width": 242,
            "height": 128,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-on",
            "chunk_id": "51682e16-57e0-406f-9504-e15ba298250a",
            "group_text": "## 4.1. On Perceptual Compression Tradeoffs\n\nThis section analyzes the behavior of our LDMs with different downsampling factors $f \\in \\{1, 2, 4, 8, 16, 32\\}$ (abbreviated as *LDM-f*, where *LDM-1* corresponds to pixel-based DMs). To obtain a comparable test-field, we fix the computational resources to a single NVIDIA A100 for all experiments in this section and train all models for the same number of steps and with the same number of parameters.\n\nTab. 8 shows hyperparameters and reconstruction performance of the first stage models used for the *LDMs* com-\n\npared in this section. Fig. 6 shows sample quality as a function of training progress for 2M steps of class-conditional models on the ImageNet [12] dataset. We see that, i) small downsampling factors for _LDM-{1,2}_ result in slow training progress, whereas ii) overly large values of _f_ cause stagnating fidelity after comparably few training steps. Revisiting the analysis above (Fig. 1 and 2) we attribute this to i) leaving most of perceptual compression to the diffusion model and ii) too strong first stage compression resulting in information loss and thus limiting the achievable quality. _LDM-{4-16}_ strike a good balance between efficiency and perceptually faithful results, which manifests in a significant FID [29] gap of 38 between pixel-based diffusion (_LDM-1_) and _LDM-8_ after 2M training steps.\n\nIn Fig. 7, we compare models trained on CelebA-HQ [39] and ImageNet in terms sampling speed for different numbers of denoising steps with the DDIM sampler [84] and plot it against FID-scores [29]. _LDM-{4-8}_ outperform models with unsuitable ratios of perceptual and conceptual compression. Especially compared to pixel-based _LDM-1_, they achieve much lower FID scores while simultaneously significantly increasing sample throughput. Complex datasets such as ImageNet require reduced compression rates to avoid reducing quality. In summary, _LDM-4_ and -8 offer the best conditions for achieving high-quality synthesis results."
        },
        {
            "text": "pared in this section. Fig. 6 shows sample quality as a function of training progress for 2M steps of class-conditional models on the ImageNet [12] dataset. We see that, i) small downsampling factors for _LDM-{1,2}_ result in slow training progress, whereas ii) overly large values of _f_ cause stagnating fidelity after comparably few training steps. Revisiting the analysis above (Fig. 1 and 2) we attribute this to i) leaving most of perceptual compression to the diffusion model and ii) too strong first stage compression resulting in information loss and thus limiting the achievable quality. _LDM-{4-16}_ strike a good balance between efficiency and perceptually faithful results, which manifests in a significant FID [29] gap of 38 between pixel-based diffusion (_LDM-1_) and _LDM-8_ after 2M training steps.",
            "page": 5,
            "x": 306,
            "y": 239,
            "width": 242,
            "height": 167,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-on",
            "chunk_id": "0db7fc44-5d89-454b-898e-e55bff7e8dd8",
            "group_text": "## 4.1. On Perceptual Compression Tradeoffs\n\nThis section analyzes the behavior of our LDMs with different downsampling factors $f \\in \\{1, 2, 4, 8, 16, 32\\}$ (abbreviated as *LDM-f*, where *LDM-1* corresponds to pixel-based DMs). To obtain a comparable test-field, we fix the computational resources to a single NVIDIA A100 for all experiments in this section and train all models for the same number of steps and with the same number of parameters.\n\nTab. 8 shows hyperparameters and reconstruction performance of the first stage models used for the *LDMs* com-\n\npared in this section. Fig. 6 shows sample quality as a function of training progress for 2M steps of class-conditional models on the ImageNet [12] dataset. We see that, i) small downsampling factors for _LDM-{1,2}_ result in slow training progress, whereas ii) overly large values of _f_ cause stagnating fidelity after comparably few training steps. Revisiting the analysis above (Fig. 1 and 2) we attribute this to i) leaving most of perceptual compression to the diffusion model and ii) too strong first stage compression resulting in information loss and thus limiting the achievable quality. _LDM-{4-16}_ strike a good balance between efficiency and perceptually faithful results, which manifests in a significant FID [29] gap of 38 between pixel-based diffusion (_LDM-1_) and _LDM-8_ after 2M training steps.\n\nIn Fig. 7, we compare models trained on CelebA-HQ [39] and ImageNet in terms sampling speed for different numbers of denoising steps with the DDIM sampler [84] and plot it against FID-scores [29]. _LDM-{4-8}_ outperform models with unsuitable ratios of perceptual and conceptual compression. Especially compared to pixel-based _LDM-1_, they achieve much lower FID scores while simultaneously significantly increasing sample throughput. Complex datasets such as ImageNet require reduced compression rates to avoid reducing quality. In summary, _LDM-4_ and -8 offer the best conditions for achieving high-quality synthesis results."
        },
        {
            "text": "In Fig. 7, we compare models trained on CelebA-HQ [39] and ImageNet in terms sampling speed for different numbers of denoising steps with the DDIM sampler [84] and plot it against FID-scores [29]. _LDM-{4-8}_ outperform models with unsuitable ratios of perceptual and conceptual compression. Especially compared to pixel-based _LDM-1_, they achieve much lower FID scores while simultaneously significantly increasing sample throughput. Complex datasets such as ImageNet require reduced compression rates to avoid reducing quality. In summary, _LDM-4_ and -8 offer the best conditions for achieving high-quality synthesis results.",
            "page": 5,
            "x": 305,
            "y": 407,
            "width": 242,
            "height": 143,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-on",
            "chunk_id": "dd0c4565-916a-47e1-84e1-783f4c39f016",
            "group_text": "## 4.1. On Perceptual Compression Tradeoffs\n\nThis section analyzes the behavior of our LDMs with different downsampling factors $f \\in \\{1, 2, 4, 8, 16, 32\\}$ (abbreviated as *LDM-f*, where *LDM-1* corresponds to pixel-based DMs). To obtain a comparable test-field, we fix the computational resources to a single NVIDIA A100 for all experiments in this section and train all models for the same number of steps and with the same number of parameters.\n\nTab. 8 shows hyperparameters and reconstruction performance of the first stage models used for the *LDMs* com-\n\npared in this section. Fig. 6 shows sample quality as a function of training progress for 2M steps of class-conditional models on the ImageNet [12] dataset. We see that, i) small downsampling factors for _LDM-{1,2}_ result in slow training progress, whereas ii) overly large values of _f_ cause stagnating fidelity after comparably few training steps. Revisiting the analysis above (Fig. 1 and 2) we attribute this to i) leaving most of perceptual compression to the diffusion model and ii) too strong first stage compression resulting in information loss and thus limiting the achievable quality. _LDM-{4-16}_ strike a good balance between efficiency and perceptually faithful results, which manifests in a significant FID [29] gap of 38 between pixel-based diffusion (_LDM-1_) and _LDM-8_ after 2M training steps.\n\nIn Fig. 7, we compare models trained on CelebA-HQ [39] and ImageNet in terms sampling speed for different numbers of denoising steps with the DDIM sampler [84] and plot it against FID-scores [29]. _LDM-{4-8}_ outperform models with unsuitable ratios of perceptual and conceptual compression. Especially compared to pixel-based _LDM-1_, they achieve much lower FID scores while simultaneously significantly increasing sample throughput. Complex datasets such as ImageNet require reduced compression rates to avoid reducing quality. In summary, _LDM-4_ and -8 offer the best conditions for achieving high-quality synthesis results."
        },
        {
            "text": "## 4.2. Image Generation with Latent Diffusion\n\nWe train unconditional models of $256^2$ images on CelebA-HQ [39], FFHQ [41], LSUN-Churches and -Bedrooms [102] and evaluate the i) sample quality and ii) their coverage of the data manifold using ii) FID [29] and ii) Precision-and-Recall [50]. Tab. 1 summarizes our results. On CelebA-HQ, we report a new state-of-the-art FID of 5.11, outperforming previous likelihood-based models as well as GANs. We also outperform LSGM [93] where a latent diffusion model is trained jointly together with the first stage. In contrast, we train diffusion models in a fixed space",
            "page": 5,
            "x": 305,
            "y": 579,
            "width": 242,
            "height": 136,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "10-image",
            "chunk_id": "21a39019-bfa6-4479-a45f-64ff2c5ee420",
            "group_text": "## 4.2. Image Generation with Latent Diffusion\n\nWe train unconditional models of $256^2$ images on CelebA-HQ [39], FFHQ [41], LSUN-Churches and -Bedrooms [102] and evaluate the i) sample quality and ii) their coverage of the data manifold using ii) FID [29] and ii) Precision-and-Recall [50]. Tab. 1 summarizes our results. On CelebA-HQ, we report a new state-of-the-art FID of 5.11, outperforming previous likelihood-based models as well as GANs. We also outperform LSGM [93] where a latent diffusion model is trained jointly together with the first stage. In contrast, we train diffusion models in a fixed space\n\nText-to-Image Synthesis on LAION. 1.45B Model.\n\nFigure 5.  Samples for user-defined text prompts from our model for text-to-image synthesis, _LDM-8 (KL)_, which was trained on the LAION [78] database. Samples generated with 200 DDIM steps and $\\eta = 1.0$. We use unconditional guidance [32] with $s = 10.0$.\n\nand avoid the difficulty of weighing reconstruction quality\nagainst learning the prior over the latent space, see Fig. 1-2.\nWe outperform prior diffusion based approaches on all\nbut the LSUN-Bedrooms dataset, where our score is close\nto ADM [15], despite utilizing half its parameters and re-\nquiring 4-times less train resources (see Appendix E.3.5).\n\nTable 1.  Evaluation metrics for unconditional image synthesis.\nCelebA-HQ results reproduced from [43, 63, 100], FFHQ from\n[42, 43]. \u2020: $N$-s refers to $N$ sampling steps with the DDIM [84]\nsampler. *: trained in $KL$-regularized latent space. Additional re-\nsults can be found in the supplementary.\n\nTable 2.  Evaluation of text-conditional image synthesis on the 256 \u00d7 256-sized MS-COCO [51] dataset: with 250 DDIM [84] steps our model is on par with the most recent diffusion [59] and autoregressive [26] methods despite using significantly less parameters. \u2020/*: Numbers from [109]/[26]\n\nMoreover, *LDMs* consistently improve upon GAN-based methods in Precision and Recall, thus confirming the advantages of their mode-covering likelihood-based training objective over adversarial approaches. In Fig. 4 we also show qualitative results on each dataset."
        },
        {
            "text": "Text-to-Image Synthesis on LAION. 1.45B Model.",
            "page": 6,
            "x": 204,
            "y": 75,
            "width": 190,
            "height": 11,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "10-image",
            "chunk_id": "be4bb30c-e378-4542-9484-a485bb495294",
            "group_text": "## 4.2. Image Generation with Latent Diffusion\n\nWe train unconditional models of $256^2$ images on CelebA-HQ [39], FFHQ [41], LSUN-Churches and -Bedrooms [102] and evaluate the i) sample quality and ii) their coverage of the data manifold using ii) FID [29] and ii) Precision-and-Recall [50]. Tab. 1 summarizes our results. On CelebA-HQ, we report a new state-of-the-art FID of 5.11, outperforming previous likelihood-based models as well as GANs. We also outperform LSGM [93] where a latent diffusion model is trained jointly together with the first stage. In contrast, we train diffusion models in a fixed space\n\nText-to-Image Synthesis on LAION. 1.45B Model.\n\nFigure 5.  Samples for user-defined text prompts from our model for text-to-image synthesis, _LDM-8 (KL)_, which was trained on the LAION [78] database. Samples generated with 200 DDIM steps and $\\eta = 1.0$. We use unconditional guidance [32] with $s = 10.0$.\n\nand avoid the difficulty of weighing reconstruction quality\nagainst learning the prior over the latent space, see Fig. 1-2.\nWe outperform prior diffusion based approaches on all\nbut the LSUN-Bedrooms dataset, where our score is close\nto ADM [15], despite utilizing half its parameters and re-\nquiring 4-times less train resources (see Appendix E.3.5).\n\nTable 1.  Evaluation metrics for unconditional image synthesis.\nCelebA-HQ results reproduced from [43, 63, 100], FFHQ from\n[42, 43]. \u2020: $N$-s refers to $N$ sampling steps with the DDIM [84]\nsampler. *: trained in $KL$-regularized latent space. Additional re-\nsults can be found in the supplementary.\n\nTable 2.  Evaluation of text-conditional image synthesis on the 256 \u00d7 256-sized MS-COCO [51] dataset: with 250 DDIM [84] steps our model is on par with the most recent diffusion [59] and autoregressive [26] methods despite using significantly less parameters. \u2020/*: Numbers from [109]/[26]\n\nMoreover, *LDMs* consistently improve upon GAN-based methods in Precision and Recall, thus confirming the advantages of their mode-covering likelihood-based training objective over adversarial approaches. In Fig. 4 we also show qualitative results on each dataset."
        },
        {
            "text": "Figure 5.  Samples for user-defined text prompts from our model for text-to-image synthesis, _LDM-8 (KL)_, which was trained on the LAION [78] database. Samples generated with 200 DDIM steps and $\\eta = 1.0$. We use unconditional guidance [32] with $s = 10.0$.",
            "page": 6,
            "x": 46,
            "y": 260,
            "width": 502,
            "height": 26,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "10-image",
            "chunk_id": "2adf2a7f-2659-4be3-807b-216452624b33",
            "group_text": "## 4.2. Image Generation with Latent Diffusion\n\nWe train unconditional models of $256^2$ images on CelebA-HQ [39], FFHQ [41], LSUN-Churches and -Bedrooms [102] and evaluate the i) sample quality and ii) their coverage of the data manifold using ii) FID [29] and ii) Precision-and-Recall [50]. Tab. 1 summarizes our results. On CelebA-HQ, we report a new state-of-the-art FID of 5.11, outperforming previous likelihood-based models as well as GANs. We also outperform LSGM [93] where a latent diffusion model is trained jointly together with the first stage. In contrast, we train diffusion models in a fixed space\n\nText-to-Image Synthesis on LAION. 1.45B Model.\n\nFigure 5.  Samples for user-defined text prompts from our model for text-to-image synthesis, _LDM-8 (KL)_, which was trained on the LAION [78] database. Samples generated with 200 DDIM steps and $\\eta = 1.0$. We use unconditional guidance [32] with $s = 10.0$.\n\nand avoid the difficulty of weighing reconstruction quality\nagainst learning the prior over the latent space, see Fig. 1-2.\nWe outperform prior diffusion based approaches on all\nbut the LSUN-Bedrooms dataset, where our score is close\nto ADM [15], despite utilizing half its parameters and re-\nquiring 4-times less train resources (see Appendix E.3.5).\n\nTable 1.  Evaluation metrics for unconditional image synthesis.\nCelebA-HQ results reproduced from [43, 63, 100], FFHQ from\n[42, 43]. \u2020: $N$-s refers to $N$ sampling steps with the DDIM [84]\nsampler. *: trained in $KL$-regularized latent space. Additional re-\nsults can be found in the supplementary.\n\nTable 2.  Evaluation of text-conditional image synthesis on the 256 \u00d7 256-sized MS-COCO [51] dataset: with 250 DDIM [84] steps our model is on par with the most recent diffusion [59] and autoregressive [26] methods despite using significantly less parameters. \u2020/*: Numbers from [109]/[26]\n\nMoreover, *LDMs* consistently improve upon GAN-based methods in Precision and Recall, thus confirming the advantages of their mode-covering likelihood-based training objective over adversarial approaches. In Fig. 4 we also show qualitative results on each dataset."
        },
        {
            "text": "and avoid the difficulty of weighing reconstruction quality\nagainst learning the prior over the latent space, see Fig. 1-2.\nWe outperform prior diffusion based approaches on all\nbut the LSUN-Bedrooms dataset, where our score is close\nto ADM [15], despite utilizing half its parameters and re-\nquiring 4-times less train resources (see Appendix E.3.5).",
            "page": 6,
            "x": 46,
            "y": 642,
            "width": 244,
            "height": 74,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "10-image",
            "chunk_id": "46ae6a64-8cee-44cf-a289-23c50fe7530d",
            "group_text": "## 4.2. Image Generation with Latent Diffusion\n\nWe train unconditional models of $256^2$ images on CelebA-HQ [39], FFHQ [41], LSUN-Churches and -Bedrooms [102] and evaluate the i) sample quality and ii) their coverage of the data manifold using ii) FID [29] and ii) Precision-and-Recall [50]. Tab. 1 summarizes our results. On CelebA-HQ, we report a new state-of-the-art FID of 5.11, outperforming previous likelihood-based models as well as GANs. We also outperform LSGM [93] where a latent diffusion model is trained jointly together with the first stage. In contrast, we train diffusion models in a fixed space\n\nText-to-Image Synthesis on LAION. 1.45B Model.\n\nFigure 5.  Samples for user-defined text prompts from our model for text-to-image synthesis, _LDM-8 (KL)_, which was trained on the LAION [78] database. Samples generated with 200 DDIM steps and $\\eta = 1.0$. We use unconditional guidance [32] with $s = 10.0$.\n\nand avoid the difficulty of weighing reconstruction quality\nagainst learning the prior over the latent space, see Fig. 1-2.\nWe outperform prior diffusion based approaches on all\nbut the LSUN-Bedrooms dataset, where our score is close\nto ADM [15], despite utilizing half its parameters and re-\nquiring 4-times less train resources (see Appendix E.3.5).\n\nTable 1.  Evaluation metrics for unconditional image synthesis.\nCelebA-HQ results reproduced from [43, 63, 100], FFHQ from\n[42, 43]. \u2020: $N$-s refers to $N$ sampling steps with the DDIM [84]\nsampler. *: trained in $KL$-regularized latent space. Additional re-\nsults can be found in the supplementary.\n\nTable 2.  Evaluation of text-conditional image synthesis on the 256 \u00d7 256-sized MS-COCO [51] dataset: with 250 DDIM [84] steps our model is on par with the most recent diffusion [59] and autoregressive [26] methods despite using significantly less parameters. \u2020/*: Numbers from [109]/[26]\n\nMoreover, *LDMs* consistently improve upon GAN-based methods in Precision and Recall, thus confirming the advantages of their mode-covering likelihood-based training objective over adversarial approaches. In Fig. 4 we also show qualitative results on each dataset."
        },
        {
            "text": "Table 1.  Evaluation metrics for unconditional image synthesis.\nCelebA-HQ results reproduced from [43, 63, 100], FFHQ from\n[42, 43]. \u2020: $N$-s refers to $N$ sampling steps with the DDIM [84]\nsampler. *: trained in $KL$-regularized latent space. Additional re-\nsults can be found in the supplementary.",
            "page": 6,
            "x": 305,
            "y": 431,
            "width": 242,
            "height": 57,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "10-image",
            "chunk_id": "9f2f534e-ed3c-4da3-a512-e6014b5d175a",
            "group_text": "## 4.2. Image Generation with Latent Diffusion\n\nWe train unconditional models of $256^2$ images on CelebA-HQ [39], FFHQ [41], LSUN-Churches and -Bedrooms [102] and evaluate the i) sample quality and ii) their coverage of the data manifold using ii) FID [29] and ii) Precision-and-Recall [50]. Tab. 1 summarizes our results. On CelebA-HQ, we report a new state-of-the-art FID of 5.11, outperforming previous likelihood-based models as well as GANs. We also outperform LSGM [93] where a latent diffusion model is trained jointly together with the first stage. In contrast, we train diffusion models in a fixed space\n\nText-to-Image Synthesis on LAION. 1.45B Model.\n\nFigure 5.  Samples for user-defined text prompts from our model for text-to-image synthesis, _LDM-8 (KL)_, which was trained on the LAION [78] database. Samples generated with 200 DDIM steps and $\\eta = 1.0$. We use unconditional guidance [32] with $s = 10.0$.\n\nand avoid the difficulty of weighing reconstruction quality\nagainst learning the prior over the latent space, see Fig. 1-2.\nWe outperform prior diffusion based approaches on all\nbut the LSUN-Bedrooms dataset, where our score is close\nto ADM [15], despite utilizing half its parameters and re-\nquiring 4-times less train resources (see Appendix E.3.5).\n\nTable 1.  Evaluation metrics for unconditional image synthesis.\nCelebA-HQ results reproduced from [43, 63, 100], FFHQ from\n[42, 43]. \u2020: $N$-s refers to $N$ sampling steps with the DDIM [84]\nsampler. *: trained in $KL$-regularized latent space. Additional re-\nsults can be found in the supplementary.\n\nTable 2.  Evaluation of text-conditional image synthesis on the 256 \u00d7 256-sized MS-COCO [51] dataset: with 250 DDIM [84] steps our model is on par with the most recent diffusion [59] and autoregressive [26] methods despite using significantly less parameters. \u2020/*: Numbers from [109]/[26]\n\nMoreover, *LDMs* consistently improve upon GAN-based methods in Precision and Recall, thus confirming the advantages of their mode-covering likelihood-based training objective over adversarial approaches. In Fig. 4 we also show qualitative results on each dataset."
        },
        {
            "text": "Table 2.  Evaluation of text-conditional image synthesis on the 256 \u00d7 256-sized MS-COCO [51] dataset: with 250 DDIM [84] steps our model is on par with the most recent diffusion [59] and autoregressive [26] methods despite using significantly less parameters. \u2020/*: Numbers from [109]/[26]",
            "page": 6,
            "x": 305,
            "y": 572,
            "width": 242,
            "height": 57,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "10-image",
            "chunk_id": "8d586e60-e91f-4ae7-a52a-c05a908f9345",
            "group_text": "## 4.2. Image Generation with Latent Diffusion\n\nWe train unconditional models of $256^2$ images on CelebA-HQ [39], FFHQ [41], LSUN-Churches and -Bedrooms [102] and evaluate the i) sample quality and ii) their coverage of the data manifold using ii) FID [29] and ii) Precision-and-Recall [50]. Tab. 1 summarizes our results. On CelebA-HQ, we report a new state-of-the-art FID of 5.11, outperforming previous likelihood-based models as well as GANs. We also outperform LSGM [93] where a latent diffusion model is trained jointly together with the first stage. In contrast, we train diffusion models in a fixed space\n\nText-to-Image Synthesis on LAION. 1.45B Model.\n\nFigure 5.  Samples for user-defined text prompts from our model for text-to-image synthesis, _LDM-8 (KL)_, which was trained on the LAION [78] database. Samples generated with 200 DDIM steps and $\\eta = 1.0$. We use unconditional guidance [32] with $s = 10.0$.\n\nand avoid the difficulty of weighing reconstruction quality\nagainst learning the prior over the latent space, see Fig. 1-2.\nWe outperform prior diffusion based approaches on all\nbut the LSUN-Bedrooms dataset, where our score is close\nto ADM [15], despite utilizing half its parameters and re-\nquiring 4-times less train resources (see Appendix E.3.5).\n\nTable 1.  Evaluation metrics for unconditional image synthesis.\nCelebA-HQ results reproduced from [43, 63, 100], FFHQ from\n[42, 43]. \u2020: $N$-s refers to $N$ sampling steps with the DDIM [84]\nsampler. *: trained in $KL$-regularized latent space. Additional re-\nsults can be found in the supplementary.\n\nTable 2.  Evaluation of text-conditional image synthesis on the 256 \u00d7 256-sized MS-COCO [51] dataset: with 250 DDIM [84] steps our model is on par with the most recent diffusion [59] and autoregressive [26] methods despite using significantly less parameters. \u2020/*: Numbers from [109]/[26]\n\nMoreover, *LDMs* consistently improve upon GAN-based methods in Precision and Recall, thus confirming the advantages of their mode-covering likelihood-based training objective over adversarial approaches. In Fig. 4 we also show qualitative results on each dataset."
        },
        {
            "text": "Moreover, *LDMs* consistently improve upon GAN-based methods in Precision and Recall, thus confirming the advantages of their mode-covering likelihood-based training objective over adversarial approaches. In Fig. 4 we also show qualitative results on each dataset.",
            "page": 6,
            "x": 306,
            "y": 653,
            "width": 242,
            "height": 61,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "10-image",
            "chunk_id": "dd54e853-43bc-43fb-94bd-153dad8733c5",
            "group_text": "## 4.2. Image Generation with Latent Diffusion\n\nWe train unconditional models of $256^2$ images on CelebA-HQ [39], FFHQ [41], LSUN-Churches and -Bedrooms [102] and evaluate the i) sample quality and ii) their coverage of the data manifold using ii) FID [29] and ii) Precision-and-Recall [50]. Tab. 1 summarizes our results. On CelebA-HQ, we report a new state-of-the-art FID of 5.11, outperforming previous likelihood-based models as well as GANs. We also outperform LSGM [93] where a latent diffusion model is trained jointly together with the first stage. In contrast, we train diffusion models in a fixed space\n\nText-to-Image Synthesis on LAION. 1.45B Model.\n\nFigure 5.  Samples for user-defined text prompts from our model for text-to-image synthesis, _LDM-8 (KL)_, which was trained on the LAION [78] database. Samples generated with 200 DDIM steps and $\\eta = 1.0$. We use unconditional guidance [32] with $s = 10.0$.\n\nand avoid the difficulty of weighing reconstruction quality\nagainst learning the prior over the latent space, see Fig. 1-2.\nWe outperform prior diffusion based approaches on all\nbut the LSUN-Bedrooms dataset, where our score is close\nto ADM [15], despite utilizing half its parameters and re-\nquiring 4-times less train resources (see Appendix E.3.5).\n\nTable 1.  Evaluation metrics for unconditional image synthesis.\nCelebA-HQ results reproduced from [43, 63, 100], FFHQ from\n[42, 43]. \u2020: $N$-s refers to $N$ sampling steps with the DDIM [84]\nsampler. *: trained in $KL$-regularized latent space. Additional re-\nsults can be found in the supplementary.\n\nTable 2.  Evaluation of text-conditional image synthesis on the 256 \u00d7 256-sized MS-COCO [51] dataset: with 250 DDIM [84] steps our model is on par with the most recent diffusion [59] and autoregressive [26] methods despite using significantly less parameters. \u2020/*: Numbers from [109]/[26]\n\nMoreover, *LDMs* consistently improve upon GAN-based methods in Precision and Recall, thus confirming the advantages of their mode-covering likelihood-based training objective over adversarial approaches. In Fig. 4 we also show qualitative results on each dataset."
        },
        {
            "text": "4.3. Conditional Latent Diffusion\n\n4.3.1 Transformer Encoders for LDMs\nBy introducing cross-attention based conditioning into LDMs we open them up for various conditioning modalities previously unexplored for diffusion models. For **text-to-image** image modeling, we train a 1.45B parameter *KL*-regularized *LDM* conditioned on language prompts on LAION-400M [78]. We employ the BERT-tokenizer [14] and implement $\\tau_\\theta$ as a transformer [97] to infer a latent code which is mapped into the UNet via (multi-head) cross-attention (Sec. 3.3). This combination of domain specific experts for learning a language representation and visual synthesis results in a powerful model, which generalizes well to complex, user-defined text prompts, *cf.* Fig. 8 and 5. For quantitative analysis, we follow prior work and evaluate text-to-image generation on the MS-COCO [51] validation set, where our model improves upon powerful AR [17, 66] and GAN-based [109] methods, *cf.* Tab. 2. We note that applying classifier-free diffusion guidance [32] greatly boosts sample quality, such that the guided *LDM-KL-8-G* is on par with the recent state-of-the-art AR [26] and diffusion models [59] for text-to-image synthesis, while substantially reducing parameter count. To further analyze the flexibility of the cross-attention based conditioning mechanism we also train models to synthesize images based on **semantic layouts** on OpenImages [49], and finetune on COCO [4], see Fig. 8. See Sec. D.3 for the quantitative evaluation and implementation details.",
            "page": 7,
            "x": 45,
            "y": 253,
            "width": 245,
            "height": 346,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-conditional",
            "chunk_id": "a96706db-80c7-4665-8ef9-92ccf5a1e948",
            "group_text": "4.3. Conditional Latent Diffusion\n\n4.3.1 Transformer Encoders for LDMs\nBy introducing cross-attention based conditioning into LDMs we open them up for various conditioning modalities previously unexplored for diffusion models. For **text-to-image** image modeling, we train a 1.45B parameter *KL*-regularized *LDM* conditioned on language prompts on LAION-400M [78]. We employ the BERT-tokenizer [14] and implement $\\tau_\\theta$ as a transformer [97] to infer a latent code which is mapped into the UNet via (multi-head) cross-attention (Sec. 3.3). This combination of domain specific experts for learning a language representation and visual synthesis results in a powerful model, which generalizes well to complex, user-defined text prompts, *cf.* Fig. 8 and 5. For quantitative analysis, we follow prior work and evaluate text-to-image generation on the MS-COCO [51] validation set, where our model improves upon powerful AR [17, 66] and GAN-based [109] methods, *cf.* Tab. 2. We note that applying classifier-free diffusion guidance [32] greatly boosts sample quality, such that the guided *LDM-KL-8-G* is on par with the recent state-of-the-art AR [26] and diffusion models [59] for text-to-image synthesis, while substantially reducing parameter count. To further analyze the flexibility of the cross-attention based conditioning mechanism we also train models to synthesize images based on **semantic layouts** on OpenImages [49], and finetune on COCO [4], see Fig. 8. See Sec. D.3 for the quantitative evaluation and implementation details.\n\nLastly, following prior work [3, 15, 21, 23], we evaluate our best-performing **class-conditional** ImageNet models with $f \\in \\{4, 8\\}$ from Sec. 4.1 in Tab. 3, Fig. 4 and Sec. D.4. Here we outperform the state of the art diffusion model ADM [15] while significantly reducing computational requirements and parameter count, _cf._ Tab 18.\n\n**4.3.2   Convolutional Sampling Beyond $256^2$**\nBy concatenating spatially aligned conditioning information to the input of $\\epsilon_\\theta$, *LDMs* can serve as efficient general-\n\npurpose image-to-image translation models. We use this\nto train models for semantic synthesis, super-resolution\n(Sec. 4.4) and inpainting (Sec. 4.5). For semantic synthe-\nsis, we use images of landscapes paired with semantic maps\n[23, 61] and concatenate downsampled versions of the se-\nmantic maps with the latent image representation of a $f = 4$\nmodel (VQ-reg., see Tab. 8). We train on an input resolution\nof $256^2$ (crops from $384^2$) but find that our model general-\nizes to larger resolutions and can generate images up to the\nmegapixel regime when evaluated in a convolutional man-\nner (see Fig. 9). We exploit this behavior to also apply the\nsuper-resolution models in Sec. 4.4 and the inpainting mod-\nels in Sec. 4.5 to generate large images between $512^2$ and\n$1024^2$. For this application, the signal-to-noise ratio (in-\nduced by the scale of the latent space) significantly affects\nthe results. In Sec. D.1 we illustrate this when learning an\nLDM on (i) the latent space as provided by a $f = 4$ model\n(KL-reg., see Tab. 8), and (ii) a rescaled version, scaled by\nthe component-wise standard deviation.\n\nThe latter, in combination with classifier-free guidance [32], also enables the direct synthesis of > 256\u00b2 images for the text-conditional *LDM-KL-8-G* as in Fig. 13."
        },
        {
            "text": "Lastly, following prior work [3, 15, 21, 23], we evaluate our best-performing **class-conditional** ImageNet models with $f \\in \\{4, 8\\}$ from Sec. 4.1 in Tab. 3, Fig. 4 and Sec. D.4. Here we outperform the state of the art diffusion model ADM [15] while significantly reducing computational requirements and parameter count, _cf._ Tab 18.",
            "page": 7,
            "x": 46,
            "y": 600,
            "width": 244,
            "height": 73,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-conditional",
            "chunk_id": "da2ce1c3-5912-4989-80cb-18687d405dbd",
            "group_text": "4.3. Conditional Latent Diffusion\n\n4.3.1 Transformer Encoders for LDMs\nBy introducing cross-attention based conditioning into LDMs we open them up for various conditioning modalities previously unexplored for diffusion models. For **text-to-image** image modeling, we train a 1.45B parameter *KL*-regularized *LDM* conditioned on language prompts on LAION-400M [78]. We employ the BERT-tokenizer [14] and implement $\\tau_\\theta$ as a transformer [97] to infer a latent code which is mapped into the UNet via (multi-head) cross-attention (Sec. 3.3). This combination of domain specific experts for learning a language representation and visual synthesis results in a powerful model, which generalizes well to complex, user-defined text prompts, *cf.* Fig. 8 and 5. For quantitative analysis, we follow prior work and evaluate text-to-image generation on the MS-COCO [51] validation set, where our model improves upon powerful AR [17, 66] and GAN-based [109] methods, *cf.* Tab. 2. We note that applying classifier-free diffusion guidance [32] greatly boosts sample quality, such that the guided *LDM-KL-8-G* is on par with the recent state-of-the-art AR [26] and diffusion models [59] for text-to-image synthesis, while substantially reducing parameter count. To further analyze the flexibility of the cross-attention based conditioning mechanism we also train models to synthesize images based on **semantic layouts** on OpenImages [49], and finetune on COCO [4], see Fig. 8. See Sec. D.3 for the quantitative evaluation and implementation details.\n\nLastly, following prior work [3, 15, 21, 23], we evaluate our best-performing **class-conditional** ImageNet models with $f \\in \\{4, 8\\}$ from Sec. 4.1 in Tab. 3, Fig. 4 and Sec. D.4. Here we outperform the state of the art diffusion model ADM [15] while significantly reducing computational requirements and parameter count, _cf._ Tab 18.\n\n**4.3.2   Convolutional Sampling Beyond $256^2$**\nBy concatenating spatially aligned conditioning information to the input of $\\epsilon_\\theta$, *LDMs* can serve as efficient general-\n\npurpose image-to-image translation models. We use this\nto train models for semantic synthesis, super-resolution\n(Sec. 4.4) and inpainting (Sec. 4.5). For semantic synthe-\nsis, we use images of landscapes paired with semantic maps\n[23, 61] and concatenate downsampled versions of the se-\nmantic maps with the latent image representation of a $f = 4$\nmodel (VQ-reg., see Tab. 8). We train on an input resolution\nof $256^2$ (crops from $384^2$) but find that our model general-\nizes to larger resolutions and can generate images up to the\nmegapixel regime when evaluated in a convolutional man-\nner (see Fig. 9). We exploit this behavior to also apply the\nsuper-resolution models in Sec. 4.4 and the inpainting mod-\nels in Sec. 4.5 to generate large images between $512^2$ and\n$1024^2$. For this application, the signal-to-noise ratio (in-\nduced by the scale of the latent space) significantly affects\nthe results. In Sec. D.1 we illustrate this when learning an\nLDM on (i) the latent space as provided by a $f = 4$ model\n(KL-reg., see Tab. 8), and (ii) a rescaled version, scaled by\nthe component-wise standard deviation.\n\nThe latter, in combination with classifier-free guidance [32], also enables the direct synthesis of > 256\u00b2 images for the text-conditional *LDM-KL-8-G* as in Fig. 13."
        },
        {
            "text": "**4.3.2   Convolutional Sampling Beyond $256^2$**\nBy concatenating spatially aligned conditioning information to the input of $\\epsilon_\\theta$, *LDMs* can serve as efficient general-",
            "page": 7,
            "x": 46,
            "y": 676,
            "width": 244,
            "height": 40,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-conditional",
            "chunk_id": "b6fafd30-dc56-4be4-87b4-408c29bddbec",
            "group_text": "4.3. Conditional Latent Diffusion\n\n4.3.1 Transformer Encoders for LDMs\nBy introducing cross-attention based conditioning into LDMs we open them up for various conditioning modalities previously unexplored for diffusion models. For **text-to-image** image modeling, we train a 1.45B parameter *KL*-regularized *LDM* conditioned on language prompts on LAION-400M [78]. We employ the BERT-tokenizer [14] and implement $\\tau_\\theta$ as a transformer [97] to infer a latent code which is mapped into the UNet via (multi-head) cross-attention (Sec. 3.3). This combination of domain specific experts for learning a language representation and visual synthesis results in a powerful model, which generalizes well to complex, user-defined text prompts, *cf.* Fig. 8 and 5. For quantitative analysis, we follow prior work and evaluate text-to-image generation on the MS-COCO [51] validation set, where our model improves upon powerful AR [17, 66] and GAN-based [109] methods, *cf.* Tab. 2. We note that applying classifier-free diffusion guidance [32] greatly boosts sample quality, such that the guided *LDM-KL-8-G* is on par with the recent state-of-the-art AR [26] and diffusion models [59] for text-to-image synthesis, while substantially reducing parameter count. To further analyze the flexibility of the cross-attention based conditioning mechanism we also train models to synthesize images based on **semantic layouts** on OpenImages [49], and finetune on COCO [4], see Fig. 8. See Sec. D.3 for the quantitative evaluation and implementation details.\n\nLastly, following prior work [3, 15, 21, 23], we evaluate our best-performing **class-conditional** ImageNet models with $f \\in \\{4, 8\\}$ from Sec. 4.1 in Tab. 3, Fig. 4 and Sec. D.4. Here we outperform the state of the art diffusion model ADM [15] while significantly reducing computational requirements and parameter count, _cf._ Tab 18.\n\n**4.3.2   Convolutional Sampling Beyond $256^2$**\nBy concatenating spatially aligned conditioning information to the input of $\\epsilon_\\theta$, *LDMs* can serve as efficient general-\n\npurpose image-to-image translation models. We use this\nto train models for semantic synthesis, super-resolution\n(Sec. 4.4) and inpainting (Sec. 4.5). For semantic synthe-\nsis, we use images of landscapes paired with semantic maps\n[23, 61] and concatenate downsampled versions of the se-\nmantic maps with the latent image representation of a $f = 4$\nmodel (VQ-reg., see Tab. 8). We train on an input resolution\nof $256^2$ (crops from $384^2$) but find that our model general-\nizes to larger resolutions and can generate images up to the\nmegapixel regime when evaluated in a convolutional man-\nner (see Fig. 9). We exploit this behavior to also apply the\nsuper-resolution models in Sec. 4.4 and the inpainting mod-\nels in Sec. 4.5 to generate large images between $512^2$ and\n$1024^2$. For this application, the signal-to-noise ratio (in-\nduced by the scale of the latent space) significantly affects\nthe results. In Sec. D.1 we illustrate this when learning an\nLDM on (i) the latent space as provided by a $f = 4$ model\n(KL-reg., see Tab. 8), and (ii) a rescaled version, scaled by\nthe component-wise standard deviation.\n\nThe latter, in combination with classifier-free guidance [32], also enables the direct synthesis of > 256\u00b2 images for the text-conditional *LDM-KL-8-G* as in Fig. 13."
        },
        {
            "text": "purpose image-to-image translation models. We use this\nto train models for semantic synthesis, super-resolution\n(Sec. 4.4) and inpainting (Sec. 4.5). For semantic synthe-\nsis, we use images of landscapes paired with semantic maps\n[23, 61] and concatenate downsampled versions of the se-\nmantic maps with the latent image representation of a $f = 4$\nmodel (VQ-reg., see Tab. 8). We train on an input resolution\nof $256^2$ (crops from $384^2$) but find that our model general-\nizes to larger resolutions and can generate images up to the\nmegapixel regime when evaluated in a convolutional man-\nner (see Fig. 9). We exploit this behavior to also apply the\nsuper-resolution models in Sec. 4.4 and the inpainting mod-\nels in Sec. 4.5 to generate large images between $512^2$ and\n$1024^2$. For this application, the signal-to-noise ratio (in-\nduced by the scale of the latent space) significantly affects\nthe results. In Sec. D.1 we illustrate this when learning an\nLDM on (i) the latent space as provided by a $f = 4$ model\n(KL-reg., see Tab. 8), and (ii) a rescaled version, scaled by\nthe component-wise standard deviation.",
            "page": 7,
            "x": 305,
            "y": 204,
            "width": 243,
            "height": 227,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-conditional",
            "chunk_id": "0c7bc9ed-596f-4ebc-b261-777549e44ada",
            "group_text": "4.3. Conditional Latent Diffusion\n\n4.3.1 Transformer Encoders for LDMs\nBy introducing cross-attention based conditioning into LDMs we open them up for various conditioning modalities previously unexplored for diffusion models. For **text-to-image** image modeling, we train a 1.45B parameter *KL*-regularized *LDM* conditioned on language prompts on LAION-400M [78]. We employ the BERT-tokenizer [14] and implement $\\tau_\\theta$ as a transformer [97] to infer a latent code which is mapped into the UNet via (multi-head) cross-attention (Sec. 3.3). This combination of domain specific experts for learning a language representation and visual synthesis results in a powerful model, which generalizes well to complex, user-defined text prompts, *cf.* Fig. 8 and 5. For quantitative analysis, we follow prior work and evaluate text-to-image generation on the MS-COCO [51] validation set, where our model improves upon powerful AR [17, 66] and GAN-based [109] methods, *cf.* Tab. 2. We note that applying classifier-free diffusion guidance [32] greatly boosts sample quality, such that the guided *LDM-KL-8-G* is on par with the recent state-of-the-art AR [26] and diffusion models [59] for text-to-image synthesis, while substantially reducing parameter count. To further analyze the flexibility of the cross-attention based conditioning mechanism we also train models to synthesize images based on **semantic layouts** on OpenImages [49], and finetune on COCO [4], see Fig. 8. See Sec. D.3 for the quantitative evaluation and implementation details.\n\nLastly, following prior work [3, 15, 21, 23], we evaluate our best-performing **class-conditional** ImageNet models with $f \\in \\{4, 8\\}$ from Sec. 4.1 in Tab. 3, Fig. 4 and Sec. D.4. Here we outperform the state of the art diffusion model ADM [15] while significantly reducing computational requirements and parameter count, _cf._ Tab 18.\n\n**4.3.2   Convolutional Sampling Beyond $256^2$**\nBy concatenating spatially aligned conditioning information to the input of $\\epsilon_\\theta$, *LDMs* can serve as efficient general-\n\npurpose image-to-image translation models. We use this\nto train models for semantic synthesis, super-resolution\n(Sec. 4.4) and inpainting (Sec. 4.5). For semantic synthe-\nsis, we use images of landscapes paired with semantic maps\n[23, 61] and concatenate downsampled versions of the se-\nmantic maps with the latent image representation of a $f = 4$\nmodel (VQ-reg., see Tab. 8). We train on an input resolution\nof $256^2$ (crops from $384^2$) but find that our model general-\nizes to larger resolutions and can generate images up to the\nmegapixel regime when evaluated in a convolutional man-\nner (see Fig. 9). We exploit this behavior to also apply the\nsuper-resolution models in Sec. 4.4 and the inpainting mod-\nels in Sec. 4.5 to generate large images between $512^2$ and\n$1024^2$. For this application, the signal-to-noise ratio (in-\nduced by the scale of the latent space) significantly affects\nthe results. In Sec. D.1 we illustrate this when learning an\nLDM on (i) the latent space as provided by a $f = 4$ model\n(KL-reg., see Tab. 8), and (ii) a rescaled version, scaled by\nthe component-wise standard deviation.\n\nThe latter, in combination with classifier-free guidance [32], also enables the direct synthesis of > 256\u00b2 images for the text-conditional *LDM-KL-8-G* as in Fig. 13."
        },
        {
            "text": "The latter, in combination with classifier-free guidance [32], also enables the direct synthesis of > 256\u00b2 images for the text-conditional *LDM-KL-8-G* as in Fig. 13.",
            "page": 7,
            "x": 305,
            "y": 432,
            "width": 244,
            "height": 37,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-conditional",
            "chunk_id": "8476c6f0-7baf-48ea-b7a8-15e74363fde8",
            "group_text": "4.3. Conditional Latent Diffusion\n\n4.3.1 Transformer Encoders for LDMs\nBy introducing cross-attention based conditioning into LDMs we open them up for various conditioning modalities previously unexplored for diffusion models. For **text-to-image** image modeling, we train a 1.45B parameter *KL*-regularized *LDM* conditioned on language prompts on LAION-400M [78]. We employ the BERT-tokenizer [14] and implement $\\tau_\\theta$ as a transformer [97] to infer a latent code which is mapped into the UNet via (multi-head) cross-attention (Sec. 3.3). This combination of domain specific experts for learning a language representation and visual synthesis results in a powerful model, which generalizes well to complex, user-defined text prompts, *cf.* Fig. 8 and 5. For quantitative analysis, we follow prior work and evaluate text-to-image generation on the MS-COCO [51] validation set, where our model improves upon powerful AR [17, 66] and GAN-based [109] methods, *cf.* Tab. 2. We note that applying classifier-free diffusion guidance [32] greatly boosts sample quality, such that the guided *LDM-KL-8-G* is on par with the recent state-of-the-art AR [26] and diffusion models [59] for text-to-image synthesis, while substantially reducing parameter count. To further analyze the flexibility of the cross-attention based conditioning mechanism we also train models to synthesize images based on **semantic layouts** on OpenImages [49], and finetune on COCO [4], see Fig. 8. See Sec. D.3 for the quantitative evaluation and implementation details.\n\nLastly, following prior work [3, 15, 21, 23], we evaluate our best-performing **class-conditional** ImageNet models with $f \\in \\{4, 8\\}$ from Sec. 4.1 in Tab. 3, Fig. 4 and Sec. D.4. Here we outperform the state of the art diffusion model ADM [15] while significantly reducing computational requirements and parameter count, _cf._ Tab 18.\n\n**4.3.2   Convolutional Sampling Beyond $256^2$**\nBy concatenating spatially aligned conditioning information to the input of $\\epsilon_\\theta$, *LDMs* can serve as efficient general-\n\npurpose image-to-image translation models. We use this\nto train models for semantic synthesis, super-resolution\n(Sec. 4.4) and inpainting (Sec. 4.5). For semantic synthe-\nsis, we use images of landscapes paired with semantic maps\n[23, 61] and concatenate downsampled versions of the se-\nmantic maps with the latent image representation of a $f = 4$\nmodel (VQ-reg., see Tab. 8). We train on an input resolution\nof $256^2$ (crops from $384^2$) but find that our model general-\nizes to larger resolutions and can generate images up to the\nmegapixel regime when evaluated in a convolutional man-\nner (see Fig. 9). We exploit this behavior to also apply the\nsuper-resolution models in Sec. 4.4 and the inpainting mod-\nels in Sec. 4.5 to generate large images between $512^2$ and\n$1024^2$. For this application, the signal-to-noise ratio (in-\nduced by the scale of the latent space) significantly affects\nthe results. In Sec. D.1 we illustrate this when learning an\nLDM on (i) the latent space as provided by a $f = 4$ model\n(KL-reg., see Tab. 8), and (ii) a rescaled version, scaled by\nthe component-wise standard deviation.\n\nThe latter, in combination with classifier-free guidance [32], also enables the direct synthesis of > 256\u00b2 images for the text-conditional *LDM-KL-8-G* as in Fig. 13."
        },
        {
            "text": "### 4.4. Super-Resolution with Latent Diffusion\nLDMs can be efficiently trained for super-resolution by directly conditioning on low-resolution images via concatenation (*cf.* Sec. 3.3). In a first experiment, we follow SR3",
            "page": 7,
            "x": 305,
            "y": 663,
            "width": 242,
            "height": 53,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-super",
            "chunk_id": "fc04bc84-9f44-4d7e-ba7b-4323a58016d4",
            "group_text": "### 4.4. Super-Resolution with Latent Diffusion\nLDMs can be efficiently trained for super-resolution by directly conditioning on low-resolution images via concatenation (*cf.* Sec. 3.3). In a first experiment, we follow SR3\n\n[72] and fix the image degradation to a bicubic interpolation\nwith 4\u00d7-downsampling and train on ImageNet follow-\ning SR3\u2019s data processing pipeline. We use the $f = 4$ au-\ntoencoding model pretrained on OpenImages (VQ-reg., cf.\nTab. 8) and concatenate the low-resolution conditioning $y$\nand the inputs to the UNet, i.e. $\\tau_\\theta$ is the identity. Our qualita-\ntive and quantitative results (see Fig. 10 and Tab. 5) show\ncompetitive performance and LDM-SR outperforms SR3\nin FID while SR3 has a better IS. A simple image regres-\nsion model achieves the highest PSNR and SSIM scores;\nhowever these metrics do not align well with human per-\nception [106] and favor blurriness over imperfectly aligned\nhigh frequency details [72]. Further, we conduct a user\nstudy comparing the pixel-baseline with LDM-SR. We fol-\nlow SR3 [72] where human subjects were shown a low-res\nimage in between two high-res images and asked for pref-\nerence. The results in Tab. 4 affirm the good performance\nof LDM-SR. PSNR and SSIM can be pushed by using a\npost-hoc guiding mechanism [15] and we implement this\nimage-based guider via a perceptual loss, see Sec. D.6.\n\n<table>\n  <tr>\n    <th></th>\n    <th colspan=\"2\">SR on ImageNet</th>\n    <th colspan=\"2\">Inpainting on Places</th>\n  </tr>\n  <tr>\n    <th>User Study</th>\n    <th>Pixel-DM (f1)</th>\n    <th><em>LDM-4</em></th>\n    <th>LAMA [88]</th>\n    <th><em>LDM-4</em></th>\n  </tr>\n  <tr>\n    <td><strong>Task 1: Preference vs GT \u2191</strong></td>\n    <td>16.0%</td>\n    <td><strong>30.4%</strong></td>\n    <td>13.6%</td>\n    <td><strong>21.0%</strong></td>\n  </tr>\n  <tr>\n    <td><strong>Task 2: Preference Score \u2191</strong></td>\n    <td>29.4%</td>\n    <td><strong>70.6%</strong></td>\n    <td>31.9%</td>\n    <td><strong>68.1%</strong></td>\n  </tr>\n</table>\n\nTable 4. Task 1: Subjects were shown ground truth and generated image and asked for preference. Task 2: Subjects had to decide between two generated images. More details in E.3.6\n\nSince the bicubic degradation process does not generalize\nwell to images which do not follow this pre-processing, we\nalso train a generic model, _LDM-BSR_, by using more di-\nverse degradation. The results are shown in Sec. D.6.1."
        },
        {
            "text": "[72] and fix the image degradation to a bicubic interpolation\nwith 4\u00d7-downsampling and train on ImageNet follow-\ning SR3\u2019s data processing pipeline. We use the $f = 4$ au-\ntoencoding model pretrained on OpenImages (VQ-reg., cf.\nTab. 8) and concatenate the low-resolution conditioning $y$\nand the inputs to the UNet, i.e. $\\tau_\\theta$ is the identity. Our qualita-\ntive and quantitative results (see Fig. 10 and Tab. 5) show\ncompetitive performance and LDM-SR outperforms SR3\nin FID while SR3 has a better IS. A simple image regres-\nsion model achieves the highest PSNR and SSIM scores;\nhowever these metrics do not align well with human per-\nception [106] and favor blurriness over imperfectly aligned\nhigh frequency details [72]. Further, we conduct a user\nstudy comparing the pixel-baseline with LDM-SR. We fol-\nlow SR3 [72] where human subjects were shown a low-res\nimage in between two high-res images and asked for pref-\nerence. The results in Tab. 4 affirm the good performance\nof LDM-SR. PSNR and SSIM can be pushed by using a\npost-hoc guiding mechanism [15] and we implement this\nimage-based guider via a perceptual loss, see Sec. D.6.",
            "page": 8,
            "x": 45,
            "y": 304,
            "width": 245,
            "height": 245,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-super",
            "chunk_id": "197648cf-74f3-4379-9222-d4a3eb0a5f27",
            "group_text": "### 4.4. Super-Resolution with Latent Diffusion\nLDMs can be efficiently trained for super-resolution by directly conditioning on low-resolution images via concatenation (*cf.* Sec. 3.3). In a first experiment, we follow SR3\n\n[72] and fix the image degradation to a bicubic interpolation\nwith 4\u00d7-downsampling and train on ImageNet follow-\ning SR3\u2019s data processing pipeline. We use the $f = 4$ au-\ntoencoding model pretrained on OpenImages (VQ-reg., cf.\nTab. 8) and concatenate the low-resolution conditioning $y$\nand the inputs to the UNet, i.e. $\\tau_\\theta$ is the identity. Our qualita-\ntive and quantitative results (see Fig. 10 and Tab. 5) show\ncompetitive performance and LDM-SR outperforms SR3\nin FID while SR3 has a better IS. A simple image regres-\nsion model achieves the highest PSNR and SSIM scores;\nhowever these metrics do not align well with human per-\nception [106] and favor blurriness over imperfectly aligned\nhigh frequency details [72]. Further, we conduct a user\nstudy comparing the pixel-baseline with LDM-SR. We fol-\nlow SR3 [72] where human subjects were shown a low-res\nimage in between two high-res images and asked for pref-\nerence. The results in Tab. 4 affirm the good performance\nof LDM-SR. PSNR and SSIM can be pushed by using a\npost-hoc guiding mechanism [15] and we implement this\nimage-based guider via a perceptual loss, see Sec. D.6.\n\n<table>\n  <tr>\n    <th></th>\n    <th colspan=\"2\">SR on ImageNet</th>\n    <th colspan=\"2\">Inpainting on Places</th>\n  </tr>\n  <tr>\n    <th>User Study</th>\n    <th>Pixel-DM (f1)</th>\n    <th><em>LDM-4</em></th>\n    <th>LAMA [88]</th>\n    <th><em>LDM-4</em></th>\n  </tr>\n  <tr>\n    <td><strong>Task 1: Preference vs GT \u2191</strong></td>\n    <td>16.0%</td>\n    <td><strong>30.4%</strong></td>\n    <td>13.6%</td>\n    <td><strong>21.0%</strong></td>\n  </tr>\n  <tr>\n    <td><strong>Task 2: Preference Score \u2191</strong></td>\n    <td>29.4%</td>\n    <td><strong>70.6%</strong></td>\n    <td>31.9%</td>\n    <td><strong>68.1%</strong></td>\n  </tr>\n</table>\n\nTable 4. Task 1: Subjects were shown ground truth and generated image and asked for preference. Task 2: Subjects had to decide between two generated images. More details in E.3.6\n\nSince the bicubic degradation process does not generalize\nwell to images which do not follow this pre-processing, we\nalso train a generic model, _LDM-BSR_, by using more di-\nverse degradation. The results are shown in Sec. D.6.1."
        },
        {
            "text": "<table>\n  <tr>\n    <th></th>\n    <th colspan=\"2\">SR on ImageNet</th>\n    <th colspan=\"2\">Inpainting on Places</th>\n  </tr>\n  <tr>\n    <th>User Study</th>\n    <th>Pixel-DM (f1)</th>\n    <th><em>LDM-4</em></th>\n    <th>LAMA [88]</th>\n    <th><em>LDM-4</em></th>\n  </tr>\n  <tr>\n    <td><strong>Task 1: Preference vs GT \u2191</strong></td>\n    <td>16.0%</td>\n    <td><strong>30.4%</strong></td>\n    <td>13.6%</td>\n    <td><strong>21.0%</strong></td>\n  </tr>\n  <tr>\n    <td><strong>Task 2: Preference Score \u2191</strong></td>\n    <td>29.4%</td>\n    <td><strong>70.6%</strong></td>\n    <td>31.9%</td>\n    <td><strong>68.1%</strong></td>\n  </tr>\n</table>\n\nTable 4. Task 1: Subjects were shown ground truth and generated image and asked for preference. Task 2: Subjects had to decide between two generated images. More details in E.3.6",
            "page": 8,
            "x": 46,
            "y": 567,
            "width": 243,
            "height": 84,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-super",
            "chunk_id": "42a3c616-e760-43a1-9070-12f2d47ecfc2",
            "group_text": "### 4.4. Super-Resolution with Latent Diffusion\nLDMs can be efficiently trained for super-resolution by directly conditioning on low-resolution images via concatenation (*cf.* Sec. 3.3). In a first experiment, we follow SR3\n\n[72] and fix the image degradation to a bicubic interpolation\nwith 4\u00d7-downsampling and train on ImageNet follow-\ning SR3\u2019s data processing pipeline. We use the $f = 4$ au-\ntoencoding model pretrained on OpenImages (VQ-reg., cf.\nTab. 8) and concatenate the low-resolution conditioning $y$\nand the inputs to the UNet, i.e. $\\tau_\\theta$ is the identity. Our qualita-\ntive and quantitative results (see Fig. 10 and Tab. 5) show\ncompetitive performance and LDM-SR outperforms SR3\nin FID while SR3 has a better IS. A simple image regres-\nsion model achieves the highest PSNR and SSIM scores;\nhowever these metrics do not align well with human per-\nception [106] and favor blurriness over imperfectly aligned\nhigh frequency details [72]. Further, we conduct a user\nstudy comparing the pixel-baseline with LDM-SR. We fol-\nlow SR3 [72] where human subjects were shown a low-res\nimage in between two high-res images and asked for pref-\nerence. The results in Tab. 4 affirm the good performance\nof LDM-SR. PSNR and SSIM can be pushed by using a\npost-hoc guiding mechanism [15] and we implement this\nimage-based guider via a perceptual loss, see Sec. D.6.\n\n<table>\n  <tr>\n    <th></th>\n    <th colspan=\"2\">SR on ImageNet</th>\n    <th colspan=\"2\">Inpainting on Places</th>\n  </tr>\n  <tr>\n    <th>User Study</th>\n    <th>Pixel-DM (f1)</th>\n    <th><em>LDM-4</em></th>\n    <th>LAMA [88]</th>\n    <th><em>LDM-4</em></th>\n  </tr>\n  <tr>\n    <td><strong>Task 1: Preference vs GT \u2191</strong></td>\n    <td>16.0%</td>\n    <td><strong>30.4%</strong></td>\n    <td>13.6%</td>\n    <td><strong>21.0%</strong></td>\n  </tr>\n  <tr>\n    <td><strong>Task 2: Preference Score \u2191</strong></td>\n    <td>29.4%</td>\n    <td><strong>70.6%</strong></td>\n    <td>31.9%</td>\n    <td><strong>68.1%</strong></td>\n  </tr>\n</table>\n\nTable 4. Task 1: Subjects were shown ground truth and generated image and asked for preference. Task 2: Subjects had to decide between two generated images. More details in E.3.6\n\nSince the bicubic degradation process does not generalize\nwell to images which do not follow this pre-processing, we\nalso train a generic model, _LDM-BSR_, by using more di-\nverse degradation. The results are shown in Sec. D.6.1."
        },
        {
            "text": "Since the bicubic degradation process does not generalize\nwell to images which do not follow this pre-processing, we\nalso train a generic model, _LDM-BSR_, by using more di-\nverse degradation. The results are shown in Sec. D.6.1.",
            "page": 8,
            "x": 46,
            "y": 663,
            "width": 243,
            "height": 53,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-super",
            "chunk_id": "57b1a6dc-e498-4108-88eb-f6730362c6a3",
            "group_text": "### 4.4. Super-Resolution with Latent Diffusion\nLDMs can be efficiently trained for super-resolution by directly conditioning on low-resolution images via concatenation (*cf.* Sec. 3.3). In a first experiment, we follow SR3\n\n[72] and fix the image degradation to a bicubic interpolation\nwith 4\u00d7-downsampling and train on ImageNet follow-\ning SR3\u2019s data processing pipeline. We use the $f = 4$ au-\ntoencoding model pretrained on OpenImages (VQ-reg., cf.\nTab. 8) and concatenate the low-resolution conditioning $y$\nand the inputs to the UNet, i.e. $\\tau_\\theta$ is the identity. Our qualita-\ntive and quantitative results (see Fig. 10 and Tab. 5) show\ncompetitive performance and LDM-SR outperforms SR3\nin FID while SR3 has a better IS. A simple image regres-\nsion model achieves the highest PSNR and SSIM scores;\nhowever these metrics do not align well with human per-\nception [106] and favor blurriness over imperfectly aligned\nhigh frequency details [72]. Further, we conduct a user\nstudy comparing the pixel-baseline with LDM-SR. We fol-\nlow SR3 [72] where human subjects were shown a low-res\nimage in between two high-res images and asked for pref-\nerence. The results in Tab. 4 affirm the good performance\nof LDM-SR. PSNR and SSIM can be pushed by using a\npost-hoc guiding mechanism [15] and we implement this\nimage-based guider via a perceptual loss, see Sec. D.6.\n\n<table>\n  <tr>\n    <th></th>\n    <th colspan=\"2\">SR on ImageNet</th>\n    <th colspan=\"2\">Inpainting on Places</th>\n  </tr>\n  <tr>\n    <th>User Study</th>\n    <th>Pixel-DM (f1)</th>\n    <th><em>LDM-4</em></th>\n    <th>LAMA [88]</th>\n    <th><em>LDM-4</em></th>\n  </tr>\n  <tr>\n    <td><strong>Task 1: Preference vs GT \u2191</strong></td>\n    <td>16.0%</td>\n    <td><strong>30.4%</strong></td>\n    <td>13.6%</td>\n    <td><strong>21.0%</strong></td>\n  </tr>\n  <tr>\n    <td><strong>Task 2: Preference Score \u2191</strong></td>\n    <td>29.4%</td>\n    <td><strong>70.6%</strong></td>\n    <td>31.9%</td>\n    <td><strong>68.1%</strong></td>\n  </tr>\n</table>\n\nTable 4. Task 1: Subjects were shown ground truth and generated image and asked for preference. Task 2: Subjects had to decide between two generated images. More details in E.3.6\n\nSince the bicubic degradation process does not generalize\nwell to images which do not follow this pre-processing, we\nalso train a generic model, _LDM-BSR_, by using more di-\nverse degradation. The results are shown in Sec. D.6.1."
        },
        {
            "text": "### 4.5. Inpainting with Latent Diffusion\n\nInpainting is the task of filling masked regions of an image with new content either because parts of the image are are corrupted or to replace existing but undesired content within the image. We evaluate how our general approach for conditional image generation compares to more specialized, state-of-the-art approaches for this task. Our evaluation follows the protocol of LaMa [88], a recent inpainting model that introduces a specialized architecture relying on Fast Fourier Convolutions [8]. The exact training & evaluation protocol on Places [108] is described in Sec. E.2.2.",
            "page": 8,
            "x": 304,
            "y": 239,
            "width": 244,
            "height": 136,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "13-inpainting",
            "chunk_id": "dc1259e7-55c6-4135-b4f5-03ff77cdfb72",
            "group_text": "### 4.5. Inpainting with Latent Diffusion\n\nInpainting is the task of filling masked regions of an image with new content either because parts of the image are are corrupted or to replace existing but undesired content within the image. We evaluate how our general approach for conditional image generation compares to more specialized, state-of-the-art approaches for this task. Our evaluation follows the protocol of LaMa [88], a recent inpainting model that introduces a specialized architecture relying on Fast Fourier Convolutions [8]. The exact training & evaluation protocol on Places [108] is described in Sec. E.2.2.\n\nWe first analyze the effect of different design choices for the first stage. In particular, we compare the inpainting efficiency of *LDM-1* (*i.e.* a pixel-based conditional DM) with *LDM-4*, for both *KL* and *VQ* regularizations, as well as *VQ-LDM-4* without any attention in the first stage (see Tab. 8), where the latter reduces GPU memory for decoding at high resolutions. For comparability, we fix the number of parameters for all models. Tab. 6 reports the training and sampling throughput at resolution $256^2$ and $512^2$, the total training time in hours per epoch and the FID score on the validation split after six epochs. Overall, we observe a speed-up of at least $2.7\\times$ between pixel- and latent-based diffusion models while improving FID scores by a factor of at least $1.6\\times$.\n\nThe comparison with other inpainting approaches in\nTab. 7 shows that our model with attention improves the\noverall image quality as measured by FID over that of [88].\nLPIPS between the unmasked images and our samples is\nslightly higher than that of [88]. We attribute this to [88]\nonly producing a single result which tends to recover more\nof an average image compared to the diverse results pro-\nduced by our LDM _cf._ Fig. 21. Additionally in a user study\n(Tab. 4) human subjects favor our results over those of [88].\n\nBased on these initial results, we also trained a larger dif-\nfusion model (_big_ in Tab. 7) in the latent space of the VQ-\nregularized first stage without attention. Following [15],\nthe UNet of this diffusion model uses attention layers on\nthree levels of its feature hierarchy, the BigGAN [3] residual\nblock for up- and downsampling and has 387M parameters\n\ninstead of 215M. After training, we noticed a discrepancy\nin the quality of samples produced at resolutions 256\u00b2 and\n512\u00b2, which we hypothesize to be caused by the additional\nattention modules. However, fine-tuning the model for half\nan epoch at resolution 512\u00b2 allows the model to adjust to\nthe new feature statistics and sets a new state of the art FID\non image inpainting (*big, w/o attn, w/f* in Tab. 7, Fig. 11.)."
        },
        {
            "text": "We first analyze the effect of different design choices for the first stage. In particular, we compare the inpainting efficiency of *LDM-1* (*i.e.* a pixel-based conditional DM) with *LDM-4*, for both *KL* and *VQ* regularizations, as well as *VQ-LDM-4* without any attention in the first stage (see Tab. 8), where the latter reduces GPU memory for decoding at high resolutions. For comparability, we fix the number of parameters for all models. Tab. 6 reports the training and sampling throughput at resolution $256^2$ and $512^2$, the total training time in hours per epoch and the FID score on the validation split after six epochs. Overall, we observe a speed-up of at least $2.7\\times$ between pixel- and latent-based diffusion models while improving FID scores by a factor of at least $1.6\\times$.",
            "page": 8,
            "x": 304,
            "y": 376,
            "width": 245,
            "height": 156,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "13-inpainting",
            "chunk_id": "c73c310d-72ae-4d38-9fa0-99a405a41ea1",
            "group_text": "### 4.5. Inpainting with Latent Diffusion\n\nInpainting is the task of filling masked regions of an image with new content either because parts of the image are are corrupted or to replace existing but undesired content within the image. We evaluate how our general approach for conditional image generation compares to more specialized, state-of-the-art approaches for this task. Our evaluation follows the protocol of LaMa [88], a recent inpainting model that introduces a specialized architecture relying on Fast Fourier Convolutions [8]. The exact training & evaluation protocol on Places [108] is described in Sec. E.2.2.\n\nWe first analyze the effect of different design choices for the first stage. In particular, we compare the inpainting efficiency of *LDM-1* (*i.e.* a pixel-based conditional DM) with *LDM-4*, for both *KL* and *VQ* regularizations, as well as *VQ-LDM-4* without any attention in the first stage (see Tab. 8), where the latter reduces GPU memory for decoding at high resolutions. For comparability, we fix the number of parameters for all models. Tab. 6 reports the training and sampling throughput at resolution $256^2$ and $512^2$, the total training time in hours per epoch and the FID score on the validation split after six epochs. Overall, we observe a speed-up of at least $2.7\\times$ between pixel- and latent-based diffusion models while improving FID scores by a factor of at least $1.6\\times$.\n\nThe comparison with other inpainting approaches in\nTab. 7 shows that our model with attention improves the\noverall image quality as measured by FID over that of [88].\nLPIPS between the unmasked images and our samples is\nslightly higher than that of [88]. We attribute this to [88]\nonly producing a single result which tends to recover more\nof an average image compared to the diverse results pro-\nduced by our LDM _cf._ Fig. 21. Additionally in a user study\n(Tab. 4) human subjects favor our results over those of [88].\n\nBased on these initial results, we also trained a larger dif-\nfusion model (_big_ in Tab. 7) in the latent space of the VQ-\nregularized first stage without attention. Following [15],\nthe UNet of this diffusion model uses attention layers on\nthree levels of its feature hierarchy, the BigGAN [3] residual\nblock for up- and downsampling and has 387M parameters\n\ninstead of 215M. After training, we noticed a discrepancy\nin the quality of samples produced at resolutions 256\u00b2 and\n512\u00b2, which we hypothesize to be caused by the additional\nattention modules. However, fine-tuning the model for half\nan epoch at resolution 512\u00b2 allows the model to adjust to\nthe new feature statistics and sets a new state of the art FID\non image inpainting (*big, w/o attn, w/f* in Tab. 7, Fig. 11.)."
        },
        {
            "text": "The comparison with other inpainting approaches in\nTab. 7 shows that our model with attention improves the\noverall image quality as measured by FID over that of [88].\nLPIPS between the unmasked images and our samples is\nslightly higher than that of [88]. We attribute this to [88]\nonly producing a single result which tends to recover more\nof an average image compared to the diverse results pro-\nduced by our LDM _cf._ Fig. 21. Additionally in a user study\n(Tab. 4) human subjects favor our results over those of [88].\n\nBased on these initial results, we also trained a larger dif-\nfusion model (_big_ in Tab. 7) in the latent space of the VQ-\nregularized first stage without attention. Following [15],\nthe UNet of this diffusion model uses attention layers on\nthree levels of its feature hierarchy, the BigGAN [3] residual\nblock for up- and downsampling and has 387M parameters",
            "page": 8,
            "x": 304,
            "y": 533,
            "width": 244,
            "height": 181,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "13-inpainting",
            "chunk_id": "f6c3467b-a998-453b-915f-f7f96dd1e4c4",
            "group_text": "### 4.5. Inpainting with Latent Diffusion\n\nInpainting is the task of filling masked regions of an image with new content either because parts of the image are are corrupted or to replace existing but undesired content within the image. We evaluate how our general approach for conditional image generation compares to more specialized, state-of-the-art approaches for this task. Our evaluation follows the protocol of LaMa [88], a recent inpainting model that introduces a specialized architecture relying on Fast Fourier Convolutions [8]. The exact training & evaluation protocol on Places [108] is described in Sec. E.2.2.\n\nWe first analyze the effect of different design choices for the first stage. In particular, we compare the inpainting efficiency of *LDM-1* (*i.e.* a pixel-based conditional DM) with *LDM-4*, for both *KL* and *VQ* regularizations, as well as *VQ-LDM-4* without any attention in the first stage (see Tab. 8), where the latter reduces GPU memory for decoding at high resolutions. For comparability, we fix the number of parameters for all models. Tab. 6 reports the training and sampling throughput at resolution $256^2$ and $512^2$, the total training time in hours per epoch and the FID score on the validation split after six epochs. Overall, we observe a speed-up of at least $2.7\\times$ between pixel- and latent-based diffusion models while improving FID scores by a factor of at least $1.6\\times$.\n\nThe comparison with other inpainting approaches in\nTab. 7 shows that our model with attention improves the\noverall image quality as measured by FID over that of [88].\nLPIPS between the unmasked images and our samples is\nslightly higher than that of [88]. We attribute this to [88]\nonly producing a single result which tends to recover more\nof an average image compared to the diverse results pro-\nduced by our LDM _cf._ Fig. 21. Additionally in a user study\n(Tab. 4) human subjects favor our results over those of [88].\n\nBased on these initial results, we also trained a larger dif-\nfusion model (_big_ in Tab. 7) in the latent space of the VQ-\nregularized first stage without attention. Following [15],\nthe UNet of this diffusion model uses attention layers on\nthree levels of its feature hierarchy, the BigGAN [3] residual\nblock for up- and downsampling and has 387M parameters\n\ninstead of 215M. After training, we noticed a discrepancy\nin the quality of samples produced at resolutions 256\u00b2 and\n512\u00b2, which we hypothesize to be caused by the additional\nattention modules. However, fine-tuning the model for half\nan epoch at resolution 512\u00b2 allows the model to adjust to\nthe new feature statistics and sets a new state of the art FID\non image inpainting (*big, w/o attn, w/f* in Tab. 7, Fig. 11.)."
        },
        {
            "text": "instead of 215M. After training, we noticed a discrepancy\nin the quality of samples produced at resolutions 256\u00b2 and\n512\u00b2, which we hypothesize to be caused by the additional\nattention modules. However, fine-tuning the model for half\nan epoch at resolution 512\u00b2 allows the model to adjust to\nthe new feature statistics and sets a new state of the art FID\non image inpainting (*big, w/o attn, w/f* in Tab. 7, Fig. 11.).",
            "page": 9,
            "x": 47,
            "y": 461,
            "width": 242,
            "height": 87,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "13-inpainting",
            "chunk_id": "022b3e6d-a869-4e4c-97db-daa9bb09e76c",
            "group_text": "### 4.5. Inpainting with Latent Diffusion\n\nInpainting is the task of filling masked regions of an image with new content either because parts of the image are are corrupted or to replace existing but undesired content within the image. We evaluate how our general approach for conditional image generation compares to more specialized, state-of-the-art approaches for this task. Our evaluation follows the protocol of LaMa [88], a recent inpainting model that introduces a specialized architecture relying on Fast Fourier Convolutions [8]. The exact training & evaluation protocol on Places [108] is described in Sec. E.2.2.\n\nWe first analyze the effect of different design choices for the first stage. In particular, we compare the inpainting efficiency of *LDM-1* (*i.e.* a pixel-based conditional DM) with *LDM-4*, for both *KL* and *VQ* regularizations, as well as *VQ-LDM-4* without any attention in the first stage (see Tab. 8), where the latter reduces GPU memory for decoding at high resolutions. For comparability, we fix the number of parameters for all models. Tab. 6 reports the training and sampling throughput at resolution $256^2$ and $512^2$, the total training time in hours per epoch and the FID score on the validation split after six epochs. Overall, we observe a speed-up of at least $2.7\\times$ between pixel- and latent-based diffusion models while improving FID scores by a factor of at least $1.6\\times$.\n\nThe comparison with other inpainting approaches in\nTab. 7 shows that our model with attention improves the\noverall image quality as measured by FID over that of [88].\nLPIPS between the unmasked images and our samples is\nslightly higher than that of [88]. We attribute this to [88]\nonly producing a single result which tends to recover more\nof an average image compared to the diverse results pro-\nduced by our LDM _cf._ Fig. 21. Additionally in a user study\n(Tab. 4) human subjects favor our results over those of [88].\n\nBased on these initial results, we also trained a larger dif-\nfusion model (_big_ in Tab. 7) in the latent space of the VQ-\nregularized first stage without attention. Following [15],\nthe UNet of this diffusion model uses attention layers on\nthree levels of its feature hierarchy, the BigGAN [3] residual\nblock for up- and downsampling and has 387M parameters\n\ninstead of 215M. After training, we noticed a discrepancy\nin the quality of samples produced at resolutions 256\u00b2 and\n512\u00b2, which we hypothesize to be caused by the additional\nattention modules. However, fine-tuning the model for half\nan epoch at resolution 512\u00b2 allows the model to adjust to\nthe new feature statistics and sets a new state of the art FID\non image inpainting (*big, w/o attn, w/f* in Tab. 7, Fig. 11.)."
        },
        {
            "text": "## 5. Limitations & Societal Impact\n\n**Limitations**  While LDMs significantly reduce computational requirements compared to pixel-based approaches, their sequential sampling process is still slower than that of GANs. Moreover, the use of LDMs can be questionable when high precision is required: although the loss of image quality is very small in our $f = 4$ autoencoding models (see Fig. 1), their reconstruction capability can become a bottleneck for tasks that require fine-grained accuracy in pixel space. We assume that our superresolution models (Sec. 4.4) are already somewhat limited in this respect.",
            "page": 9,
            "x": 47,
            "y": 552,
            "width": 242,
            "height": 143,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "14-limitations",
            "chunk_id": "338f0288-b02d-4763-8adb-720d0aaa9b21",
            "group_text": "## 5. Limitations & Societal Impact\n\n**Limitations**  While LDMs significantly reduce computational requirements compared to pixel-based approaches, their sequential sampling process is still slower than that of GANs. Moreover, the use of LDMs can be questionable when high precision is required: although the loss of image quality is very small in our $f = 4$ autoencoding models (see Fig. 1), their reconstruction capability can become a bottleneck for tasks that require fine-grained accuracy in pixel space. We assume that our superresolution models (Sec. 4.4) are already somewhat limited in this respect.\n\n**Societal Impact**  Generative models for media like imagery are a double-edged sword: On the one hand, they\n\nenable various creative applications, and in particular ap-\nproaches like ours that reduce the cost of training and in-\nference have the potential to facilitate access to this tech-\nnology and democratize its exploration. On the other hand,\nit also means that it becomes easier to create and dissemi-\nnate manipulated data or spread misinformation and spam.\nIn particular, the deliberate manipulation of images (\u201cdeep\nfakes\u201d) is a common problem in this context, and women in\nparticular are disproportionately affected by it [13, 24].\n\nGenerative models can also reveal their training data [5, 90], which is of great concern when the data contain sensitive or personal information and were collected without explicit consent. However, the extent to which this also applies to DMs of images is not yet fully understood.\n\nFinally, deep learning modules tend to reproduce or exacerbate biases that are already present in the data [22, 38, 91]. While diffusion models achieve better coverage of the data distribution than _e.g._ GAN-based approaches, the extent to which our two-stage approach that combines adversarial training and a likelihood-based objective misrepresents the data remains an important research question.\n\nFor a more general, detailed discussion of the ethical considerations of deep generative models, see *e.g.* [13]."
        },
        {
            "text": "**Societal Impact**  Generative models for media like imagery are a double-edged sword: On the one hand, they",
            "page": 9,
            "x": 46,
            "y": 700,
            "width": 242,
            "height": 27,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "14-limitations",
            "chunk_id": "71f72bd0-be34-4e3f-b98e-29468c6ea379",
            "group_text": "## 5. Limitations & Societal Impact\n\n**Limitations**  While LDMs significantly reduce computational requirements compared to pixel-based approaches, their sequential sampling process is still slower than that of GANs. Moreover, the use of LDMs can be questionable when high precision is required: although the loss of image quality is very small in our $f = 4$ autoencoding models (see Fig. 1), their reconstruction capability can become a bottleneck for tasks that require fine-grained accuracy in pixel space. We assume that our superresolution models (Sec. 4.4) are already somewhat limited in this respect.\n\n**Societal Impact**  Generative models for media like imagery are a double-edged sword: On the one hand, they\n\nenable various creative applications, and in particular ap-\nproaches like ours that reduce the cost of training and in-\nference have the potential to facilitate access to this tech-\nnology and democratize its exploration. On the other hand,\nit also means that it becomes easier to create and dissemi-\nnate manipulated data or spread misinformation and spam.\nIn particular, the deliberate manipulation of images (\u201cdeep\nfakes\u201d) is a common problem in this context, and women in\nparticular are disproportionately affected by it [13, 24].\n\nGenerative models can also reveal their training data [5, 90], which is of great concern when the data contain sensitive or personal information and were collected without explicit consent. However, the extent to which this also applies to DMs of images is not yet fully understood.\n\nFinally, deep learning modules tend to reproduce or exacerbate biases that are already present in the data [22, 38, 91]. While diffusion models achieve better coverage of the data distribution than _e.g._ GAN-based approaches, the extent to which our two-stage approach that combines adversarial training and a likelihood-based objective misrepresents the data remains an important research question.\n\nFor a more general, detailed discussion of the ethical considerations of deep generative models, see *e.g.* [13]."
        },
        {
            "text": "enable various creative applications, and in particular ap-\nproaches like ours that reduce the cost of training and in-\nference have the potential to facilitate access to this tech-\nnology and democratize its exploration. On the other hand,\nit also means that it becomes easier to create and dissemi-\nnate manipulated data or spread misinformation and spam.\nIn particular, the deliberate manipulation of images (\u201cdeep\nfakes\u201d) is a common problem in this context, and women in\nparticular are disproportionately affected by it [13, 24].",
            "page": 9,
            "x": 306,
            "y": 265,
            "width": 242,
            "height": 108,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "14-limitations",
            "chunk_id": "40314ea9-bc89-472f-bc26-84c5f618b45d",
            "group_text": "## 5. Limitations & Societal Impact\n\n**Limitations**  While LDMs significantly reduce computational requirements compared to pixel-based approaches, their sequential sampling process is still slower than that of GANs. Moreover, the use of LDMs can be questionable when high precision is required: although the loss of image quality is very small in our $f = 4$ autoencoding models (see Fig. 1), their reconstruction capability can become a bottleneck for tasks that require fine-grained accuracy in pixel space. We assume that our superresolution models (Sec. 4.4) are already somewhat limited in this respect.\n\n**Societal Impact**  Generative models for media like imagery are a double-edged sword: On the one hand, they\n\nenable various creative applications, and in particular ap-\nproaches like ours that reduce the cost of training and in-\nference have the potential to facilitate access to this tech-\nnology and democratize its exploration. On the other hand,\nit also means that it becomes easier to create and dissemi-\nnate manipulated data or spread misinformation and spam.\nIn particular, the deliberate manipulation of images (\u201cdeep\nfakes\u201d) is a common problem in this context, and women in\nparticular are disproportionately affected by it [13, 24].\n\nGenerative models can also reveal their training data [5, 90], which is of great concern when the data contain sensitive or personal information and were collected without explicit consent. However, the extent to which this also applies to DMs of images is not yet fully understood.\n\nFinally, deep learning modules tend to reproduce or exacerbate biases that are already present in the data [22, 38, 91]. While diffusion models achieve better coverage of the data distribution than _e.g._ GAN-based approaches, the extent to which our two-stage approach that combines adversarial training and a likelihood-based objective misrepresents the data remains an important research question.\n\nFor a more general, detailed discussion of the ethical considerations of deep generative models, see *e.g.* [13]."
        },
        {
            "text": "Generative models can also reveal their training data [5, 90], which is of great concern when the data contain sensitive or personal information and were collected without explicit consent. However, the extent to which this also applies to DMs of images is not yet fully understood.",
            "page": 9,
            "x": 306,
            "y": 373,
            "width": 241,
            "height": 60,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "14-limitations",
            "chunk_id": "8123ddab-5d71-4202-829c-1500cf61209e",
            "group_text": "## 5. Limitations & Societal Impact\n\n**Limitations**  While LDMs significantly reduce computational requirements compared to pixel-based approaches, their sequential sampling process is still slower than that of GANs. Moreover, the use of LDMs can be questionable when high precision is required: although the loss of image quality is very small in our $f = 4$ autoencoding models (see Fig. 1), their reconstruction capability can become a bottleneck for tasks that require fine-grained accuracy in pixel space. We assume that our superresolution models (Sec. 4.4) are already somewhat limited in this respect.\n\n**Societal Impact**  Generative models for media like imagery are a double-edged sword: On the one hand, they\n\nenable various creative applications, and in particular ap-\nproaches like ours that reduce the cost of training and in-\nference have the potential to facilitate access to this tech-\nnology and democratize its exploration. On the other hand,\nit also means that it becomes easier to create and dissemi-\nnate manipulated data or spread misinformation and spam.\nIn particular, the deliberate manipulation of images (\u201cdeep\nfakes\u201d) is a common problem in this context, and women in\nparticular are disproportionately affected by it [13, 24].\n\nGenerative models can also reveal their training data [5, 90], which is of great concern when the data contain sensitive or personal information and were collected without explicit consent. However, the extent to which this also applies to DMs of images is not yet fully understood.\n\nFinally, deep learning modules tend to reproduce or exacerbate biases that are already present in the data [22, 38, 91]. While diffusion models achieve better coverage of the data distribution than _e.g._ GAN-based approaches, the extent to which our two-stage approach that combines adversarial training and a likelihood-based objective misrepresents the data remains an important research question.\n\nFor a more general, detailed discussion of the ethical considerations of deep generative models, see *e.g.* [13]."
        },
        {
            "text": "Finally, deep learning modules tend to reproduce or exacerbate biases that are already present in the data [22, 38, 91]. While diffusion models achieve better coverage of the data distribution than _e.g._ GAN-based approaches, the extent to which our two-stage approach that combines adversarial training and a likelihood-based objective misrepresents the data remains an important research question.",
            "page": 9,
            "x": 306,
            "y": 433,
            "width": 242,
            "height": 83,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "14-limitations",
            "chunk_id": "b33e6f5f-c58d-45d0-a134-11cb7cd2c644",
            "group_text": "## 5. Limitations & Societal Impact\n\n**Limitations**  While LDMs significantly reduce computational requirements compared to pixel-based approaches, their sequential sampling process is still slower than that of GANs. Moreover, the use of LDMs can be questionable when high precision is required: although the loss of image quality is very small in our $f = 4$ autoencoding models (see Fig. 1), their reconstruction capability can become a bottleneck for tasks that require fine-grained accuracy in pixel space. We assume that our superresolution models (Sec. 4.4) are already somewhat limited in this respect.\n\n**Societal Impact**  Generative models for media like imagery are a double-edged sword: On the one hand, they\n\nenable various creative applications, and in particular ap-\nproaches like ours that reduce the cost of training and in-\nference have the potential to facilitate access to this tech-\nnology and democratize its exploration. On the other hand,\nit also means that it becomes easier to create and dissemi-\nnate manipulated data or spread misinformation and spam.\nIn particular, the deliberate manipulation of images (\u201cdeep\nfakes\u201d) is a common problem in this context, and women in\nparticular are disproportionately affected by it [13, 24].\n\nGenerative models can also reveal their training data [5, 90], which is of great concern when the data contain sensitive or personal information and were collected without explicit consent. However, the extent to which this also applies to DMs of images is not yet fully understood.\n\nFinally, deep learning modules tend to reproduce or exacerbate biases that are already present in the data [22, 38, 91]. While diffusion models achieve better coverage of the data distribution than _e.g._ GAN-based approaches, the extent to which our two-stage approach that combines adversarial training and a likelihood-based objective misrepresents the data remains an important research question.\n\nFor a more general, detailed discussion of the ethical considerations of deep generative models, see *e.g.* [13]."
        },
        {
            "text": "For a more general, detailed discussion of the ethical considerations of deep generative models, see *e.g.* [13].",
            "page": 9,
            "x": 306,
            "y": 517,
            "width": 241,
            "height": 25,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "14-limitations",
            "chunk_id": "57846976-7e6f-46f4-b0ec-ff4a8e32c214",
            "group_text": "## 5. Limitations & Societal Impact\n\n**Limitations**  While LDMs significantly reduce computational requirements compared to pixel-based approaches, their sequential sampling process is still slower than that of GANs. Moreover, the use of LDMs can be questionable when high precision is required: although the loss of image quality is very small in our $f = 4$ autoencoding models (see Fig. 1), their reconstruction capability can become a bottleneck for tasks that require fine-grained accuracy in pixel space. We assume that our superresolution models (Sec. 4.4) are already somewhat limited in this respect.\n\n**Societal Impact**  Generative models for media like imagery are a double-edged sword: On the one hand, they\n\nenable various creative applications, and in particular ap-\nproaches like ours that reduce the cost of training and in-\nference have the potential to facilitate access to this tech-\nnology and democratize its exploration. On the other hand,\nit also means that it becomes easier to create and dissemi-\nnate manipulated data or spread misinformation and spam.\nIn particular, the deliberate manipulation of images (\u201cdeep\nfakes\u201d) is a common problem in this context, and women in\nparticular are disproportionately affected by it [13, 24].\n\nGenerative models can also reveal their training data [5, 90], which is of great concern when the data contain sensitive or personal information and were collected without explicit consent. However, the extent to which this also applies to DMs of images is not yet fully understood.\n\nFinally, deep learning modules tend to reproduce or exacerbate biases that are already present in the data [22, 38, 91]. While diffusion models achieve better coverage of the data distribution than _e.g._ GAN-based approaches, the extent to which our two-stage approach that combines adversarial training and a likelihood-based objective misrepresents the data remains an important research question.\n\nFor a more general, detailed discussion of the ethical considerations of deep generative models, see *e.g.* [13]."
        },
        {
            "text": "## 6. Conclusion\nWe have presented latent diffusion models, a simple and efficient way to significantly improve both the training and sampling efficiency of denoising diffusion models without degrading their quality. Based on this and our cross-attention conditioning mechanism, our experiments could demonstrate favorable results compared to state-of-the-art methods across a wide range of conditional image synthesis tasks without task-specific architectures.",
            "page": 9,
            "x": 306,
            "y": 546,
            "width": 242,
            "height": 111,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "15-conclusion",
            "chunk_id": "c8521bd7-92c3-4783-9638-4fde983f712e",
            "group_text": "## 6. Conclusion\nWe have presented latent diffusion models, a simple and efficient way to significantly improve both the training and sampling efficiency of denoising diffusion models without degrading their quality. Based on this and our cross-attention conditioning mechanism, our experiments could demonstrate favorable results compared to state-of-the-art methods across a wide range of conditional image synthesis tasks without task-specific architectures.\n\nThis work has been supported by the German Federal Ministry for Economic Affairs and Energy within the project 'KI-Absicherung - Safe AI for automated driving' and by the German Research Foundation (DFG) project 421703927."
        },
        {
            "text": "This work has been supported by the German Federal Ministry for Economic Affairs and Energy within the project 'KI-Absicherung - Safe AI for automated driving' and by the German Research Foundation (DFG) project 421703927.",
            "page": 9,
            "x": 307,
            "y": 686,
            "width": 240,
            "height": 40,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "15-conclusion",
            "chunk_id": "137602e7-8885-41c3-b527-59ecf70c0828",
            "group_text": "## 6. Conclusion\nWe have presented latent diffusion models, a simple and efficient way to significantly improve both the training and sampling efficiency of denoising diffusion models without degrading their quality. Based on this and our cross-attention conditioning mechanism, our experiments could demonstrate favorable results compared to state-of-the-art methods across a wide range of conditional image synthesis tasks without task-specific architectures.\n\nThis work has been supported by the German Federal Ministry for Economic Affairs and Energy within the project 'KI-Absicherung - Safe AI for automated driving' and by the German Research Foundation (DFG) project 421703927."
        }
    ]
}