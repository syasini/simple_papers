{
  "Transformer": "",
  "self-attention": "",
  "Multi-Head Attention": "",
  "recurrent neural networks": "",
  "convolutions": "",
  "encoder-decoder": "",
  "fully-connected neural networks": "",
  "encoder": "",
  "decoder": "",
  "multi-head self-attention mechanism": "",
  "feed-forward network": "",
  "residual connection": "",
  "layer normalization": "",
  "attention layer": "",
  "masking": "",
  "ReLU": "",
  "linear transformations": "",
  "position-wise": "",
  "embeddings": "",
  "softmax": "",
  "attention mechanisms": "",
  "BLEU": "",
  "recurrent networks": "",
  "computational efficiency": "",
  "sequence length": "",
  "representation dimension": "",
  "machine translation": "",
  "long-range dependencies": "",
  "attention heads": "",
  "syntax": "",
  "semantics": "",
  "WMT 2014": "",
  "NVIDIA P100 GPUs": "",
  "Adam optimizer": "",
  "learning rate": "",
  "warmup phase": "",
  "Residual Dropout": "",
  "Label Smoothing": "",
  "dropout": "",
  "position encoding": "",
  "English Constituency Parsing": "",
  "Wall Street Journal dataset": "",
  "self-attention mechanisms": "",
  "recurrent layers": "",
  "convolutional architectures": "",
  "English-to-German": "",
  "English-to-French translation tasks": "",
  "Recurrent neural networks": "",
  "LSTMs": "",
  "GRUs": "",
  "language tasks": "",
  "translation": "",
  "recurrence": "",
  "parallelization": "",
  "queries": "",
  "keys": "",
  "values": "",
  "Scaled Dot-Product Attention": "",
  "dot product": "",
  "scaling factor": "",
  "representation subspaces": "",
  "encoder-decoder attention": "",
  "masked self-attention": "",
  "positional encoding": "",
  "sine": "",
  "cosine waves": "",
  "word embeddings": ""
}