{
    "annotations": [
        {
            "text": "XGBoost: A Scalable Tree Boosting System",
            "page": 1,
            "x": 114,
            "y": 66,
            "width": 380,
            "height": 25,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "0-title",
            "chunk_id": "9e2ffb56-283f-4f26-8847-8649f9343497",
            "group_text": "XGBoost: A Scalable Tree Boosting System\n\nTianqi Chen\nUniversity of Washington\ntqchen@cs.washington.edu\n\nCarlos Guestrin  \nUniversity of Washington  \nguestrin@cs.washington.edu"
        },
        {
            "text": "Tianqi Chen\nUniversity of Washington\ntqchen@cs.washington.edu",
            "page": 1,
            "x": 114,
            "y": 111,
            "width": 156,
            "height": 44,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "0-title",
            "chunk_id": "9914f9a7-bd97-414e-b62b-2da7e1c4ddba",
            "group_text": "XGBoost: A Scalable Tree Boosting System\n\nTianqi Chen\nUniversity of Washington\ntqchen@cs.washington.edu\n\nCarlos Guestrin  \nUniversity of Washington  \nguestrin@cs.washington.edu"
        },
        {
            "text": "Carlos Guestrin  \nUniversity of Washington  \nguestrin@cs.washington.edu",
            "page": 1,
            "x": 336,
            "y": 112,
            "width": 162,
            "height": 42,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "0-title",
            "chunk_id": "19d4f864-3555-4c1e-a1ce-71cfd963f930",
            "group_text": "XGBoost: A Scalable Tree Boosting System\n\nTianqi Chen\nUniversity of Washington\ntqchen@cs.washington.edu\n\nCarlos Guestrin  \nUniversity of Washington  \nguestrin@cs.washington.edu"
        },
        {
            "text": "# ABSTRACT\nTree boosting is a highly effective and widely used machine\nlearning method. In this paper, we describe a scalable end-\nto-end tree boosting system called XGBoost, which is used\nwidely by data scientists to achieve state-of-the-art results\non many machine learning challenges. We propose a novel\nsparsity-aware algorithm for sparse data and weighted quan-\ntile sketch for approximate tree learning. More importantly,\nwe provide insights on cache access patterns, data compres-\nsion and sharding to build a scalable tree boosting system.\nBy combining these insights, XGBoost scales beyond billions\nof examples using far fewer resources than existing systems.",
            "page": 1,
            "x": 50,
            "y": 202,
            "width": 245,
            "height": 136,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "1-abstract",
            "chunk_id": "6ca6b73e-1e59-4fc0-ae42-d32fcfbe79d3",
            "group_text": "# ABSTRACT\nTree boosting is a highly effective and widely used machine\nlearning method. In this paper, we describe a scalable end-\nto-end tree boosting system called XGBoost, which is used\nwidely by data scientists to achieve state-of-the-art results\non many machine learning challenges. We propose a novel\nsparsity-aware algorithm for sparse data and weighted quan-\ntile sketch for approximate tree learning. More importantly,\nwe provide insights on cache access patterns, data compres-\nsion and sharding to build a scalable tree boosting system.\nBy combining these insights, XGBoost scales beyond billions\nof examples using far fewer resources than existing systems.\n\n**Keywords**\nLarge-scale Machine Learning"
        },
        {
            "text": "**Keywords**\nLarge-scale Machine Learning",
            "page": 1,
            "x": 50,
            "y": 348,
            "width": 126,
            "height": 30,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "1-abstract",
            "chunk_id": "4c48d62e-42f6-465a-a8ce-b681d421e397",
            "group_text": "# ABSTRACT\nTree boosting is a highly effective and widely used machine\nlearning method. In this paper, we describe a scalable end-\nto-end tree boosting system called XGBoost, which is used\nwidely by data scientists to achieve state-of-the-art results\non many machine learning challenges. We propose a novel\nsparsity-aware algorithm for sparse data and weighted quan-\ntile sketch for approximate tree learning. More importantly,\nwe provide insights on cache access patterns, data compres-\nsion and sharding to build a scalable tree boosting system.\nBy combining these insights, XGBoost scales beyond billions\nof examples using far fewer resources than existing systems.\n\n**Keywords**\nLarge-scale Machine Learning"
        },
        {
            "text": "1. INTRODUCTION\n\n    Machine learning and data-driven approaches are becoming very important in many areas.  Smart spam classifiers protect our email by learning from massive amounts of spam data and user feedback; advertising systems learn to match the right ads with the right context; fraud detection systems protect banks from malicious attackers; anomaly event detection systems help experimental physicists to find events that lead to new physics.  There are two important factors that drive these successful applications: usage of effective (statistical) models that capture the complex data dependencies and scalable learning systems that learn the model of interest from large datasets.",
            "page": 1,
            "x": 49,
            "y": 386,
            "width": 247,
            "height": 141,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "1c69fbb3-f814-4ea6-96fb-cd355012bb78",
            "group_text": "1. INTRODUCTION\n\n    Machine learning and data-driven approaches are becoming very important in many areas.  Smart spam classifiers protect our email by learning from massive amounts of spam data and user feedback; advertising systems learn to match the right ads with the right context; fraud detection systems protect banks from malicious attackers; anomaly event detection systems help experimental physicists to find events that lead to new physics.  There are two important factors that drive these successful applications: usage of effective (statistical) models that capture the complex data dependencies and scalable learning systems that learn the model of interest from large datasets.\n\nAmong the machine learning methods used in practice,\ngradient tree boosting [10]\\textsuperscript{1} is one technique that shines\nin many applications.  Tree boosting has been shown to\ngive state-of-the-art results on many standard classification\nbenchmarks [16]. LambdaMART [5], a variant of tree boosting for ranking, achieves state-of-the-art result for ranking\n\n\u00b9Gradient tree boosting is also known as gradient boosting\nmachine (GBM) or gradient boosted regression tree (GBRT)\n\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\n\n*KDD \u201916, August 13-17, 2016, San Francisco, CA, USA*\n\n\u00a9 2016 Copyright held by the owner/author(s).\n\nACM ISBN .\n\nproblems. Besides being used as a stand-alone predictor, it\nis also incorporated into real-world production pipelines for\nad click through rate prediction [15]. Finally, it is the de-\nfacto choice of ensemble method and is used in challenges\nsuch as the Netflix prize [3].\n\nIn this paper, we describe XGBoost, a scalable machine\nlearning system for tree boosting. The system is available as\nan open source package\\textsuperscript{2}. The impact of the system has been\nwidely recognized in a number of machine learning and data\nmining challenges. Take the challenges hosted by the ma-\nchine learning competition site Kaggle for example. Among\nthe 29 challenge winning solutions\\textsuperscript{3} published at Kaggle\u2019s\nblog during 2015, 17 solutions used XGBoost. Among these\nsolutions, eight solely used XGBoost to train the model,\nwhile most others combined XGBoost with neural nets in en-\nsembles. For comparison, the second most popular method,\ndeep neural nets, was used in 11 solutions. The success\nof the system was also witnessed in KDDCup 2015, where\nXGBoost was used by every winning team in the top-10.\nMoreover, the winning teams reported that ensemble meth-\nods outperform a well-configured XGBoost by only a small\namount\\textsuperscript{[1]}.\n\nThese results demonstrate that our system gives state-of-\nthe-art results on a wide range of problems. Examples of\nthe problems in these winning solutions include: store sales\nprediction; high energy physics event classification; web text\nclassification; customer behavior prediction; motion detec-\ntion; ad click through rate prediction; malware classification;\nproduct categorization; hazard risk prediction; massive on-\nline course dropout rate prediction. While domain depen-\ndent data analysis and feature engineering play an important\nrole in these solutions, the fact that XGBoost is the consen-\nsus choice of learner shows the impact and importance of\nour system and tree boosting.\n\nThe most important factor behind the success of XGBoost\nis its scalability in all scenarios. The system runs more than\nten times faster than existing popular solutions on a single\nmachine and scales to billions of examples in distributed or\nmemory-limited settings. The scalability of XGBoost is due\nto several important systems and algorithmic optimizations.\nThese innovations include: a novel tree learning algorithm\nis for handling _sparse data_; a theoretically justified weighted\nquantile sketch procedure enables handling instance weights\nin approximate tree learning. Parallel and distributed com-\nputing makes learning faster which enables quicker model ex-\nploration. More importantly, XGBoost exploits out-of-core\n\n\u00b2https://github.com/dmlc/xgboost  \n\u00b3Solutions come from of top-3 teams of each competitions.\n\ncomputation and enables data scientists to process hundred\nmillions of examples on a desktop.  Finally, it is even more\nexciting to combine these techniques to make an end-to-end\nsystem that scales to even larger data with the least amount\nof cluster resources.  The major contributions of this paper\nis listed as follows:\n\n- \u2022 We design and build a highly scalable end-to-end tree boosting system.\n\n- \u2022 We propose a theoretically justified weighted quantile sketch for efficient proposal calculation.\n\n- \u2022 We introduce a novel sparsity-aware algorithm for parallel tree learning.\n\n- \u2022 We propose an effective cache-aware block structure for out-of-core tree learning.\n\nWhile there are some existing works on parallel tree boosting\n[22, 23, 19], the directions such as out-of-core compu-\ntation, cache-aware and sparsity-aware learning have not\nbeen explored. More importantly, an end-to-end system\nthat combines all of these aspects gives a novel solution for\nreal-world use-cases. This enables data scientists as well as\nresearchers to build powerful variants of tree boosting al-\ngorithms [7, 8]. Besides these major contributions, we also\nmake additional improvements in proposing a regularized\nlearning objective, which we will include for completeness.\n\nThe remainder of the paper is organized as follows. We\nwill first review tree boosting and introduce a regularized\nobjective in Sec. 2. We then describe the split finding meth-\nods in Sec. 3 as well as the system design in Sec. 4, including\nexperimental results when relevant to provide quantitative\nsupport for each optimization we describe. Related work\nis discussed in Sec. 5. Detailed end-to-end evaluations are\nincluded in Sec. 6. Finally we conclude the paper in Sec. 7."
        },
        {
            "text": "Among the machine learning methods used in practice,\ngradient tree boosting [10]\\textsuperscript{1} is one technique that shines\nin many applications.  Tree boosting has been shown to\ngive state-of-the-art results on many standard classification\nbenchmarks [16]. LambdaMART [5], a variant of tree boosting for ranking, achieves state-of-the-art result for ranking",
            "page": 1,
            "x": 51,
            "y": 528,
            "width": 244,
            "height": 66,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "c9159b7e-1dc5-4e9e-a6c6-67d433a16a1f",
            "group_text": "1. INTRODUCTION\n\n    Machine learning and data-driven approaches are becoming very important in many areas.  Smart spam classifiers protect our email by learning from massive amounts of spam data and user feedback; advertising systems learn to match the right ads with the right context; fraud detection systems protect banks from malicious attackers; anomaly event detection systems help experimental physicists to find events that lead to new physics.  There are two important factors that drive these successful applications: usage of effective (statistical) models that capture the complex data dependencies and scalable learning systems that learn the model of interest from large datasets.\n\nAmong the machine learning methods used in practice,\ngradient tree boosting [10]\\textsuperscript{1} is one technique that shines\nin many applications.  Tree boosting has been shown to\ngive state-of-the-art results on many standard classification\nbenchmarks [16]. LambdaMART [5], a variant of tree boosting for ranking, achieves state-of-the-art result for ranking\n\n\u00b9Gradient tree boosting is also known as gradient boosting\nmachine (GBM) or gradient boosted regression tree (GBRT)\n\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\n\n*KDD \u201916, August 13-17, 2016, San Francisco, CA, USA*\n\n\u00a9 2016 Copyright held by the owner/author(s).\n\nACM ISBN .\n\nproblems. Besides being used as a stand-alone predictor, it\nis also incorporated into real-world production pipelines for\nad click through rate prediction [15]. Finally, it is the de-\nfacto choice of ensemble method and is used in challenges\nsuch as the Netflix prize [3].\n\nIn this paper, we describe XGBoost, a scalable machine\nlearning system for tree boosting. The system is available as\nan open source package\\textsuperscript{2}. The impact of the system has been\nwidely recognized in a number of machine learning and data\nmining challenges. Take the challenges hosted by the ma-\nchine learning competition site Kaggle for example. Among\nthe 29 challenge winning solutions\\textsuperscript{3} published at Kaggle\u2019s\nblog during 2015, 17 solutions used XGBoost. Among these\nsolutions, eight solely used XGBoost to train the model,\nwhile most others combined XGBoost with neural nets in en-\nsembles. For comparison, the second most popular method,\ndeep neural nets, was used in 11 solutions. The success\nof the system was also witnessed in KDDCup 2015, where\nXGBoost was used by every winning team in the top-10.\nMoreover, the winning teams reported that ensemble meth-\nods outperform a well-configured XGBoost by only a small\namount\\textsuperscript{[1]}.\n\nThese results demonstrate that our system gives state-of-\nthe-art results on a wide range of problems. Examples of\nthe problems in these winning solutions include: store sales\nprediction; high energy physics event classification; web text\nclassification; customer behavior prediction; motion detec-\ntion; ad click through rate prediction; malware classification;\nproduct categorization; hazard risk prediction; massive on-\nline course dropout rate prediction. While domain depen-\ndent data analysis and feature engineering play an important\nrole in these solutions, the fact that XGBoost is the consen-\nsus choice of learner shows the impact and importance of\nour system and tree boosting.\n\nThe most important factor behind the success of XGBoost\nis its scalability in all scenarios. The system runs more than\nten times faster than existing popular solutions on a single\nmachine and scales to billions of examples in distributed or\nmemory-limited settings. The scalability of XGBoost is due\nto several important systems and algorithmic optimizations.\nThese innovations include: a novel tree learning algorithm\nis for handling _sparse data_; a theoretically justified weighted\nquantile sketch procedure enables handling instance weights\nin approximate tree learning. Parallel and distributed com-\nputing makes learning faster which enables quicker model ex-\nploration. More importantly, XGBoost exploits out-of-core\n\n\u00b2https://github.com/dmlc/xgboost  \n\u00b3Solutions come from of top-3 teams of each competitions.\n\ncomputation and enables data scientists to process hundred\nmillions of examples on a desktop.  Finally, it is even more\nexciting to combine these techniques to make an end-to-end\nsystem that scales to even larger data with the least amount\nof cluster resources.  The major contributions of this paper\nis listed as follows:\n\n- \u2022 We design and build a highly scalable end-to-end tree boosting system.\n\n- \u2022 We propose a theoretically justified weighted quantile sketch for efficient proposal calculation.\n\n- \u2022 We introduce a novel sparsity-aware algorithm for parallel tree learning.\n\n- \u2022 We propose an effective cache-aware block structure for out-of-core tree learning.\n\nWhile there are some existing works on parallel tree boosting\n[22, 23, 19], the directions such as out-of-core compu-\ntation, cache-aware and sparsity-aware learning have not\nbeen explored. More importantly, an end-to-end system\nthat combines all of these aspects gives a novel solution for\nreal-world use-cases. This enables data scientists as well as\nresearchers to build powerful variants of tree boosting al-\ngorithms [7, 8]. Besides these major contributions, we also\nmake additional improvements in proposing a regularized\nlearning objective, which we will include for completeness.\n\nThe remainder of the paper is organized as follows. We\nwill first review tree boosting and introduce a regularized\nobjective in Sec. 2. We then describe the split finding meth-\nods in Sec. 3 as well as the system design in Sec. 4, including\nexperimental results when relevant to provide quantitative\nsupport for each optimization we describe. Related work\nis discussed in Sec. 5. Detailed end-to-end evaluations are\nincluded in Sec. 6. Finally we conclude the paper in Sec. 7."
        },
        {
            "text": "\u00b9Gradient tree boosting is also known as gradient boosting\nmachine (GBM) or gradient boosted regression tree (GBRT)",
            "page": 1,
            "x": 51,
            "y": 596,
            "width": 243,
            "height": 27,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "ef0df6f1-0774-4b51-9f49-9b99780b8244",
            "group_text": "1. INTRODUCTION\n\n    Machine learning and data-driven approaches are becoming very important in many areas.  Smart spam classifiers protect our email by learning from massive amounts of spam data and user feedback; advertising systems learn to match the right ads with the right context; fraud detection systems protect banks from malicious attackers; anomaly event detection systems help experimental physicists to find events that lead to new physics.  There are two important factors that drive these successful applications: usage of effective (statistical) models that capture the complex data dependencies and scalable learning systems that learn the model of interest from large datasets.\n\nAmong the machine learning methods used in practice,\ngradient tree boosting [10]\\textsuperscript{1} is one technique that shines\nin many applications.  Tree boosting has been shown to\ngive state-of-the-art results on many standard classification\nbenchmarks [16]. LambdaMART [5], a variant of tree boosting for ranking, achieves state-of-the-art result for ranking\n\n\u00b9Gradient tree boosting is also known as gradient boosting\nmachine (GBM) or gradient boosted regression tree (GBRT)\n\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\n\n*KDD \u201916, August 13-17, 2016, San Francisco, CA, USA*\n\n\u00a9 2016 Copyright held by the owner/author(s).\n\nACM ISBN .\n\nproblems. Besides being used as a stand-alone predictor, it\nis also incorporated into real-world production pipelines for\nad click through rate prediction [15]. Finally, it is the de-\nfacto choice of ensemble method and is used in challenges\nsuch as the Netflix prize [3].\n\nIn this paper, we describe XGBoost, a scalable machine\nlearning system for tree boosting. The system is available as\nan open source package\\textsuperscript{2}. The impact of the system has been\nwidely recognized in a number of machine learning and data\nmining challenges. Take the challenges hosted by the ma-\nchine learning competition site Kaggle for example. Among\nthe 29 challenge winning solutions\\textsuperscript{3} published at Kaggle\u2019s\nblog during 2015, 17 solutions used XGBoost. Among these\nsolutions, eight solely used XGBoost to train the model,\nwhile most others combined XGBoost with neural nets in en-\nsembles. For comparison, the second most popular method,\ndeep neural nets, was used in 11 solutions. The success\nof the system was also witnessed in KDDCup 2015, where\nXGBoost was used by every winning team in the top-10.\nMoreover, the winning teams reported that ensemble meth-\nods outperform a well-configured XGBoost by only a small\namount\\textsuperscript{[1]}.\n\nThese results demonstrate that our system gives state-of-\nthe-art results on a wide range of problems. Examples of\nthe problems in these winning solutions include: store sales\nprediction; high energy physics event classification; web text\nclassification; customer behavior prediction; motion detec-\ntion; ad click through rate prediction; malware classification;\nproduct categorization; hazard risk prediction; massive on-\nline course dropout rate prediction. While domain depen-\ndent data analysis and feature engineering play an important\nrole in these solutions, the fact that XGBoost is the consen-\nsus choice of learner shows the impact and importance of\nour system and tree boosting.\n\nThe most important factor behind the success of XGBoost\nis its scalability in all scenarios. The system runs more than\nten times faster than existing popular solutions on a single\nmachine and scales to billions of examples in distributed or\nmemory-limited settings. The scalability of XGBoost is due\nto several important systems and algorithmic optimizations.\nThese innovations include: a novel tree learning algorithm\nis for handling _sparse data_; a theoretically justified weighted\nquantile sketch procedure enables handling instance weights\nin approximate tree learning. Parallel and distributed com-\nputing makes learning faster which enables quicker model ex-\nploration. More importantly, XGBoost exploits out-of-core\n\n\u00b2https://github.com/dmlc/xgboost  \n\u00b3Solutions come from of top-3 teams of each competitions.\n\ncomputation and enables data scientists to process hundred\nmillions of examples on a desktop.  Finally, it is even more\nexciting to combine these techniques to make an end-to-end\nsystem that scales to even larger data with the least amount\nof cluster resources.  The major contributions of this paper\nis listed as follows:\n\n- \u2022 We design and build a highly scalable end-to-end tree boosting system.\n\n- \u2022 We propose a theoretically justified weighted quantile sketch for efficient proposal calculation.\n\n- \u2022 We introduce a novel sparsity-aware algorithm for parallel tree learning.\n\n- \u2022 We propose an effective cache-aware block structure for out-of-core tree learning.\n\nWhile there are some existing works on parallel tree boosting\n[22, 23, 19], the directions such as out-of-core compu-\ntation, cache-aware and sparsity-aware learning have not\nbeen explored. More importantly, an end-to-end system\nthat combines all of these aspects gives a novel solution for\nreal-world use-cases. This enables data scientists as well as\nresearchers to build powerful variants of tree boosting al-\ngorithms [7, 8]. Besides these major contributions, we also\nmake additional improvements in proposing a regularized\nlearning objective, which we will include for completeness.\n\nThe remainder of the paper is organized as follows. We\nwill first review tree boosting and introduce a regularized\nobjective in Sec. 2. We then describe the split finding meth-\nods in Sec. 3 as well as the system design in Sec. 4, including\nexperimental results when relevant to provide quantitative\nsupport for each optimization we describe. Related work\nis discussed in Sec. 5. Detailed end-to-end evaluations are\nincluded in Sec. 6. Finally we conclude the paper in Sec. 7."
        },
        {
            "text": "Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).",
            "page": 1,
            "x": 51,
            "y": 636,
            "width": 243,
            "height": 41,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "6ff2f03d-42fe-4fba-a494-7d56e68ece9a",
            "group_text": "1. INTRODUCTION\n\n    Machine learning and data-driven approaches are becoming very important in many areas.  Smart spam classifiers protect our email by learning from massive amounts of spam data and user feedback; advertising systems learn to match the right ads with the right context; fraud detection systems protect banks from malicious attackers; anomaly event detection systems help experimental physicists to find events that lead to new physics.  There are two important factors that drive these successful applications: usage of effective (statistical) models that capture the complex data dependencies and scalable learning systems that learn the model of interest from large datasets.\n\nAmong the machine learning methods used in practice,\ngradient tree boosting [10]\\textsuperscript{1} is one technique that shines\nin many applications.  Tree boosting has been shown to\ngive state-of-the-art results on many standard classification\nbenchmarks [16]. LambdaMART [5], a variant of tree boosting for ranking, achieves state-of-the-art result for ranking\n\n\u00b9Gradient tree boosting is also known as gradient boosting\nmachine (GBM) or gradient boosted regression tree (GBRT)\n\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\n\n*KDD \u201916, August 13-17, 2016, San Francisco, CA, USA*\n\n\u00a9 2016 Copyright held by the owner/author(s).\n\nACM ISBN .\n\nproblems. Besides being used as a stand-alone predictor, it\nis also incorporated into real-world production pipelines for\nad click through rate prediction [15]. Finally, it is the de-\nfacto choice of ensemble method and is used in challenges\nsuch as the Netflix prize [3].\n\nIn this paper, we describe XGBoost, a scalable machine\nlearning system for tree boosting. The system is available as\nan open source package\\textsuperscript{2}. The impact of the system has been\nwidely recognized in a number of machine learning and data\nmining challenges. Take the challenges hosted by the ma-\nchine learning competition site Kaggle for example. Among\nthe 29 challenge winning solutions\\textsuperscript{3} published at Kaggle\u2019s\nblog during 2015, 17 solutions used XGBoost. Among these\nsolutions, eight solely used XGBoost to train the model,\nwhile most others combined XGBoost with neural nets in en-\nsembles. For comparison, the second most popular method,\ndeep neural nets, was used in 11 solutions. The success\nof the system was also witnessed in KDDCup 2015, where\nXGBoost was used by every winning team in the top-10.\nMoreover, the winning teams reported that ensemble meth-\nods outperform a well-configured XGBoost by only a small\namount\\textsuperscript{[1]}.\n\nThese results demonstrate that our system gives state-of-\nthe-art results on a wide range of problems. Examples of\nthe problems in these winning solutions include: store sales\nprediction; high energy physics event classification; web text\nclassification; customer behavior prediction; motion detec-\ntion; ad click through rate prediction; malware classification;\nproduct categorization; hazard risk prediction; massive on-\nline course dropout rate prediction. While domain depen-\ndent data analysis and feature engineering play an important\nrole in these solutions, the fact that XGBoost is the consen-\nsus choice of learner shows the impact and importance of\nour system and tree boosting.\n\nThe most important factor behind the success of XGBoost\nis its scalability in all scenarios. The system runs more than\nten times faster than existing popular solutions on a single\nmachine and scales to billions of examples in distributed or\nmemory-limited settings. The scalability of XGBoost is due\nto several important systems and algorithmic optimizations.\nThese innovations include: a novel tree learning algorithm\nis for handling _sparse data_; a theoretically justified weighted\nquantile sketch procedure enables handling instance weights\nin approximate tree learning. Parallel and distributed com-\nputing makes learning faster which enables quicker model ex-\nploration. More importantly, XGBoost exploits out-of-core\n\n\u00b2https://github.com/dmlc/xgboost  \n\u00b3Solutions come from of top-3 teams of each competitions.\n\ncomputation and enables data scientists to process hundred\nmillions of examples on a desktop.  Finally, it is even more\nexciting to combine these techniques to make an end-to-end\nsystem that scales to even larger data with the least amount\nof cluster resources.  The major contributions of this paper\nis listed as follows:\n\n- \u2022 We design and build a highly scalable end-to-end tree boosting system.\n\n- \u2022 We propose a theoretically justified weighted quantile sketch for efficient proposal calculation.\n\n- \u2022 We introduce a novel sparsity-aware algorithm for parallel tree learning.\n\n- \u2022 We propose an effective cache-aware block structure for out-of-core tree learning.\n\nWhile there are some existing works on parallel tree boosting\n[22, 23, 19], the directions such as out-of-core compu-\ntation, cache-aware and sparsity-aware learning have not\nbeen explored. More importantly, an end-to-end system\nthat combines all of these aspects gives a novel solution for\nreal-world use-cases. This enables data scientists as well as\nresearchers to build powerful variants of tree boosting al-\ngorithms [7, 8]. Besides these major contributions, we also\nmake additional improvements in proposing a regularized\nlearning objective, which we will include for completeness.\n\nThe remainder of the paper is organized as follows. We\nwill first review tree boosting and introduce a regularized\nobjective in Sec. 2. We then describe the split finding meth-\nods in Sec. 3 as well as the system design in Sec. 4, including\nexperimental results when relevant to provide quantitative\nsupport for each optimization we describe. Related work\nis discussed in Sec. 5. Detailed end-to-end evaluations are\nincluded in Sec. 6. Finally we conclude the paper in Sec. 7."
        },
        {
            "text": "*KDD \u201916, August 13-17, 2016, San Francisco, CA, USA*\n\n\u00a9 2016 Copyright held by the owner/author(s).\n\nACM ISBN .",
            "page": 1,
            "x": 52,
            "y": 679,
            "width": 184,
            "height": 34,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "2a1cdd82-a910-4dec-bb00-9579a5f8a798",
            "group_text": "1. INTRODUCTION\n\n    Machine learning and data-driven approaches are becoming very important in many areas.  Smart spam classifiers protect our email by learning from massive amounts of spam data and user feedback; advertising systems learn to match the right ads with the right context; fraud detection systems protect banks from malicious attackers; anomaly event detection systems help experimental physicists to find events that lead to new physics.  There are two important factors that drive these successful applications: usage of effective (statistical) models that capture the complex data dependencies and scalable learning systems that learn the model of interest from large datasets.\n\nAmong the machine learning methods used in practice,\ngradient tree boosting [10]\\textsuperscript{1} is one technique that shines\nin many applications.  Tree boosting has been shown to\ngive state-of-the-art results on many standard classification\nbenchmarks [16]. LambdaMART [5], a variant of tree boosting for ranking, achieves state-of-the-art result for ranking\n\n\u00b9Gradient tree boosting is also known as gradient boosting\nmachine (GBM) or gradient boosted regression tree (GBRT)\n\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\n\n*KDD \u201916, August 13-17, 2016, San Francisco, CA, USA*\n\n\u00a9 2016 Copyright held by the owner/author(s).\n\nACM ISBN .\n\nproblems. Besides being used as a stand-alone predictor, it\nis also incorporated into real-world production pipelines for\nad click through rate prediction [15]. Finally, it is the de-\nfacto choice of ensemble method and is used in challenges\nsuch as the Netflix prize [3].\n\nIn this paper, we describe XGBoost, a scalable machine\nlearning system for tree boosting. The system is available as\nan open source package\\textsuperscript{2}. The impact of the system has been\nwidely recognized in a number of machine learning and data\nmining challenges. Take the challenges hosted by the ma-\nchine learning competition site Kaggle for example. Among\nthe 29 challenge winning solutions\\textsuperscript{3} published at Kaggle\u2019s\nblog during 2015, 17 solutions used XGBoost. Among these\nsolutions, eight solely used XGBoost to train the model,\nwhile most others combined XGBoost with neural nets in en-\nsembles. For comparison, the second most popular method,\ndeep neural nets, was used in 11 solutions. The success\nof the system was also witnessed in KDDCup 2015, where\nXGBoost was used by every winning team in the top-10.\nMoreover, the winning teams reported that ensemble meth-\nods outperform a well-configured XGBoost by only a small\namount\\textsuperscript{[1]}.\n\nThese results demonstrate that our system gives state-of-\nthe-art results on a wide range of problems. Examples of\nthe problems in these winning solutions include: store sales\nprediction; high energy physics event classification; web text\nclassification; customer behavior prediction; motion detec-\ntion; ad click through rate prediction; malware classification;\nproduct categorization; hazard risk prediction; massive on-\nline course dropout rate prediction. While domain depen-\ndent data analysis and feature engineering play an important\nrole in these solutions, the fact that XGBoost is the consen-\nsus choice of learner shows the impact and importance of\nour system and tree boosting.\n\nThe most important factor behind the success of XGBoost\nis its scalability in all scenarios. The system runs more than\nten times faster than existing popular solutions on a single\nmachine and scales to billions of examples in distributed or\nmemory-limited settings. The scalability of XGBoost is due\nto several important systems and algorithmic optimizations.\nThese innovations include: a novel tree learning algorithm\nis for handling _sparse data_; a theoretically justified weighted\nquantile sketch procedure enables handling instance weights\nin approximate tree learning. Parallel and distributed com-\nputing makes learning faster which enables quicker model ex-\nploration. More importantly, XGBoost exploits out-of-core\n\n\u00b2https://github.com/dmlc/xgboost  \n\u00b3Solutions come from of top-3 teams of each competitions.\n\ncomputation and enables data scientists to process hundred\nmillions of examples on a desktop.  Finally, it is even more\nexciting to combine these techniques to make an end-to-end\nsystem that scales to even larger data with the least amount\nof cluster resources.  The major contributions of this paper\nis listed as follows:\n\n- \u2022 We design and build a highly scalable end-to-end tree boosting system.\n\n- \u2022 We propose a theoretically justified weighted quantile sketch for efficient proposal calculation.\n\n- \u2022 We introduce a novel sparsity-aware algorithm for parallel tree learning.\n\n- \u2022 We propose an effective cache-aware block structure for out-of-core tree learning.\n\nWhile there are some existing works on parallel tree boosting\n[22, 23, 19], the directions such as out-of-core compu-\ntation, cache-aware and sparsity-aware learning have not\nbeen explored. More importantly, an end-to-end system\nthat combines all of these aspects gives a novel solution for\nreal-world use-cases. This enables data scientists as well as\nresearchers to build powerful variants of tree boosting al-\ngorithms [7, 8]. Besides these major contributions, we also\nmake additional improvements in proposing a regularized\nlearning objective, which we will include for completeness.\n\nThe remainder of the paper is organized as follows. We\nwill first review tree boosting and introduce a regularized\nobjective in Sec. 2. We then describe the split finding meth-\nods in Sec. 3 as well as the system design in Sec. 4, including\nexperimental results when relevant to provide quantitative\nsupport for each optimization we describe. Related work\nis discussed in Sec. 5. Detailed end-to-end evaluations are\nincluded in Sec. 6. Finally we conclude the paper in Sec. 7."
        },
        {
            "text": "problems. Besides being used as a stand-alone predictor, it\nis also incorporated into real-world production pipelines for\nad click through rate prediction [15]. Finally, it is the de-\nfacto choice of ensemble method and is used in challenges\nsuch as the Netflix prize [3].",
            "page": 1,
            "x": 314,
            "y": 207,
            "width": 243,
            "height": 52,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "37174d59-856a-4cd3-8e3e-c0519bfb025d",
            "group_text": "1. INTRODUCTION\n\n    Machine learning and data-driven approaches are becoming very important in many areas.  Smart spam classifiers protect our email by learning from massive amounts of spam data and user feedback; advertising systems learn to match the right ads with the right context; fraud detection systems protect banks from malicious attackers; anomaly event detection systems help experimental physicists to find events that lead to new physics.  There are two important factors that drive these successful applications: usage of effective (statistical) models that capture the complex data dependencies and scalable learning systems that learn the model of interest from large datasets.\n\nAmong the machine learning methods used in practice,\ngradient tree boosting [10]\\textsuperscript{1} is one technique that shines\nin many applications.  Tree boosting has been shown to\ngive state-of-the-art results on many standard classification\nbenchmarks [16]. LambdaMART [5], a variant of tree boosting for ranking, achieves state-of-the-art result for ranking\n\n\u00b9Gradient tree boosting is also known as gradient boosting\nmachine (GBM) or gradient boosted regression tree (GBRT)\n\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\n\n*KDD \u201916, August 13-17, 2016, San Francisco, CA, USA*\n\n\u00a9 2016 Copyright held by the owner/author(s).\n\nACM ISBN .\n\nproblems. Besides being used as a stand-alone predictor, it\nis also incorporated into real-world production pipelines for\nad click through rate prediction [15]. Finally, it is the de-\nfacto choice of ensemble method and is used in challenges\nsuch as the Netflix prize [3].\n\nIn this paper, we describe XGBoost, a scalable machine\nlearning system for tree boosting. The system is available as\nan open source package\\textsuperscript{2}. The impact of the system has been\nwidely recognized in a number of machine learning and data\nmining challenges. Take the challenges hosted by the ma-\nchine learning competition site Kaggle for example. Among\nthe 29 challenge winning solutions\\textsuperscript{3} published at Kaggle\u2019s\nblog during 2015, 17 solutions used XGBoost. Among these\nsolutions, eight solely used XGBoost to train the model,\nwhile most others combined XGBoost with neural nets in en-\nsembles. For comparison, the second most popular method,\ndeep neural nets, was used in 11 solutions. The success\nof the system was also witnessed in KDDCup 2015, where\nXGBoost was used by every winning team in the top-10.\nMoreover, the winning teams reported that ensemble meth-\nods outperform a well-configured XGBoost by only a small\namount\\textsuperscript{[1]}.\n\nThese results demonstrate that our system gives state-of-\nthe-art results on a wide range of problems. Examples of\nthe problems in these winning solutions include: store sales\nprediction; high energy physics event classification; web text\nclassification; customer behavior prediction; motion detec-\ntion; ad click through rate prediction; malware classification;\nproduct categorization; hazard risk prediction; massive on-\nline course dropout rate prediction. While domain depen-\ndent data analysis and feature engineering play an important\nrole in these solutions, the fact that XGBoost is the consen-\nsus choice of learner shows the impact and importance of\nour system and tree boosting.\n\nThe most important factor behind the success of XGBoost\nis its scalability in all scenarios. The system runs more than\nten times faster than existing popular solutions on a single\nmachine and scales to billions of examples in distributed or\nmemory-limited settings. The scalability of XGBoost is due\nto several important systems and algorithmic optimizations.\nThese innovations include: a novel tree learning algorithm\nis for handling _sparse data_; a theoretically justified weighted\nquantile sketch procedure enables handling instance weights\nin approximate tree learning. Parallel and distributed com-\nputing makes learning faster which enables quicker model ex-\nploration. More importantly, XGBoost exploits out-of-core\n\n\u00b2https://github.com/dmlc/xgboost  \n\u00b3Solutions come from of top-3 teams of each competitions.\n\ncomputation and enables data scientists to process hundred\nmillions of examples on a desktop.  Finally, it is even more\nexciting to combine these techniques to make an end-to-end\nsystem that scales to even larger data with the least amount\nof cluster resources.  The major contributions of this paper\nis listed as follows:\n\n- \u2022 We design and build a highly scalable end-to-end tree boosting system.\n\n- \u2022 We propose a theoretically justified weighted quantile sketch for efficient proposal calculation.\n\n- \u2022 We introduce a novel sparsity-aware algorithm for parallel tree learning.\n\n- \u2022 We propose an effective cache-aware block structure for out-of-core tree learning.\n\nWhile there are some existing works on parallel tree boosting\n[22, 23, 19], the directions such as out-of-core compu-\ntation, cache-aware and sparsity-aware learning have not\nbeen explored. More importantly, an end-to-end system\nthat combines all of these aspects gives a novel solution for\nreal-world use-cases. This enables data scientists as well as\nresearchers to build powerful variants of tree boosting al-\ngorithms [7, 8]. Besides these major contributions, we also\nmake additional improvements in proposing a regularized\nlearning objective, which we will include for completeness.\n\nThe remainder of the paper is organized as follows. We\nwill first review tree boosting and introduce a regularized\nobjective in Sec. 2. We then describe the split finding meth-\nods in Sec. 3 as well as the system design in Sec. 4, including\nexperimental results when relevant to provide quantitative\nsupport for each optimization we describe. Related work\nis discussed in Sec. 5. Detailed end-to-end evaluations are\nincluded in Sec. 6. Finally we conclude the paper in Sec. 7."
        },
        {
            "text": "In this paper, we describe XGBoost, a scalable machine\nlearning system for tree boosting. The system is available as\nan open source package\\textsuperscript{2}. The impact of the system has been\nwidely recognized in a number of machine learning and data\nmining challenges. Take the challenges hosted by the ma-\nchine learning competition site Kaggle for example. Among\nthe 29 challenge winning solutions\\textsuperscript{3} published at Kaggle\u2019s\nblog during 2015, 17 solutions used XGBoost. Among these\nsolutions, eight solely used XGBoost to train the model,\nwhile most others combined XGBoost with neural nets in en-\nsembles. For comparison, the second most popular method,\ndeep neural nets, was used in 11 solutions. The success\nof the system was also witnessed in KDDCup 2015, where\nXGBoost was used by every winning team in the top-10.\nMoreover, the winning teams reported that ensemble meth-\nods outperform a well-configured XGBoost by only a small\namount\\textsuperscript{[1]}.",
            "page": 1,
            "x": 314,
            "y": 261,
            "width": 243,
            "height": 176,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "001c7ece-12e3-4516-88eb-a2ceefa35939",
            "group_text": "1. INTRODUCTION\n\n    Machine learning and data-driven approaches are becoming very important in many areas.  Smart spam classifiers protect our email by learning from massive amounts of spam data and user feedback; advertising systems learn to match the right ads with the right context; fraud detection systems protect banks from malicious attackers; anomaly event detection systems help experimental physicists to find events that lead to new physics.  There are two important factors that drive these successful applications: usage of effective (statistical) models that capture the complex data dependencies and scalable learning systems that learn the model of interest from large datasets.\n\nAmong the machine learning methods used in practice,\ngradient tree boosting [10]\\textsuperscript{1} is one technique that shines\nin many applications.  Tree boosting has been shown to\ngive state-of-the-art results on many standard classification\nbenchmarks [16]. LambdaMART [5], a variant of tree boosting for ranking, achieves state-of-the-art result for ranking\n\n\u00b9Gradient tree boosting is also known as gradient boosting\nmachine (GBM) or gradient boosted regression tree (GBRT)\n\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\n\n*KDD \u201916, August 13-17, 2016, San Francisco, CA, USA*\n\n\u00a9 2016 Copyright held by the owner/author(s).\n\nACM ISBN .\n\nproblems. Besides being used as a stand-alone predictor, it\nis also incorporated into real-world production pipelines for\nad click through rate prediction [15]. Finally, it is the de-\nfacto choice of ensemble method and is used in challenges\nsuch as the Netflix prize [3].\n\nIn this paper, we describe XGBoost, a scalable machine\nlearning system for tree boosting. The system is available as\nan open source package\\textsuperscript{2}. The impact of the system has been\nwidely recognized in a number of machine learning and data\nmining challenges. Take the challenges hosted by the ma-\nchine learning competition site Kaggle for example. Among\nthe 29 challenge winning solutions\\textsuperscript{3} published at Kaggle\u2019s\nblog during 2015, 17 solutions used XGBoost. Among these\nsolutions, eight solely used XGBoost to train the model,\nwhile most others combined XGBoost with neural nets in en-\nsembles. For comparison, the second most popular method,\ndeep neural nets, was used in 11 solutions. The success\nof the system was also witnessed in KDDCup 2015, where\nXGBoost was used by every winning team in the top-10.\nMoreover, the winning teams reported that ensemble meth-\nods outperform a well-configured XGBoost by only a small\namount\\textsuperscript{[1]}.\n\nThese results demonstrate that our system gives state-of-\nthe-art results on a wide range of problems. Examples of\nthe problems in these winning solutions include: store sales\nprediction; high energy physics event classification; web text\nclassification; customer behavior prediction; motion detec-\ntion; ad click through rate prediction; malware classification;\nproduct categorization; hazard risk prediction; massive on-\nline course dropout rate prediction. While domain depen-\ndent data analysis and feature engineering play an important\nrole in these solutions, the fact that XGBoost is the consen-\nsus choice of learner shows the impact and importance of\nour system and tree boosting.\n\nThe most important factor behind the success of XGBoost\nis its scalability in all scenarios. The system runs more than\nten times faster than existing popular solutions on a single\nmachine and scales to billions of examples in distributed or\nmemory-limited settings. The scalability of XGBoost is due\nto several important systems and algorithmic optimizations.\nThese innovations include: a novel tree learning algorithm\nis for handling _sparse data_; a theoretically justified weighted\nquantile sketch procedure enables handling instance weights\nin approximate tree learning. Parallel and distributed com-\nputing makes learning faster which enables quicker model ex-\nploration. More importantly, XGBoost exploits out-of-core\n\n\u00b2https://github.com/dmlc/xgboost  \n\u00b3Solutions come from of top-3 teams of each competitions.\n\ncomputation and enables data scientists to process hundred\nmillions of examples on a desktop.  Finally, it is even more\nexciting to combine these techniques to make an end-to-end\nsystem that scales to even larger data with the least amount\nof cluster resources.  The major contributions of this paper\nis listed as follows:\n\n- \u2022 We design and build a highly scalable end-to-end tree boosting system.\n\n- \u2022 We propose a theoretically justified weighted quantile sketch for efficient proposal calculation.\n\n- \u2022 We introduce a novel sparsity-aware algorithm for parallel tree learning.\n\n- \u2022 We propose an effective cache-aware block structure for out-of-core tree learning.\n\nWhile there are some existing works on parallel tree boosting\n[22, 23, 19], the directions such as out-of-core compu-\ntation, cache-aware and sparsity-aware learning have not\nbeen explored. More importantly, an end-to-end system\nthat combines all of these aspects gives a novel solution for\nreal-world use-cases. This enables data scientists as well as\nresearchers to build powerful variants of tree boosting al-\ngorithms [7, 8]. Besides these major contributions, we also\nmake additional improvements in proposing a regularized\nlearning objective, which we will include for completeness.\n\nThe remainder of the paper is organized as follows. We\nwill first review tree boosting and introduce a regularized\nobjective in Sec. 2. We then describe the split finding meth-\nods in Sec. 3 as well as the system design in Sec. 4, including\nexperimental results when relevant to provide quantitative\nsupport for each optimization we describe. Related work\nis discussed in Sec. 5. Detailed end-to-end evaluations are\nincluded in Sec. 6. Finally we conclude the paper in Sec. 7."
        },
        {
            "text": "These results demonstrate that our system gives state-of-\nthe-art results on a wide range of problems. Examples of\nthe problems in these winning solutions include: store sales\nprediction; high energy physics event classification; web text\nclassification; customer behavior prediction; motion detec-\ntion; ad click through rate prediction; malware classification;\nproduct categorization; hazard risk prediction; massive on-\nline course dropout rate prediction. While domain depen-\ndent data analysis and feature engineering play an important\nrole in these solutions, the fact that XGBoost is the consen-\nsus choice of learner shows the impact and importance of\nour system and tree boosting.",
            "page": 1,
            "x": 314,
            "y": 438,
            "width": 243,
            "height": 124,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "9e071a97-14aa-4cc3-8247-b1da493c7b68",
            "group_text": "1. INTRODUCTION\n\n    Machine learning and data-driven approaches are becoming very important in many areas.  Smart spam classifiers protect our email by learning from massive amounts of spam data and user feedback; advertising systems learn to match the right ads with the right context; fraud detection systems protect banks from malicious attackers; anomaly event detection systems help experimental physicists to find events that lead to new physics.  There are two important factors that drive these successful applications: usage of effective (statistical) models that capture the complex data dependencies and scalable learning systems that learn the model of interest from large datasets.\n\nAmong the machine learning methods used in practice,\ngradient tree boosting [10]\\textsuperscript{1} is one technique that shines\nin many applications.  Tree boosting has been shown to\ngive state-of-the-art results on many standard classification\nbenchmarks [16]. LambdaMART [5], a variant of tree boosting for ranking, achieves state-of-the-art result for ranking\n\n\u00b9Gradient tree boosting is also known as gradient boosting\nmachine (GBM) or gradient boosted regression tree (GBRT)\n\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\n\n*KDD \u201916, August 13-17, 2016, San Francisco, CA, USA*\n\n\u00a9 2016 Copyright held by the owner/author(s).\n\nACM ISBN .\n\nproblems. Besides being used as a stand-alone predictor, it\nis also incorporated into real-world production pipelines for\nad click through rate prediction [15]. Finally, it is the de-\nfacto choice of ensemble method and is used in challenges\nsuch as the Netflix prize [3].\n\nIn this paper, we describe XGBoost, a scalable machine\nlearning system for tree boosting. The system is available as\nan open source package\\textsuperscript{2}. The impact of the system has been\nwidely recognized in a number of machine learning and data\nmining challenges. Take the challenges hosted by the ma-\nchine learning competition site Kaggle for example. Among\nthe 29 challenge winning solutions\\textsuperscript{3} published at Kaggle\u2019s\nblog during 2015, 17 solutions used XGBoost. Among these\nsolutions, eight solely used XGBoost to train the model,\nwhile most others combined XGBoost with neural nets in en-\nsembles. For comparison, the second most popular method,\ndeep neural nets, was used in 11 solutions. The success\nof the system was also witnessed in KDDCup 2015, where\nXGBoost was used by every winning team in the top-10.\nMoreover, the winning teams reported that ensemble meth-\nods outperform a well-configured XGBoost by only a small\namount\\textsuperscript{[1]}.\n\nThese results demonstrate that our system gives state-of-\nthe-art results on a wide range of problems. Examples of\nthe problems in these winning solutions include: store sales\nprediction; high energy physics event classification; web text\nclassification; customer behavior prediction; motion detec-\ntion; ad click through rate prediction; malware classification;\nproduct categorization; hazard risk prediction; massive on-\nline course dropout rate prediction. While domain depen-\ndent data analysis and feature engineering play an important\nrole in these solutions, the fact that XGBoost is the consen-\nsus choice of learner shows the impact and importance of\nour system and tree boosting.\n\nThe most important factor behind the success of XGBoost\nis its scalability in all scenarios. The system runs more than\nten times faster than existing popular solutions on a single\nmachine and scales to billions of examples in distributed or\nmemory-limited settings. The scalability of XGBoost is due\nto several important systems and algorithmic optimizations.\nThese innovations include: a novel tree learning algorithm\nis for handling _sparse data_; a theoretically justified weighted\nquantile sketch procedure enables handling instance weights\nin approximate tree learning. Parallel and distributed com-\nputing makes learning faster which enables quicker model ex-\nploration. More importantly, XGBoost exploits out-of-core\n\n\u00b2https://github.com/dmlc/xgboost  \n\u00b3Solutions come from of top-3 teams of each competitions.\n\ncomputation and enables data scientists to process hundred\nmillions of examples on a desktop.  Finally, it is even more\nexciting to combine these techniques to make an end-to-end\nsystem that scales to even larger data with the least amount\nof cluster resources.  The major contributions of this paper\nis listed as follows:\n\n- \u2022 We design and build a highly scalable end-to-end tree boosting system.\n\n- \u2022 We propose a theoretically justified weighted quantile sketch for efficient proposal calculation.\n\n- \u2022 We introduce a novel sparsity-aware algorithm for parallel tree learning.\n\n- \u2022 We propose an effective cache-aware block structure for out-of-core tree learning.\n\nWhile there are some existing works on parallel tree boosting\n[22, 23, 19], the directions such as out-of-core compu-\ntation, cache-aware and sparsity-aware learning have not\nbeen explored. More importantly, an end-to-end system\nthat combines all of these aspects gives a novel solution for\nreal-world use-cases. This enables data scientists as well as\nresearchers to build powerful variants of tree boosting al-\ngorithms [7, 8]. Besides these major contributions, we also\nmake additional improvements in proposing a regularized\nlearning objective, which we will include for completeness.\n\nThe remainder of the paper is organized as follows. We\nwill first review tree boosting and introduce a regularized\nobjective in Sec. 2. We then describe the split finding meth-\nods in Sec. 3 as well as the system design in Sec. 4, including\nexperimental results when relevant to provide quantitative\nsupport for each optimization we describe. Related work\nis discussed in Sec. 5. Detailed end-to-end evaluations are\nincluded in Sec. 6. Finally we conclude the paper in Sec. 7."
        },
        {
            "text": "The most important factor behind the success of XGBoost\nis its scalability in all scenarios. The system runs more than\nten times faster than existing popular solutions on a single\nmachine and scales to billions of examples in distributed or\nmemory-limited settings. The scalability of XGBoost is due\nto several important systems and algorithmic optimizations.\nThese innovations include: a novel tree learning algorithm\nis for handling _sparse data_; a theoretically justified weighted\nquantile sketch procedure enables handling instance weights\nin approximate tree learning. Parallel and distributed com-\nputing makes learning faster which enables quicker model ex-\nploration. More importantly, XGBoost exploits out-of-core",
            "page": 1,
            "x": 315,
            "y": 564,
            "width": 242,
            "height": 126,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "982c3725-2ab7-4e16-b38b-259c6762e202",
            "group_text": "1. INTRODUCTION\n\n    Machine learning and data-driven approaches are becoming very important in many areas.  Smart spam classifiers protect our email by learning from massive amounts of spam data and user feedback; advertising systems learn to match the right ads with the right context; fraud detection systems protect banks from malicious attackers; anomaly event detection systems help experimental physicists to find events that lead to new physics.  There are two important factors that drive these successful applications: usage of effective (statistical) models that capture the complex data dependencies and scalable learning systems that learn the model of interest from large datasets.\n\nAmong the machine learning methods used in practice,\ngradient tree boosting [10]\\textsuperscript{1} is one technique that shines\nin many applications.  Tree boosting has been shown to\ngive state-of-the-art results on many standard classification\nbenchmarks [16]. LambdaMART [5], a variant of tree boosting for ranking, achieves state-of-the-art result for ranking\n\n\u00b9Gradient tree boosting is also known as gradient boosting\nmachine (GBM) or gradient boosted regression tree (GBRT)\n\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\n\n*KDD \u201916, August 13-17, 2016, San Francisco, CA, USA*\n\n\u00a9 2016 Copyright held by the owner/author(s).\n\nACM ISBN .\n\nproblems. Besides being used as a stand-alone predictor, it\nis also incorporated into real-world production pipelines for\nad click through rate prediction [15]. Finally, it is the de-\nfacto choice of ensemble method and is used in challenges\nsuch as the Netflix prize [3].\n\nIn this paper, we describe XGBoost, a scalable machine\nlearning system for tree boosting. The system is available as\nan open source package\\textsuperscript{2}. The impact of the system has been\nwidely recognized in a number of machine learning and data\nmining challenges. Take the challenges hosted by the ma-\nchine learning competition site Kaggle for example. Among\nthe 29 challenge winning solutions\\textsuperscript{3} published at Kaggle\u2019s\nblog during 2015, 17 solutions used XGBoost. Among these\nsolutions, eight solely used XGBoost to train the model,\nwhile most others combined XGBoost with neural nets in en-\nsembles. For comparison, the second most popular method,\ndeep neural nets, was used in 11 solutions. The success\nof the system was also witnessed in KDDCup 2015, where\nXGBoost was used by every winning team in the top-10.\nMoreover, the winning teams reported that ensemble meth-\nods outperform a well-configured XGBoost by only a small\namount\\textsuperscript{[1]}.\n\nThese results demonstrate that our system gives state-of-\nthe-art results on a wide range of problems. Examples of\nthe problems in these winning solutions include: store sales\nprediction; high energy physics event classification; web text\nclassification; customer behavior prediction; motion detec-\ntion; ad click through rate prediction; malware classification;\nproduct categorization; hazard risk prediction; massive on-\nline course dropout rate prediction. While domain depen-\ndent data analysis and feature engineering play an important\nrole in these solutions, the fact that XGBoost is the consen-\nsus choice of learner shows the impact and importance of\nour system and tree boosting.\n\nThe most important factor behind the success of XGBoost\nis its scalability in all scenarios. The system runs more than\nten times faster than existing popular solutions on a single\nmachine and scales to billions of examples in distributed or\nmemory-limited settings. The scalability of XGBoost is due\nto several important systems and algorithmic optimizations.\nThese innovations include: a novel tree learning algorithm\nis for handling _sparse data_; a theoretically justified weighted\nquantile sketch procedure enables handling instance weights\nin approximate tree learning. Parallel and distributed com-\nputing makes learning faster which enables quicker model ex-\nploration. More importantly, XGBoost exploits out-of-core\n\n\u00b2https://github.com/dmlc/xgboost  \n\u00b3Solutions come from of top-3 teams of each competitions.\n\ncomputation and enables data scientists to process hundred\nmillions of examples on a desktop.  Finally, it is even more\nexciting to combine these techniques to make an end-to-end\nsystem that scales to even larger data with the least amount\nof cluster resources.  The major contributions of this paper\nis listed as follows:\n\n- \u2022 We design and build a highly scalable end-to-end tree boosting system.\n\n- \u2022 We propose a theoretically justified weighted quantile sketch for efficient proposal calculation.\n\n- \u2022 We introduce a novel sparsity-aware algorithm for parallel tree learning.\n\n- \u2022 We propose an effective cache-aware block structure for out-of-core tree learning.\n\nWhile there are some existing works on parallel tree boosting\n[22, 23, 19], the directions such as out-of-core compu-\ntation, cache-aware and sparsity-aware learning have not\nbeen explored. More importantly, an end-to-end system\nthat combines all of these aspects gives a novel solution for\nreal-world use-cases. This enables data scientists as well as\nresearchers to build powerful variants of tree boosting al-\ngorithms [7, 8]. Besides these major contributions, we also\nmake additional improvements in proposing a regularized\nlearning objective, which we will include for completeness.\n\nThe remainder of the paper is organized as follows. We\nwill first review tree boosting and introduce a regularized\nobjective in Sec. 2. We then describe the split finding meth-\nods in Sec. 3 as well as the system design in Sec. 4, including\nexperimental results when relevant to provide quantitative\nsupport for each optimization we describe. Related work\nis discussed in Sec. 5. Detailed end-to-end evaluations are\nincluded in Sec. 6. Finally we conclude the paper in Sec. 7."
        },
        {
            "text": "\u00b2https://github.com/dmlc/xgboost  \n\u00b3Solutions come from of top-3 teams of each competitions.",
            "page": 1,
            "x": 315,
            "y": 697,
            "width": 238,
            "height": 22,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "fad08b57-ebe7-45f8-90a8-95852ef48c10",
            "group_text": "1. INTRODUCTION\n\n    Machine learning and data-driven approaches are becoming very important in many areas.  Smart spam classifiers protect our email by learning from massive amounts of spam data and user feedback; advertising systems learn to match the right ads with the right context; fraud detection systems protect banks from malicious attackers; anomaly event detection systems help experimental physicists to find events that lead to new physics.  There are two important factors that drive these successful applications: usage of effective (statistical) models that capture the complex data dependencies and scalable learning systems that learn the model of interest from large datasets.\n\nAmong the machine learning methods used in practice,\ngradient tree boosting [10]\\textsuperscript{1} is one technique that shines\nin many applications.  Tree boosting has been shown to\ngive state-of-the-art results on many standard classification\nbenchmarks [16]. LambdaMART [5], a variant of tree boosting for ranking, achieves state-of-the-art result for ranking\n\n\u00b9Gradient tree boosting is also known as gradient boosting\nmachine (GBM) or gradient boosted regression tree (GBRT)\n\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\n\n*KDD \u201916, August 13-17, 2016, San Francisco, CA, USA*\n\n\u00a9 2016 Copyright held by the owner/author(s).\n\nACM ISBN .\n\nproblems. Besides being used as a stand-alone predictor, it\nis also incorporated into real-world production pipelines for\nad click through rate prediction [15]. Finally, it is the de-\nfacto choice of ensemble method and is used in challenges\nsuch as the Netflix prize [3].\n\nIn this paper, we describe XGBoost, a scalable machine\nlearning system for tree boosting. The system is available as\nan open source package\\textsuperscript{2}. The impact of the system has been\nwidely recognized in a number of machine learning and data\nmining challenges. Take the challenges hosted by the ma-\nchine learning competition site Kaggle for example. Among\nthe 29 challenge winning solutions\\textsuperscript{3} published at Kaggle\u2019s\nblog during 2015, 17 solutions used XGBoost. Among these\nsolutions, eight solely used XGBoost to train the model,\nwhile most others combined XGBoost with neural nets in en-\nsembles. For comparison, the second most popular method,\ndeep neural nets, was used in 11 solutions. The success\nof the system was also witnessed in KDDCup 2015, where\nXGBoost was used by every winning team in the top-10.\nMoreover, the winning teams reported that ensemble meth-\nods outperform a well-configured XGBoost by only a small\namount\\textsuperscript{[1]}.\n\nThese results demonstrate that our system gives state-of-\nthe-art results on a wide range of problems. Examples of\nthe problems in these winning solutions include: store sales\nprediction; high energy physics event classification; web text\nclassification; customer behavior prediction; motion detec-\ntion; ad click through rate prediction; malware classification;\nproduct categorization; hazard risk prediction; massive on-\nline course dropout rate prediction. While domain depen-\ndent data analysis and feature engineering play an important\nrole in these solutions, the fact that XGBoost is the consen-\nsus choice of learner shows the impact and importance of\nour system and tree boosting.\n\nThe most important factor behind the success of XGBoost\nis its scalability in all scenarios. The system runs more than\nten times faster than existing popular solutions on a single\nmachine and scales to billions of examples in distributed or\nmemory-limited settings. The scalability of XGBoost is due\nto several important systems and algorithmic optimizations.\nThese innovations include: a novel tree learning algorithm\nis for handling _sparse data_; a theoretically justified weighted\nquantile sketch procedure enables handling instance weights\nin approximate tree learning. Parallel and distributed com-\nputing makes learning faster which enables quicker model ex-\nploration. More importantly, XGBoost exploits out-of-core\n\n\u00b2https://github.com/dmlc/xgboost  \n\u00b3Solutions come from of top-3 teams of each competitions.\n\ncomputation and enables data scientists to process hundred\nmillions of examples on a desktop.  Finally, it is even more\nexciting to combine these techniques to make an end-to-end\nsystem that scales to even larger data with the least amount\nof cluster resources.  The major contributions of this paper\nis listed as follows:\n\n- \u2022 We design and build a highly scalable end-to-end tree boosting system.\n\n- \u2022 We propose a theoretically justified weighted quantile sketch for efficient proposal calculation.\n\n- \u2022 We introduce a novel sparsity-aware algorithm for parallel tree learning.\n\n- \u2022 We propose an effective cache-aware block structure for out-of-core tree learning.\n\nWhile there are some existing works on parallel tree boosting\n[22, 23, 19], the directions such as out-of-core compu-\ntation, cache-aware and sparsity-aware learning have not\nbeen explored. More importantly, an end-to-end system\nthat combines all of these aspects gives a novel solution for\nreal-world use-cases. This enables data scientists as well as\nresearchers to build powerful variants of tree boosting al-\ngorithms [7, 8]. Besides these major contributions, we also\nmake additional improvements in proposing a regularized\nlearning objective, which we will include for completeness.\n\nThe remainder of the paper is organized as follows. We\nwill first review tree boosting and introduce a regularized\nobjective in Sec. 2. We then describe the split finding meth-\nods in Sec. 3 as well as the system design in Sec. 4, including\nexperimental results when relevant to provide quantitative\nsupport for each optimization we describe. Related work\nis discussed in Sec. 5. Detailed end-to-end evaluations are\nincluded in Sec. 6. Finally we conclude the paper in Sec. 7."
        },
        {
            "text": "computation and enables data scientists to process hundred\nmillions of examples on a desktop.  Finally, it is even more\nexciting to combine these techniques to make an end-to-end\nsystem that scales to even larger data with the least amount\nof cluster resources.  The major contributions of this paper\nis listed as follows:",
            "page": 2,
            "x": 49,
            "y": 54,
            "width": 246,
            "height": 65,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "fa5c94c0-bc2d-4042-9151-20f16132ecc9",
            "group_text": "1. INTRODUCTION\n\n    Machine learning and data-driven approaches are becoming very important in many areas.  Smart spam classifiers protect our email by learning from massive amounts of spam data and user feedback; advertising systems learn to match the right ads with the right context; fraud detection systems protect banks from malicious attackers; anomaly event detection systems help experimental physicists to find events that lead to new physics.  There are two important factors that drive these successful applications: usage of effective (statistical) models that capture the complex data dependencies and scalable learning systems that learn the model of interest from large datasets.\n\nAmong the machine learning methods used in practice,\ngradient tree boosting [10]\\textsuperscript{1} is one technique that shines\nin many applications.  Tree boosting has been shown to\ngive state-of-the-art results on many standard classification\nbenchmarks [16]. LambdaMART [5], a variant of tree boosting for ranking, achieves state-of-the-art result for ranking\n\n\u00b9Gradient tree boosting is also known as gradient boosting\nmachine (GBM) or gradient boosted regression tree (GBRT)\n\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\n\n*KDD \u201916, August 13-17, 2016, San Francisco, CA, USA*\n\n\u00a9 2016 Copyright held by the owner/author(s).\n\nACM ISBN .\n\nproblems. Besides being used as a stand-alone predictor, it\nis also incorporated into real-world production pipelines for\nad click through rate prediction [15]. Finally, it is the de-\nfacto choice of ensemble method and is used in challenges\nsuch as the Netflix prize [3].\n\nIn this paper, we describe XGBoost, a scalable machine\nlearning system for tree boosting. The system is available as\nan open source package\\textsuperscript{2}. The impact of the system has been\nwidely recognized in a number of machine learning and data\nmining challenges. Take the challenges hosted by the ma-\nchine learning competition site Kaggle for example. Among\nthe 29 challenge winning solutions\\textsuperscript{3} published at Kaggle\u2019s\nblog during 2015, 17 solutions used XGBoost. Among these\nsolutions, eight solely used XGBoost to train the model,\nwhile most others combined XGBoost with neural nets in en-\nsembles. For comparison, the second most popular method,\ndeep neural nets, was used in 11 solutions. The success\nof the system was also witnessed in KDDCup 2015, where\nXGBoost was used by every winning team in the top-10.\nMoreover, the winning teams reported that ensemble meth-\nods outperform a well-configured XGBoost by only a small\namount\\textsuperscript{[1]}.\n\nThese results demonstrate that our system gives state-of-\nthe-art results on a wide range of problems. Examples of\nthe problems in these winning solutions include: store sales\nprediction; high energy physics event classification; web text\nclassification; customer behavior prediction; motion detec-\ntion; ad click through rate prediction; malware classification;\nproduct categorization; hazard risk prediction; massive on-\nline course dropout rate prediction. While domain depen-\ndent data analysis and feature engineering play an important\nrole in these solutions, the fact that XGBoost is the consen-\nsus choice of learner shows the impact and importance of\nour system and tree boosting.\n\nThe most important factor behind the success of XGBoost\nis its scalability in all scenarios. The system runs more than\nten times faster than existing popular solutions on a single\nmachine and scales to billions of examples in distributed or\nmemory-limited settings. The scalability of XGBoost is due\nto several important systems and algorithmic optimizations.\nThese innovations include: a novel tree learning algorithm\nis for handling _sparse data_; a theoretically justified weighted\nquantile sketch procedure enables handling instance weights\nin approximate tree learning. Parallel and distributed com-\nputing makes learning faster which enables quicker model ex-\nploration. More importantly, XGBoost exploits out-of-core\n\n\u00b2https://github.com/dmlc/xgboost  \n\u00b3Solutions come from of top-3 teams of each competitions.\n\ncomputation and enables data scientists to process hundred\nmillions of examples on a desktop.  Finally, it is even more\nexciting to combine these techniques to make an end-to-end\nsystem that scales to even larger data with the least amount\nof cluster resources.  The major contributions of this paper\nis listed as follows:\n\n- \u2022 We design and build a highly scalable end-to-end tree boosting system.\n\n- \u2022 We propose a theoretically justified weighted quantile sketch for efficient proposal calculation.\n\n- \u2022 We introduce a novel sparsity-aware algorithm for parallel tree learning.\n\n- \u2022 We propose an effective cache-aware block structure for out-of-core tree learning.\n\nWhile there are some existing works on parallel tree boosting\n[22, 23, 19], the directions such as out-of-core compu-\ntation, cache-aware and sparsity-aware learning have not\nbeen explored. More importantly, an end-to-end system\nthat combines all of these aspects gives a novel solution for\nreal-world use-cases. This enables data scientists as well as\nresearchers to build powerful variants of tree boosting al-\ngorithms [7, 8]. Besides these major contributions, we also\nmake additional improvements in proposing a regularized\nlearning objective, which we will include for completeness.\n\nThe remainder of the paper is organized as follows. We\nwill first review tree boosting and introduce a regularized\nobjective in Sec. 2. We then describe the split finding meth-\nods in Sec. 3 as well as the system design in Sec. 4, including\nexperimental results when relevant to provide quantitative\nsupport for each optimization we describe. Related work\nis discussed in Sec. 5. Detailed end-to-end evaluations are\nincluded in Sec. 6. Finally we conclude the paper in Sec. 7."
        },
        {
            "text": "- \u2022 We design and build a highly scalable end-to-end tree boosting system.\n\n- \u2022 We propose a theoretically justified weighted quantile sketch for efficient proposal calculation.\n\n- \u2022 We introduce a novel sparsity-aware algorithm for parallel tree learning.\n\n- \u2022 We propose an effective cache-aware block structure for out-of-core tree learning.",
            "page": 2,
            "x": 62,
            "y": 123,
            "width": 235,
            "height": 112,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "85b9ee9a-1e5c-465a-8bc9-ee6acca3ce33",
            "group_text": "1. INTRODUCTION\n\n    Machine learning and data-driven approaches are becoming very important in many areas.  Smart spam classifiers protect our email by learning from massive amounts of spam data and user feedback; advertising systems learn to match the right ads with the right context; fraud detection systems protect banks from malicious attackers; anomaly event detection systems help experimental physicists to find events that lead to new physics.  There are two important factors that drive these successful applications: usage of effective (statistical) models that capture the complex data dependencies and scalable learning systems that learn the model of interest from large datasets.\n\nAmong the machine learning methods used in practice,\ngradient tree boosting [10]\\textsuperscript{1} is one technique that shines\nin many applications.  Tree boosting has been shown to\ngive state-of-the-art results on many standard classification\nbenchmarks [16]. LambdaMART [5], a variant of tree boosting for ranking, achieves state-of-the-art result for ranking\n\n\u00b9Gradient tree boosting is also known as gradient boosting\nmachine (GBM) or gradient boosted regression tree (GBRT)\n\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\n\n*KDD \u201916, August 13-17, 2016, San Francisco, CA, USA*\n\n\u00a9 2016 Copyright held by the owner/author(s).\n\nACM ISBN .\n\nproblems. Besides being used as a stand-alone predictor, it\nis also incorporated into real-world production pipelines for\nad click through rate prediction [15]. Finally, it is the de-\nfacto choice of ensemble method and is used in challenges\nsuch as the Netflix prize [3].\n\nIn this paper, we describe XGBoost, a scalable machine\nlearning system for tree boosting. The system is available as\nan open source package\\textsuperscript{2}. The impact of the system has been\nwidely recognized in a number of machine learning and data\nmining challenges. Take the challenges hosted by the ma-\nchine learning competition site Kaggle for example. Among\nthe 29 challenge winning solutions\\textsuperscript{3} published at Kaggle\u2019s\nblog during 2015, 17 solutions used XGBoost. Among these\nsolutions, eight solely used XGBoost to train the model,\nwhile most others combined XGBoost with neural nets in en-\nsembles. For comparison, the second most popular method,\ndeep neural nets, was used in 11 solutions. The success\nof the system was also witnessed in KDDCup 2015, where\nXGBoost was used by every winning team in the top-10.\nMoreover, the winning teams reported that ensemble meth-\nods outperform a well-configured XGBoost by only a small\namount\\textsuperscript{[1]}.\n\nThese results demonstrate that our system gives state-of-\nthe-art results on a wide range of problems. Examples of\nthe problems in these winning solutions include: store sales\nprediction; high energy physics event classification; web text\nclassification; customer behavior prediction; motion detec-\ntion; ad click through rate prediction; malware classification;\nproduct categorization; hazard risk prediction; massive on-\nline course dropout rate prediction. While domain depen-\ndent data analysis and feature engineering play an important\nrole in these solutions, the fact that XGBoost is the consen-\nsus choice of learner shows the impact and importance of\nour system and tree boosting.\n\nThe most important factor behind the success of XGBoost\nis its scalability in all scenarios. The system runs more than\nten times faster than existing popular solutions on a single\nmachine and scales to billions of examples in distributed or\nmemory-limited settings. The scalability of XGBoost is due\nto several important systems and algorithmic optimizations.\nThese innovations include: a novel tree learning algorithm\nis for handling _sparse data_; a theoretically justified weighted\nquantile sketch procedure enables handling instance weights\nin approximate tree learning. Parallel and distributed com-\nputing makes learning faster which enables quicker model ex-\nploration. More importantly, XGBoost exploits out-of-core\n\n\u00b2https://github.com/dmlc/xgboost  \n\u00b3Solutions come from of top-3 teams of each competitions.\n\ncomputation and enables data scientists to process hundred\nmillions of examples on a desktop.  Finally, it is even more\nexciting to combine these techniques to make an end-to-end\nsystem that scales to even larger data with the least amount\nof cluster resources.  The major contributions of this paper\nis listed as follows:\n\n- \u2022 We design and build a highly scalable end-to-end tree boosting system.\n\n- \u2022 We propose a theoretically justified weighted quantile sketch for efficient proposal calculation.\n\n- \u2022 We introduce a novel sparsity-aware algorithm for parallel tree learning.\n\n- \u2022 We propose an effective cache-aware block structure for out-of-core tree learning.\n\nWhile there are some existing works on parallel tree boosting\n[22, 23, 19], the directions such as out-of-core compu-\ntation, cache-aware and sparsity-aware learning have not\nbeen explored. More importantly, an end-to-end system\nthat combines all of these aspects gives a novel solution for\nreal-world use-cases. This enables data scientists as well as\nresearchers to build powerful variants of tree boosting al-\ngorithms [7, 8]. Besides these major contributions, we also\nmake additional improvements in proposing a regularized\nlearning objective, which we will include for completeness.\n\nThe remainder of the paper is organized as follows. We\nwill first review tree boosting and introduce a regularized\nobjective in Sec. 2. We then describe the split finding meth-\nods in Sec. 3 as well as the system design in Sec. 4, including\nexperimental results when relevant to provide quantitative\nsupport for each optimization we describe. Related work\nis discussed in Sec. 5. Detailed end-to-end evaluations are\nincluded in Sec. 6. Finally we conclude the paper in Sec. 7."
        },
        {
            "text": "While there are some existing works on parallel tree boosting\n[22, 23, 19], the directions such as out-of-core compu-\ntation, cache-aware and sparsity-aware learning have not\nbeen explored. More importantly, an end-to-end system\nthat combines all of these aspects gives a novel solution for\nreal-world use-cases. This enables data scientists as well as\nresearchers to build powerful variants of tree boosting al-\ngorithms [7, 8]. Besides these major contributions, we also\nmake additional improvements in proposing a regularized\nlearning objective, which we will include for completeness.",
            "page": 2,
            "x": 50,
            "y": 240,
            "width": 245,
            "height": 106,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "1e76662a-175c-48c2-b231-469b052a7ba3",
            "group_text": "1. INTRODUCTION\n\n    Machine learning and data-driven approaches are becoming very important in many areas.  Smart spam classifiers protect our email by learning from massive amounts of spam data and user feedback; advertising systems learn to match the right ads with the right context; fraud detection systems protect banks from malicious attackers; anomaly event detection systems help experimental physicists to find events that lead to new physics.  There are two important factors that drive these successful applications: usage of effective (statistical) models that capture the complex data dependencies and scalable learning systems that learn the model of interest from large datasets.\n\nAmong the machine learning methods used in practice,\ngradient tree boosting [10]\\textsuperscript{1} is one technique that shines\nin many applications.  Tree boosting has been shown to\ngive state-of-the-art results on many standard classification\nbenchmarks [16]. LambdaMART [5], a variant of tree boosting for ranking, achieves state-of-the-art result for ranking\n\n\u00b9Gradient tree boosting is also known as gradient boosting\nmachine (GBM) or gradient boosted regression tree (GBRT)\n\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\n\n*KDD \u201916, August 13-17, 2016, San Francisco, CA, USA*\n\n\u00a9 2016 Copyright held by the owner/author(s).\n\nACM ISBN .\n\nproblems. Besides being used as a stand-alone predictor, it\nis also incorporated into real-world production pipelines for\nad click through rate prediction [15]. Finally, it is the de-\nfacto choice of ensemble method and is used in challenges\nsuch as the Netflix prize [3].\n\nIn this paper, we describe XGBoost, a scalable machine\nlearning system for tree boosting. The system is available as\nan open source package\\textsuperscript{2}. The impact of the system has been\nwidely recognized in a number of machine learning and data\nmining challenges. Take the challenges hosted by the ma-\nchine learning competition site Kaggle for example. Among\nthe 29 challenge winning solutions\\textsuperscript{3} published at Kaggle\u2019s\nblog during 2015, 17 solutions used XGBoost. Among these\nsolutions, eight solely used XGBoost to train the model,\nwhile most others combined XGBoost with neural nets in en-\nsembles. For comparison, the second most popular method,\ndeep neural nets, was used in 11 solutions. The success\nof the system was also witnessed in KDDCup 2015, where\nXGBoost was used by every winning team in the top-10.\nMoreover, the winning teams reported that ensemble meth-\nods outperform a well-configured XGBoost by only a small\namount\\textsuperscript{[1]}.\n\nThese results demonstrate that our system gives state-of-\nthe-art results on a wide range of problems. Examples of\nthe problems in these winning solutions include: store sales\nprediction; high energy physics event classification; web text\nclassification; customer behavior prediction; motion detec-\ntion; ad click through rate prediction; malware classification;\nproduct categorization; hazard risk prediction; massive on-\nline course dropout rate prediction. While domain depen-\ndent data analysis and feature engineering play an important\nrole in these solutions, the fact that XGBoost is the consen-\nsus choice of learner shows the impact and importance of\nour system and tree boosting.\n\nThe most important factor behind the success of XGBoost\nis its scalability in all scenarios. The system runs more than\nten times faster than existing popular solutions on a single\nmachine and scales to billions of examples in distributed or\nmemory-limited settings. The scalability of XGBoost is due\nto several important systems and algorithmic optimizations.\nThese innovations include: a novel tree learning algorithm\nis for handling _sparse data_; a theoretically justified weighted\nquantile sketch procedure enables handling instance weights\nin approximate tree learning. Parallel and distributed com-\nputing makes learning faster which enables quicker model ex-\nploration. More importantly, XGBoost exploits out-of-core\n\n\u00b2https://github.com/dmlc/xgboost  \n\u00b3Solutions come from of top-3 teams of each competitions.\n\ncomputation and enables data scientists to process hundred\nmillions of examples on a desktop.  Finally, it is even more\nexciting to combine these techniques to make an end-to-end\nsystem that scales to even larger data with the least amount\nof cluster resources.  The major contributions of this paper\nis listed as follows:\n\n- \u2022 We design and build a highly scalable end-to-end tree boosting system.\n\n- \u2022 We propose a theoretically justified weighted quantile sketch for efficient proposal calculation.\n\n- \u2022 We introduce a novel sparsity-aware algorithm for parallel tree learning.\n\n- \u2022 We propose an effective cache-aware block structure for out-of-core tree learning.\n\nWhile there are some existing works on parallel tree boosting\n[22, 23, 19], the directions such as out-of-core compu-\ntation, cache-aware and sparsity-aware learning have not\nbeen explored. More importantly, an end-to-end system\nthat combines all of these aspects gives a novel solution for\nreal-world use-cases. This enables data scientists as well as\nresearchers to build powerful variants of tree boosting al-\ngorithms [7, 8]. Besides these major contributions, we also\nmake additional improvements in proposing a regularized\nlearning objective, which we will include for completeness.\n\nThe remainder of the paper is organized as follows. We\nwill first review tree boosting and introduce a regularized\nobjective in Sec. 2. We then describe the split finding meth-\nods in Sec. 3 as well as the system design in Sec. 4, including\nexperimental results when relevant to provide quantitative\nsupport for each optimization we describe. Related work\nis discussed in Sec. 5. Detailed end-to-end evaluations are\nincluded in Sec. 6. Finally we conclude the paper in Sec. 7."
        },
        {
            "text": "The remainder of the paper is organized as follows. We\nwill first review tree boosting and introduce a regularized\nobjective in Sec. 2. We then describe the split finding meth-\nods in Sec. 3 as well as the system design in Sec. 4, including\nexperimental results when relevant to provide quantitative\nsupport for each optimization we describe. Related work\nis discussed in Sec. 5. Detailed end-to-end evaluations are\nincluded in Sec. 6. Finally we conclude the paper in Sec. 7.",
            "page": 2,
            "x": 51,
            "y": 347,
            "width": 244,
            "height": 84,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "1fed8597-092d-468c-9612-37a7aadd4de4",
            "group_text": "1. INTRODUCTION\n\n    Machine learning and data-driven approaches are becoming very important in many areas.  Smart spam classifiers protect our email by learning from massive amounts of spam data and user feedback; advertising systems learn to match the right ads with the right context; fraud detection systems protect banks from malicious attackers; anomaly event detection systems help experimental physicists to find events that lead to new physics.  There are two important factors that drive these successful applications: usage of effective (statistical) models that capture the complex data dependencies and scalable learning systems that learn the model of interest from large datasets.\n\nAmong the machine learning methods used in practice,\ngradient tree boosting [10]\\textsuperscript{1} is one technique that shines\nin many applications.  Tree boosting has been shown to\ngive state-of-the-art results on many standard classification\nbenchmarks [16]. LambdaMART [5], a variant of tree boosting for ranking, achieves state-of-the-art result for ranking\n\n\u00b9Gradient tree boosting is also known as gradient boosting\nmachine (GBM) or gradient boosted regression tree (GBRT)\n\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\n\n*KDD \u201916, August 13-17, 2016, San Francisco, CA, USA*\n\n\u00a9 2016 Copyright held by the owner/author(s).\n\nACM ISBN .\n\nproblems. Besides being used as a stand-alone predictor, it\nis also incorporated into real-world production pipelines for\nad click through rate prediction [15]. Finally, it is the de-\nfacto choice of ensemble method and is used in challenges\nsuch as the Netflix prize [3].\n\nIn this paper, we describe XGBoost, a scalable machine\nlearning system for tree boosting. The system is available as\nan open source package\\textsuperscript{2}. The impact of the system has been\nwidely recognized in a number of machine learning and data\nmining challenges. Take the challenges hosted by the ma-\nchine learning competition site Kaggle for example. Among\nthe 29 challenge winning solutions\\textsuperscript{3} published at Kaggle\u2019s\nblog during 2015, 17 solutions used XGBoost. Among these\nsolutions, eight solely used XGBoost to train the model,\nwhile most others combined XGBoost with neural nets in en-\nsembles. For comparison, the second most popular method,\ndeep neural nets, was used in 11 solutions. The success\nof the system was also witnessed in KDDCup 2015, where\nXGBoost was used by every winning team in the top-10.\nMoreover, the winning teams reported that ensemble meth-\nods outperform a well-configured XGBoost by only a small\namount\\textsuperscript{[1]}.\n\nThese results demonstrate that our system gives state-of-\nthe-art results on a wide range of problems. Examples of\nthe problems in these winning solutions include: store sales\nprediction; high energy physics event classification; web text\nclassification; customer behavior prediction; motion detec-\ntion; ad click through rate prediction; malware classification;\nproduct categorization; hazard risk prediction; massive on-\nline course dropout rate prediction. While domain depen-\ndent data analysis and feature engineering play an important\nrole in these solutions, the fact that XGBoost is the consen-\nsus choice of learner shows the impact and importance of\nour system and tree boosting.\n\nThe most important factor behind the success of XGBoost\nis its scalability in all scenarios. The system runs more than\nten times faster than existing popular solutions on a single\nmachine and scales to billions of examples in distributed or\nmemory-limited settings. The scalability of XGBoost is due\nto several important systems and algorithmic optimizations.\nThese innovations include: a novel tree learning algorithm\nis for handling _sparse data_; a theoretically justified weighted\nquantile sketch procedure enables handling instance weights\nin approximate tree learning. Parallel and distributed com-\nputing makes learning faster which enables quicker model ex-\nploration. More importantly, XGBoost exploits out-of-core\n\n\u00b2https://github.com/dmlc/xgboost  \n\u00b3Solutions come from of top-3 teams of each competitions.\n\ncomputation and enables data scientists to process hundred\nmillions of examples on a desktop.  Finally, it is even more\nexciting to combine these techniques to make an end-to-end\nsystem that scales to even larger data with the least amount\nof cluster resources.  The major contributions of this paper\nis listed as follows:\n\n- \u2022 We design and build a highly scalable end-to-end tree boosting system.\n\n- \u2022 We propose a theoretically justified weighted quantile sketch for efficient proposal calculation.\n\n- \u2022 We introduce a novel sparsity-aware algorithm for parallel tree learning.\n\n- \u2022 We propose an effective cache-aware block structure for out-of-core tree learning.\n\nWhile there are some existing works on parallel tree boosting\n[22, 23, 19], the directions such as out-of-core compu-\ntation, cache-aware and sparsity-aware learning have not\nbeen explored. More importantly, an end-to-end system\nthat combines all of these aspects gives a novel solution for\nreal-world use-cases. This enables data scientists as well as\nresearchers to build powerful variants of tree boosting al-\ngorithms [7, 8]. Besides these major contributions, we also\nmake additional improvements in proposing a regularized\nlearning objective, which we will include for completeness.\n\nThe remainder of the paper is organized as follows. We\nwill first review tree boosting and introduce a regularized\nobjective in Sec. 2. We then describe the split finding meth-\nods in Sec. 3 as well as the system design in Sec. 4, including\nexperimental results when relevant to provide quantitative\nsupport for each optimization we describe. Related work\nis discussed in Sec. 5. Detailed end-to-end evaluations are\nincluded in Sec. 6. Finally we conclude the paper in Sec. 7."
        },
        {
            "text": "2.  TREE BOOSTING IN A NUTSHELL\n\n    We review gradient tree boosting algorithms in this section. The derivation follows from the same idea in existing literatures in gradient boosting. Specicially the second order method is originated from Friedman et al. [12]. We make minor improvements in the regularized objective, which were found helpful in practice.",
            "page": 2,
            "x": 50,
            "y": 440,
            "width": 245,
            "height": 81,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "3-tree",
            "chunk_id": "8713957b-44af-4ffc-a46a-9a1068d2e042",
            "group_text": "2.  TREE BOOSTING IN A NUTSHELL\n\n    We review gradient tree boosting algorithms in this section. The derivation follows from the same idea in existing literatures in gradient boosting. Specicially the second order method is originated from Friedman et al. [12]. We make minor improvements in the regularized objective, which were found helpful in practice."
        },
        {
            "text": "## 2.1 Regularized Learning Objective\n\nFor a given data set with $n$ examples and $m$ features $\\mathcal{D} = \\{(\\mathbf{x}_i, y_i)\\}$ ($|\\mathcal{D}| = n, \\mathbf{x}_i \\in \\mathbb{R}^m, y_i \\in \\mathbb{R}$), a tree ensemble model (shown in Fig. 1) uses $K$ additive functions to predict the output.",
            "page": 2,
            "x": 50,
            "y": 525,
            "width": 244,
            "height": 59,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "4-regularized",
            "chunk_id": "738660fc-7e0c-410d-aaf3-b778d1f78419",
            "group_text": "## 2.1 Regularized Learning Objective\n\nFor a given data set with $n$ examples and $m$ features $\\mathcal{D} = \\{(\\mathbf{x}_i, y_i)\\}$ ($|\\mathcal{D}| = n, \\mathbf{x}_i \\in \\mathbb{R}^m, y_i \\in \\mathbb{R}$), a tree ensemble model (shown in Fig. 1) uses $K$ additive functions to predict the output.\n\n$ \\hat{y}_i = \\phi(\\mathbf{x}_i) = \\sum_{k=1}^{K} f_k(\\mathbf{x}_i), \\quad f_k \\in \\mathcal{F}, \\tag{1} $\n\nwhere $\\mathcal{F} = \\{f(\\mathbf{x}) = w_{q(\\mathbf{x})}\\}\\{q : \\mathbb{R}^m \\to T, w \\in \\mathbb{R}^T\\}$ is the space of regression trees (also known as CART). Here $q$ represents the structure of each tree that maps an example to the corresponding leaf index. $T$ is the number of leaves in the tree. Each $f_k$ corresponds to an independent tree structure $q$ and leaf weights $w$. Unlike decision trees, each regression tree contains a continuous score on each of the leaf, we use $w_i$ to represent score on $i$-th leaf. For a given example, we will use the decision rules in the trees (given by $q$) to classify\n\nit into the leaves and calculate the final prediction by sum-\nming up the score in the corresponding leaves (given by $w$).\nTo learn the set of functions used in the model, we minimize\nthe following *regularized* objective.\n\n$\\mathcal{L}(\\phi) = \\sum_i l(\\hat{y}_i, y_i) + \\sum_k \\Omega(f_k)$\n\nwhere $\\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda \\|w\\|^2$\n  \n\\hspace{8cm} (2)\n\nHere $l$ is a differentiable convex loss function that measures\nthe difference between the prediction $\\hat{y}_i$ and the target $y_i$.\nThe second term $\\Omega$ penalizes the complexity of the model\n(i.e., the regression tree functions). The additional regular-\nization term helps to smooth the final learnt weights to avoid\nover-fitting. Intuitively, the regularized objective will tend\nto select a model employing simple and predictive functions.\nA similar regularization technique has been used in Regularized greedy forest (RGF) [25] model. Our objective and\nthe corresponding learning algorithm is simpler than RGF\nand easier to parallelize. When the regularization parameter is set to zero, the objective falls back to the traditional\ngradient tree boosting."
        },
        {
            "text": "$ \\hat{y}_i = \\phi(\\mathbf{x}_i) = \\sum_{k=1}^{K} f_k(\\mathbf{x}_i), \\quad f_k \\in \\mathcal{F}, \\tag{1} $",
            "page": 2,
            "x": 103,
            "y": 588,
            "width": 191,
            "height": 33,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "4-regularized",
            "chunk_id": "aca66f29-dc29-4660-87a8-e171a2cc3e38",
            "group_text": "## 2.1 Regularized Learning Objective\n\nFor a given data set with $n$ examples and $m$ features $\\mathcal{D} = \\{(\\mathbf{x}_i, y_i)\\}$ ($|\\mathcal{D}| = n, \\mathbf{x}_i \\in \\mathbb{R}^m, y_i \\in \\mathbb{R}$), a tree ensemble model (shown in Fig. 1) uses $K$ additive functions to predict the output.\n\n$ \\hat{y}_i = \\phi(\\mathbf{x}_i) = \\sum_{k=1}^{K} f_k(\\mathbf{x}_i), \\quad f_k \\in \\mathcal{F}, \\tag{1} $\n\nwhere $\\mathcal{F} = \\{f(\\mathbf{x}) = w_{q(\\mathbf{x})}\\}\\{q : \\mathbb{R}^m \\to T, w \\in \\mathbb{R}^T\\}$ is the space of regression trees (also known as CART). Here $q$ represents the structure of each tree that maps an example to the corresponding leaf index. $T$ is the number of leaves in the tree. Each $f_k$ corresponds to an independent tree structure $q$ and leaf weights $w$. Unlike decision trees, each regression tree contains a continuous score on each of the leaf, we use $w_i$ to represent score on $i$-th leaf. For a given example, we will use the decision rules in the trees (given by $q$) to classify\n\nit into the leaves and calculate the final prediction by sum-\nming up the score in the corresponding leaves (given by $w$).\nTo learn the set of functions used in the model, we minimize\nthe following *regularized* objective.\n\n$\\mathcal{L}(\\phi) = \\sum_i l(\\hat{y}_i, y_i) + \\sum_k \\Omega(f_k)$\n\nwhere $\\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda \\|w\\|^2$\n  \n\\hspace{8cm} (2)\n\nHere $l$ is a differentiable convex loss function that measures\nthe difference between the prediction $\\hat{y}_i$ and the target $y_i$.\nThe second term $\\Omega$ penalizes the complexity of the model\n(i.e., the regression tree functions). The additional regular-\nization term helps to smooth the final learnt weights to avoid\nover-fitting. Intuitively, the regularized objective will tend\nto select a model employing simple and predictive functions.\nA similar regularization technique has been used in Regularized greedy forest (RGF) [25] model. Our objective and\nthe corresponding learning algorithm is simpler than RGF\nand easier to parallelize. When the regularization parameter is set to zero, the objective falls back to the traditional\ngradient tree boosting."
        },
        {
            "text": "where $\\mathcal{F} = \\{f(\\mathbf{x}) = w_{q(\\mathbf{x})}\\}\\{q : \\mathbb{R}^m \\to T, w \\in \\mathbb{R}^T\\}$ is the space of regression trees (also known as CART). Here $q$ represents the structure of each tree that maps an example to the corresponding leaf index. $T$ is the number of leaves in the tree. Each $f_k$ corresponds to an independent tree structure $q$ and leaf weights $w$. Unlike decision trees, each regression tree contains a continuous score on each of the leaf, we use $w_i$ to represent score on $i$-th leaf. For a given example, we will use the decision rules in the trees (given by $q$) to classify",
            "page": 2,
            "x": 51,
            "y": 624,
            "width": 244,
            "height": 95,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "4-regularized",
            "chunk_id": "ade8796c-727b-4f9e-8ee6-15701cffc350",
            "group_text": "## 2.1 Regularized Learning Objective\n\nFor a given data set with $n$ examples and $m$ features $\\mathcal{D} = \\{(\\mathbf{x}_i, y_i)\\}$ ($|\\mathcal{D}| = n, \\mathbf{x}_i \\in \\mathbb{R}^m, y_i \\in \\mathbb{R}$), a tree ensemble model (shown in Fig. 1) uses $K$ additive functions to predict the output.\n\n$ \\hat{y}_i = \\phi(\\mathbf{x}_i) = \\sum_{k=1}^{K} f_k(\\mathbf{x}_i), \\quad f_k \\in \\mathcal{F}, \\tag{1} $\n\nwhere $\\mathcal{F} = \\{f(\\mathbf{x}) = w_{q(\\mathbf{x})}\\}\\{q : \\mathbb{R}^m \\to T, w \\in \\mathbb{R}^T\\}$ is the space of regression trees (also known as CART). Here $q$ represents the structure of each tree that maps an example to the corresponding leaf index. $T$ is the number of leaves in the tree. Each $f_k$ corresponds to an independent tree structure $q$ and leaf weights $w$. Unlike decision trees, each regression tree contains a continuous score on each of the leaf, we use $w_i$ to represent score on $i$-th leaf. For a given example, we will use the decision rules in the trees (given by $q$) to classify\n\nit into the leaves and calculate the final prediction by sum-\nming up the score in the corresponding leaves (given by $w$).\nTo learn the set of functions used in the model, we minimize\nthe following *regularized* objective.\n\n$\\mathcal{L}(\\phi) = \\sum_i l(\\hat{y}_i, y_i) + \\sum_k \\Omega(f_k)$\n\nwhere $\\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda \\|w\\|^2$\n  \n\\hspace{8cm} (2)\n\nHere $l$ is a differentiable convex loss function that measures\nthe difference between the prediction $\\hat{y}_i$ and the target $y_i$.\nThe second term $\\Omega$ penalizes the complexity of the model\n(i.e., the regression tree functions). The additional regular-\nization term helps to smooth the final learnt weights to avoid\nover-fitting. Intuitively, the regularized objective will tend\nto select a model employing simple and predictive functions.\nA similar regularization technique has been used in Regularized greedy forest (RGF) [25] model. Our objective and\nthe corresponding learning algorithm is simpler than RGF\nand easier to parallelize. When the regularization parameter is set to zero, the objective falls back to the traditional\ngradient tree boosting."
        },
        {
            "text": "it into the leaves and calculate the final prediction by sum-\nming up the score in the corresponding leaves (given by $w$).\nTo learn the set of functions used in the model, we minimize\nthe following *regularized* objective.",
            "page": 2,
            "x": 314,
            "y": 194,
            "width": 244,
            "height": 43,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "4-regularized",
            "chunk_id": "5b4787d7-cebe-45e9-8ecf-a60425e8e688",
            "group_text": "## 2.1 Regularized Learning Objective\n\nFor a given data set with $n$ examples and $m$ features $\\mathcal{D} = \\{(\\mathbf{x}_i, y_i)\\}$ ($|\\mathcal{D}| = n, \\mathbf{x}_i \\in \\mathbb{R}^m, y_i \\in \\mathbb{R}$), a tree ensemble model (shown in Fig. 1) uses $K$ additive functions to predict the output.\n\n$ \\hat{y}_i = \\phi(\\mathbf{x}_i) = \\sum_{k=1}^{K} f_k(\\mathbf{x}_i), \\quad f_k \\in \\mathcal{F}, \\tag{1} $\n\nwhere $\\mathcal{F} = \\{f(\\mathbf{x}) = w_{q(\\mathbf{x})}\\}\\{q : \\mathbb{R}^m \\to T, w \\in \\mathbb{R}^T\\}$ is the space of regression trees (also known as CART). Here $q$ represents the structure of each tree that maps an example to the corresponding leaf index. $T$ is the number of leaves in the tree. Each $f_k$ corresponds to an independent tree structure $q$ and leaf weights $w$. Unlike decision trees, each regression tree contains a continuous score on each of the leaf, we use $w_i$ to represent score on $i$-th leaf. For a given example, we will use the decision rules in the trees (given by $q$) to classify\n\nit into the leaves and calculate the final prediction by sum-\nming up the score in the corresponding leaves (given by $w$).\nTo learn the set of functions used in the model, we minimize\nthe following *regularized* objective.\n\n$\\mathcal{L}(\\phi) = \\sum_i l(\\hat{y}_i, y_i) + \\sum_k \\Omega(f_k)$\n\nwhere $\\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda \\|w\\|^2$\n  \n\\hspace{8cm} (2)\n\nHere $l$ is a differentiable convex loss function that measures\nthe difference between the prediction $\\hat{y}_i$ and the target $y_i$.\nThe second term $\\Omega$ penalizes the complexity of the model\n(i.e., the regression tree functions). The additional regular-\nization term helps to smooth the final learnt weights to avoid\nover-fitting. Intuitively, the regularized objective will tend\nto select a model employing simple and predictive functions.\nA similar regularization technique has been used in Regularized greedy forest (RGF) [25] model. Our objective and\nthe corresponding learning algorithm is simpler than RGF\nand easier to parallelize. When the regularization parameter is set to zero, the objective falls back to the traditional\ngradient tree boosting."
        },
        {
            "text": "$\\mathcal{L}(\\phi) = \\sum_i l(\\hat{y}_i, y_i) + \\sum_k \\Omega(f_k)$\n\nwhere $\\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda \\|w\\|^2$\n  \n\\hspace{8cm} (2)",
            "page": 2,
            "x": 372,
            "y": 240,
            "width": 185,
            "height": 49,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "4-regularized",
            "chunk_id": "77516f38-76b1-40f7-ba63-26790e4f0273",
            "group_text": "## 2.1 Regularized Learning Objective\n\nFor a given data set with $n$ examples and $m$ features $\\mathcal{D} = \\{(\\mathbf{x}_i, y_i)\\}$ ($|\\mathcal{D}| = n, \\mathbf{x}_i \\in \\mathbb{R}^m, y_i \\in \\mathbb{R}$), a tree ensemble model (shown in Fig. 1) uses $K$ additive functions to predict the output.\n\n$ \\hat{y}_i = \\phi(\\mathbf{x}_i) = \\sum_{k=1}^{K} f_k(\\mathbf{x}_i), \\quad f_k \\in \\mathcal{F}, \\tag{1} $\n\nwhere $\\mathcal{F} = \\{f(\\mathbf{x}) = w_{q(\\mathbf{x})}\\}\\{q : \\mathbb{R}^m \\to T, w \\in \\mathbb{R}^T\\}$ is the space of regression trees (also known as CART). Here $q$ represents the structure of each tree that maps an example to the corresponding leaf index. $T$ is the number of leaves in the tree. Each $f_k$ corresponds to an independent tree structure $q$ and leaf weights $w$. Unlike decision trees, each regression tree contains a continuous score on each of the leaf, we use $w_i$ to represent score on $i$-th leaf. For a given example, we will use the decision rules in the trees (given by $q$) to classify\n\nit into the leaves and calculate the final prediction by sum-\nming up the score in the corresponding leaves (given by $w$).\nTo learn the set of functions used in the model, we minimize\nthe following *regularized* objective.\n\n$\\mathcal{L}(\\phi) = \\sum_i l(\\hat{y}_i, y_i) + \\sum_k \\Omega(f_k)$\n\nwhere $\\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda \\|w\\|^2$\n  \n\\hspace{8cm} (2)\n\nHere $l$ is a differentiable convex loss function that measures\nthe difference between the prediction $\\hat{y}_i$ and the target $y_i$.\nThe second term $\\Omega$ penalizes the complexity of the model\n(i.e., the regression tree functions). The additional regular-\nization term helps to smooth the final learnt weights to avoid\nover-fitting. Intuitively, the regularized objective will tend\nto select a model employing simple and predictive functions.\nA similar regularization technique has been used in Regularized greedy forest (RGF) [25] model. Our objective and\nthe corresponding learning algorithm is simpler than RGF\nand easier to parallelize. When the regularization parameter is set to zero, the objective falls back to the traditional\ngradient tree boosting."
        },
        {
            "text": "Here $l$ is a differentiable convex loss function that measures\nthe difference between the prediction $\\hat{y}_i$ and the target $y_i$.\nThe second term $\\Omega$ penalizes the complexity of the model\n(i.e., the regression tree functions). The additional regular-\nization term helps to smooth the final learnt weights to avoid\nover-fitting. Intuitively, the regularized objective will tend\nto select a model employing simple and predictive functions.\nA similar regularization technique has been used in Regularized greedy forest (RGF) [25] model. Our objective and\nthe corresponding learning algorithm is simpler than RGF\nand easier to parallelize. When the regularization parameter is set to zero, the objective falls back to the traditional\ngradient tree boosting.",
            "page": 2,
            "x": 314,
            "y": 293,
            "width": 244,
            "height": 138,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "4-regularized",
            "chunk_id": "d8cad8de-a2d3-4bbc-a7a8-1e6cd9fd836e",
            "group_text": "## 2.1 Regularized Learning Objective\n\nFor a given data set with $n$ examples and $m$ features $\\mathcal{D} = \\{(\\mathbf{x}_i, y_i)\\}$ ($|\\mathcal{D}| = n, \\mathbf{x}_i \\in \\mathbb{R}^m, y_i \\in \\mathbb{R}$), a tree ensemble model (shown in Fig. 1) uses $K$ additive functions to predict the output.\n\n$ \\hat{y}_i = \\phi(\\mathbf{x}_i) = \\sum_{k=1}^{K} f_k(\\mathbf{x}_i), \\quad f_k \\in \\mathcal{F}, \\tag{1} $\n\nwhere $\\mathcal{F} = \\{f(\\mathbf{x}) = w_{q(\\mathbf{x})}\\}\\{q : \\mathbb{R}^m \\to T, w \\in \\mathbb{R}^T\\}$ is the space of regression trees (also known as CART). Here $q$ represents the structure of each tree that maps an example to the corresponding leaf index. $T$ is the number of leaves in the tree. Each $f_k$ corresponds to an independent tree structure $q$ and leaf weights $w$. Unlike decision trees, each regression tree contains a continuous score on each of the leaf, we use $w_i$ to represent score on $i$-th leaf. For a given example, we will use the decision rules in the trees (given by $q$) to classify\n\nit into the leaves and calculate the final prediction by sum-\nming up the score in the corresponding leaves (given by $w$).\nTo learn the set of functions used in the model, we minimize\nthe following *regularized* objective.\n\n$\\mathcal{L}(\\phi) = \\sum_i l(\\hat{y}_i, y_i) + \\sum_k \\Omega(f_k)$\n\nwhere $\\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda \\|w\\|^2$\n  \n\\hspace{8cm} (2)\n\nHere $l$ is a differentiable convex loss function that measures\nthe difference between the prediction $\\hat{y}_i$ and the target $y_i$.\nThe second term $\\Omega$ penalizes the complexity of the model\n(i.e., the regression tree functions). The additional regular-\nization term helps to smooth the final learnt weights to avoid\nover-fitting. Intuitively, the regularized objective will tend\nto select a model employing simple and predictive functions.\nA similar regularization technique has been used in Regularized greedy forest (RGF) [25] model. Our objective and\nthe corresponding learning algorithm is simpler than RGF\nand easier to parallelize. When the regularization parameter is set to zero, the objective falls back to the traditional\ngradient tree boosting."
        },
        {
            "text": "## 2.2  Gradient Tree Boosting\n\nThe tree ensemble model in Eq. (2) includes functions as parameters and cannot be optimized using traditional optimization methods in Euclidean space. Instead, the model is trained in an additive manner. Formally, let $\\hat{y}_i^{(t)}$ be the prediction of the $i$-th instance at the $t$-th iteration, we will need to add $f_t$ to minimize the following objective.",
            "page": 2,
            "x": 314,
            "y": 436,
            "width": 244,
            "height": 82,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-gradient",
            "chunk_id": "6ac295bb-7599-4394-b21c-ece053b6f4d2",
            "group_text": "## 2.2  Gradient Tree Boosting\n\nThe tree ensemble model in Eq. (2) includes functions as parameters and cannot be optimized using traditional optimization methods in Euclidean space. Instead, the model is trained in an additive manner. Formally, let $\\hat{y}_i^{(t)}$ be the prediction of the $i$-th instance at the $t$-th iteration, we will need to add $f_t$ to minimize the following objective.\n\n$\\mathcal{L}^{(t)} = \\sum_{i=1}^{n} l(y_i, \\hat{y}_i^{(t-1)} + f_t(\\mathbf{x}_i)) + \\Omega(f_t)$\n\nThis means we greedily add the $f_t$ that most improves our\nmodel according to Eq. (2). Second-order approximation\ncan be used to quickly optimize the objective in the general\nsetting [12].\n\n$\\mathcal{L}^{(t)} \\simeq \\sum_{i=1}^{n} [l(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$\n\nwhere $g_{i} = \\partial_{\\hat{y}^{(t-1)}} l(y_{i}, \\hat{y}_{i}^{(t-1)})$ and $h_{i} = \\partial_{\\hat{y}^{(t-1)}}^{2} l(y_{i}, \\hat{y}_{i}^{(t-1)})$  \nare first and second order gradient statistics on the loss function. We can remove the constant terms to obtain the following simplified objective at step $t$.\n\n$\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$\n\nFigure 2: Structure Score Calculation.  We only\nneed to sum up the gradient and second order gra-\ndient statistics on each leaf, then apply the scoring\nformula to get the quality score.\n\nDefine $I_j = \\{i|q(\\mathbf{x}_i) = j\\}$ as the instance set of leaf $j$. We can rewrite Eq (3) by expanding $\\Omega$ as follows\n\n$\n\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2 \\\\\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (4) \\\\\n= \\sum_{j=1}^{T} [(\\sum_{i \\in I_j} g_i) w_j + \\frac{1}{2} (\\sum_{i \\in I_j} h_i + \\lambda) w_j^2] + \\gamma T\n$\n\nFor a fixed structure $q(\\mathbf{x})$, we can compute the optimal weight $w_j^*$ of leaf $j$ by\n\n$w_j^* = -\\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda}, \\qquad (5)$\n\nand calculate the corresponding optimal value by\n\n$\\tilde{\\mathcal{L}}^{(t)}(q) = -\\frac{1}{2} \\sum_{j=1}^{T} \\frac{\\left(\\sum_{i \\in I_j} g_i\\right)^2}{\\sum_{i \\in I_j} h_i + \\lambda} + \\gamma T.$\n\nEq (6) can be used as a scoring function to measure the\nquality of a tree structure $q$. This score is like the impurity\nscore for evaluating decision trees, except that it is derived\nfor a wider range of objective functions. Fig. 2 illustrates\nhow this score can be calculated.\n\nNormally it is impossible to enumerate all the possible\ntree structures $q$. A greedy algorithm that starts from a\nsingle leaf and iteratively adds branches to the tree is used\ninstead. Assume that $I_L$ and $I_R$ are the instance sets of left\nand right nodes after the split. Lettting $I = I_L \\cup I_R$, then\nthe loss reduction after the split is given by\n\n$\\mathcal{L}_{\\text{split}} = \\frac{1}{2} \\left[ \\frac{\\left( \\sum_{i \\in I_L} g_i \\right)^2}{\\sum_{i \\in I_L} h_i + \\lambda} + \\frac{\\left( \\sum_{i \\in I_R} g_i \\right)^2}{\\sum_{i \\in I_R} h_i + \\lambda} - \\frac{\\left( \\sum_{i \\in I} g_i \\right)^2}{\\sum_{i \\in I} h_i + \\lambda} \\right] - \\gamma \\tag{7}$\n\nThis formula is usually used in practice for evaluating the\nsplit candidates."
        },
        {
            "text": "$\\mathcal{L}^{(t)} = \\sum_{i=1}^{n} l(y_i, \\hat{y}_i^{(t-1)} + f_t(\\mathbf{x}_i)) + \\Omega(f_t)$",
            "page": 2,
            "x": 354,
            "y": 524,
            "width": 163,
            "height": 29,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-gradient",
            "chunk_id": "90fb5cc2-7001-40fa-b4a9-71105e58974d",
            "group_text": "## 2.2  Gradient Tree Boosting\n\nThe tree ensemble model in Eq. (2) includes functions as parameters and cannot be optimized using traditional optimization methods in Euclidean space. Instead, the model is trained in an additive manner. Formally, let $\\hat{y}_i^{(t)}$ be the prediction of the $i$-th instance at the $t$-th iteration, we will need to add $f_t$ to minimize the following objective.\n\n$\\mathcal{L}^{(t)} = \\sum_{i=1}^{n} l(y_i, \\hat{y}_i^{(t-1)} + f_t(\\mathbf{x}_i)) + \\Omega(f_t)$\n\nThis means we greedily add the $f_t$ that most improves our\nmodel according to Eq. (2). Second-order approximation\ncan be used to quickly optimize the objective in the general\nsetting [12].\n\n$\\mathcal{L}^{(t)} \\simeq \\sum_{i=1}^{n} [l(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$\n\nwhere $g_{i} = \\partial_{\\hat{y}^{(t-1)}} l(y_{i}, \\hat{y}_{i}^{(t-1)})$ and $h_{i} = \\partial_{\\hat{y}^{(t-1)}}^{2} l(y_{i}, \\hat{y}_{i}^{(t-1)})$  \nare first and second order gradient statistics on the loss function. We can remove the constant terms to obtain the following simplified objective at step $t$.\n\n$\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$\n\nFigure 2: Structure Score Calculation.  We only\nneed to sum up the gradient and second order gra-\ndient statistics on each leaf, then apply the scoring\nformula to get the quality score.\n\nDefine $I_j = \\{i|q(\\mathbf{x}_i) = j\\}$ as the instance set of leaf $j$. We can rewrite Eq (3) by expanding $\\Omega$ as follows\n\n$\n\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2 \\\\\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (4) \\\\\n= \\sum_{j=1}^{T} [(\\sum_{i \\in I_j} g_i) w_j + \\frac{1}{2} (\\sum_{i \\in I_j} h_i + \\lambda) w_j^2] + \\gamma T\n$\n\nFor a fixed structure $q(\\mathbf{x})$, we can compute the optimal weight $w_j^*$ of leaf $j$ by\n\n$w_j^* = -\\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda}, \\qquad (5)$\n\nand calculate the corresponding optimal value by\n\n$\\tilde{\\mathcal{L}}^{(t)}(q) = -\\frac{1}{2} \\sum_{j=1}^{T} \\frac{\\left(\\sum_{i \\in I_j} g_i\\right)^2}{\\sum_{i \\in I_j} h_i + \\lambda} + \\gamma T.$\n\nEq (6) can be used as a scoring function to measure the\nquality of a tree structure $q$. This score is like the impurity\nscore for evaluating decision trees, except that it is derived\nfor a wider range of objective functions. Fig. 2 illustrates\nhow this score can be calculated.\n\nNormally it is impossible to enumerate all the possible\ntree structures $q$. A greedy algorithm that starts from a\nsingle leaf and iteratively adds branches to the tree is used\ninstead. Assume that $I_L$ and $I_R$ are the instance sets of left\nand right nodes after the split. Lettting $I = I_L \\cup I_R$, then\nthe loss reduction after the split is given by\n\n$\\mathcal{L}_{\\text{split}} = \\frac{1}{2} \\left[ \\frac{\\left( \\sum_{i \\in I_L} g_i \\right)^2}{\\sum_{i \\in I_L} h_i + \\lambda} + \\frac{\\left( \\sum_{i \\in I_R} g_i \\right)^2}{\\sum_{i \\in I_R} h_i + \\lambda} - \\frac{\\left( \\sum_{i \\in I} g_i \\right)^2}{\\sum_{i \\in I} h_i + \\lambda} \\right] - \\gamma \\tag{7}$\n\nThis formula is usually used in practice for evaluating the\nsplit candidates."
        },
        {
            "text": "This means we greedily add the $f_t$ that most improves our\nmodel according to Eq. (2). Second-order approximation\ncan be used to quickly optimize the objective in the general\nsetting [12].",
            "page": 2,
            "x": 314,
            "y": 556,
            "width": 243,
            "height": 44,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-gradient",
            "chunk_id": "4a78a9ff-e36e-4c88-9228-273fa5ff765f",
            "group_text": "## 2.2  Gradient Tree Boosting\n\nThe tree ensemble model in Eq. (2) includes functions as parameters and cannot be optimized using traditional optimization methods in Euclidean space. Instead, the model is trained in an additive manner. Formally, let $\\hat{y}_i^{(t)}$ be the prediction of the $i$-th instance at the $t$-th iteration, we will need to add $f_t$ to minimize the following objective.\n\n$\\mathcal{L}^{(t)} = \\sum_{i=1}^{n} l(y_i, \\hat{y}_i^{(t-1)} + f_t(\\mathbf{x}_i)) + \\Omega(f_t)$\n\nThis means we greedily add the $f_t$ that most improves our\nmodel according to Eq. (2). Second-order approximation\ncan be used to quickly optimize the objective in the general\nsetting [12].\n\n$\\mathcal{L}^{(t)} \\simeq \\sum_{i=1}^{n} [l(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$\n\nwhere $g_{i} = \\partial_{\\hat{y}^{(t-1)}} l(y_{i}, \\hat{y}_{i}^{(t-1)})$ and $h_{i} = \\partial_{\\hat{y}^{(t-1)}}^{2} l(y_{i}, \\hat{y}_{i}^{(t-1)})$  \nare first and second order gradient statistics on the loss function. We can remove the constant terms to obtain the following simplified objective at step $t$.\n\n$\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$\n\nFigure 2: Structure Score Calculation.  We only\nneed to sum up the gradient and second order gra-\ndient statistics on each leaf, then apply the scoring\nformula to get the quality score.\n\nDefine $I_j = \\{i|q(\\mathbf{x}_i) = j\\}$ as the instance set of leaf $j$. We can rewrite Eq (3) by expanding $\\Omega$ as follows\n\n$\n\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2 \\\\\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (4) \\\\\n= \\sum_{j=1}^{T} [(\\sum_{i \\in I_j} g_i) w_j + \\frac{1}{2} (\\sum_{i \\in I_j} h_i + \\lambda) w_j^2] + \\gamma T\n$\n\nFor a fixed structure $q(\\mathbf{x})$, we can compute the optimal weight $w_j^*$ of leaf $j$ by\n\n$w_j^* = -\\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda}, \\qquad (5)$\n\nand calculate the corresponding optimal value by\n\n$\\tilde{\\mathcal{L}}^{(t)}(q) = -\\frac{1}{2} \\sum_{j=1}^{T} \\frac{\\left(\\sum_{i \\in I_j} g_i\\right)^2}{\\sum_{i \\in I_j} h_i + \\lambda} + \\gamma T.$\n\nEq (6) can be used as a scoring function to measure the\nquality of a tree structure $q$. This score is like the impurity\nscore for evaluating decision trees, except that it is derived\nfor a wider range of objective functions. Fig. 2 illustrates\nhow this score can be calculated.\n\nNormally it is impossible to enumerate all the possible\ntree structures $q$. A greedy algorithm that starts from a\nsingle leaf and iteratively adds branches to the tree is used\ninstead. Assume that $I_L$ and $I_R$ are the instance sets of left\nand right nodes after the split. Lettting $I = I_L \\cup I_R$, then\nthe loss reduction after the split is given by\n\n$\\mathcal{L}_{\\text{split}} = \\frac{1}{2} \\left[ \\frac{\\left( \\sum_{i \\in I_L} g_i \\right)^2}{\\sum_{i \\in I_L} h_i + \\lambda} + \\frac{\\left( \\sum_{i \\in I_R} g_i \\right)^2}{\\sum_{i \\in I_R} h_i + \\lambda} - \\frac{\\left( \\sum_{i \\in I} g_i \\right)^2}{\\sum_{i \\in I} h_i + \\lambda} \\right] - \\gamma \\tag{7}$\n\nThis formula is usually used in practice for evaluating the\nsplit candidates."
        },
        {
            "text": "$\\mathcal{L}^{(t)} \\simeq \\sum_{i=1}^{n} [l(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$",
            "page": 2,
            "x": 323,
            "y": 606,
            "width": 229,
            "height": 33,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-gradient",
            "chunk_id": "2fc85be3-c0bf-4238-a077-6de6cedf8087",
            "group_text": "## 2.2  Gradient Tree Boosting\n\nThe tree ensemble model in Eq. (2) includes functions as parameters and cannot be optimized using traditional optimization methods in Euclidean space. Instead, the model is trained in an additive manner. Formally, let $\\hat{y}_i^{(t)}$ be the prediction of the $i$-th instance at the $t$-th iteration, we will need to add $f_t$ to minimize the following objective.\n\n$\\mathcal{L}^{(t)} = \\sum_{i=1}^{n} l(y_i, \\hat{y}_i^{(t-1)} + f_t(\\mathbf{x}_i)) + \\Omega(f_t)$\n\nThis means we greedily add the $f_t$ that most improves our\nmodel according to Eq. (2). Second-order approximation\ncan be used to quickly optimize the objective in the general\nsetting [12].\n\n$\\mathcal{L}^{(t)} \\simeq \\sum_{i=1}^{n} [l(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$\n\nwhere $g_{i} = \\partial_{\\hat{y}^{(t-1)}} l(y_{i}, \\hat{y}_{i}^{(t-1)})$ and $h_{i} = \\partial_{\\hat{y}^{(t-1)}}^{2} l(y_{i}, \\hat{y}_{i}^{(t-1)})$  \nare first and second order gradient statistics on the loss function. We can remove the constant terms to obtain the following simplified objective at step $t$.\n\n$\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$\n\nFigure 2: Structure Score Calculation.  We only\nneed to sum up the gradient and second order gra-\ndient statistics on each leaf, then apply the scoring\nformula to get the quality score.\n\nDefine $I_j = \\{i|q(\\mathbf{x}_i) = j\\}$ as the instance set of leaf $j$. We can rewrite Eq (3) by expanding $\\Omega$ as follows\n\n$\n\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2 \\\\\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (4) \\\\\n= \\sum_{j=1}^{T} [(\\sum_{i \\in I_j} g_i) w_j + \\frac{1}{2} (\\sum_{i \\in I_j} h_i + \\lambda) w_j^2] + \\gamma T\n$\n\nFor a fixed structure $q(\\mathbf{x})$, we can compute the optimal weight $w_j^*$ of leaf $j$ by\n\n$w_j^* = -\\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda}, \\qquad (5)$\n\nand calculate the corresponding optimal value by\n\n$\\tilde{\\mathcal{L}}^{(t)}(q) = -\\frac{1}{2} \\sum_{j=1}^{T} \\frac{\\left(\\sum_{i \\in I_j} g_i\\right)^2}{\\sum_{i \\in I_j} h_i + \\lambda} + \\gamma T.$\n\nEq (6) can be used as a scoring function to measure the\nquality of a tree structure $q$. This score is like the impurity\nscore for evaluating decision trees, except that it is derived\nfor a wider range of objective functions. Fig. 2 illustrates\nhow this score can be calculated.\n\nNormally it is impossible to enumerate all the possible\ntree structures $q$. A greedy algorithm that starts from a\nsingle leaf and iteratively adds branches to the tree is used\ninstead. Assume that $I_L$ and $I_R$ are the instance sets of left\nand right nodes after the split. Lettting $I = I_L \\cup I_R$, then\nthe loss reduction after the split is given by\n\n$\\mathcal{L}_{\\text{split}} = \\frac{1}{2} \\left[ \\frac{\\left( \\sum_{i \\in I_L} g_i \\right)^2}{\\sum_{i \\in I_L} h_i + \\lambda} + \\frac{\\left( \\sum_{i \\in I_R} g_i \\right)^2}{\\sum_{i \\in I_R} h_i + \\lambda} - \\frac{\\left( \\sum_{i \\in I} g_i \\right)^2}{\\sum_{i \\in I} h_i + \\lambda} \\right] - \\gamma \\tag{7}$\n\nThis formula is usually used in practice for evaluating the\nsplit candidates."
        },
        {
            "text": "where $g_{i} = \\partial_{\\hat{y}^{(t-1)}} l(y_{i}, \\hat{y}_{i}^{(t-1)})$ and $h_{i} = \\partial_{\\hat{y}^{(t-1)}}^{2} l(y_{i}, \\hat{y}_{i}^{(t-1)})$  \nare first and second order gradient statistics on the loss function. We can remove the constant terms to obtain the following simplified objective at step $t$.",
            "page": 2,
            "x": 314,
            "y": 641,
            "width": 243,
            "height": 47,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-gradient",
            "chunk_id": "9614bce7-8bb7-4693-9012-373af9c1ff3c",
            "group_text": "## 2.2  Gradient Tree Boosting\n\nThe tree ensemble model in Eq. (2) includes functions as parameters and cannot be optimized using traditional optimization methods in Euclidean space. Instead, the model is trained in an additive manner. Formally, let $\\hat{y}_i^{(t)}$ be the prediction of the $i$-th instance at the $t$-th iteration, we will need to add $f_t$ to minimize the following objective.\n\n$\\mathcal{L}^{(t)} = \\sum_{i=1}^{n} l(y_i, \\hat{y}_i^{(t-1)} + f_t(\\mathbf{x}_i)) + \\Omega(f_t)$\n\nThis means we greedily add the $f_t$ that most improves our\nmodel according to Eq. (2). Second-order approximation\ncan be used to quickly optimize the objective in the general\nsetting [12].\n\n$\\mathcal{L}^{(t)} \\simeq \\sum_{i=1}^{n} [l(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$\n\nwhere $g_{i} = \\partial_{\\hat{y}^{(t-1)}} l(y_{i}, \\hat{y}_{i}^{(t-1)})$ and $h_{i} = \\partial_{\\hat{y}^{(t-1)}}^{2} l(y_{i}, \\hat{y}_{i}^{(t-1)})$  \nare first and second order gradient statistics on the loss function. We can remove the constant terms to obtain the following simplified objective at step $t$.\n\n$\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$\n\nFigure 2: Structure Score Calculation.  We only\nneed to sum up the gradient and second order gra-\ndient statistics on each leaf, then apply the scoring\nformula to get the quality score.\n\nDefine $I_j = \\{i|q(\\mathbf{x}_i) = j\\}$ as the instance set of leaf $j$. We can rewrite Eq (3) by expanding $\\Omega$ as follows\n\n$\n\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2 \\\\\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (4) \\\\\n= \\sum_{j=1}^{T} [(\\sum_{i \\in I_j} g_i) w_j + \\frac{1}{2} (\\sum_{i \\in I_j} h_i + \\lambda) w_j^2] + \\gamma T\n$\n\nFor a fixed structure $q(\\mathbf{x})$, we can compute the optimal weight $w_j^*$ of leaf $j$ by\n\n$w_j^* = -\\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda}, \\qquad (5)$\n\nand calculate the corresponding optimal value by\n\n$\\tilde{\\mathcal{L}}^{(t)}(q) = -\\frac{1}{2} \\sum_{j=1}^{T} \\frac{\\left(\\sum_{i \\in I_j} g_i\\right)^2}{\\sum_{i \\in I_j} h_i + \\lambda} + \\gamma T.$\n\nEq (6) can be used as a scoring function to measure the\nquality of a tree structure $q$. This score is like the impurity\nscore for evaluating decision trees, except that it is derived\nfor a wider range of objective functions. Fig. 2 illustrates\nhow this score can be calculated.\n\nNormally it is impossible to enumerate all the possible\ntree structures $q$. A greedy algorithm that starts from a\nsingle leaf and iteratively adds branches to the tree is used\ninstead. Assume that $I_L$ and $I_R$ are the instance sets of left\nand right nodes after the split. Lettting $I = I_L \\cup I_R$, then\nthe loss reduction after the split is given by\n\n$\\mathcal{L}_{\\text{split}} = \\frac{1}{2} \\left[ \\frac{\\left( \\sum_{i \\in I_L} g_i \\right)^2}{\\sum_{i \\in I_L} h_i + \\lambda} + \\frac{\\left( \\sum_{i \\in I_R} g_i \\right)^2}{\\sum_{i \\in I_R} h_i + \\lambda} - \\frac{\\left( \\sum_{i \\in I} g_i \\right)^2}{\\sum_{i \\in I} h_i + \\lambda} \\right] - \\gamma \\tag{7}$\n\nThis formula is usually used in practice for evaluating the\nsplit candidates."
        },
        {
            "text": "$\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$",
            "page": 2,
            "x": 352,
            "y": 692,
            "width": 204,
            "height": 31,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-gradient",
            "chunk_id": "86459f2f-cbd4-4897-b752-542be1e87f82",
            "group_text": "## 2.2  Gradient Tree Boosting\n\nThe tree ensemble model in Eq. (2) includes functions as parameters and cannot be optimized using traditional optimization methods in Euclidean space. Instead, the model is trained in an additive manner. Formally, let $\\hat{y}_i^{(t)}$ be the prediction of the $i$-th instance at the $t$-th iteration, we will need to add $f_t$ to minimize the following objective.\n\n$\\mathcal{L}^{(t)} = \\sum_{i=1}^{n} l(y_i, \\hat{y}_i^{(t-1)} + f_t(\\mathbf{x}_i)) + \\Omega(f_t)$\n\nThis means we greedily add the $f_t$ that most improves our\nmodel according to Eq. (2). Second-order approximation\ncan be used to quickly optimize the objective in the general\nsetting [12].\n\n$\\mathcal{L}^{(t)} \\simeq \\sum_{i=1}^{n} [l(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$\n\nwhere $g_{i} = \\partial_{\\hat{y}^{(t-1)}} l(y_{i}, \\hat{y}_{i}^{(t-1)})$ and $h_{i} = \\partial_{\\hat{y}^{(t-1)}}^{2} l(y_{i}, \\hat{y}_{i}^{(t-1)})$  \nare first and second order gradient statistics on the loss function. We can remove the constant terms to obtain the following simplified objective at step $t$.\n\n$\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$\n\nFigure 2: Structure Score Calculation.  We only\nneed to sum up the gradient and second order gra-\ndient statistics on each leaf, then apply the scoring\nformula to get the quality score.\n\nDefine $I_j = \\{i|q(\\mathbf{x}_i) = j\\}$ as the instance set of leaf $j$. We can rewrite Eq (3) by expanding $\\Omega$ as follows\n\n$\n\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2 \\\\\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (4) \\\\\n= \\sum_{j=1}^{T} [(\\sum_{i \\in I_j} g_i) w_j + \\frac{1}{2} (\\sum_{i \\in I_j} h_i + \\lambda) w_j^2] + \\gamma T\n$\n\nFor a fixed structure $q(\\mathbf{x})$, we can compute the optimal weight $w_j^*$ of leaf $j$ by\n\n$w_j^* = -\\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda}, \\qquad (5)$\n\nand calculate the corresponding optimal value by\n\n$\\tilde{\\mathcal{L}}^{(t)}(q) = -\\frac{1}{2} \\sum_{j=1}^{T} \\frac{\\left(\\sum_{i \\in I_j} g_i\\right)^2}{\\sum_{i \\in I_j} h_i + \\lambda} + \\gamma T.$\n\nEq (6) can be used as a scoring function to measure the\nquality of a tree structure $q$. This score is like the impurity\nscore for evaluating decision trees, except that it is derived\nfor a wider range of objective functions. Fig. 2 illustrates\nhow this score can be calculated.\n\nNormally it is impossible to enumerate all the possible\ntree structures $q$. A greedy algorithm that starts from a\nsingle leaf and iteratively adds branches to the tree is used\ninstead. Assume that $I_L$ and $I_R$ are the instance sets of left\nand right nodes after the split. Lettting $I = I_L \\cup I_R$, then\nthe loss reduction after the split is given by\n\n$\\mathcal{L}_{\\text{split}} = \\frac{1}{2} \\left[ \\frac{\\left( \\sum_{i \\in I_L} g_i \\right)^2}{\\sum_{i \\in I_L} h_i + \\lambda} + \\frac{\\left( \\sum_{i \\in I_R} g_i \\right)^2}{\\sum_{i \\in I_R} h_i + \\lambda} - \\frac{\\left( \\sum_{i \\in I} g_i \\right)^2}{\\sum_{i \\in I} h_i + \\lambda} \\right] - \\gamma \\tag{7}$\n\nThis formula is usually used in practice for evaluating the\nsplit candidates."
        },
        {
            "text": "Figure 2: Structure Score Calculation.  We only\nneed to sum up the gradient and second order gra-\ndient statistics on each leaf, then apply the scoring\nformula to get the quality score.",
            "page": 3,
            "x": 50,
            "y": 146,
            "width": 246,
            "height": 45,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-gradient",
            "chunk_id": "ea7a64da-aab3-40ae-b8f4-2a417209d220",
            "group_text": "## 2.2  Gradient Tree Boosting\n\nThe tree ensemble model in Eq. (2) includes functions as parameters and cannot be optimized using traditional optimization methods in Euclidean space. Instead, the model is trained in an additive manner. Formally, let $\\hat{y}_i^{(t)}$ be the prediction of the $i$-th instance at the $t$-th iteration, we will need to add $f_t$ to minimize the following objective.\n\n$\\mathcal{L}^{(t)} = \\sum_{i=1}^{n} l(y_i, \\hat{y}_i^{(t-1)} + f_t(\\mathbf{x}_i)) + \\Omega(f_t)$\n\nThis means we greedily add the $f_t$ that most improves our\nmodel according to Eq. (2). Second-order approximation\ncan be used to quickly optimize the objective in the general\nsetting [12].\n\n$\\mathcal{L}^{(t)} \\simeq \\sum_{i=1}^{n} [l(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$\n\nwhere $g_{i} = \\partial_{\\hat{y}^{(t-1)}} l(y_{i}, \\hat{y}_{i}^{(t-1)})$ and $h_{i} = \\partial_{\\hat{y}^{(t-1)}}^{2} l(y_{i}, \\hat{y}_{i}^{(t-1)})$  \nare first and second order gradient statistics on the loss function. We can remove the constant terms to obtain the following simplified objective at step $t$.\n\n$\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$\n\nFigure 2: Structure Score Calculation.  We only\nneed to sum up the gradient and second order gra-\ndient statistics on each leaf, then apply the scoring\nformula to get the quality score.\n\nDefine $I_j = \\{i|q(\\mathbf{x}_i) = j\\}$ as the instance set of leaf $j$. We can rewrite Eq (3) by expanding $\\Omega$ as follows\n\n$\n\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2 \\\\\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (4) \\\\\n= \\sum_{j=1}^{T} [(\\sum_{i \\in I_j} g_i) w_j + \\frac{1}{2} (\\sum_{i \\in I_j} h_i + \\lambda) w_j^2] + \\gamma T\n$\n\nFor a fixed structure $q(\\mathbf{x})$, we can compute the optimal weight $w_j^*$ of leaf $j$ by\n\n$w_j^* = -\\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda}, \\qquad (5)$\n\nand calculate the corresponding optimal value by\n\n$\\tilde{\\mathcal{L}}^{(t)}(q) = -\\frac{1}{2} \\sum_{j=1}^{T} \\frac{\\left(\\sum_{i \\in I_j} g_i\\right)^2}{\\sum_{i \\in I_j} h_i + \\lambda} + \\gamma T.$\n\nEq (6) can be used as a scoring function to measure the\nquality of a tree structure $q$. This score is like the impurity\nscore for evaluating decision trees, except that it is derived\nfor a wider range of objective functions. Fig. 2 illustrates\nhow this score can be calculated.\n\nNormally it is impossible to enumerate all the possible\ntree structures $q$. A greedy algorithm that starts from a\nsingle leaf and iteratively adds branches to the tree is used\ninstead. Assume that $I_L$ and $I_R$ are the instance sets of left\nand right nodes after the split. Lettting $I = I_L \\cup I_R$, then\nthe loss reduction after the split is given by\n\n$\\mathcal{L}_{\\text{split}} = \\frac{1}{2} \\left[ \\frac{\\left( \\sum_{i \\in I_L} g_i \\right)^2}{\\sum_{i \\in I_L} h_i + \\lambda} + \\frac{\\left( \\sum_{i \\in I_R} g_i \\right)^2}{\\sum_{i \\in I_R} h_i + \\lambda} - \\frac{\\left( \\sum_{i \\in I} g_i \\right)^2}{\\sum_{i \\in I} h_i + \\lambda} \\right] - \\gamma \\tag{7}$\n\nThis formula is usually used in practice for evaluating the\nsplit candidates."
        },
        {
            "text": "Define $I_j = \\{i|q(\\mathbf{x}_i) = j\\}$ as the instance set of leaf $j$. We can rewrite Eq (3) by expanding $\\Omega$ as follows",
            "page": 3,
            "x": 50,
            "y": 198,
            "width": 245,
            "height": 26,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-gradient",
            "chunk_id": "05d529ea-74aa-40de-857e-8bc0b856b475",
            "group_text": "## 2.2  Gradient Tree Boosting\n\nThe tree ensemble model in Eq. (2) includes functions as parameters and cannot be optimized using traditional optimization methods in Euclidean space. Instead, the model is trained in an additive manner. Formally, let $\\hat{y}_i^{(t)}$ be the prediction of the $i$-th instance at the $t$-th iteration, we will need to add $f_t$ to minimize the following objective.\n\n$\\mathcal{L}^{(t)} = \\sum_{i=1}^{n} l(y_i, \\hat{y}_i^{(t-1)} + f_t(\\mathbf{x}_i)) + \\Omega(f_t)$\n\nThis means we greedily add the $f_t$ that most improves our\nmodel according to Eq. (2). Second-order approximation\ncan be used to quickly optimize the objective in the general\nsetting [12].\n\n$\\mathcal{L}^{(t)} \\simeq \\sum_{i=1}^{n} [l(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$\n\nwhere $g_{i} = \\partial_{\\hat{y}^{(t-1)}} l(y_{i}, \\hat{y}_{i}^{(t-1)})$ and $h_{i} = \\partial_{\\hat{y}^{(t-1)}}^{2} l(y_{i}, \\hat{y}_{i}^{(t-1)})$  \nare first and second order gradient statistics on the loss function. We can remove the constant terms to obtain the following simplified objective at step $t$.\n\n$\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$\n\nFigure 2: Structure Score Calculation.  We only\nneed to sum up the gradient and second order gra-\ndient statistics on each leaf, then apply the scoring\nformula to get the quality score.\n\nDefine $I_j = \\{i|q(\\mathbf{x}_i) = j\\}$ as the instance set of leaf $j$. We can rewrite Eq (3) by expanding $\\Omega$ as follows\n\n$\n\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2 \\\\\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (4) \\\\\n= \\sum_{j=1}^{T} [(\\sum_{i \\in I_j} g_i) w_j + \\frac{1}{2} (\\sum_{i \\in I_j} h_i + \\lambda) w_j^2] + \\gamma T\n$\n\nFor a fixed structure $q(\\mathbf{x})$, we can compute the optimal weight $w_j^*$ of leaf $j$ by\n\n$w_j^* = -\\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda}, \\qquad (5)$\n\nand calculate the corresponding optimal value by\n\n$\\tilde{\\mathcal{L}}^{(t)}(q) = -\\frac{1}{2} \\sum_{j=1}^{T} \\frac{\\left(\\sum_{i \\in I_j} g_i\\right)^2}{\\sum_{i \\in I_j} h_i + \\lambda} + \\gamma T.$\n\nEq (6) can be used as a scoring function to measure the\nquality of a tree structure $q$. This score is like the impurity\nscore for evaluating decision trees, except that it is derived\nfor a wider range of objective functions. Fig. 2 illustrates\nhow this score can be calculated.\n\nNormally it is impossible to enumerate all the possible\ntree structures $q$. A greedy algorithm that starts from a\nsingle leaf and iteratively adds branches to the tree is used\ninstead. Assume that $I_L$ and $I_R$ are the instance sets of left\nand right nodes after the split. Lettting $I = I_L \\cup I_R$, then\nthe loss reduction after the split is given by\n\n$\\mathcal{L}_{\\text{split}} = \\frac{1}{2} \\left[ \\frac{\\left( \\sum_{i \\in I_L} g_i \\right)^2}{\\sum_{i \\in I_L} h_i + \\lambda} + \\frac{\\left( \\sum_{i \\in I_R} g_i \\right)^2}{\\sum_{i \\in I_R} h_i + \\lambda} - \\frac{\\left( \\sum_{i \\in I} g_i \\right)^2}{\\sum_{i \\in I} h_i + \\lambda} \\right] - \\gamma \\tag{7}$\n\nThis formula is usually used in practice for evaluating the\nsplit candidates."
        },
        {
            "text": "$\n\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2 \\\\\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (4) \\\\\n= \\sum_{j=1}^{T} [(\\sum_{i \\in I_j} g_i) w_j + \\frac{1}{2} (\\sum_{i \\in I_j} h_i + \\lambda) w_j^2] + \\gamma T\n$",
            "page": 3,
            "x": 60,
            "y": 229,
            "width": 234,
            "height": 66,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-gradient",
            "chunk_id": "f8c56016-97a6-4f99-83fe-bc99a1cabc7d",
            "group_text": "## 2.2  Gradient Tree Boosting\n\nThe tree ensemble model in Eq. (2) includes functions as parameters and cannot be optimized using traditional optimization methods in Euclidean space. Instead, the model is trained in an additive manner. Formally, let $\\hat{y}_i^{(t)}$ be the prediction of the $i$-th instance at the $t$-th iteration, we will need to add $f_t$ to minimize the following objective.\n\n$\\mathcal{L}^{(t)} = \\sum_{i=1}^{n} l(y_i, \\hat{y}_i^{(t-1)} + f_t(\\mathbf{x}_i)) + \\Omega(f_t)$\n\nThis means we greedily add the $f_t$ that most improves our\nmodel according to Eq. (2). Second-order approximation\ncan be used to quickly optimize the objective in the general\nsetting [12].\n\n$\\mathcal{L}^{(t)} \\simeq \\sum_{i=1}^{n} [l(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$\n\nwhere $g_{i} = \\partial_{\\hat{y}^{(t-1)}} l(y_{i}, \\hat{y}_{i}^{(t-1)})$ and $h_{i} = \\partial_{\\hat{y}^{(t-1)}}^{2} l(y_{i}, \\hat{y}_{i}^{(t-1)})$  \nare first and second order gradient statistics on the loss function. We can remove the constant terms to obtain the following simplified objective at step $t$.\n\n$\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$\n\nFigure 2: Structure Score Calculation.  We only\nneed to sum up the gradient and second order gra-\ndient statistics on each leaf, then apply the scoring\nformula to get the quality score.\n\nDefine $I_j = \\{i|q(\\mathbf{x}_i) = j\\}$ as the instance set of leaf $j$. We can rewrite Eq (3) by expanding $\\Omega$ as follows\n\n$\n\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2 \\\\\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (4) \\\\\n= \\sum_{j=1}^{T} [(\\sum_{i \\in I_j} g_i) w_j + \\frac{1}{2} (\\sum_{i \\in I_j} h_i + \\lambda) w_j^2] + \\gamma T\n$\n\nFor a fixed structure $q(\\mathbf{x})$, we can compute the optimal weight $w_j^*$ of leaf $j$ by\n\n$w_j^* = -\\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda}, \\qquad (5)$\n\nand calculate the corresponding optimal value by\n\n$\\tilde{\\mathcal{L}}^{(t)}(q) = -\\frac{1}{2} \\sum_{j=1}^{T} \\frac{\\left(\\sum_{i \\in I_j} g_i\\right)^2}{\\sum_{i \\in I_j} h_i + \\lambda} + \\gamma T.$\n\nEq (6) can be used as a scoring function to measure the\nquality of a tree structure $q$. This score is like the impurity\nscore for evaluating decision trees, except that it is derived\nfor a wider range of objective functions. Fig. 2 illustrates\nhow this score can be calculated.\n\nNormally it is impossible to enumerate all the possible\ntree structures $q$. A greedy algorithm that starts from a\nsingle leaf and iteratively adds branches to the tree is used\ninstead. Assume that $I_L$ and $I_R$ are the instance sets of left\nand right nodes after the split. Lettting $I = I_L \\cup I_R$, then\nthe loss reduction after the split is given by\n\n$\\mathcal{L}_{\\text{split}} = \\frac{1}{2} \\left[ \\frac{\\left( \\sum_{i \\in I_L} g_i \\right)^2}{\\sum_{i \\in I_L} h_i + \\lambda} + \\frac{\\left( \\sum_{i \\in I_R} g_i \\right)^2}{\\sum_{i \\in I_R} h_i + \\lambda} - \\frac{\\left( \\sum_{i \\in I} g_i \\right)^2}{\\sum_{i \\in I} h_i + \\lambda} \\right] - \\gamma \\tag{7}$\n\nThis formula is usually used in practice for evaluating the\nsplit candidates."
        },
        {
            "text": "For a fixed structure $q(\\mathbf{x})$, we can compute the optimal weight $w_j^*$ of leaf $j$ by",
            "page": 3,
            "x": 51,
            "y": 300,
            "width": 244,
            "height": 25,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-gradient",
            "chunk_id": "ac88943b-a332-40e6-a04f-c118c7c4b010",
            "group_text": "## 2.2  Gradient Tree Boosting\n\nThe tree ensemble model in Eq. (2) includes functions as parameters and cannot be optimized using traditional optimization methods in Euclidean space. Instead, the model is trained in an additive manner. Formally, let $\\hat{y}_i^{(t)}$ be the prediction of the $i$-th instance at the $t$-th iteration, we will need to add $f_t$ to minimize the following objective.\n\n$\\mathcal{L}^{(t)} = \\sum_{i=1}^{n} l(y_i, \\hat{y}_i^{(t-1)} + f_t(\\mathbf{x}_i)) + \\Omega(f_t)$\n\nThis means we greedily add the $f_t$ that most improves our\nmodel according to Eq. (2). Second-order approximation\ncan be used to quickly optimize the objective in the general\nsetting [12].\n\n$\\mathcal{L}^{(t)} \\simeq \\sum_{i=1}^{n} [l(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$\n\nwhere $g_{i} = \\partial_{\\hat{y}^{(t-1)}} l(y_{i}, \\hat{y}_{i}^{(t-1)})$ and $h_{i} = \\partial_{\\hat{y}^{(t-1)}}^{2} l(y_{i}, \\hat{y}_{i}^{(t-1)})$  \nare first and second order gradient statistics on the loss function. We can remove the constant terms to obtain the following simplified objective at step $t$.\n\n$\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$\n\nFigure 2: Structure Score Calculation.  We only\nneed to sum up the gradient and second order gra-\ndient statistics on each leaf, then apply the scoring\nformula to get the quality score.\n\nDefine $I_j = \\{i|q(\\mathbf{x}_i) = j\\}$ as the instance set of leaf $j$. We can rewrite Eq (3) by expanding $\\Omega$ as follows\n\n$\n\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2 \\\\\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (4) \\\\\n= \\sum_{j=1}^{T} [(\\sum_{i \\in I_j} g_i) w_j + \\frac{1}{2} (\\sum_{i \\in I_j} h_i + \\lambda) w_j^2] + \\gamma T\n$\n\nFor a fixed structure $q(\\mathbf{x})$, we can compute the optimal weight $w_j^*$ of leaf $j$ by\n\n$w_j^* = -\\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda}, \\qquad (5)$\n\nand calculate the corresponding optimal value by\n\n$\\tilde{\\mathcal{L}}^{(t)}(q) = -\\frac{1}{2} \\sum_{j=1}^{T} \\frac{\\left(\\sum_{i \\in I_j} g_i\\right)^2}{\\sum_{i \\in I_j} h_i + \\lambda} + \\gamma T.$\n\nEq (6) can be used as a scoring function to measure the\nquality of a tree structure $q$. This score is like the impurity\nscore for evaluating decision trees, except that it is derived\nfor a wider range of objective functions. Fig. 2 illustrates\nhow this score can be calculated.\n\nNormally it is impossible to enumerate all the possible\ntree structures $q$. A greedy algorithm that starts from a\nsingle leaf and iteratively adds branches to the tree is used\ninstead. Assume that $I_L$ and $I_R$ are the instance sets of left\nand right nodes after the split. Lettting $I = I_L \\cup I_R$, then\nthe loss reduction after the split is given by\n\n$\\mathcal{L}_{\\text{split}} = \\frac{1}{2} \\left[ \\frac{\\left( \\sum_{i \\in I_L} g_i \\right)^2}{\\sum_{i \\in I_L} h_i + \\lambda} + \\frac{\\left( \\sum_{i \\in I_R} g_i \\right)^2}{\\sum_{i \\in I_R} h_i + \\lambda} - \\frac{\\left( \\sum_{i \\in I} g_i \\right)^2}{\\sum_{i \\in I} h_i + \\lambda} \\right] - \\gamma \\tag{7}$\n\nThis formula is usually used in practice for evaluating the\nsplit candidates."
        },
        {
            "text": "$w_j^* = -\\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda}, \\qquad (5)$",
            "page": 3,
            "x": 125,
            "y": 328,
            "width": 169,
            "height": 32,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-gradient",
            "chunk_id": "3d7b8657-cdd6-4b77-94eb-08734e71cf2e",
            "group_text": "## 2.2  Gradient Tree Boosting\n\nThe tree ensemble model in Eq. (2) includes functions as parameters and cannot be optimized using traditional optimization methods in Euclidean space. Instead, the model is trained in an additive manner. Formally, let $\\hat{y}_i^{(t)}$ be the prediction of the $i$-th instance at the $t$-th iteration, we will need to add $f_t$ to minimize the following objective.\n\n$\\mathcal{L}^{(t)} = \\sum_{i=1}^{n} l(y_i, \\hat{y}_i^{(t-1)} + f_t(\\mathbf{x}_i)) + \\Omega(f_t)$\n\nThis means we greedily add the $f_t$ that most improves our\nmodel according to Eq. (2). Second-order approximation\ncan be used to quickly optimize the objective in the general\nsetting [12].\n\n$\\mathcal{L}^{(t)} \\simeq \\sum_{i=1}^{n} [l(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$\n\nwhere $g_{i} = \\partial_{\\hat{y}^{(t-1)}} l(y_{i}, \\hat{y}_{i}^{(t-1)})$ and $h_{i} = \\partial_{\\hat{y}^{(t-1)}}^{2} l(y_{i}, \\hat{y}_{i}^{(t-1)})$  \nare first and second order gradient statistics on the loss function. We can remove the constant terms to obtain the following simplified objective at step $t$.\n\n$\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$\n\nFigure 2: Structure Score Calculation.  We only\nneed to sum up the gradient and second order gra-\ndient statistics on each leaf, then apply the scoring\nformula to get the quality score.\n\nDefine $I_j = \\{i|q(\\mathbf{x}_i) = j\\}$ as the instance set of leaf $j$. We can rewrite Eq (3) by expanding $\\Omega$ as follows\n\n$\n\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2 \\\\\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (4) \\\\\n= \\sum_{j=1}^{T} [(\\sum_{i \\in I_j} g_i) w_j + \\frac{1}{2} (\\sum_{i \\in I_j} h_i + \\lambda) w_j^2] + \\gamma T\n$\n\nFor a fixed structure $q(\\mathbf{x})$, we can compute the optimal weight $w_j^*$ of leaf $j$ by\n\n$w_j^* = -\\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda}, \\qquad (5)$\n\nand calculate the corresponding optimal value by\n\n$\\tilde{\\mathcal{L}}^{(t)}(q) = -\\frac{1}{2} \\sum_{j=1}^{T} \\frac{\\left(\\sum_{i \\in I_j} g_i\\right)^2}{\\sum_{i \\in I_j} h_i + \\lambda} + \\gamma T.$\n\nEq (6) can be used as a scoring function to measure the\nquality of a tree structure $q$. This score is like the impurity\nscore for evaluating decision trees, except that it is derived\nfor a wider range of objective functions. Fig. 2 illustrates\nhow this score can be calculated.\n\nNormally it is impossible to enumerate all the possible\ntree structures $q$. A greedy algorithm that starts from a\nsingle leaf and iteratively adds branches to the tree is used\ninstead. Assume that $I_L$ and $I_R$ are the instance sets of left\nand right nodes after the split. Lettting $I = I_L \\cup I_R$, then\nthe loss reduction after the split is given by\n\n$\\mathcal{L}_{\\text{split}} = \\frac{1}{2} \\left[ \\frac{\\left( \\sum_{i \\in I_L} g_i \\right)^2}{\\sum_{i \\in I_L} h_i + \\lambda} + \\frac{\\left( \\sum_{i \\in I_R} g_i \\right)^2}{\\sum_{i \\in I_R} h_i + \\lambda} - \\frac{\\left( \\sum_{i \\in I} g_i \\right)^2}{\\sum_{i \\in I} h_i + \\lambda} \\right] - \\gamma \\tag{7}$\n\nThis formula is usually used in practice for evaluating the\nsplit candidates."
        },
        {
            "text": "and calculate the corresponding optimal value by",
            "page": 3,
            "x": 51,
            "y": 364,
            "width": 204,
            "height": 13,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-gradient",
            "chunk_id": "07309917-10ca-4af6-97e0-5808fc12289d",
            "group_text": "## 2.2  Gradient Tree Boosting\n\nThe tree ensemble model in Eq. (2) includes functions as parameters and cannot be optimized using traditional optimization methods in Euclidean space. Instead, the model is trained in an additive manner. Formally, let $\\hat{y}_i^{(t)}$ be the prediction of the $i$-th instance at the $t$-th iteration, we will need to add $f_t$ to minimize the following objective.\n\n$\\mathcal{L}^{(t)} = \\sum_{i=1}^{n} l(y_i, \\hat{y}_i^{(t-1)} + f_t(\\mathbf{x}_i)) + \\Omega(f_t)$\n\nThis means we greedily add the $f_t$ that most improves our\nmodel according to Eq. (2). Second-order approximation\ncan be used to quickly optimize the objective in the general\nsetting [12].\n\n$\\mathcal{L}^{(t)} \\simeq \\sum_{i=1}^{n} [l(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$\n\nwhere $g_{i} = \\partial_{\\hat{y}^{(t-1)}} l(y_{i}, \\hat{y}_{i}^{(t-1)})$ and $h_{i} = \\partial_{\\hat{y}^{(t-1)}}^{2} l(y_{i}, \\hat{y}_{i}^{(t-1)})$  \nare first and second order gradient statistics on the loss function. We can remove the constant terms to obtain the following simplified objective at step $t$.\n\n$\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$\n\nFigure 2: Structure Score Calculation.  We only\nneed to sum up the gradient and second order gra-\ndient statistics on each leaf, then apply the scoring\nformula to get the quality score.\n\nDefine $I_j = \\{i|q(\\mathbf{x}_i) = j\\}$ as the instance set of leaf $j$. We can rewrite Eq (3) by expanding $\\Omega$ as follows\n\n$\n\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2 \\\\\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (4) \\\\\n= \\sum_{j=1}^{T} [(\\sum_{i \\in I_j} g_i) w_j + \\frac{1}{2} (\\sum_{i \\in I_j} h_i + \\lambda) w_j^2] + \\gamma T\n$\n\nFor a fixed structure $q(\\mathbf{x})$, we can compute the optimal weight $w_j^*$ of leaf $j$ by\n\n$w_j^* = -\\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda}, \\qquad (5)$\n\nand calculate the corresponding optimal value by\n\n$\\tilde{\\mathcal{L}}^{(t)}(q) = -\\frac{1}{2} \\sum_{j=1}^{T} \\frac{\\left(\\sum_{i \\in I_j} g_i\\right)^2}{\\sum_{i \\in I_j} h_i + \\lambda} + \\gamma T.$\n\nEq (6) can be used as a scoring function to measure the\nquality of a tree structure $q$. This score is like the impurity\nscore for evaluating decision trees, except that it is derived\nfor a wider range of objective functions. Fig. 2 illustrates\nhow this score can be calculated.\n\nNormally it is impossible to enumerate all the possible\ntree structures $q$. A greedy algorithm that starts from a\nsingle leaf and iteratively adds branches to the tree is used\ninstead. Assume that $I_L$ and $I_R$ are the instance sets of left\nand right nodes after the split. Lettting $I = I_L \\cup I_R$, then\nthe loss reduction after the split is given by\n\n$\\mathcal{L}_{\\text{split}} = \\frac{1}{2} \\left[ \\frac{\\left( \\sum_{i \\in I_L} g_i \\right)^2}{\\sum_{i \\in I_L} h_i + \\lambda} + \\frac{\\left( \\sum_{i \\in I_R} g_i \\right)^2}{\\sum_{i \\in I_R} h_i + \\lambda} - \\frac{\\left( \\sum_{i \\in I} g_i \\right)^2}{\\sum_{i \\in I} h_i + \\lambda} \\right] - \\gamma \\tag{7}$\n\nThis formula is usually used in practice for evaluating the\nsplit candidates."
        },
        {
            "text": "$\\tilde{\\mathcal{L}}^{(t)}(q) = -\\frac{1}{2} \\sum_{j=1}^{T} \\frac{\\left(\\sum_{i \\in I_j} g_i\\right)^2}{\\sum_{i \\in I_j} h_i + \\lambda} + \\gamma T.$",
            "page": 3,
            "x": 96,
            "y": 381,
            "width": 197,
            "height": 32,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-gradient",
            "chunk_id": "8d33a1be-bc8b-4e1d-b9ff-b6357f7a81d9",
            "group_text": "## 2.2  Gradient Tree Boosting\n\nThe tree ensemble model in Eq. (2) includes functions as parameters and cannot be optimized using traditional optimization methods in Euclidean space. Instead, the model is trained in an additive manner. Formally, let $\\hat{y}_i^{(t)}$ be the prediction of the $i$-th instance at the $t$-th iteration, we will need to add $f_t$ to minimize the following objective.\n\n$\\mathcal{L}^{(t)} = \\sum_{i=1}^{n} l(y_i, \\hat{y}_i^{(t-1)} + f_t(\\mathbf{x}_i)) + \\Omega(f_t)$\n\nThis means we greedily add the $f_t$ that most improves our\nmodel according to Eq. (2). Second-order approximation\ncan be used to quickly optimize the objective in the general\nsetting [12].\n\n$\\mathcal{L}^{(t)} \\simeq \\sum_{i=1}^{n} [l(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$\n\nwhere $g_{i} = \\partial_{\\hat{y}^{(t-1)}} l(y_{i}, \\hat{y}_{i}^{(t-1)})$ and $h_{i} = \\partial_{\\hat{y}^{(t-1)}}^{2} l(y_{i}, \\hat{y}_{i}^{(t-1)})$  \nare first and second order gradient statistics on the loss function. We can remove the constant terms to obtain the following simplified objective at step $t$.\n\n$\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$\n\nFigure 2: Structure Score Calculation.  We only\nneed to sum up the gradient and second order gra-\ndient statistics on each leaf, then apply the scoring\nformula to get the quality score.\n\nDefine $I_j = \\{i|q(\\mathbf{x}_i) = j\\}$ as the instance set of leaf $j$. We can rewrite Eq (3) by expanding $\\Omega$ as follows\n\n$\n\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2 \\\\\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (4) \\\\\n= \\sum_{j=1}^{T} [(\\sum_{i \\in I_j} g_i) w_j + \\frac{1}{2} (\\sum_{i \\in I_j} h_i + \\lambda) w_j^2] + \\gamma T\n$\n\nFor a fixed structure $q(\\mathbf{x})$, we can compute the optimal weight $w_j^*$ of leaf $j$ by\n\n$w_j^* = -\\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda}, \\qquad (5)$\n\nand calculate the corresponding optimal value by\n\n$\\tilde{\\mathcal{L}}^{(t)}(q) = -\\frac{1}{2} \\sum_{j=1}^{T} \\frac{\\left(\\sum_{i \\in I_j} g_i\\right)^2}{\\sum_{i \\in I_j} h_i + \\lambda} + \\gamma T.$\n\nEq (6) can be used as a scoring function to measure the\nquality of a tree structure $q$. This score is like the impurity\nscore for evaluating decision trees, except that it is derived\nfor a wider range of objective functions. Fig. 2 illustrates\nhow this score can be calculated.\n\nNormally it is impossible to enumerate all the possible\ntree structures $q$. A greedy algorithm that starts from a\nsingle leaf and iteratively adds branches to the tree is used\ninstead. Assume that $I_L$ and $I_R$ are the instance sets of left\nand right nodes after the split. Lettting $I = I_L \\cup I_R$, then\nthe loss reduction after the split is given by\n\n$\\mathcal{L}_{\\text{split}} = \\frac{1}{2} \\left[ \\frac{\\left( \\sum_{i \\in I_L} g_i \\right)^2}{\\sum_{i \\in I_L} h_i + \\lambda} + \\frac{\\left( \\sum_{i \\in I_R} g_i \\right)^2}{\\sum_{i \\in I_R} h_i + \\lambda} - \\frac{\\left( \\sum_{i \\in I} g_i \\right)^2}{\\sum_{i \\in I} h_i + \\lambda} \\right] - \\gamma \\tag{7}$\n\nThis formula is usually used in practice for evaluating the\nsplit candidates."
        },
        {
            "text": "Eq (6) can be used as a scoring function to measure the\nquality of a tree structure $q$. This score is like the impurity\nscore for evaluating decision trees, except that it is derived\nfor a wider range of objective functions. Fig. 2 illustrates\nhow this score can be calculated.",
            "page": 3,
            "x": 51,
            "y": 419,
            "width": 244,
            "height": 53,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-gradient",
            "chunk_id": "6d8504b8-a5b0-4cf2-adad-4053e662efc0",
            "group_text": "## 2.2  Gradient Tree Boosting\n\nThe tree ensemble model in Eq. (2) includes functions as parameters and cannot be optimized using traditional optimization methods in Euclidean space. Instead, the model is trained in an additive manner. Formally, let $\\hat{y}_i^{(t)}$ be the prediction of the $i$-th instance at the $t$-th iteration, we will need to add $f_t$ to minimize the following objective.\n\n$\\mathcal{L}^{(t)} = \\sum_{i=1}^{n} l(y_i, \\hat{y}_i^{(t-1)} + f_t(\\mathbf{x}_i)) + \\Omega(f_t)$\n\nThis means we greedily add the $f_t$ that most improves our\nmodel according to Eq. (2). Second-order approximation\ncan be used to quickly optimize the objective in the general\nsetting [12].\n\n$\\mathcal{L}^{(t)} \\simeq \\sum_{i=1}^{n} [l(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$\n\nwhere $g_{i} = \\partial_{\\hat{y}^{(t-1)}} l(y_{i}, \\hat{y}_{i}^{(t-1)})$ and $h_{i} = \\partial_{\\hat{y}^{(t-1)}}^{2} l(y_{i}, \\hat{y}_{i}^{(t-1)})$  \nare first and second order gradient statistics on the loss function. We can remove the constant terms to obtain the following simplified objective at step $t$.\n\n$\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$\n\nFigure 2: Structure Score Calculation.  We only\nneed to sum up the gradient and second order gra-\ndient statistics on each leaf, then apply the scoring\nformula to get the quality score.\n\nDefine $I_j = \\{i|q(\\mathbf{x}_i) = j\\}$ as the instance set of leaf $j$. We can rewrite Eq (3) by expanding $\\Omega$ as follows\n\n$\n\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2 \\\\\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (4) \\\\\n= \\sum_{j=1}^{T} [(\\sum_{i \\in I_j} g_i) w_j + \\frac{1}{2} (\\sum_{i \\in I_j} h_i + \\lambda) w_j^2] + \\gamma T\n$\n\nFor a fixed structure $q(\\mathbf{x})$, we can compute the optimal weight $w_j^*$ of leaf $j$ by\n\n$w_j^* = -\\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda}, \\qquad (5)$\n\nand calculate the corresponding optimal value by\n\n$\\tilde{\\mathcal{L}}^{(t)}(q) = -\\frac{1}{2} \\sum_{j=1}^{T} \\frac{\\left(\\sum_{i \\in I_j} g_i\\right)^2}{\\sum_{i \\in I_j} h_i + \\lambda} + \\gamma T.$\n\nEq (6) can be used as a scoring function to measure the\nquality of a tree structure $q$. This score is like the impurity\nscore for evaluating decision trees, except that it is derived\nfor a wider range of objective functions. Fig. 2 illustrates\nhow this score can be calculated.\n\nNormally it is impossible to enumerate all the possible\ntree structures $q$. A greedy algorithm that starts from a\nsingle leaf and iteratively adds branches to the tree is used\ninstead. Assume that $I_L$ and $I_R$ are the instance sets of left\nand right nodes after the split. Lettting $I = I_L \\cup I_R$, then\nthe loss reduction after the split is given by\n\n$\\mathcal{L}_{\\text{split}} = \\frac{1}{2} \\left[ \\frac{\\left( \\sum_{i \\in I_L} g_i \\right)^2}{\\sum_{i \\in I_L} h_i + \\lambda} + \\frac{\\left( \\sum_{i \\in I_R} g_i \\right)^2}{\\sum_{i \\in I_R} h_i + \\lambda} - \\frac{\\left( \\sum_{i \\in I} g_i \\right)^2}{\\sum_{i \\in I} h_i + \\lambda} \\right] - \\gamma \\tag{7}$\n\nThis formula is usually used in practice for evaluating the\nsplit candidates."
        },
        {
            "text": "Normally it is impossible to enumerate all the possible\ntree structures $q$. A greedy algorithm that starts from a\nsingle leaf and iteratively adds branches to the tree is used\ninstead. Assume that $I_L$ and $I_R$ are the instance sets of left\nand right nodes after the split. Lettting $I = I_L \\cup I_R$, then\nthe loss reduction after the split is given by",
            "page": 3,
            "x": 51,
            "y": 473,
            "width": 243,
            "height": 64,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-gradient",
            "chunk_id": "6f5e47dd-fca1-4aa5-b198-485ff555e6b3",
            "group_text": "## 2.2  Gradient Tree Boosting\n\nThe tree ensemble model in Eq. (2) includes functions as parameters and cannot be optimized using traditional optimization methods in Euclidean space. Instead, the model is trained in an additive manner. Formally, let $\\hat{y}_i^{(t)}$ be the prediction of the $i$-th instance at the $t$-th iteration, we will need to add $f_t$ to minimize the following objective.\n\n$\\mathcal{L}^{(t)} = \\sum_{i=1}^{n} l(y_i, \\hat{y}_i^{(t-1)} + f_t(\\mathbf{x}_i)) + \\Omega(f_t)$\n\nThis means we greedily add the $f_t$ that most improves our\nmodel according to Eq. (2). Second-order approximation\ncan be used to quickly optimize the objective in the general\nsetting [12].\n\n$\\mathcal{L}^{(t)} \\simeq \\sum_{i=1}^{n} [l(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$\n\nwhere $g_{i} = \\partial_{\\hat{y}^{(t-1)}} l(y_{i}, \\hat{y}_{i}^{(t-1)})$ and $h_{i} = \\partial_{\\hat{y}^{(t-1)}}^{2} l(y_{i}, \\hat{y}_{i}^{(t-1)})$  \nare first and second order gradient statistics on the loss function. We can remove the constant terms to obtain the following simplified objective at step $t$.\n\n$\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$\n\nFigure 2: Structure Score Calculation.  We only\nneed to sum up the gradient and second order gra-\ndient statistics on each leaf, then apply the scoring\nformula to get the quality score.\n\nDefine $I_j = \\{i|q(\\mathbf{x}_i) = j\\}$ as the instance set of leaf $j$. We can rewrite Eq (3) by expanding $\\Omega$ as follows\n\n$\n\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2 \\\\\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (4) \\\\\n= \\sum_{j=1}^{T} [(\\sum_{i \\in I_j} g_i) w_j + \\frac{1}{2} (\\sum_{i \\in I_j} h_i + \\lambda) w_j^2] + \\gamma T\n$\n\nFor a fixed structure $q(\\mathbf{x})$, we can compute the optimal weight $w_j^*$ of leaf $j$ by\n\n$w_j^* = -\\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda}, \\qquad (5)$\n\nand calculate the corresponding optimal value by\n\n$\\tilde{\\mathcal{L}}^{(t)}(q) = -\\frac{1}{2} \\sum_{j=1}^{T} \\frac{\\left(\\sum_{i \\in I_j} g_i\\right)^2}{\\sum_{i \\in I_j} h_i + \\lambda} + \\gamma T.$\n\nEq (6) can be used as a scoring function to measure the\nquality of a tree structure $q$. This score is like the impurity\nscore for evaluating decision trees, except that it is derived\nfor a wider range of objective functions. Fig. 2 illustrates\nhow this score can be calculated.\n\nNormally it is impossible to enumerate all the possible\ntree structures $q$. A greedy algorithm that starts from a\nsingle leaf and iteratively adds branches to the tree is used\ninstead. Assume that $I_L$ and $I_R$ are the instance sets of left\nand right nodes after the split. Lettting $I = I_L \\cup I_R$, then\nthe loss reduction after the split is given by\n\n$\\mathcal{L}_{\\text{split}} = \\frac{1}{2} \\left[ \\frac{\\left( \\sum_{i \\in I_L} g_i \\right)^2}{\\sum_{i \\in I_L} h_i + \\lambda} + \\frac{\\left( \\sum_{i \\in I_R} g_i \\right)^2}{\\sum_{i \\in I_R} h_i + \\lambda} - \\frac{\\left( \\sum_{i \\in I} g_i \\right)^2}{\\sum_{i \\in I} h_i + \\lambda} \\right] - \\gamma \\tag{7}$\n\nThis formula is usually used in practice for evaluating the\nsplit candidates."
        },
        {
            "text": "$\\mathcal{L}_{\\text{split}} = \\frac{1}{2} \\left[ \\frac{\\left( \\sum_{i \\in I_L} g_i \\right)^2}{\\sum_{i \\in I_L} h_i + \\lambda} + \\frac{\\left( \\sum_{i \\in I_R} g_i \\right)^2}{\\sum_{i \\in I_R} h_i + \\lambda} - \\frac{\\left( \\sum_{i \\in I} g_i \\right)^2}{\\sum_{i \\in I} h_i + \\lambda} \\right] - \\gamma \\tag{7}$",
            "page": 3,
            "x": 52,
            "y": 541,
            "width": 252,
            "height": 38,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-gradient",
            "chunk_id": "8b0fad13-6ba3-4958-8e2c-0f85be287d31",
            "group_text": "## 2.2  Gradient Tree Boosting\n\nThe tree ensemble model in Eq. (2) includes functions as parameters and cannot be optimized using traditional optimization methods in Euclidean space. Instead, the model is trained in an additive manner. Formally, let $\\hat{y}_i^{(t)}$ be the prediction of the $i$-th instance at the $t$-th iteration, we will need to add $f_t$ to minimize the following objective.\n\n$\\mathcal{L}^{(t)} = \\sum_{i=1}^{n} l(y_i, \\hat{y}_i^{(t-1)} + f_t(\\mathbf{x}_i)) + \\Omega(f_t)$\n\nThis means we greedily add the $f_t$ that most improves our\nmodel according to Eq. (2). Second-order approximation\ncan be used to quickly optimize the objective in the general\nsetting [12].\n\n$\\mathcal{L}^{(t)} \\simeq \\sum_{i=1}^{n} [l(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$\n\nwhere $g_{i} = \\partial_{\\hat{y}^{(t-1)}} l(y_{i}, \\hat{y}_{i}^{(t-1)})$ and $h_{i} = \\partial_{\\hat{y}^{(t-1)}}^{2} l(y_{i}, \\hat{y}_{i}^{(t-1)})$  \nare first and second order gradient statistics on the loss function. We can remove the constant terms to obtain the following simplified objective at step $t$.\n\n$\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$\n\nFigure 2: Structure Score Calculation.  We only\nneed to sum up the gradient and second order gra-\ndient statistics on each leaf, then apply the scoring\nformula to get the quality score.\n\nDefine $I_j = \\{i|q(\\mathbf{x}_i) = j\\}$ as the instance set of leaf $j$. We can rewrite Eq (3) by expanding $\\Omega$ as follows\n\n$\n\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2 \\\\\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (4) \\\\\n= \\sum_{j=1}^{T} [(\\sum_{i \\in I_j} g_i) w_j + \\frac{1}{2} (\\sum_{i \\in I_j} h_i + \\lambda) w_j^2] + \\gamma T\n$\n\nFor a fixed structure $q(\\mathbf{x})$, we can compute the optimal weight $w_j^*$ of leaf $j$ by\n\n$w_j^* = -\\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda}, \\qquad (5)$\n\nand calculate the corresponding optimal value by\n\n$\\tilde{\\mathcal{L}}^{(t)}(q) = -\\frac{1}{2} \\sum_{j=1}^{T} \\frac{\\left(\\sum_{i \\in I_j} g_i\\right)^2}{\\sum_{i \\in I_j} h_i + \\lambda} + \\gamma T.$\n\nEq (6) can be used as a scoring function to measure the\nquality of a tree structure $q$. This score is like the impurity\nscore for evaluating decision trees, except that it is derived\nfor a wider range of objective functions. Fig. 2 illustrates\nhow this score can be calculated.\n\nNormally it is impossible to enumerate all the possible\ntree structures $q$. A greedy algorithm that starts from a\nsingle leaf and iteratively adds branches to the tree is used\ninstead. Assume that $I_L$ and $I_R$ are the instance sets of left\nand right nodes after the split. Lettting $I = I_L \\cup I_R$, then\nthe loss reduction after the split is given by\n\n$\\mathcal{L}_{\\text{split}} = \\frac{1}{2} \\left[ \\frac{\\left( \\sum_{i \\in I_L} g_i \\right)^2}{\\sum_{i \\in I_L} h_i + \\lambda} + \\frac{\\left( \\sum_{i \\in I_R} g_i \\right)^2}{\\sum_{i \\in I_R} h_i + \\lambda} - \\frac{\\left( \\sum_{i \\in I} g_i \\right)^2}{\\sum_{i \\in I} h_i + \\lambda} \\right] - \\gamma \\tag{7}$\n\nThis formula is usually used in practice for evaluating the\nsplit candidates."
        },
        {
            "text": "This formula is usually used in practice for evaluating the\nsplit candidates.",
            "page": 3,
            "x": 51,
            "y": 579,
            "width": 244,
            "height": 24,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-gradient",
            "chunk_id": "9a53c6bc-a7fd-4f42-9c76-c85f2aa9977a",
            "group_text": "## 2.2  Gradient Tree Boosting\n\nThe tree ensemble model in Eq. (2) includes functions as parameters and cannot be optimized using traditional optimization methods in Euclidean space. Instead, the model is trained in an additive manner. Formally, let $\\hat{y}_i^{(t)}$ be the prediction of the $i$-th instance at the $t$-th iteration, we will need to add $f_t$ to minimize the following objective.\n\n$\\mathcal{L}^{(t)} = \\sum_{i=1}^{n} l(y_i, \\hat{y}_i^{(t-1)} + f_t(\\mathbf{x}_i)) + \\Omega(f_t)$\n\nThis means we greedily add the $f_t$ that most improves our\nmodel according to Eq. (2). Second-order approximation\ncan be used to quickly optimize the objective in the general\nsetting [12].\n\n$\\mathcal{L}^{(t)} \\simeq \\sum_{i=1}^{n} [l(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$\n\nwhere $g_{i} = \\partial_{\\hat{y}^{(t-1)}} l(y_{i}, \\hat{y}_{i}^{(t-1)})$ and $h_{i} = \\partial_{\\hat{y}^{(t-1)}}^{2} l(y_{i}, \\hat{y}_{i}^{(t-1)})$  \nare first and second order gradient statistics on the loss function. We can remove the constant terms to obtain the following simplified objective at step $t$.\n\n$\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$\n\nFigure 2: Structure Score Calculation.  We only\nneed to sum up the gradient and second order gra-\ndient statistics on each leaf, then apply the scoring\nformula to get the quality score.\n\nDefine $I_j = \\{i|q(\\mathbf{x}_i) = j\\}$ as the instance set of leaf $j$. We can rewrite Eq (3) by expanding $\\Omega$ as follows\n\n$\n\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2 \\\\\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (4) \\\\\n= \\sum_{j=1}^{T} [(\\sum_{i \\in I_j} g_i) w_j + \\frac{1}{2} (\\sum_{i \\in I_j} h_i + \\lambda) w_j^2] + \\gamma T\n$\n\nFor a fixed structure $q(\\mathbf{x})$, we can compute the optimal weight $w_j^*$ of leaf $j$ by\n\n$w_j^* = -\\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda}, \\qquad (5)$\n\nand calculate the corresponding optimal value by\n\n$\\tilde{\\mathcal{L}}^{(t)}(q) = -\\frac{1}{2} \\sum_{j=1}^{T} \\frac{\\left(\\sum_{i \\in I_j} g_i\\right)^2}{\\sum_{i \\in I_j} h_i + \\lambda} + \\gamma T.$\n\nEq (6) can be used as a scoring function to measure the\nquality of a tree structure $q$. This score is like the impurity\nscore for evaluating decision trees, except that it is derived\nfor a wider range of objective functions. Fig. 2 illustrates\nhow this score can be calculated.\n\nNormally it is impossible to enumerate all the possible\ntree structures $q$. A greedy algorithm that starts from a\nsingle leaf and iteratively adds branches to the tree is used\ninstead. Assume that $I_L$ and $I_R$ are the instance sets of left\nand right nodes after the split. Lettting $I = I_L \\cup I_R$, then\nthe loss reduction after the split is given by\n\n$\\mathcal{L}_{\\text{split}} = \\frac{1}{2} \\left[ \\frac{\\left( \\sum_{i \\in I_L} g_i \\right)^2}{\\sum_{i \\in I_L} h_i + \\lambda} + \\frac{\\left( \\sum_{i \\in I_R} g_i \\right)^2}{\\sum_{i \\in I_R} h_i + \\lambda} - \\frac{\\left( \\sum_{i \\in I} g_i \\right)^2}{\\sum_{i \\in I} h_i + \\lambda} \\right] - \\gamma \\tag{7}$\n\nThis formula is usually used in practice for evaluating the\nsplit candidates."
        },
        {
            "text": "## 2.3  Shrinkage and Column Subsampling\n\nBesides the regularized objective mentioned in Sec. 2.1, two additional techniques are used to further prevent overfitting. The first technique is shrinkage introduced by Friedman [11]. Shrinkage scales newly added weights by a factor $\\eta$ after each step of tree boosting. Similar to a learning rate in stochastic optimization, shrinkage reduces the influence of each individual tree and leaves space for future trees to improve the model. The second technique is column (feature) subsampling. This technique is used in RandomForest [4,",
            "page": 3,
            "x": 51,
            "y": 609,
            "width": 244,
            "height": 110,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "6-shrinkage",
            "chunk_id": "b4d15b63-74cc-420c-a43d-d30c80d1fe0d",
            "group_text": "## 2.3  Shrinkage and Column Subsampling\n\nBesides the regularized objective mentioned in Sec. 2.1, two additional techniques are used to further prevent overfitting. The first technique is shrinkage introduced by Friedman [11]. Shrinkage scales newly added weights by a factor $\\eta$ after each step of tree boosting. Similar to a learning rate in stochastic optimization, shrinkage reduces the influence of each individual tree and leaves space for future trees to improve the model. The second technique is column (feature) subsampling. This technique is used in RandomForest [4,\n\nAlgorithm 1: Exact Greedy Algorithm for Split Finding\nInput: $I$, instance set of current node\nInput: $d$, feature dimension\n$gain \\gets 0$\n$G \\gets \\sum_{i \\in I} g_i$, $H \\gets \\sum_{i \\in I} h_i$\nfor $k = 1$ to $m$ do\n  $G_L \\gets 0$, $H_L \\gets 0$\n  for $j$ in $sorted(I, \\text{ by } x_{jk})$ do\n    $G_L \\gets G_L + g_j$, $H_L \\gets H_L + h_j$\n    $G_R \\gets G - G_L$, $H_R \\gets H - H_L$\n    $score \\gets \\max(score, \\frac{G_L^2}{H_L+\\lambda} + \\frac{G_R^2}{H_R+\\lambda} - \\frac{G^2}{H+\\lambda})$\n  end\nend\nOutput: Split with max score\n\nAlgorithm 2: Approximate Algorithm for Split Finding\nfor $k = 1$ to $m$ do\n\u2003Propose $S_k = \\{s_{k1}, s_{k2}, \\cdots s_{kl}\\}$ by percentiles on feature $k$.\n\u2003Proposal can be done per tree (global), or per split (local).\nend\nfor $k = 1$ to $m$ do\n\u2003$G_{kv} \\leftarrow \\sum_{j \\in \\{j|s_{k,v} \\geq x_{jk} > s_{k,v-1}\\}} g_j$\n\u2003$H_{kv} \\leftarrow \\sum_{j \\in \\{j|s_{k,v} \\geq x_{jk} > s_{k,v-1}\\}} h_j$\nend\nFollow same step as in previous section to find max\nscore only among proposed splits.\n\n13], It is implemented in a commercial software TreeNet for gradient boosting, but is not implemented in existing opensource packages. According to user feedback, using column sub-sampling prevents over-fitting even more so than the traditional row sub-sampling (which is also supported). The usage of column sub-samples also speeds up computations of the parallel algorithm described later."
        },
        {
            "text": "Algorithm 1: Exact Greedy Algorithm for Split Finding\nInput: $I$, instance set of current node\nInput: $d$, feature dimension\n$gain \\gets 0$\n$G \\gets \\sum_{i \\in I} g_i$, $H \\gets \\sum_{i \\in I} h_i$\nfor $k = 1$ to $m$ do\n  $G_L \\gets 0$, $H_L \\gets 0$\n  for $j$ in $sorted(I, \\text{ by } x_{jk})$ do\n    $G_L \\gets G_L + g_j$, $H_L \\gets H_L + h_j$\n    $G_R \\gets G - G_L$, $H_R \\gets H - H_L$\n    $score \\gets \\max(score, \\frac{G_L^2}{H_L+\\lambda} + \\frac{G_R^2}{H_R+\\lambda} - \\frac{G^2}{H+\\lambda})$\n  end\nend\nOutput: Split with max score",
            "page": 3,
            "x": 316,
            "y": 55,
            "width": 242,
            "height": 157,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "6-shrinkage",
            "chunk_id": "3a30adac-94c4-4aa0-b280-bddc235aa033",
            "group_text": "## 2.3  Shrinkage and Column Subsampling\n\nBesides the regularized objective mentioned in Sec. 2.1, two additional techniques are used to further prevent overfitting. The first technique is shrinkage introduced by Friedman [11]. Shrinkage scales newly added weights by a factor $\\eta$ after each step of tree boosting. Similar to a learning rate in stochastic optimization, shrinkage reduces the influence of each individual tree and leaves space for future trees to improve the model. The second technique is column (feature) subsampling. This technique is used in RandomForest [4,\n\nAlgorithm 1: Exact Greedy Algorithm for Split Finding\nInput: $I$, instance set of current node\nInput: $d$, feature dimension\n$gain \\gets 0$\n$G \\gets \\sum_{i \\in I} g_i$, $H \\gets \\sum_{i \\in I} h_i$\nfor $k = 1$ to $m$ do\n  $G_L \\gets 0$, $H_L \\gets 0$\n  for $j$ in $sorted(I, \\text{ by } x_{jk})$ do\n    $G_L \\gets G_L + g_j$, $H_L \\gets H_L + h_j$\n    $G_R \\gets G - G_L$, $H_R \\gets H - H_L$\n    $score \\gets \\max(score, \\frac{G_L^2}{H_L+\\lambda} + \\frac{G_R^2}{H_R+\\lambda} - \\frac{G^2}{H+\\lambda})$\n  end\nend\nOutput: Split with max score\n\nAlgorithm 2: Approximate Algorithm for Split Finding\nfor $k = 1$ to $m$ do\n\u2003Propose $S_k = \\{s_{k1}, s_{k2}, \\cdots s_{kl}\\}$ by percentiles on feature $k$.\n\u2003Proposal can be done per tree (global), or per split (local).\nend\nfor $k = 1$ to $m$ do\n\u2003$G_{kv} \\leftarrow \\sum_{j \\in \\{j|s_{k,v} \\geq x_{jk} > s_{k,v-1}\\}} g_j$\n\u2003$H_{kv} \\leftarrow \\sum_{j \\in \\{j|s_{k,v} \\geq x_{jk} > s_{k,v-1}\\}} h_j$\nend\nFollow same step as in previous section to find max\nscore only among proposed splits.\n\n13], It is implemented in a commercial software TreeNet for gradient boosting, but is not implemented in existing opensource packages. According to user feedback, using column sub-sampling prevents over-fitting even more so than the traditional row sub-sampling (which is also supported). The usage of column sub-samples also speeds up computations of the parallel algorithm described later."
        },
        {
            "text": "Algorithm 2: Approximate Algorithm for Split Finding\nfor $k = 1$ to $m$ do\n\u2003Propose $S_k = \\{s_{k1}, s_{k2}, \\cdots s_{kl}\\}$ by percentiles on feature $k$.\n\u2003Proposal can be done per tree (global), or per split (local).\nend\nfor $k = 1$ to $m$ do\n\u2003$G_{kv} \\leftarrow \\sum_{j \\in \\{j|s_{k,v} \\geq x_{jk} > s_{k,v-1}\\}} g_j$\n\u2003$H_{kv} \\leftarrow \\sum_{j \\in \\{j|s_{k,v} \\geq x_{jk} > s_{k,v-1}\\}} h_j$\nend\nFollow same step as in previous section to find max\nscore only among proposed splits.",
            "page": 3,
            "x": 316,
            "y": 230,
            "width": 261,
            "height": 125,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "6-shrinkage",
            "chunk_id": "01b181c4-ed7a-499e-a477-270c29d4d4ae",
            "group_text": "## 2.3  Shrinkage and Column Subsampling\n\nBesides the regularized objective mentioned in Sec. 2.1, two additional techniques are used to further prevent overfitting. The first technique is shrinkage introduced by Friedman [11]. Shrinkage scales newly added weights by a factor $\\eta$ after each step of tree boosting. Similar to a learning rate in stochastic optimization, shrinkage reduces the influence of each individual tree and leaves space for future trees to improve the model. The second technique is column (feature) subsampling. This technique is used in RandomForest [4,\n\nAlgorithm 1: Exact Greedy Algorithm for Split Finding\nInput: $I$, instance set of current node\nInput: $d$, feature dimension\n$gain \\gets 0$\n$G \\gets \\sum_{i \\in I} g_i$, $H \\gets \\sum_{i \\in I} h_i$\nfor $k = 1$ to $m$ do\n  $G_L \\gets 0$, $H_L \\gets 0$\n  for $j$ in $sorted(I, \\text{ by } x_{jk})$ do\n    $G_L \\gets G_L + g_j$, $H_L \\gets H_L + h_j$\n    $G_R \\gets G - G_L$, $H_R \\gets H - H_L$\n    $score \\gets \\max(score, \\frac{G_L^2}{H_L+\\lambda} + \\frac{G_R^2}{H_R+\\lambda} - \\frac{G^2}{H+\\lambda})$\n  end\nend\nOutput: Split with max score\n\nAlgorithm 2: Approximate Algorithm for Split Finding\nfor $k = 1$ to $m$ do\n\u2003Propose $S_k = \\{s_{k1}, s_{k2}, \\cdots s_{kl}\\}$ by percentiles on feature $k$.\n\u2003Proposal can be done per tree (global), or per split (local).\nend\nfor $k = 1$ to $m$ do\n\u2003$G_{kv} \\leftarrow \\sum_{j \\in \\{j|s_{k,v} \\geq x_{jk} > s_{k,v-1}\\}} g_j$\n\u2003$H_{kv} \\leftarrow \\sum_{j \\in \\{j|s_{k,v} \\geq x_{jk} > s_{k,v-1}\\}} h_j$\nend\nFollow same step as in previous section to find max\nscore only among proposed splits.\n\n13], It is implemented in a commercial software TreeNet for gradient boosting, but is not implemented in existing opensource packages. According to user feedback, using column sub-sampling prevents over-fitting even more so than the traditional row sub-sampling (which is also supported). The usage of column sub-samples also speeds up computations of the parallel algorithm described later."
        },
        {
            "text": "13], It is implemented in a commercial software TreeNet for gradient boosting, but is not implemented in existing opensource packages. According to user feedback, using column sub-sampling prevents over-fitting even more so than the traditional row sub-sampling (which is also supported). The usage of column sub-samples also speeds up computations of the parallel algorithm described later.",
            "page": 3,
            "x": 314,
            "y": 379,
            "width": 244,
            "height": 75,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "6-shrinkage",
            "chunk_id": "4cb1b5e0-7362-436e-9bbc-9b3004a990d2",
            "group_text": "## 2.3  Shrinkage and Column Subsampling\n\nBesides the regularized objective mentioned in Sec. 2.1, two additional techniques are used to further prevent overfitting. The first technique is shrinkage introduced by Friedman [11]. Shrinkage scales newly added weights by a factor $\\eta$ after each step of tree boosting. Similar to a learning rate in stochastic optimization, shrinkage reduces the influence of each individual tree and leaves space for future trees to improve the model. The second technique is column (feature) subsampling. This technique is used in RandomForest [4,\n\nAlgorithm 1: Exact Greedy Algorithm for Split Finding\nInput: $I$, instance set of current node\nInput: $d$, feature dimension\n$gain \\gets 0$\n$G \\gets \\sum_{i \\in I} g_i$, $H \\gets \\sum_{i \\in I} h_i$\nfor $k = 1$ to $m$ do\n  $G_L \\gets 0$, $H_L \\gets 0$\n  for $j$ in $sorted(I, \\text{ by } x_{jk})$ do\n    $G_L \\gets G_L + g_j$, $H_L \\gets H_L + h_j$\n    $G_R \\gets G - G_L$, $H_R \\gets H - H_L$\n    $score \\gets \\max(score, \\frac{G_L^2}{H_L+\\lambda} + \\frac{G_R^2}{H_R+\\lambda} - \\frac{G^2}{H+\\lambda})$\n  end\nend\nOutput: Split with max score\n\nAlgorithm 2: Approximate Algorithm for Split Finding\nfor $k = 1$ to $m$ do\n\u2003Propose $S_k = \\{s_{k1}, s_{k2}, \\cdots s_{kl}\\}$ by percentiles on feature $k$.\n\u2003Proposal can be done per tree (global), or per split (local).\nend\nfor $k = 1$ to $m$ do\n\u2003$G_{kv} \\leftarrow \\sum_{j \\in \\{j|s_{k,v} \\geq x_{jk} > s_{k,v-1}\\}} g_j$\n\u2003$H_{kv} \\leftarrow \\sum_{j \\in \\{j|s_{k,v} \\geq x_{jk} > s_{k,v-1}\\}} h_j$\nend\nFollow same step as in previous section to find max\nscore only among proposed splits.\n\n13], It is implemented in a commercial software TreeNet for gradient boosting, but is not implemented in existing opensource packages. According to user feedback, using column sub-sampling prevents over-fitting even more so than the traditional row sub-sampling (which is also supported). The usage of column sub-samples also speeds up computations of the parallel algorithm described later."
        },
        {
            "text": "3.    SPLIT FINDING ALGORITHMS\n\n3.1   Basic Exact Greedy Algorithm\n\n    One of the key problems in tree learning is to find the best split as indicated by Eq (7). In order to do so, a split finding algorithm enumerates over all the possible splits on all the features. We call this the _exact greedy algorithm_. Most existing single machine tree boosting implementations, such as scikit-learn [20], R\u2019s gbm [21] as well as the single machine version of XGBoost support the exact greedy algorithm. The exact greedy algorithm is shown in Alg. 1. It is computationally demanding to enumerate all the possible splits for continuous features. In order to do so efficiently, the algorithm must first sort the data according to feature values and visit the data in sorted order to accumulate the gradient statistics for the structure score in Eq (7).",
            "page": 3,
            "x": 314,
            "y": 464,
            "width": 243,
            "height": 174,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-split",
            "chunk_id": "53384dcd-8972-40ed-99fc-9dcb5e713773",
            "group_text": "3.    SPLIT FINDING ALGORITHMS\n\n3.1   Basic Exact Greedy Algorithm\n\n    One of the key problems in tree learning is to find the best split as indicated by Eq (7). In order to do so, a split finding algorithm enumerates over all the possible splits on all the features. We call this the _exact greedy algorithm_. Most existing single machine tree boosting implementations, such as scikit-learn [20], R\u2019s gbm [21] as well as the single machine version of XGBoost support the exact greedy algorithm. The exact greedy algorithm is shown in Alg. 1. It is computationally demanding to enumerate all the possible splits for continuous features. In order to do so efficiently, the algorithm must first sort the data according to feature values and visit the data in sorted order to accumulate the gradient statistics for the structure score in Eq (7)."
        },
        {
            "text": "3.2  Approximate Algorithm\n\n    The exact greedy algorithm is very powerful since it enumerates over all possible splitting points greedily. However, it is impossible to efficiently do so when the data does not fit entirely into memory. Same problem also arises in the dis-",
            "page": 3,
            "x": 314,
            "y": 644,
            "width": 242,
            "height": 60,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "8-approximate",
            "chunk_id": "5cb76fe1-5337-4f68-887e-251c79750ccf",
            "group_text": "3.2  Approximate Algorithm\n\n    The exact greedy algorithm is very powerful since it enumerates over all possible splitting points greedily. However, it is impossible to efficiently do so when the data does not fit entirely into memory. Same problem also arises in the dis-\n\n4https://www.salford-systems.com/products/treenet\n\ntributed setting. To support effective gradient tree boosting\nin these two settings, an approximate algorithm is needed.\n\n   We summarize an approximate framework, which resembles the ideas proposed in past literatures [17, 2, 22], in Alg. 2. To summarize, the algorithm first proposes candidate splitting points according to percentiles of feature distribution (a specific criteria will be given in Sec. 3.3).\nThe algorithm then maps the continuous features into buckets split by these candidate points, aggregates the statistics and finds the best solution among proposals based on the aggregated statistics.\n\nThere are two variants of the algorithm, depending on\nwhen the proposal is given. The global variant proposes all\nthe candidate splits during the initial phase of tree construc-\ntion, and uses the same proposals for split finding at all lev-\nels. The local variant re-proposes after each split. The global\nmethod requires less proposal steps than the local method.\nHowever, usually more candidate points are needed for the\nglobal proposal because candidates are not refined after each\nsplit. The local proposal refines the candidates after splits,\nand can potentially be more appropriate for deeper trees. A\ncomparison of different algorithms on a Higgs boson dataset\nis given by Fig. 3. We find that the local proposal indeed\nrequires fewer candidates. The global proposal can be as\naccurate as the local one given enough candidates.\n\nMost existing approximate algorithms for distributed tree\nlearning also follow this framework. Notably, it is also possi-\nble to directly construct approximate histograms of gradient\nstatistics [22]. It is also possible to use other variants of bin-\nning strategies instead of quantile [17]. Quantile strategy\nbenefit from being distributable and recomputable, which\nwe will detail in next subsection. From Fig. 3, we also find\nthat the quantile strategy can get the same accuracy as exact\ngreedy given reasonable approximation level.\n\nOur system efficiently supports exact greedy for the single\nmachine setting, as well as approximate algorithm with both\nlocal and global proposal methods for all settings. Users can\nfreely choose between the methods according to their needs."
        },
        {
            "text": "4https://www.salford-systems.com/products/treenet",
            "page": 3,
            "x": 316,
            "y": 707,
            "width": 213,
            "height": 13,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "8-approximate",
            "chunk_id": "763c004c-b6b7-4c39-9587-20f4deaf2b36",
            "group_text": "3.2  Approximate Algorithm\n\n    The exact greedy algorithm is very powerful since it enumerates over all possible splitting points greedily. However, it is impossible to efficiently do so when the data does not fit entirely into memory. Same problem also arises in the dis-\n\n4https://www.salford-systems.com/products/treenet\n\ntributed setting. To support effective gradient tree boosting\nin these two settings, an approximate algorithm is needed.\n\n   We summarize an approximate framework, which resembles the ideas proposed in past literatures [17, 2, 22], in Alg. 2. To summarize, the algorithm first proposes candidate splitting points according to percentiles of feature distribution (a specific criteria will be given in Sec. 3.3).\nThe algorithm then maps the continuous features into buckets split by these candidate points, aggregates the statistics and finds the best solution among proposals based on the aggregated statistics.\n\nThere are two variants of the algorithm, depending on\nwhen the proposal is given. The global variant proposes all\nthe candidate splits during the initial phase of tree construc-\ntion, and uses the same proposals for split finding at all lev-\nels. The local variant re-proposes after each split. The global\nmethod requires less proposal steps than the local method.\nHowever, usually more candidate points are needed for the\nglobal proposal because candidates are not refined after each\nsplit. The local proposal refines the candidates after splits,\nand can potentially be more appropriate for deeper trees. A\ncomparison of different algorithms on a Higgs boson dataset\nis given by Fig. 3. We find that the local proposal indeed\nrequires fewer candidates. The global proposal can be as\naccurate as the local one given enough candidates.\n\nMost existing approximate algorithms for distributed tree\nlearning also follow this framework. Notably, it is also possi-\nble to directly construct approximate histograms of gradient\nstatistics [22]. It is also possible to use other variants of bin-\nning strategies instead of quantile [17]. Quantile strategy\nbenefit from being distributable and recomputable, which\nwe will detail in next subsection. From Fig. 3, we also find\nthat the quantile strategy can get the same accuracy as exact\ngreedy given reasonable approximation level.\n\nOur system efficiently supports exact greedy for the single\nmachine setting, as well as approximate algorithm with both\nlocal and global proposal methods for all settings. Users can\nfreely choose between the methods according to their needs."
        },
        {
            "text": "tributed setting. To support effective gradient tree boosting\nin these two settings, an approximate algorithm is needed.\n\n   We summarize an approximate framework, which resembles the ideas proposed in past literatures [17, 2, 22], in Alg. 2. To summarize, the algorithm first proposes candidate splitting points according to percentiles of feature distribution (a specific criteria will be given in Sec. 3.3).\nThe algorithm then maps the continuous features into buckets split by these candidate points, aggregates the statistics and finds the best solution among proposals based on the aggregated statistics.",
            "page": 4,
            "x": 49,
            "y": 275,
            "width": 246,
            "height": 115,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "8-approximate",
            "chunk_id": "071330e9-54cb-4b26-86f9-5014d220525c",
            "group_text": "3.2  Approximate Algorithm\n\n    The exact greedy algorithm is very powerful since it enumerates over all possible splitting points greedily. However, it is impossible to efficiently do so when the data does not fit entirely into memory. Same problem also arises in the dis-\n\n4https://www.salford-systems.com/products/treenet\n\ntributed setting. To support effective gradient tree boosting\nin these two settings, an approximate algorithm is needed.\n\n   We summarize an approximate framework, which resembles the ideas proposed in past literatures [17, 2, 22], in Alg. 2. To summarize, the algorithm first proposes candidate splitting points according to percentiles of feature distribution (a specific criteria will be given in Sec. 3.3).\nThe algorithm then maps the continuous features into buckets split by these candidate points, aggregates the statistics and finds the best solution among proposals based on the aggregated statistics.\n\nThere are two variants of the algorithm, depending on\nwhen the proposal is given. The global variant proposes all\nthe candidate splits during the initial phase of tree construc-\ntion, and uses the same proposals for split finding at all lev-\nels. The local variant re-proposes after each split. The global\nmethod requires less proposal steps than the local method.\nHowever, usually more candidate points are needed for the\nglobal proposal because candidates are not refined after each\nsplit. The local proposal refines the candidates after splits,\nand can potentially be more appropriate for deeper trees. A\ncomparison of different algorithms on a Higgs boson dataset\nis given by Fig. 3. We find that the local proposal indeed\nrequires fewer candidates. The global proposal can be as\naccurate as the local one given enough candidates.\n\nMost existing approximate algorithms for distributed tree\nlearning also follow this framework. Notably, it is also possi-\nble to directly construct approximate histograms of gradient\nstatistics [22]. It is also possible to use other variants of bin-\nning strategies instead of quantile [17]. Quantile strategy\nbenefit from being distributable and recomputable, which\nwe will detail in next subsection. From Fig. 3, we also find\nthat the quantile strategy can get the same accuracy as exact\ngreedy given reasonable approximation level.\n\nOur system efficiently supports exact greedy for the single\nmachine setting, as well as approximate algorithm with both\nlocal and global proposal methods for all settings. Users can\nfreely choose between the methods according to their needs."
        },
        {
            "text": "There are two variants of the algorithm, depending on\nwhen the proposal is given. The global variant proposes all\nthe candidate splits during the initial phase of tree construc-\ntion, and uses the same proposals for split finding at all lev-\nels. The local variant re-proposes after each split. The global\nmethod requires less proposal steps than the local method.\nHowever, usually more candidate points are needed for the\nglobal proposal because candidates are not refined after each\nsplit. The local proposal refines the candidates after splits,\nand can potentially be more appropriate for deeper trees. A\ncomparison of different algorithms on a Higgs boson dataset\nis given by Fig. 3. We find that the local proposal indeed\nrequires fewer candidates. The global proposal can be as\naccurate as the local one given enough candidates.",
            "page": 4,
            "x": 50,
            "y": 392,
            "width": 245,
            "height": 145,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "8-approximate",
            "chunk_id": "38e1effb-d6bb-41bf-bf01-14d6b7cc0f7d",
            "group_text": "3.2  Approximate Algorithm\n\n    The exact greedy algorithm is very powerful since it enumerates over all possible splitting points greedily. However, it is impossible to efficiently do so when the data does not fit entirely into memory. Same problem also arises in the dis-\n\n4https://www.salford-systems.com/products/treenet\n\ntributed setting. To support effective gradient tree boosting\nin these two settings, an approximate algorithm is needed.\n\n   We summarize an approximate framework, which resembles the ideas proposed in past literatures [17, 2, 22], in Alg. 2. To summarize, the algorithm first proposes candidate splitting points according to percentiles of feature distribution (a specific criteria will be given in Sec. 3.3).\nThe algorithm then maps the continuous features into buckets split by these candidate points, aggregates the statistics and finds the best solution among proposals based on the aggregated statistics.\n\nThere are two variants of the algorithm, depending on\nwhen the proposal is given. The global variant proposes all\nthe candidate splits during the initial phase of tree construc-\ntion, and uses the same proposals for split finding at all lev-\nels. The local variant re-proposes after each split. The global\nmethod requires less proposal steps than the local method.\nHowever, usually more candidate points are needed for the\nglobal proposal because candidates are not refined after each\nsplit. The local proposal refines the candidates after splits,\nand can potentially be more appropriate for deeper trees. A\ncomparison of different algorithms on a Higgs boson dataset\nis given by Fig. 3. We find that the local proposal indeed\nrequires fewer candidates. The global proposal can be as\naccurate as the local one given enough candidates.\n\nMost existing approximate algorithms for distributed tree\nlearning also follow this framework. Notably, it is also possi-\nble to directly construct approximate histograms of gradient\nstatistics [22]. It is also possible to use other variants of bin-\nning strategies instead of quantile [17]. Quantile strategy\nbenefit from being distributable and recomputable, which\nwe will detail in next subsection. From Fig. 3, we also find\nthat the quantile strategy can get the same accuracy as exact\ngreedy given reasonable approximation level.\n\nOur system efficiently supports exact greedy for the single\nmachine setting, as well as approximate algorithm with both\nlocal and global proposal methods for all settings. Users can\nfreely choose between the methods according to their needs."
        },
        {
            "text": "Most existing approximate algorithms for distributed tree\nlearning also follow this framework. Notably, it is also possi-\nble to directly construct approximate histograms of gradient\nstatistics [22]. It is also possible to use other variants of bin-\nning strategies instead of quantile [17]. Quantile strategy\nbenefit from being distributable and recomputable, which\nwe will detail in next subsection. From Fig. 3, we also find\nthat the quantile strategy can get the same accuracy as exact\ngreedy given reasonable approximation level.",
            "page": 4,
            "x": 51,
            "y": 538,
            "width": 243,
            "height": 94,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "8-approximate",
            "chunk_id": "12289929-318e-4f4e-81f0-d93c301cce0a",
            "group_text": "3.2  Approximate Algorithm\n\n    The exact greedy algorithm is very powerful since it enumerates over all possible splitting points greedily. However, it is impossible to efficiently do so when the data does not fit entirely into memory. Same problem also arises in the dis-\n\n4https://www.salford-systems.com/products/treenet\n\ntributed setting. To support effective gradient tree boosting\nin these two settings, an approximate algorithm is needed.\n\n   We summarize an approximate framework, which resembles the ideas proposed in past literatures [17, 2, 22], in Alg. 2. To summarize, the algorithm first proposes candidate splitting points according to percentiles of feature distribution (a specific criteria will be given in Sec. 3.3).\nThe algorithm then maps the continuous features into buckets split by these candidate points, aggregates the statistics and finds the best solution among proposals based on the aggregated statistics.\n\nThere are two variants of the algorithm, depending on\nwhen the proposal is given. The global variant proposes all\nthe candidate splits during the initial phase of tree construc-\ntion, and uses the same proposals for split finding at all lev-\nels. The local variant re-proposes after each split. The global\nmethod requires less proposal steps than the local method.\nHowever, usually more candidate points are needed for the\nglobal proposal because candidates are not refined after each\nsplit. The local proposal refines the candidates after splits,\nand can potentially be more appropriate for deeper trees. A\ncomparison of different algorithms on a Higgs boson dataset\nis given by Fig. 3. We find that the local proposal indeed\nrequires fewer candidates. The global proposal can be as\naccurate as the local one given enough candidates.\n\nMost existing approximate algorithms for distributed tree\nlearning also follow this framework. Notably, it is also possi-\nble to directly construct approximate histograms of gradient\nstatistics [22]. It is also possible to use other variants of bin-\nning strategies instead of quantile [17]. Quantile strategy\nbenefit from being distributable and recomputable, which\nwe will detail in next subsection. From Fig. 3, we also find\nthat the quantile strategy can get the same accuracy as exact\ngreedy given reasonable approximation level.\n\nOur system efficiently supports exact greedy for the single\nmachine setting, as well as approximate algorithm with both\nlocal and global proposal methods for all settings. Users can\nfreely choose between the methods according to their needs."
        },
        {
            "text": "Our system efficiently supports exact greedy for the single\nmachine setting, as well as approximate algorithm with both\nlocal and global proposal methods for all settings. Users can\nfreely choose between the methods according to their needs.",
            "page": 4,
            "x": 51,
            "y": 632,
            "width": 243,
            "height": 42,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "8-approximate",
            "chunk_id": "5c58ffd7-1d7a-4410-8f02-ab32a5de3e29",
            "group_text": "3.2  Approximate Algorithm\n\n    The exact greedy algorithm is very powerful since it enumerates over all possible splitting points greedily. However, it is impossible to efficiently do so when the data does not fit entirely into memory. Same problem also arises in the dis-\n\n4https://www.salford-systems.com/products/treenet\n\ntributed setting. To support effective gradient tree boosting\nin these two settings, an approximate algorithm is needed.\n\n   We summarize an approximate framework, which resembles the ideas proposed in past literatures [17, 2, 22], in Alg. 2. To summarize, the algorithm first proposes candidate splitting points according to percentiles of feature distribution (a specific criteria will be given in Sec. 3.3).\nThe algorithm then maps the continuous features into buckets split by these candidate points, aggregates the statistics and finds the best solution among proposals based on the aggregated statistics.\n\nThere are two variants of the algorithm, depending on\nwhen the proposal is given. The global variant proposes all\nthe candidate splits during the initial phase of tree construc-\ntion, and uses the same proposals for split finding at all lev-\nels. The local variant re-proposes after each split. The global\nmethod requires less proposal steps than the local method.\nHowever, usually more candidate points are needed for the\nglobal proposal because candidates are not refined after each\nsplit. The local proposal refines the candidates after splits,\nand can potentially be more appropriate for deeper trees. A\ncomparison of different algorithms on a Higgs boson dataset\nis given by Fig. 3. We find that the local proposal indeed\nrequires fewer candidates. The global proposal can be as\naccurate as the local one given enough candidates.\n\nMost existing approximate algorithms for distributed tree\nlearning also follow this framework. Notably, it is also possi-\nble to directly construct approximate histograms of gradient\nstatistics [22]. It is also possible to use other variants of bin-\nning strategies instead of quantile [17]. Quantile strategy\nbenefit from being distributable and recomputable, which\nwe will detail in next subsection. From Fig. 3, we also find\nthat the quantile strategy can get the same accuracy as exact\ngreedy given reasonable approximation level.\n\nOur system efficiently supports exact greedy for the single\nmachine setting, as well as approximate algorithm with both\nlocal and global proposal methods for all settings. Users can\nfreely choose between the methods according to their needs."
        },
        {
            "text": "3.3  Weighted Quantile Sketch  \n    One important step in the approximate algorithm is to  \npropose candidate split points. Usually percentiles of a fea-",
            "page": 4,
            "x": 51,
            "y": 682,
            "width": 243,
            "height": 39,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-weighted",
            "chunk_id": "69cc1985-dc8a-437d-a263-aa76aba01a47",
            "group_text": "3.3  Weighted Quantile Sketch  \n    One important step in the approximate algorithm is to  \npropose candidate split points. Usually percentiles of a fea-\n\nFigure 4: Tree structure with default directions. An example will be classified into the default direction when the feature needed for the split is missing.\n\nture are used to make candidates distribute evenly on the\ndata. Formally, let multi-set $\\mathcal{D}_k = \\{(x_{1k}, h_1), (x_{2k}, h_2) \\cdots (x_{nk}, h_n)\\}$\nrepresent the $k$-th feature values and second order gradient\nstatistics of each training instances. We can define a rank\nfunctions $r_k : \\mathbb{R} \\rightarrow [0, +\\infty)$ as\n\n$r_k(z) = \\frac{1}{\\sum_{(x,h)\\in\\mathcal{D}_k} h} \\sum_{(x,h)\\in\\mathcal{D}_k,\\,x<z} h,$\n\nwhich represents the proportion of instances whose feature\nvalue $k$ is smaller than $z$. The goal is to find candidate split\npoints $\\{s_{k1}, s_{k2}, \\cdots s_{kl}\\}$, such that\n\n$|r_k(s_{k,j}) - r_k(s_{k,j+1})| < \\epsilon,\\quad s_{k1} = \\min_i \\mathbf{x}_{ik}, s_{kl} = \\max_i \\mathbf{x}_{ik}.$\n\nHere $\\epsilon$ is an approximation factor. Intuitively, this means\nthat there is roughly $1/\\epsilon$ candidate points. Here each data\npoint is weighted by $h_i$. To see why $h_i$ represents the weight,\nwe can rewrite Eq (3) as\n\n$\\sum_{i=1}^{n} \\frac{1}{2} h_i (f_t(\\mathbf{x}_i) - g_i / h_i)^2 + \\Omega(f_t) + constant,$\n\nwhich is exactly weighted squared loss with labels $g_{i}/h_{i}$\nand weights $h_{i}$. For large datasets, it is non-trivial to find\ncandidate splits that satisfy the criteria. When every in-\nstance has equal weights, an existing algorithm called quan-\ntile sketch [14, 24] solves the problem. However, there is no\nexisting quantile sketch for the weighted datasets. There-\nfore, most existing approximate algorithms either resorted\nto sorting on a random subset of data which have a chance of\nfailure or heuristics that do not have theoretical guarantee.\n\nTo solve this problem, we introduced a novel distributed\nweighted quantile sketch algorithm that can handle weighted\ndata with a _provable theoretical guarantee_. The general idea\nis to propose a data structure that supports _merge_ and _prune_\noperations, with each operation proven to maintain a certain\naccuracy level. A detailed description of the algorithm as\nwell as proofs are given in the appendix."
        },
        {
            "text": "Figure 4: Tree structure with default directions. An example will be classified into the default direction when the feature needed for the split is missing.",
            "page": 4,
            "x": 313,
            "y": 137,
            "width": 247,
            "height": 36,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-weighted",
            "chunk_id": "350fd743-2528-4807-a5cd-cb755a8ca58e",
            "group_text": "3.3  Weighted Quantile Sketch  \n    One important step in the approximate algorithm is to  \npropose candidate split points. Usually percentiles of a fea-\n\nFigure 4: Tree structure with default directions. An example will be classified into the default direction when the feature needed for the split is missing.\n\nture are used to make candidates distribute evenly on the\ndata. Formally, let multi-set $\\mathcal{D}_k = \\{(x_{1k}, h_1), (x_{2k}, h_2) \\cdots (x_{nk}, h_n)\\}$\nrepresent the $k$-th feature values and second order gradient\nstatistics of each training instances. We can define a rank\nfunctions $r_k : \\mathbb{R} \\rightarrow [0, +\\infty)$ as\n\n$r_k(z) = \\frac{1}{\\sum_{(x,h)\\in\\mathcal{D}_k} h} \\sum_{(x,h)\\in\\mathcal{D}_k,\\,x<z} h,$\n\nwhich represents the proportion of instances whose feature\nvalue $k$ is smaller than $z$. The goal is to find candidate split\npoints $\\{s_{k1}, s_{k2}, \\cdots s_{kl}\\}$, such that\n\n$|r_k(s_{k,j}) - r_k(s_{k,j+1})| < \\epsilon,\\quad s_{k1} = \\min_i \\mathbf{x}_{ik}, s_{kl} = \\max_i \\mathbf{x}_{ik}.$\n\nHere $\\epsilon$ is an approximation factor. Intuitively, this means\nthat there is roughly $1/\\epsilon$ candidate points. Here each data\npoint is weighted by $h_i$. To see why $h_i$ represents the weight,\nwe can rewrite Eq (3) as\n\n$\\sum_{i=1}^{n} \\frac{1}{2} h_i (f_t(\\mathbf{x}_i) - g_i / h_i)^2 + \\Omega(f_t) + constant,$\n\nwhich is exactly weighted squared loss with labels $g_{i}/h_{i}$\nand weights $h_{i}$. For large datasets, it is non-trivial to find\ncandidate splits that satisfy the criteria. When every in-\nstance has equal weights, an existing algorithm called quan-\ntile sketch [14, 24] solves the problem. However, there is no\nexisting quantile sketch for the weighted datasets. There-\nfore, most existing approximate algorithms either resorted\nto sorting on a random subset of data which have a chance of\nfailure or heuristics that do not have theoretical guarantee.\n\nTo solve this problem, we introduced a novel distributed\nweighted quantile sketch algorithm that can handle weighted\ndata with a _provable theoretical guarantee_. The general idea\nis to propose a data structure that supports _merge_ and _prune_\noperations, with each operation proven to maintain a certain\naccuracy level. A detailed description of the algorithm as\nwell as proofs are given in the appendix."
        },
        {
            "text": "ture are used to make candidates distribute evenly on the\ndata. Formally, let multi-set $\\mathcal{D}_k = \\{(x_{1k}, h_1), (x_{2k}, h_2) \\cdots (x_{nk}, h_n)\\}$\nrepresent the $k$-th feature values and second order gradient\nstatistics of each training instances. We can define a rank\nfunctions $r_k : \\mathbb{R} \\rightarrow [0, +\\infty)$ as",
            "page": 4,
            "x": 313,
            "y": 189,
            "width": 274,
            "height": 54,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-weighted",
            "chunk_id": "34b7a1ff-dd45-4ed3-a049-fe90846e5fac",
            "group_text": "3.3  Weighted Quantile Sketch  \n    One important step in the approximate algorithm is to  \npropose candidate split points. Usually percentiles of a fea-\n\nFigure 4: Tree structure with default directions. An example will be classified into the default direction when the feature needed for the split is missing.\n\nture are used to make candidates distribute evenly on the\ndata. Formally, let multi-set $\\mathcal{D}_k = \\{(x_{1k}, h_1), (x_{2k}, h_2) \\cdots (x_{nk}, h_n)\\}$\nrepresent the $k$-th feature values and second order gradient\nstatistics of each training instances. We can define a rank\nfunctions $r_k : \\mathbb{R} \\rightarrow [0, +\\infty)$ as\n\n$r_k(z) = \\frac{1}{\\sum_{(x,h)\\in\\mathcal{D}_k} h} \\sum_{(x,h)\\in\\mathcal{D}_k,\\,x<z} h,$\n\nwhich represents the proportion of instances whose feature\nvalue $k$ is smaller than $z$. The goal is to find candidate split\npoints $\\{s_{k1}, s_{k2}, \\cdots s_{kl}\\}$, such that\n\n$|r_k(s_{k,j}) - r_k(s_{k,j+1})| < \\epsilon,\\quad s_{k1} = \\min_i \\mathbf{x}_{ik}, s_{kl} = \\max_i \\mathbf{x}_{ik}.$\n\nHere $\\epsilon$ is an approximation factor. Intuitively, this means\nthat there is roughly $1/\\epsilon$ candidate points. Here each data\npoint is weighted by $h_i$. To see why $h_i$ represents the weight,\nwe can rewrite Eq (3) as\n\n$\\sum_{i=1}^{n} \\frac{1}{2} h_i (f_t(\\mathbf{x}_i) - g_i / h_i)^2 + \\Omega(f_t) + constant,$\n\nwhich is exactly weighted squared loss with labels $g_{i}/h_{i}$\nand weights $h_{i}$. For large datasets, it is non-trivial to find\ncandidate splits that satisfy the criteria. When every in-\nstance has equal weights, an existing algorithm called quan-\ntile sketch [14, 24] solves the problem. However, there is no\nexisting quantile sketch for the weighted datasets. There-\nfore, most existing approximate algorithms either resorted\nto sorting on a random subset of data which have a chance of\nfailure or heuristics that do not have theoretical guarantee.\n\nTo solve this problem, we introduced a novel distributed\nweighted quantile sketch algorithm that can handle weighted\ndata with a _provable theoretical guarantee_. The general idea\nis to propose a data structure that supports _merge_ and _prune_\noperations, with each operation proven to maintain a certain\naccuracy level. A detailed description of the algorithm as\nwell as proofs are given in the appendix."
        },
        {
            "text": "$r_k(z) = \\frac{1}{\\sum_{(x,h)\\in\\mathcal{D}_k} h} \\sum_{(x,h)\\in\\mathcal{D}_k,\\,x<z} h,$",
            "page": 4,
            "x": 361,
            "y": 246,
            "width": 197,
            "height": 29,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-weighted",
            "chunk_id": "ebda3eb3-b483-4035-bdb0-f88f2facbfdb",
            "group_text": "3.3  Weighted Quantile Sketch  \n    One important step in the approximate algorithm is to  \npropose candidate split points. Usually percentiles of a fea-\n\nFigure 4: Tree structure with default directions. An example will be classified into the default direction when the feature needed for the split is missing.\n\nture are used to make candidates distribute evenly on the\ndata. Formally, let multi-set $\\mathcal{D}_k = \\{(x_{1k}, h_1), (x_{2k}, h_2) \\cdots (x_{nk}, h_n)\\}$\nrepresent the $k$-th feature values and second order gradient\nstatistics of each training instances. We can define a rank\nfunctions $r_k : \\mathbb{R} \\rightarrow [0, +\\infty)$ as\n\n$r_k(z) = \\frac{1}{\\sum_{(x,h)\\in\\mathcal{D}_k} h} \\sum_{(x,h)\\in\\mathcal{D}_k,\\,x<z} h,$\n\nwhich represents the proportion of instances whose feature\nvalue $k$ is smaller than $z$. The goal is to find candidate split\npoints $\\{s_{k1}, s_{k2}, \\cdots s_{kl}\\}$, such that\n\n$|r_k(s_{k,j}) - r_k(s_{k,j+1})| < \\epsilon,\\quad s_{k1} = \\min_i \\mathbf{x}_{ik}, s_{kl} = \\max_i \\mathbf{x}_{ik}.$\n\nHere $\\epsilon$ is an approximation factor. Intuitively, this means\nthat there is roughly $1/\\epsilon$ candidate points. Here each data\npoint is weighted by $h_i$. To see why $h_i$ represents the weight,\nwe can rewrite Eq (3) as\n\n$\\sum_{i=1}^{n} \\frac{1}{2} h_i (f_t(\\mathbf{x}_i) - g_i / h_i)^2 + \\Omega(f_t) + constant,$\n\nwhich is exactly weighted squared loss with labels $g_{i}/h_{i}$\nand weights $h_{i}$. For large datasets, it is non-trivial to find\ncandidate splits that satisfy the criteria. When every in-\nstance has equal weights, an existing algorithm called quan-\ntile sketch [14, 24] solves the problem. However, there is no\nexisting quantile sketch for the weighted datasets. There-\nfore, most existing approximate algorithms either resorted\nto sorting on a random subset of data which have a chance of\nfailure or heuristics that do not have theoretical guarantee.\n\nTo solve this problem, we introduced a novel distributed\nweighted quantile sketch algorithm that can handle weighted\ndata with a _provable theoretical guarantee_. The general idea\nis to propose a data structure that supports _merge_ and _prune_\noperations, with each operation proven to maintain a certain\naccuracy level. A detailed description of the algorithm as\nwell as proofs are given in the appendix."
        },
        {
            "text": "which represents the proportion of instances whose feature\nvalue $k$ is smaller than $z$. The goal is to find candidate split\npoints $\\{s_{k1}, s_{k2}, \\cdots s_{kl}\\}$, such that",
            "page": 4,
            "x": 313,
            "y": 280,
            "width": 244,
            "height": 33,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-weighted",
            "chunk_id": "da025f80-2c22-447c-b9e7-ab3313828547",
            "group_text": "3.3  Weighted Quantile Sketch  \n    One important step in the approximate algorithm is to  \npropose candidate split points. Usually percentiles of a fea-\n\nFigure 4: Tree structure with default directions. An example will be classified into the default direction when the feature needed for the split is missing.\n\nture are used to make candidates distribute evenly on the\ndata. Formally, let multi-set $\\mathcal{D}_k = \\{(x_{1k}, h_1), (x_{2k}, h_2) \\cdots (x_{nk}, h_n)\\}$\nrepresent the $k$-th feature values and second order gradient\nstatistics of each training instances. We can define a rank\nfunctions $r_k : \\mathbb{R} \\rightarrow [0, +\\infty)$ as\n\n$r_k(z) = \\frac{1}{\\sum_{(x,h)\\in\\mathcal{D}_k} h} \\sum_{(x,h)\\in\\mathcal{D}_k,\\,x<z} h,$\n\nwhich represents the proportion of instances whose feature\nvalue $k$ is smaller than $z$. The goal is to find candidate split\npoints $\\{s_{k1}, s_{k2}, \\cdots s_{kl}\\}$, such that\n\n$|r_k(s_{k,j}) - r_k(s_{k,j+1})| < \\epsilon,\\quad s_{k1} = \\min_i \\mathbf{x}_{ik}, s_{kl} = \\max_i \\mathbf{x}_{ik}.$\n\nHere $\\epsilon$ is an approximation factor. Intuitively, this means\nthat there is roughly $1/\\epsilon$ candidate points. Here each data\npoint is weighted by $h_i$. To see why $h_i$ represents the weight,\nwe can rewrite Eq (3) as\n\n$\\sum_{i=1}^{n} \\frac{1}{2} h_i (f_t(\\mathbf{x}_i) - g_i / h_i)^2 + \\Omega(f_t) + constant,$\n\nwhich is exactly weighted squared loss with labels $g_{i}/h_{i}$\nand weights $h_{i}$. For large datasets, it is non-trivial to find\ncandidate splits that satisfy the criteria. When every in-\nstance has equal weights, an existing algorithm called quan-\ntile sketch [14, 24] solves the problem. However, there is no\nexisting quantile sketch for the weighted datasets. There-\nfore, most existing approximate algorithms either resorted\nto sorting on a random subset of data which have a chance of\nfailure or heuristics that do not have theoretical guarantee.\n\nTo solve this problem, we introduced a novel distributed\nweighted quantile sketch algorithm that can handle weighted\ndata with a _provable theoretical guarantee_. The general idea\nis to propose a data structure that supports _merge_ and _prune_\noperations, with each operation proven to maintain a certain\naccuracy level. A detailed description of the algorithm as\nwell as proofs are given in the appendix."
        },
        {
            "text": "$|r_k(s_{k,j}) - r_k(s_{k,j+1})| < \\epsilon,\\quad s_{k1} = \\min_i \\mathbf{x}_{ik}, s_{kl} = \\max_i \\mathbf{x}_{ik}.$",
            "page": 4,
            "x": 318,
            "y": 317,
            "width": 238,
            "height": 21,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-weighted",
            "chunk_id": "b1420cae-b13c-4ee6-99cf-3e76630410e2",
            "group_text": "3.3  Weighted Quantile Sketch  \n    One important step in the approximate algorithm is to  \npropose candidate split points. Usually percentiles of a fea-\n\nFigure 4: Tree structure with default directions. An example will be classified into the default direction when the feature needed for the split is missing.\n\nture are used to make candidates distribute evenly on the\ndata. Formally, let multi-set $\\mathcal{D}_k = \\{(x_{1k}, h_1), (x_{2k}, h_2) \\cdots (x_{nk}, h_n)\\}$\nrepresent the $k$-th feature values and second order gradient\nstatistics of each training instances. We can define a rank\nfunctions $r_k : \\mathbb{R} \\rightarrow [0, +\\infty)$ as\n\n$r_k(z) = \\frac{1}{\\sum_{(x,h)\\in\\mathcal{D}_k} h} \\sum_{(x,h)\\in\\mathcal{D}_k,\\,x<z} h,$\n\nwhich represents the proportion of instances whose feature\nvalue $k$ is smaller than $z$. The goal is to find candidate split\npoints $\\{s_{k1}, s_{k2}, \\cdots s_{kl}\\}$, such that\n\n$|r_k(s_{k,j}) - r_k(s_{k,j+1})| < \\epsilon,\\quad s_{k1} = \\min_i \\mathbf{x}_{ik}, s_{kl} = \\max_i \\mathbf{x}_{ik}.$\n\nHere $\\epsilon$ is an approximation factor. Intuitively, this means\nthat there is roughly $1/\\epsilon$ candidate points. Here each data\npoint is weighted by $h_i$. To see why $h_i$ represents the weight,\nwe can rewrite Eq (3) as\n\n$\\sum_{i=1}^{n} \\frac{1}{2} h_i (f_t(\\mathbf{x}_i) - g_i / h_i)^2 + \\Omega(f_t) + constant,$\n\nwhich is exactly weighted squared loss with labels $g_{i}/h_{i}$\nand weights $h_{i}$. For large datasets, it is non-trivial to find\ncandidate splits that satisfy the criteria. When every in-\nstance has equal weights, an existing algorithm called quan-\ntile sketch [14, 24] solves the problem. However, there is no\nexisting quantile sketch for the weighted datasets. There-\nfore, most existing approximate algorithms either resorted\nto sorting on a random subset of data which have a chance of\nfailure or heuristics that do not have theoretical guarantee.\n\nTo solve this problem, we introduced a novel distributed\nweighted quantile sketch algorithm that can handle weighted\ndata with a _provable theoretical guarantee_. The general idea\nis to propose a data structure that supports _merge_ and _prune_\noperations, with each operation proven to maintain a certain\naccuracy level. A detailed description of the algorithm as\nwell as proofs are given in the appendix."
        },
        {
            "text": "Here $\\epsilon$ is an approximation factor. Intuitively, this means\nthat there is roughly $1/\\epsilon$ candidate points. Here each data\npoint is weighted by $h_i$. To see why $h_i$ represents the weight,\nwe can rewrite Eq (3) as",
            "page": 4,
            "x": 314,
            "y": 340,
            "width": 243,
            "height": 46,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-weighted",
            "chunk_id": "4bf51880-6871-470f-88e1-5c793d5f6c19",
            "group_text": "3.3  Weighted Quantile Sketch  \n    One important step in the approximate algorithm is to  \npropose candidate split points. Usually percentiles of a fea-\n\nFigure 4: Tree structure with default directions. An example will be classified into the default direction when the feature needed for the split is missing.\n\nture are used to make candidates distribute evenly on the\ndata. Formally, let multi-set $\\mathcal{D}_k = \\{(x_{1k}, h_1), (x_{2k}, h_2) \\cdots (x_{nk}, h_n)\\}$\nrepresent the $k$-th feature values and second order gradient\nstatistics of each training instances. We can define a rank\nfunctions $r_k : \\mathbb{R} \\rightarrow [0, +\\infty)$ as\n\n$r_k(z) = \\frac{1}{\\sum_{(x,h)\\in\\mathcal{D}_k} h} \\sum_{(x,h)\\in\\mathcal{D}_k,\\,x<z} h,$\n\nwhich represents the proportion of instances whose feature\nvalue $k$ is smaller than $z$. The goal is to find candidate split\npoints $\\{s_{k1}, s_{k2}, \\cdots s_{kl}\\}$, such that\n\n$|r_k(s_{k,j}) - r_k(s_{k,j+1})| < \\epsilon,\\quad s_{k1} = \\min_i \\mathbf{x}_{ik}, s_{kl} = \\max_i \\mathbf{x}_{ik}.$\n\nHere $\\epsilon$ is an approximation factor. Intuitively, this means\nthat there is roughly $1/\\epsilon$ candidate points. Here each data\npoint is weighted by $h_i$. To see why $h_i$ represents the weight,\nwe can rewrite Eq (3) as\n\n$\\sum_{i=1}^{n} \\frac{1}{2} h_i (f_t(\\mathbf{x}_i) - g_i / h_i)^2 + \\Omega(f_t) + constant,$\n\nwhich is exactly weighted squared loss with labels $g_{i}/h_{i}$\nand weights $h_{i}$. For large datasets, it is non-trivial to find\ncandidate splits that satisfy the criteria. When every in-\nstance has equal weights, an existing algorithm called quan-\ntile sketch [14, 24] solves the problem. However, there is no\nexisting quantile sketch for the weighted datasets. There-\nfore, most existing approximate algorithms either resorted\nto sorting on a random subset of data which have a chance of\nfailure or heuristics that do not have theoretical guarantee.\n\nTo solve this problem, we introduced a novel distributed\nweighted quantile sketch algorithm that can handle weighted\ndata with a _provable theoretical guarantee_. The general idea\nis to propose a data structure that supports _merge_ and _prune_\noperations, with each operation proven to maintain a certain\naccuracy level. A detailed description of the algorithm as\nwell as proofs are given in the appendix."
        },
        {
            "text": "$\\sum_{i=1}^{n} \\frac{1}{2} h_i (f_t(\\mathbf{x}_i) - g_i / h_i)^2 + \\Omega(f_t) + constant,$",
            "page": 4,
            "x": 343,
            "y": 391,
            "width": 184,
            "height": 29,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-weighted",
            "chunk_id": "7cd33bd9-5c01-45d2-9685-06e39c419b9c",
            "group_text": "3.3  Weighted Quantile Sketch  \n    One important step in the approximate algorithm is to  \npropose candidate split points. Usually percentiles of a fea-\n\nFigure 4: Tree structure with default directions. An example will be classified into the default direction when the feature needed for the split is missing.\n\nture are used to make candidates distribute evenly on the\ndata. Formally, let multi-set $\\mathcal{D}_k = \\{(x_{1k}, h_1), (x_{2k}, h_2) \\cdots (x_{nk}, h_n)\\}$\nrepresent the $k$-th feature values and second order gradient\nstatistics of each training instances. We can define a rank\nfunctions $r_k : \\mathbb{R} \\rightarrow [0, +\\infty)$ as\n\n$r_k(z) = \\frac{1}{\\sum_{(x,h)\\in\\mathcal{D}_k} h} \\sum_{(x,h)\\in\\mathcal{D}_k,\\,x<z} h,$\n\nwhich represents the proportion of instances whose feature\nvalue $k$ is smaller than $z$. The goal is to find candidate split\npoints $\\{s_{k1}, s_{k2}, \\cdots s_{kl}\\}$, such that\n\n$|r_k(s_{k,j}) - r_k(s_{k,j+1})| < \\epsilon,\\quad s_{k1} = \\min_i \\mathbf{x}_{ik}, s_{kl} = \\max_i \\mathbf{x}_{ik}.$\n\nHere $\\epsilon$ is an approximation factor. Intuitively, this means\nthat there is roughly $1/\\epsilon$ candidate points. Here each data\npoint is weighted by $h_i$. To see why $h_i$ represents the weight,\nwe can rewrite Eq (3) as\n\n$\\sum_{i=1}^{n} \\frac{1}{2} h_i (f_t(\\mathbf{x}_i) - g_i / h_i)^2 + \\Omega(f_t) + constant,$\n\nwhich is exactly weighted squared loss with labels $g_{i}/h_{i}$\nand weights $h_{i}$. For large datasets, it is non-trivial to find\ncandidate splits that satisfy the criteria. When every in-\nstance has equal weights, an existing algorithm called quan-\ntile sketch [14, 24] solves the problem. However, there is no\nexisting quantile sketch for the weighted datasets. There-\nfore, most existing approximate algorithms either resorted\nto sorting on a random subset of data which have a chance of\nfailure or heuristics that do not have theoretical guarantee.\n\nTo solve this problem, we introduced a novel distributed\nweighted quantile sketch algorithm that can handle weighted\ndata with a _provable theoretical guarantee_. The general idea\nis to propose a data structure that supports _merge_ and _prune_\noperations, with each operation proven to maintain a certain\naccuracy level. A detailed description of the algorithm as\nwell as proofs are given in the appendix."
        },
        {
            "text": "which is exactly weighted squared loss with labels $g_{i}/h_{i}$\nand weights $h_{i}$. For large datasets, it is non-trivial to find\ncandidate splits that satisfy the criteria. When every in-\nstance has equal weights, an existing algorithm called quan-\ntile sketch [14, 24] solves the problem. However, there is no\nexisting quantile sketch for the weighted datasets. There-\nfore, most existing approximate algorithms either resorted\nto sorting on a random subset of data which have a chance of\nfailure or heuristics that do not have theoretical guarantee.",
            "page": 4,
            "x": 314,
            "y": 425,
            "width": 243,
            "height": 94,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-weighted",
            "chunk_id": "59b060de-501a-4af6-977e-b2396f95d524",
            "group_text": "3.3  Weighted Quantile Sketch  \n    One important step in the approximate algorithm is to  \npropose candidate split points. Usually percentiles of a fea-\n\nFigure 4: Tree structure with default directions. An example will be classified into the default direction when the feature needed for the split is missing.\n\nture are used to make candidates distribute evenly on the\ndata. Formally, let multi-set $\\mathcal{D}_k = \\{(x_{1k}, h_1), (x_{2k}, h_2) \\cdots (x_{nk}, h_n)\\}$\nrepresent the $k$-th feature values and second order gradient\nstatistics of each training instances. We can define a rank\nfunctions $r_k : \\mathbb{R} \\rightarrow [0, +\\infty)$ as\n\n$r_k(z) = \\frac{1}{\\sum_{(x,h)\\in\\mathcal{D}_k} h} \\sum_{(x,h)\\in\\mathcal{D}_k,\\,x<z} h,$\n\nwhich represents the proportion of instances whose feature\nvalue $k$ is smaller than $z$. The goal is to find candidate split\npoints $\\{s_{k1}, s_{k2}, \\cdots s_{kl}\\}$, such that\n\n$|r_k(s_{k,j}) - r_k(s_{k,j+1})| < \\epsilon,\\quad s_{k1} = \\min_i \\mathbf{x}_{ik}, s_{kl} = \\max_i \\mathbf{x}_{ik}.$\n\nHere $\\epsilon$ is an approximation factor. Intuitively, this means\nthat there is roughly $1/\\epsilon$ candidate points. Here each data\npoint is weighted by $h_i$. To see why $h_i$ represents the weight,\nwe can rewrite Eq (3) as\n\n$\\sum_{i=1}^{n} \\frac{1}{2} h_i (f_t(\\mathbf{x}_i) - g_i / h_i)^2 + \\Omega(f_t) + constant,$\n\nwhich is exactly weighted squared loss with labels $g_{i}/h_{i}$\nand weights $h_{i}$. For large datasets, it is non-trivial to find\ncandidate splits that satisfy the criteria. When every in-\nstance has equal weights, an existing algorithm called quan-\ntile sketch [14, 24] solves the problem. However, there is no\nexisting quantile sketch for the weighted datasets. There-\nfore, most existing approximate algorithms either resorted\nto sorting on a random subset of data which have a chance of\nfailure or heuristics that do not have theoretical guarantee.\n\nTo solve this problem, we introduced a novel distributed\nweighted quantile sketch algorithm that can handle weighted\ndata with a _provable theoretical guarantee_. The general idea\nis to propose a data structure that supports _merge_ and _prune_\noperations, with each operation proven to maintain a certain\naccuracy level. A detailed description of the algorithm as\nwell as proofs are given in the appendix."
        },
        {
            "text": "To solve this problem, we introduced a novel distributed\nweighted quantile sketch algorithm that can handle weighted\ndata with a _provable theoretical guarantee_. The general idea\nis to propose a data structure that supports _merge_ and _prune_\noperations, with each operation proven to maintain a certain\naccuracy level. A detailed description of the algorithm as\nwell as proofs are given in the appendix.",
            "page": 4,
            "x": 314,
            "y": 520,
            "width": 243,
            "height": 74,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-weighted",
            "chunk_id": "58510679-47d0-421b-97bd-72a29deb371d",
            "group_text": "3.3  Weighted Quantile Sketch  \n    One important step in the approximate algorithm is to  \npropose candidate split points. Usually percentiles of a fea-\n\nFigure 4: Tree structure with default directions. An example will be classified into the default direction when the feature needed for the split is missing.\n\nture are used to make candidates distribute evenly on the\ndata. Formally, let multi-set $\\mathcal{D}_k = \\{(x_{1k}, h_1), (x_{2k}, h_2) \\cdots (x_{nk}, h_n)\\}$\nrepresent the $k$-th feature values and second order gradient\nstatistics of each training instances. We can define a rank\nfunctions $r_k : \\mathbb{R} \\rightarrow [0, +\\infty)$ as\n\n$r_k(z) = \\frac{1}{\\sum_{(x,h)\\in\\mathcal{D}_k} h} \\sum_{(x,h)\\in\\mathcal{D}_k,\\,x<z} h,$\n\nwhich represents the proportion of instances whose feature\nvalue $k$ is smaller than $z$. The goal is to find candidate split\npoints $\\{s_{k1}, s_{k2}, \\cdots s_{kl}\\}$, such that\n\n$|r_k(s_{k,j}) - r_k(s_{k,j+1})| < \\epsilon,\\quad s_{k1} = \\min_i \\mathbf{x}_{ik}, s_{kl} = \\max_i \\mathbf{x}_{ik}.$\n\nHere $\\epsilon$ is an approximation factor. Intuitively, this means\nthat there is roughly $1/\\epsilon$ candidate points. Here each data\npoint is weighted by $h_i$. To see why $h_i$ represents the weight,\nwe can rewrite Eq (3) as\n\n$\\sum_{i=1}^{n} \\frac{1}{2} h_i (f_t(\\mathbf{x}_i) - g_i / h_i)^2 + \\Omega(f_t) + constant,$\n\nwhich is exactly weighted squared loss with labels $g_{i}/h_{i}$\nand weights $h_{i}$. For large datasets, it is non-trivial to find\ncandidate splits that satisfy the criteria. When every in-\nstance has equal weights, an existing algorithm called quan-\ntile sketch [14, 24] solves the problem. However, there is no\nexisting quantile sketch for the weighted datasets. There-\nfore, most existing approximate algorithms either resorted\nto sorting on a random subset of data which have a chance of\nfailure or heuristics that do not have theoretical guarantee.\n\nTo solve this problem, we introduced a novel distributed\nweighted quantile sketch algorithm that can handle weighted\ndata with a _provable theoretical guarantee_. The general idea\nis to propose a data structure that supports _merge_ and _prune_\noperations, with each operation proven to maintain a certain\naccuracy level. A detailed description of the algorithm as\nwell as proofs are given in the appendix."
        },
        {
            "text": "3.4  Sparsity-aware Split Finding\n\n    In many real-world problems, it is quite common for the input x to be sparse. There are multiple possible causes for sparsity: 1) presence of missing values in the data; 2) frequent zero entries in the statistics; and, 3) artifacts of feature engineering such as one-hot encoding. It is important to make the algorithm aware of the sparsity pattern in the data. In order to do so, we propose to add a default direction in each tree node, which is shown in Fig. 4. When a value is missing in the sparse matrix x, the instance is classified into the default direction. There are two choices",
            "page": 4,
            "x": 314,
            "y": 599,
            "width": 243,
            "height": 121,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "10-sparsity",
            "chunk_id": "f024bc52-02b9-44e1-8c55-df4287a1f4f0",
            "group_text": "3.4  Sparsity-aware Split Finding\n\n    In many real-world problems, it is quite common for the input x to be sparse. There are multiple possible causes for sparsity: 1) presence of missing values in the data; 2) frequent zero entries in the statistics; and, 3) artifacts of feature engineering such as one-hot encoding. It is important to make the algorithm aware of the sparsity pattern in the data. In order to do so, we propose to add a default direction in each tree node, which is shown in Fig. 4. When a value is missing in the sparse matrix x, the instance is classified into the default direction. There are two choices\n\nAlgorithm 3: Sparsity-aware Split Finding\n\nInput: $I$, instance set of current node  \nInput: $I_k = \\{i \\in I|x_{ik} \\neq \\text{missing}\\}$  \nInput: $d$, feature dimension  \nAlso applies to the approximate setting, only collect  \nstatistics of non-missing entries into buckets  \ngain $\\gets 0$  \n$G \\gets \\sum_{i \\in I} g_i; H \\gets \\sum_{i \\in I} h_i$  \nfor $k = 1$ to $m$ do  \n\u2003// enumerate missing value goto right  \n\u2003$G_L \\gets 0, H_L \\gets 0$  \n\u2003for $j$ in sorted($I_k$, ascent order by $x_{jk}$) do  \n\u2003\u2003$G_L \\gets G_L + g_j; H_L \\gets H_L + h_j$  \n\u2003\u2003$G_R \\gets G - G_L, H_R \\gets H - H_L$  \n\u2003\u2003score $\\gets \\max(\\text{score}, \\frac{G_L^2}{H_L+\\lambda} + \\frac{G_R^2}{H_R+\\lambda} - \\frac{G^2}{H+\\lambda})$  \n\u2003end  \n\u2003// enumerate missing value goto left  \n\u2003$G_R \\gets 0, H_R \\gets 0$  \n\u2003for $j$ in sorted($I_k$, descent order by $x_{jk}$) do  \n\u2003\u2003$G_R \\gets G_R + g_j, H_R \\gets H_R + h_j$  \n\u2003\u2003$G_L \\gets G - G_R, H_L \\gets H - H_R$  \n\u2003\u2003score $\\gets \\max(\\text{score}, \\frac{G_L^2}{H_L+\\lambda} + \\frac{G_R^2}{H_R+\\lambda} - \\frac{G^2}{H+\\lambda})$  \n\u2003end  \nend  \nOutput: Split and default directions with max gain\n\nof default direction in each branch. The optimal default di-\nrections are learnt from the data. The algorithm is shown in\nAlg. 3. The key improvement is to only visit the non-missing\nentries $I_k$. The presented algorithm treats the non-presence\nas a missing value and learns the best direction to handle\nmissing values. The same algorithm can also be applied\nwhen the non-presence corresponds to a user specified value\nby limiting the enumeration only to consistent solutions.\n\nTo the best of our knowledge, most existing tree learn-\ning algorithms are either only optimized for dense data, or\nneed specific procedures to handle limited cases such as cat-\negorical encoding. XGBoost handles all sparsity patterns in\na unified way. More importantly, our method exploits the\nsparsity to make computation complexity linear to number\nof non-missing entries in the input. Fig. 5 shows the com-\nparison of sparsity aware and a naive implementation on an\nAllstate-10K dataset (description of dataset given in Sec. 6).\nWe find that the sparsity aware algorithm runs 50 times\nfaster than the naive version. This confirms the importance\nof the sparsity aware algorithm."
        },
        {
            "text": "Algorithm 3: Sparsity-aware Split Finding\n\nInput: $I$, instance set of current node  \nInput: $I_k = \\{i \\in I|x_{ik} \\neq \\text{missing}\\}$  \nInput: $d$, feature dimension  \nAlso applies to the approximate setting, only collect  \nstatistics of non-missing entries into buckets  \ngain $\\gets 0$  \n$G \\gets \\sum_{i \\in I} g_i; H \\gets \\sum_{i \\in I} h_i$  \nfor $k = 1$ to $m$ do  \n\u2003// enumerate missing value goto right  \n\u2003$G_L \\gets 0, H_L \\gets 0$  \n\u2003for $j$ in sorted($I_k$, ascent order by $x_{jk}$) do  \n\u2003\u2003$G_L \\gets G_L + g_j; H_L \\gets H_L + h_j$  \n\u2003\u2003$G_R \\gets G - G_L, H_R \\gets H - H_L$  \n\u2003\u2003score $\\gets \\max(\\text{score}, \\frac{G_L^2}{H_L+\\lambda} + \\frac{G_R^2}{H_R+\\lambda} - \\frac{G^2}{H+\\lambda})$  \n\u2003end  \n\u2003// enumerate missing value goto left  \n\u2003$G_R \\gets 0, H_R \\gets 0$  \n\u2003for $j$ in sorted($I_k$, descent order by $x_{jk}$) do  \n\u2003\u2003$G_R \\gets G_R + g_j, H_R \\gets H_R + h_j$  \n\u2003\u2003$G_L \\gets G - G_R, H_L \\gets H - H_R$  \n\u2003\u2003score $\\gets \\max(\\text{score}, \\frac{G_L^2}{H_L+\\lambda} + \\frac{G_R^2}{H_R+\\lambda} - \\frac{G^2}{H+\\lambda})$  \n\u2003end  \nend  \nOutput: Split and default directions with max gain",
            "page": 5,
            "x": 51,
            "y": 192,
            "width": 242,
            "height": 275,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "10-sparsity",
            "chunk_id": "f648bd19-cbf9-4e08-a4e2-2587f766852e",
            "group_text": "3.4  Sparsity-aware Split Finding\n\n    In many real-world problems, it is quite common for the input x to be sparse. There are multiple possible causes for sparsity: 1) presence of missing values in the data; 2) frequent zero entries in the statistics; and, 3) artifacts of feature engineering such as one-hot encoding. It is important to make the algorithm aware of the sparsity pattern in the data. In order to do so, we propose to add a default direction in each tree node, which is shown in Fig. 4. When a value is missing in the sparse matrix x, the instance is classified into the default direction. There are two choices\n\nAlgorithm 3: Sparsity-aware Split Finding\n\nInput: $I$, instance set of current node  \nInput: $I_k = \\{i \\in I|x_{ik} \\neq \\text{missing}\\}$  \nInput: $d$, feature dimension  \nAlso applies to the approximate setting, only collect  \nstatistics of non-missing entries into buckets  \ngain $\\gets 0$  \n$G \\gets \\sum_{i \\in I} g_i; H \\gets \\sum_{i \\in I} h_i$  \nfor $k = 1$ to $m$ do  \n\u2003// enumerate missing value goto right  \n\u2003$G_L \\gets 0, H_L \\gets 0$  \n\u2003for $j$ in sorted($I_k$, ascent order by $x_{jk}$) do  \n\u2003\u2003$G_L \\gets G_L + g_j; H_L \\gets H_L + h_j$  \n\u2003\u2003$G_R \\gets G - G_L, H_R \\gets H - H_L$  \n\u2003\u2003score $\\gets \\max(\\text{score}, \\frac{G_L^2}{H_L+\\lambda} + \\frac{G_R^2}{H_R+\\lambda} - \\frac{G^2}{H+\\lambda})$  \n\u2003end  \n\u2003// enumerate missing value goto left  \n\u2003$G_R \\gets 0, H_R \\gets 0$  \n\u2003for $j$ in sorted($I_k$, descent order by $x_{jk}$) do  \n\u2003\u2003$G_R \\gets G_R + g_j, H_R \\gets H_R + h_j$  \n\u2003\u2003$G_L \\gets G - G_R, H_L \\gets H - H_R$  \n\u2003\u2003score $\\gets \\max(\\text{score}, \\frac{G_L^2}{H_L+\\lambda} + \\frac{G_R^2}{H_R+\\lambda} - \\frac{G^2}{H+\\lambda})$  \n\u2003end  \nend  \nOutput: Split and default directions with max gain\n\nof default direction in each branch. The optimal default di-\nrections are learnt from the data. The algorithm is shown in\nAlg. 3. The key improvement is to only visit the non-missing\nentries $I_k$. The presented algorithm treats the non-presence\nas a missing value and learns the best direction to handle\nmissing values. The same algorithm can also be applied\nwhen the non-presence corresponds to a user specified value\nby limiting the enumeration only to consistent solutions.\n\nTo the best of our knowledge, most existing tree learn-\ning algorithms are either only optimized for dense data, or\nneed specific procedures to handle limited cases such as cat-\negorical encoding. XGBoost handles all sparsity patterns in\na unified way. More importantly, our method exploits the\nsparsity to make computation complexity linear to number\nof non-missing entries in the input. Fig. 5 shows the com-\nparison of sparsity aware and a naive implementation on an\nAllstate-10K dataset (description of dataset given in Sec. 6).\nWe find that the sparsity aware algorithm runs 50 times\nfaster than the naive version. This confirms the importance\nof the sparsity aware algorithm."
        },
        {
            "text": "of default direction in each branch. The optimal default di-\nrections are learnt from the data. The algorithm is shown in\nAlg. 3. The key improvement is to only visit the non-missing\nentries $I_k$. The presented algorithm treats the non-presence\nas a missing value and learns the best direction to handle\nmissing values. The same algorithm can also be applied\nwhen the non-presence corresponds to a user specified value\nby limiting the enumeration only to consistent solutions.",
            "page": 5,
            "x": 50,
            "y": 507,
            "width": 245,
            "height": 84,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "10-sparsity",
            "chunk_id": "a5c8a251-4ea5-4bed-8fc6-ff73b1817f30",
            "group_text": "3.4  Sparsity-aware Split Finding\n\n    In many real-world problems, it is quite common for the input x to be sparse. There are multiple possible causes for sparsity: 1) presence of missing values in the data; 2) frequent zero entries in the statistics; and, 3) artifacts of feature engineering such as one-hot encoding. It is important to make the algorithm aware of the sparsity pattern in the data. In order to do so, we propose to add a default direction in each tree node, which is shown in Fig. 4. When a value is missing in the sparse matrix x, the instance is classified into the default direction. There are two choices\n\nAlgorithm 3: Sparsity-aware Split Finding\n\nInput: $I$, instance set of current node  \nInput: $I_k = \\{i \\in I|x_{ik} \\neq \\text{missing}\\}$  \nInput: $d$, feature dimension  \nAlso applies to the approximate setting, only collect  \nstatistics of non-missing entries into buckets  \ngain $\\gets 0$  \n$G \\gets \\sum_{i \\in I} g_i; H \\gets \\sum_{i \\in I} h_i$  \nfor $k = 1$ to $m$ do  \n\u2003// enumerate missing value goto right  \n\u2003$G_L \\gets 0, H_L \\gets 0$  \n\u2003for $j$ in sorted($I_k$, ascent order by $x_{jk}$) do  \n\u2003\u2003$G_L \\gets G_L + g_j; H_L \\gets H_L + h_j$  \n\u2003\u2003$G_R \\gets G - G_L, H_R \\gets H - H_L$  \n\u2003\u2003score $\\gets \\max(\\text{score}, \\frac{G_L^2}{H_L+\\lambda} + \\frac{G_R^2}{H_R+\\lambda} - \\frac{G^2}{H+\\lambda})$  \n\u2003end  \n\u2003// enumerate missing value goto left  \n\u2003$G_R \\gets 0, H_R \\gets 0$  \n\u2003for $j$ in sorted($I_k$, descent order by $x_{jk}$) do  \n\u2003\u2003$G_R \\gets G_R + g_j, H_R \\gets H_R + h_j$  \n\u2003\u2003$G_L \\gets G - G_R, H_L \\gets H - H_R$  \n\u2003\u2003score $\\gets \\max(\\text{score}, \\frac{G_L^2}{H_L+\\lambda} + \\frac{G_R^2}{H_R+\\lambda} - \\frac{G^2}{H+\\lambda})$  \n\u2003end  \nend  \nOutput: Split and default directions with max gain\n\nof default direction in each branch. The optimal default di-\nrections are learnt from the data. The algorithm is shown in\nAlg. 3. The key improvement is to only visit the non-missing\nentries $I_k$. The presented algorithm treats the non-presence\nas a missing value and learns the best direction to handle\nmissing values. The same algorithm can also be applied\nwhen the non-presence corresponds to a user specified value\nby limiting the enumeration only to consistent solutions.\n\nTo the best of our knowledge, most existing tree learn-\ning algorithms are either only optimized for dense data, or\nneed specific procedures to handle limited cases such as cat-\negorical encoding. XGBoost handles all sparsity patterns in\na unified way. More importantly, our method exploits the\nsparsity to make computation complexity linear to number\nof non-missing entries in the input. Fig. 5 shows the com-\nparison of sparsity aware and a naive implementation on an\nAllstate-10K dataset (description of dataset given in Sec. 6).\nWe find that the sparsity aware algorithm runs 50 times\nfaster than the naive version. This confirms the importance\nof the sparsity aware algorithm."
        },
        {
            "text": "To the best of our knowledge, most existing tree learn-\ning algorithms are either only optimized for dense data, or\nneed specific procedures to handle limited cases such as cat-\negorical encoding. XGBoost handles all sparsity patterns in\na unified way. More importantly, our method exploits the\nsparsity to make computation complexity linear to number\nof non-missing entries in the input. Fig. 5 shows the com-\nparison of sparsity aware and a naive implementation on an\nAllstate-10K dataset (description of dataset given in Sec. 6).\nWe find that the sparsity aware algorithm runs 50 times\nfaster than the naive version. This confirms the importance\nof the sparsity aware algorithm.",
            "page": 5,
            "x": 51,
            "y": 592,
            "width": 244,
            "height": 126,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "10-sparsity",
            "chunk_id": "140f697a-6267-499d-b951-6275be75e7fc",
            "group_text": "3.4  Sparsity-aware Split Finding\n\n    In many real-world problems, it is quite common for the input x to be sparse. There are multiple possible causes for sparsity: 1) presence of missing values in the data; 2) frequent zero entries in the statistics; and, 3) artifacts of feature engineering such as one-hot encoding. It is important to make the algorithm aware of the sparsity pattern in the data. In order to do so, we propose to add a default direction in each tree node, which is shown in Fig. 4. When a value is missing in the sparse matrix x, the instance is classified into the default direction. There are two choices\n\nAlgorithm 3: Sparsity-aware Split Finding\n\nInput: $I$, instance set of current node  \nInput: $I_k = \\{i \\in I|x_{ik} \\neq \\text{missing}\\}$  \nInput: $d$, feature dimension  \nAlso applies to the approximate setting, only collect  \nstatistics of non-missing entries into buckets  \ngain $\\gets 0$  \n$G \\gets \\sum_{i \\in I} g_i; H \\gets \\sum_{i \\in I} h_i$  \nfor $k = 1$ to $m$ do  \n\u2003// enumerate missing value goto right  \n\u2003$G_L \\gets 0, H_L \\gets 0$  \n\u2003for $j$ in sorted($I_k$, ascent order by $x_{jk}$) do  \n\u2003\u2003$G_L \\gets G_L + g_j; H_L \\gets H_L + h_j$  \n\u2003\u2003$G_R \\gets G - G_L, H_R \\gets H - H_L$  \n\u2003\u2003score $\\gets \\max(\\text{score}, \\frac{G_L^2}{H_L+\\lambda} + \\frac{G_R^2}{H_R+\\lambda} - \\frac{G^2}{H+\\lambda})$  \n\u2003end  \n\u2003// enumerate missing value goto left  \n\u2003$G_R \\gets 0, H_R \\gets 0$  \n\u2003for $j$ in sorted($I_k$, descent order by $x_{jk}$) do  \n\u2003\u2003$G_R \\gets G_R + g_j, H_R \\gets H_R + h_j$  \n\u2003\u2003$G_L \\gets G - G_R, H_L \\gets H - H_R$  \n\u2003\u2003score $\\gets \\max(\\text{score}, \\frac{G_L^2}{H_L+\\lambda} + \\frac{G_R^2}{H_R+\\lambda} - \\frac{G^2}{H+\\lambda})$  \n\u2003end  \nend  \nOutput: Split and default directions with max gain\n\nof default direction in each branch. The optimal default di-\nrections are learnt from the data. The algorithm is shown in\nAlg. 3. The key improvement is to only visit the non-missing\nentries $I_k$. The presented algorithm treats the non-presence\nas a missing value and learns the best direction to handle\nmissing values. The same algorithm can also be applied\nwhen the non-presence corresponds to a user specified value\nby limiting the enumeration only to consistent solutions.\n\nTo the best of our knowledge, most existing tree learn-\ning algorithms are either only optimized for dense data, or\nneed specific procedures to handle limited cases such as cat-\negorical encoding. XGBoost handles all sparsity patterns in\na unified way. More importantly, our method exploits the\nsparsity to make computation complexity linear to number\nof non-missing entries in the input. Fig. 5 shows the com-\nparison of sparsity aware and a naive implementation on an\nAllstate-10K dataset (description of dataset given in Sec. 6).\nWe find that the sparsity aware algorithm runs 50 times\nfaster than the naive version. This confirms the importance\nof the sparsity aware algorithm."
        },
        {
            "text": "4.  SYSTEM DESIGN\n\n4.1  Column Block for Parallel Learning\n\n    The most time consuming part of tree learning is to get\nthe data into sorted order. In order to reduce the cost of\nsorting, we propose to store the data in in-memory units,\nwhich we called block. Data in each block is stored in the\ncompressed column (CSC) format, with each column sorted\nby the corresponding feature value. This input data layout\nonly needs to be computed once before training, and can be\nreused in later iterations.",
            "page": 5,
            "x": 314,
            "y": 378,
            "width": 243,
            "height": 120,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-system",
            "chunk_id": "1496404b-c9f9-452c-bb4d-109c9a4639df",
            "group_text": "4.  SYSTEM DESIGN\n\n4.1  Column Block for Parallel Learning\n\n    The most time consuming part of tree learning is to get\nthe data into sorted order. In order to reduce the cost of\nsorting, we propose to store the data in in-memory units,\nwhich we called block. Data in each block is stored in the\ncompressed column (CSC) format, with each column sorted\nby the corresponding feature value. This input data layout\nonly needs to be computed once before training, and can be\nreused in later iterations.\n\nIn the exact greedy algorithm, we store the entire dataset\nin a single block and run the split search algorithm by lin-\nearly scanning over the pre-sorted entries. We do the split\nfinding of all leaves collectively, so one scan over the block\nwill collect the statistics of the split candidates in all leaf\nbranches. Fig. 6 shows how we transform a dataset into the\nformat and find the optimal split using the block structure.\n\nThe block structure also helps when using the approxi-\nmate algorithms. Multiple blocks can be used in this case,\nwith each block corresponding to subset of rows in the dataset\nDifferent blocks can be distributed across machines, or stored\non disk in the out-of-core setting. Using the sorted struc-\nture, the quantile finding step becomes a _linear scan_ over\nthe sorted columns. This is especially valuable for local pro-\nposal algorithms, where candidates are generated frequently\nat each branch. The binary search in histogram aggregation\nalso becomes a linear time merge style algorithm.\n\nCollecting statistics for each column can be *parallelized*,\ngiving us a parallel algorithm for split finding. Importantly,\nthe column block structure also supports column subsam-\npling, as it is easy to select a subset of columns in a block.\n\nFigure 7: Impact of cache-aware prefetching in exact greedy algorithm. We find that the cache-miss effect impacts the performance on the large datasets (10 million instances). Using cache aware prefetching improves the performance by factor of two when the dataset is large.\n\n**Time Complexity Analysis** Let $d$ be the maximum depth\nof the tree and $K$ be total number of trees. For the ex-\nact greedy algorithm, the time complexity of original spase\naware algorithm is $O(Kd\\|\\mathbf{x}\\|_0 \\log n)$. Here we use $\\|\\mathbf{x}\\|_0$ to\ndenote number of non-missing entries in the training data.\nOn the other hand, tree boosting on the block structure only\ncost $O(Kd\\|\\mathbf{x}\\|_0 + \\|\\mathbf{x}\\|_0 \\log n)$. Here $O(\\|\\mathbf{x}\\|_0 \\log n)$ is the one\ntime preprocessing cost that can be amortized. This analysis\nshows that the block structure helps to save an additional\n$\\log n$ factor, which is significant when $n$ is large. For the\napproximate algorithm, the time complexity of original al-\ngorithm with binary search is $O(Kd\\|\\mathbf{x}\\|_0 \\log q)$. Here $q$ is\nthe number of proposal candidates in the dataset. While $q$\nis usually between 32 and 100, the log factor still introduces\noverhead. Using the block structure, we can reduce the time\nto $O(Kd\\|\\mathbf{x}\\|_0 + \\|\\mathbf{x}\\|_0 \\log B)$, where $B$ is the maximum num-\nber of rows in each block. Again we can save the additional\n$\\log q$ factor in computation."
        },
        {
            "text": "In the exact greedy algorithm, we store the entire dataset\nin a single block and run the split search algorithm by lin-\nearly scanning over the pre-sorted entries. We do the split\nfinding of all leaves collectively, so one scan over the block\nwill collect the statistics of the split candidates in all leaf\nbranches. Fig. 6 shows how we transform a dataset into the\nformat and find the optimal split using the block structure.",
            "page": 5,
            "x": 315,
            "y": 500,
            "width": 242,
            "height": 73,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-system",
            "chunk_id": "39f4b025-1d33-488b-9c88-36b1eb7370b8",
            "group_text": "4.  SYSTEM DESIGN\n\n4.1  Column Block for Parallel Learning\n\n    The most time consuming part of tree learning is to get\nthe data into sorted order. In order to reduce the cost of\nsorting, we propose to store the data in in-memory units,\nwhich we called block. Data in each block is stored in the\ncompressed column (CSC) format, with each column sorted\nby the corresponding feature value. This input data layout\nonly needs to be computed once before training, and can be\nreused in later iterations.\n\nIn the exact greedy algorithm, we store the entire dataset\nin a single block and run the split search algorithm by lin-\nearly scanning over the pre-sorted entries. We do the split\nfinding of all leaves collectively, so one scan over the block\nwill collect the statistics of the split candidates in all leaf\nbranches. Fig. 6 shows how we transform a dataset into the\nformat and find the optimal split using the block structure.\n\nThe block structure also helps when using the approxi-\nmate algorithms. Multiple blocks can be used in this case,\nwith each block corresponding to subset of rows in the dataset\nDifferent blocks can be distributed across machines, or stored\non disk in the out-of-core setting. Using the sorted struc-\nture, the quantile finding step becomes a _linear scan_ over\nthe sorted columns. This is especially valuable for local pro-\nposal algorithms, where candidates are generated frequently\nat each branch. The binary search in histogram aggregation\nalso becomes a linear time merge style algorithm.\n\nCollecting statistics for each column can be *parallelized*,\ngiving us a parallel algorithm for split finding. Importantly,\nthe column block structure also supports column subsam-\npling, as it is easy to select a subset of columns in a block.\n\nFigure 7: Impact of cache-aware prefetching in exact greedy algorithm. We find that the cache-miss effect impacts the performance on the large datasets (10 million instances). Using cache aware prefetching improves the performance by factor of two when the dataset is large.\n\n**Time Complexity Analysis** Let $d$ be the maximum depth\nof the tree and $K$ be total number of trees. For the ex-\nact greedy algorithm, the time complexity of original spase\naware algorithm is $O(Kd\\|\\mathbf{x}\\|_0 \\log n)$. Here we use $\\|\\mathbf{x}\\|_0$ to\ndenote number of non-missing entries in the training data.\nOn the other hand, tree boosting on the block structure only\ncost $O(Kd\\|\\mathbf{x}\\|_0 + \\|\\mathbf{x}\\|_0 \\log n)$. Here $O(\\|\\mathbf{x}\\|_0 \\log n)$ is the one\ntime preprocessing cost that can be amortized. This analysis\nshows that the block structure helps to save an additional\n$\\log n$ factor, which is significant when $n$ is large. For the\napproximate algorithm, the time complexity of original al-\ngorithm with binary search is $O(Kd\\|\\mathbf{x}\\|_0 \\log q)$. Here $q$ is\nthe number of proposal candidates in the dataset. While $q$\nis usually between 32 and 100, the log factor still introduces\noverhead. Using the block structure, we can reduce the time\nto $O(Kd\\|\\mathbf{x}\\|_0 + \\|\\mathbf{x}\\|_0 \\log B)$, where $B$ is the maximum num-\nber of rows in each block. Again we can save the additional\n$\\log q$ factor in computation."
        },
        {
            "text": "The block structure also helps when using the approxi-\nmate algorithms. Multiple blocks can be used in this case,\nwith each block corresponding to subset of rows in the dataset\nDifferent blocks can be distributed across machines, or stored\non disk in the out-of-core setting. Using the sorted struc-\nture, the quantile finding step becomes a _linear scan_ over\nthe sorted columns. This is especially valuable for local pro-\nposal algorithms, where candidates are generated frequently\nat each branch. The binary search in histogram aggregation\nalso becomes a linear time merge style algorithm.",
            "page": 5,
            "x": 315,
            "y": 574,
            "width": 243,
            "height": 103,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-system",
            "chunk_id": "bb35909c-7ac2-44db-904c-7f075fb8899e",
            "group_text": "4.  SYSTEM DESIGN\n\n4.1  Column Block for Parallel Learning\n\n    The most time consuming part of tree learning is to get\nthe data into sorted order. In order to reduce the cost of\nsorting, we propose to store the data in in-memory units,\nwhich we called block. Data in each block is stored in the\ncompressed column (CSC) format, with each column sorted\nby the corresponding feature value. This input data layout\nonly needs to be computed once before training, and can be\nreused in later iterations.\n\nIn the exact greedy algorithm, we store the entire dataset\nin a single block and run the split search algorithm by lin-\nearly scanning over the pre-sorted entries. We do the split\nfinding of all leaves collectively, so one scan over the block\nwill collect the statistics of the split candidates in all leaf\nbranches. Fig. 6 shows how we transform a dataset into the\nformat and find the optimal split using the block structure.\n\nThe block structure also helps when using the approxi-\nmate algorithms. Multiple blocks can be used in this case,\nwith each block corresponding to subset of rows in the dataset\nDifferent blocks can be distributed across machines, or stored\non disk in the out-of-core setting. Using the sorted struc-\nture, the quantile finding step becomes a _linear scan_ over\nthe sorted columns. This is especially valuable for local pro-\nposal algorithms, where candidates are generated frequently\nat each branch. The binary search in histogram aggregation\nalso becomes a linear time merge style algorithm.\n\nCollecting statistics for each column can be *parallelized*,\ngiving us a parallel algorithm for split finding. Importantly,\nthe column block structure also supports column subsam-\npling, as it is easy to select a subset of columns in a block.\n\nFigure 7: Impact of cache-aware prefetching in exact greedy algorithm. We find that the cache-miss effect impacts the performance on the large datasets (10 million instances). Using cache aware prefetching improves the performance by factor of two when the dataset is large.\n\n**Time Complexity Analysis** Let $d$ be the maximum depth\nof the tree and $K$ be total number of trees. For the ex-\nact greedy algorithm, the time complexity of original spase\naware algorithm is $O(Kd\\|\\mathbf{x}\\|_0 \\log n)$. Here we use $\\|\\mathbf{x}\\|_0$ to\ndenote number of non-missing entries in the training data.\nOn the other hand, tree boosting on the block structure only\ncost $O(Kd\\|\\mathbf{x}\\|_0 + \\|\\mathbf{x}\\|_0 \\log n)$. Here $O(\\|\\mathbf{x}\\|_0 \\log n)$ is the one\ntime preprocessing cost that can be amortized. This analysis\nshows that the block structure helps to save an additional\n$\\log n$ factor, which is significant when $n$ is large. For the\napproximate algorithm, the time complexity of original al-\ngorithm with binary search is $O(Kd\\|\\mathbf{x}\\|_0 \\log q)$. Here $q$ is\nthe number of proposal candidates in the dataset. While $q$\nis usually between 32 and 100, the log factor still introduces\noverhead. Using the block structure, we can reduce the time\nto $O(Kd\\|\\mathbf{x}\\|_0 + \\|\\mathbf{x}\\|_0 \\log B)$, where $B$ is the maximum num-\nber of rows in each block. Again we can save the additional\n$\\log q$ factor in computation."
        },
        {
            "text": "Collecting statistics for each column can be *parallelized*,\ngiving us a parallel algorithm for split finding. Importantly,\nthe column block structure also supports column subsam-\npling, as it is easy to select a subset of columns in a block.",
            "page": 5,
            "x": 315,
            "y": 677,
            "width": 241,
            "height": 42,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-system",
            "chunk_id": "2f03ae72-7f50-4c3e-a091-3f571f4e5bf9",
            "group_text": "4.  SYSTEM DESIGN\n\n4.1  Column Block for Parallel Learning\n\n    The most time consuming part of tree learning is to get\nthe data into sorted order. In order to reduce the cost of\nsorting, we propose to store the data in in-memory units,\nwhich we called block. Data in each block is stored in the\ncompressed column (CSC) format, with each column sorted\nby the corresponding feature value. This input data layout\nonly needs to be computed once before training, and can be\nreused in later iterations.\n\nIn the exact greedy algorithm, we store the entire dataset\nin a single block and run the split search algorithm by lin-\nearly scanning over the pre-sorted entries. We do the split\nfinding of all leaves collectively, so one scan over the block\nwill collect the statistics of the split candidates in all leaf\nbranches. Fig. 6 shows how we transform a dataset into the\nformat and find the optimal split using the block structure.\n\nThe block structure also helps when using the approxi-\nmate algorithms. Multiple blocks can be used in this case,\nwith each block corresponding to subset of rows in the dataset\nDifferent blocks can be distributed across machines, or stored\non disk in the out-of-core setting. Using the sorted struc-\nture, the quantile finding step becomes a _linear scan_ over\nthe sorted columns. This is especially valuable for local pro-\nposal algorithms, where candidates are generated frequently\nat each branch. The binary search in histogram aggregation\nalso becomes a linear time merge style algorithm.\n\nCollecting statistics for each column can be *parallelized*,\ngiving us a parallel algorithm for split finding. Importantly,\nthe column block structure also supports column subsam-\npling, as it is easy to select a subset of columns in a block.\n\nFigure 7: Impact of cache-aware prefetching in exact greedy algorithm. We find that the cache-miss effect impacts the performance on the large datasets (10 million instances). Using cache aware prefetching improves the performance by factor of two when the dataset is large.\n\n**Time Complexity Analysis** Let $d$ be the maximum depth\nof the tree and $K$ be total number of trees. For the ex-\nact greedy algorithm, the time complexity of original spase\naware algorithm is $O(Kd\\|\\mathbf{x}\\|_0 \\log n)$. Here we use $\\|\\mathbf{x}\\|_0$ to\ndenote number of non-missing entries in the training data.\nOn the other hand, tree boosting on the block structure only\ncost $O(Kd\\|\\mathbf{x}\\|_0 + \\|\\mathbf{x}\\|_0 \\log n)$. Here $O(\\|\\mathbf{x}\\|_0 \\log n)$ is the one\ntime preprocessing cost that can be amortized. This analysis\nshows that the block structure helps to save an additional\n$\\log n$ factor, which is significant when $n$ is large. For the\napproximate algorithm, the time complexity of original al-\ngorithm with binary search is $O(Kd\\|\\mathbf{x}\\|_0 \\log q)$. Here $q$ is\nthe number of proposal candidates in the dataset. While $q$\nis usually between 32 and 100, the log factor still introduces\noverhead. Using the block structure, we can reduce the time\nto $O(Kd\\|\\mathbf{x}\\|_0 + \\|\\mathbf{x}\\|_0 \\log B)$, where $B$ is the maximum num-\nber of rows in each block. Again we can save the additional\n$\\log q$ factor in computation."
        },
        {
            "text": "Figure 7: Impact of cache-aware prefetching in exact greedy algorithm. We find that the cache-miss effect impacts the performance on the large datasets (10 million instances). Using cache aware prefetching improves the performance by factor of two when the dataset is large.",
            "page": 6,
            "x": 49,
            "y": 169,
            "width": 508,
            "height": 33,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-system",
            "chunk_id": "019792b5-d985-4888-a51e-00821f9a9e79",
            "group_text": "4.  SYSTEM DESIGN\n\n4.1  Column Block for Parallel Learning\n\n    The most time consuming part of tree learning is to get\nthe data into sorted order. In order to reduce the cost of\nsorting, we propose to store the data in in-memory units,\nwhich we called block. Data in each block is stored in the\ncompressed column (CSC) format, with each column sorted\nby the corresponding feature value. This input data layout\nonly needs to be computed once before training, and can be\nreused in later iterations.\n\nIn the exact greedy algorithm, we store the entire dataset\nin a single block and run the split search algorithm by lin-\nearly scanning over the pre-sorted entries. We do the split\nfinding of all leaves collectively, so one scan over the block\nwill collect the statistics of the split candidates in all leaf\nbranches. Fig. 6 shows how we transform a dataset into the\nformat and find the optimal split using the block structure.\n\nThe block structure also helps when using the approxi-\nmate algorithms. Multiple blocks can be used in this case,\nwith each block corresponding to subset of rows in the dataset\nDifferent blocks can be distributed across machines, or stored\non disk in the out-of-core setting. Using the sorted struc-\nture, the quantile finding step becomes a _linear scan_ over\nthe sorted columns. This is especially valuable for local pro-\nposal algorithms, where candidates are generated frequently\nat each branch. The binary search in histogram aggregation\nalso becomes a linear time merge style algorithm.\n\nCollecting statistics for each column can be *parallelized*,\ngiving us a parallel algorithm for split finding. Importantly,\nthe column block structure also supports column subsam-\npling, as it is easy to select a subset of columns in a block.\n\nFigure 7: Impact of cache-aware prefetching in exact greedy algorithm. We find that the cache-miss effect impacts the performance on the large datasets (10 million instances). Using cache aware prefetching improves the performance by factor of two when the dataset is large.\n\n**Time Complexity Analysis** Let $d$ be the maximum depth\nof the tree and $K$ be total number of trees. For the ex-\nact greedy algorithm, the time complexity of original spase\naware algorithm is $O(Kd\\|\\mathbf{x}\\|_0 \\log n)$. Here we use $\\|\\mathbf{x}\\|_0$ to\ndenote number of non-missing entries in the training data.\nOn the other hand, tree boosting on the block structure only\ncost $O(Kd\\|\\mathbf{x}\\|_0 + \\|\\mathbf{x}\\|_0 \\log n)$. Here $O(\\|\\mathbf{x}\\|_0 \\log n)$ is the one\ntime preprocessing cost that can be amortized. This analysis\nshows that the block structure helps to save an additional\n$\\log n$ factor, which is significant when $n$ is large. For the\napproximate algorithm, the time complexity of original al-\ngorithm with binary search is $O(Kd\\|\\mathbf{x}\\|_0 \\log q)$. Here $q$ is\nthe number of proposal candidates in the dataset. While $q$\nis usually between 32 and 100, the log factor still introduces\noverhead. Using the block structure, we can reduce the time\nto $O(Kd\\|\\mathbf{x}\\|_0 + \\|\\mathbf{x}\\|_0 \\log B)$, where $B$ is the maximum num-\nber of rows in each block. Again we can save the additional\n$\\log q$ factor in computation."
        },
        {
            "text": "**Time Complexity Analysis** Let $d$ be the maximum depth\nof the tree and $K$ be total number of trees. For the ex-\nact greedy algorithm, the time complexity of original spase\naware algorithm is $O(Kd\\|\\mathbf{x}\\|_0 \\log n)$. Here we use $\\|\\mathbf{x}\\|_0$ to\ndenote number of non-missing entries in the training data.\nOn the other hand, tree boosting on the block structure only\ncost $O(Kd\\|\\mathbf{x}\\|_0 + \\|\\mathbf{x}\\|_0 \\log n)$. Here $O(\\|\\mathbf{x}\\|_0 \\log n)$ is the one\ntime preprocessing cost that can be amortized. This analysis\nshows that the block structure helps to save an additional\n$\\log n$ factor, which is significant when $n$ is large. For the\napproximate algorithm, the time complexity of original al-\ngorithm with binary search is $O(Kd\\|\\mathbf{x}\\|_0 \\log q)$. Here $q$ is\nthe number of proposal candidates in the dataset. While $q$\nis usually between 32 and 100, the log factor still introduces\noverhead. Using the block structure, we can reduce the time\nto $O(Kd\\|\\mathbf{x}\\|_0 + \\|\\mathbf{x}\\|_0 \\log B)$, where $B$ is the maximum num-\nber of rows in each block. Again we can save the additional\n$\\log q$ factor in computation.",
            "page": 6,
            "x": 50,
            "y": 315,
            "width": 247,
            "height": 192,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-system",
            "chunk_id": "48d5b499-6522-467e-a50f-aeba232ae967",
            "group_text": "4.  SYSTEM DESIGN\n\n4.1  Column Block for Parallel Learning\n\n    The most time consuming part of tree learning is to get\nthe data into sorted order. In order to reduce the cost of\nsorting, we propose to store the data in in-memory units,\nwhich we called block. Data in each block is stored in the\ncompressed column (CSC) format, with each column sorted\nby the corresponding feature value. This input data layout\nonly needs to be computed once before training, and can be\nreused in later iterations.\n\nIn the exact greedy algorithm, we store the entire dataset\nin a single block and run the split search algorithm by lin-\nearly scanning over the pre-sorted entries. We do the split\nfinding of all leaves collectively, so one scan over the block\nwill collect the statistics of the split candidates in all leaf\nbranches. Fig. 6 shows how we transform a dataset into the\nformat and find the optimal split using the block structure.\n\nThe block structure also helps when using the approxi-\nmate algorithms. Multiple blocks can be used in this case,\nwith each block corresponding to subset of rows in the dataset\nDifferent blocks can be distributed across machines, or stored\non disk in the out-of-core setting. Using the sorted struc-\nture, the quantile finding step becomes a _linear scan_ over\nthe sorted columns. This is especially valuable for local pro-\nposal algorithms, where candidates are generated frequently\nat each branch. The binary search in histogram aggregation\nalso becomes a linear time merge style algorithm.\n\nCollecting statistics for each column can be *parallelized*,\ngiving us a parallel algorithm for split finding. Importantly,\nthe column block structure also supports column subsam-\npling, as it is easy to select a subset of columns in a block.\n\nFigure 7: Impact of cache-aware prefetching in exact greedy algorithm. We find that the cache-miss effect impacts the performance on the large datasets (10 million instances). Using cache aware prefetching improves the performance by factor of two when the dataset is large.\n\n**Time Complexity Analysis** Let $d$ be the maximum depth\nof the tree and $K$ be total number of trees. For the ex-\nact greedy algorithm, the time complexity of original spase\naware algorithm is $O(Kd\\|\\mathbf{x}\\|_0 \\log n)$. Here we use $\\|\\mathbf{x}\\|_0$ to\ndenote number of non-missing entries in the training data.\nOn the other hand, tree boosting on the block structure only\ncost $O(Kd\\|\\mathbf{x}\\|_0 + \\|\\mathbf{x}\\|_0 \\log n)$. Here $O(\\|\\mathbf{x}\\|_0 \\log n)$ is the one\ntime preprocessing cost that can be amortized. This analysis\nshows that the block structure helps to save an additional\n$\\log n$ factor, which is significant when $n$ is large. For the\napproximate algorithm, the time complexity of original al-\ngorithm with binary search is $O(Kd\\|\\mathbf{x}\\|_0 \\log q)$. Here $q$ is\nthe number of proposal candidates in the dataset. While $q$\nis usually between 32 and 100, the log factor still introduces\noverhead. Using the block structure, we can reduce the time\nto $O(Kd\\|\\mathbf{x}\\|_0 + \\|\\mathbf{x}\\|_0 \\log B)$, where $B$ is the maximum num-\nber of rows in each block. Again we can save the additional\n$\\log q$ factor in computation."
        },
        {
            "text": "## 4.2  Cache-aware Access\n\nWhile the proposed block structure helps optimize the computation complexity of split finding, the new algorithm requires indirect fetches of gradient statistics by row index, since these values are accessed in order of feature. This is a non-continuous memory access. A naive implementation of split enumeration introduces immediate read/write dependency between the accumulation and the non-continuous memory fetch operation (see Fig. 8). This slows down split finding when the gradient statistics do not fit into CPU cache and cache miss occur.",
            "page": 6,
            "x": 50,
            "y": 513,
            "width": 245,
            "height": 121,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-cache",
            "chunk_id": "8d58ad9c-bfba-4c36-a4ee-d20e5716fad5",
            "group_text": "## 4.2  Cache-aware Access\n\nWhile the proposed block structure helps optimize the computation complexity of split finding, the new algorithm requires indirect fetches of gradient statistics by row index, since these values are accessed in order of feature. This is a non-continuous memory access. A naive implementation of split enumeration introduces immediate read/write dependency between the accumulation and the non-continuous memory fetch operation (see Fig. 8). This slows down split finding when the gradient statistics do not fit into CPU cache and cache miss occur.\n\nFor the exact greedy algorithm, we can alleviate the problem by a cache-aware prefetching algorithm.  Specifically, we allocate an internal buffer in each thread, fetch the gradient statistics into it, and then perform accumulation in a mini-batch manner.  This prefetching changes the direct read/write dependency to a longer dependency and helps to reduce the runtime overhead when number of rows in the is large.  Figure 7 gives the comparison of cache-aware vs.\n\nnon cache-aware algorithm on the the Higgs and the All-\nstate dataset. We find that cache-aware implementation of\nthe exact greedy algorithm runs twice as fast as the naive\nversion when the dataset is large.\n\nFor approximate algorithms, we solve the problem by choosing a correct block size.  We define the block size to be maximum number of examples in contained in a block, as this reflects the cache storage cost of gradient statistics.  Choosing an overly small block size results in small workload for each thread and leads to inefficient parallelization.  On the other hand, overly large blocks result in cache misses, as the gradient statistics do not fit into the CPU cache.  A good choice of block size balances these two factors.  We compared various choices of block size on two data sets.  The results are given in Fig. 9.  This result validates our discussion and\n\nshows that choosing $2^{16}$ examples per block balances the\ncache property and parallelization."
        },
        {
            "text": "For the exact greedy algorithm, we can alleviate the problem by a cache-aware prefetching algorithm.  Specifically, we allocate an internal buffer in each thread, fetch the gradient statistics into it, and then perform accumulation in a mini-batch manner.  This prefetching changes the direct read/write dependency to a longer dependency and helps to reduce the runtime overhead when number of rows in the is large.  Figure 7 gives the comparison of cache-aware vs.",
            "page": 6,
            "x": 51,
            "y": 635,
            "width": 244,
            "height": 85,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-cache",
            "chunk_id": "c2b683ff-a2b1-4259-91c2-cd9ba4a9fdb6",
            "group_text": "## 4.2  Cache-aware Access\n\nWhile the proposed block structure helps optimize the computation complexity of split finding, the new algorithm requires indirect fetches of gradient statistics by row index, since these values are accessed in order of feature. This is a non-continuous memory access. A naive implementation of split enumeration introduces immediate read/write dependency between the accumulation and the non-continuous memory fetch operation (see Fig. 8). This slows down split finding when the gradient statistics do not fit into CPU cache and cache miss occur.\n\nFor the exact greedy algorithm, we can alleviate the problem by a cache-aware prefetching algorithm.  Specifically, we allocate an internal buffer in each thread, fetch the gradient statistics into it, and then perform accumulation in a mini-batch manner.  This prefetching changes the direct read/write dependency to a longer dependency and helps to reduce the runtime overhead when number of rows in the is large.  Figure 7 gives the comparison of cache-aware vs.\n\nnon cache-aware algorithm on the the Higgs and the All-\nstate dataset. We find that cache-aware implementation of\nthe exact greedy algorithm runs twice as fast as the naive\nversion when the dataset is large.\n\nFor approximate algorithms, we solve the problem by choosing a correct block size.  We define the block size to be maximum number of examples in contained in a block, as this reflects the cache storage cost of gradient statistics.  Choosing an overly small block size results in small workload for each thread and leads to inefficient parallelization.  On the other hand, overly large blocks result in cache misses, as the gradient statistics do not fit into the CPU cache.  A good choice of block size balances these two factors.  We compared various choices of block size on two data sets.  The results are given in Fig. 9.  This result validates our discussion and\n\nshows that choosing $2^{16}$ examples per block balances the\ncache property and parallelization."
        },
        {
            "text": "non cache-aware algorithm on the the Higgs and the All-\nstate dataset. We find that cache-aware implementation of\nthe exact greedy algorithm runs twice as fast as the naive\nversion when the dataset is large.",
            "page": 6,
            "x": 314,
            "y": 562,
            "width": 244,
            "height": 41,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-cache",
            "chunk_id": "d3d08ba7-5c05-4da8-8b42-2c752b123c15",
            "group_text": "## 4.2  Cache-aware Access\n\nWhile the proposed block structure helps optimize the computation complexity of split finding, the new algorithm requires indirect fetches of gradient statistics by row index, since these values are accessed in order of feature. This is a non-continuous memory access. A naive implementation of split enumeration introduces immediate read/write dependency between the accumulation and the non-continuous memory fetch operation (see Fig. 8). This slows down split finding when the gradient statistics do not fit into CPU cache and cache miss occur.\n\nFor the exact greedy algorithm, we can alleviate the problem by a cache-aware prefetching algorithm.  Specifically, we allocate an internal buffer in each thread, fetch the gradient statistics into it, and then perform accumulation in a mini-batch manner.  This prefetching changes the direct read/write dependency to a longer dependency and helps to reduce the runtime overhead when number of rows in the is large.  Figure 7 gives the comparison of cache-aware vs.\n\nnon cache-aware algorithm on the the Higgs and the All-\nstate dataset. We find that cache-aware implementation of\nthe exact greedy algorithm runs twice as fast as the naive\nversion when the dataset is large.\n\nFor approximate algorithms, we solve the problem by choosing a correct block size.  We define the block size to be maximum number of examples in contained in a block, as this reflects the cache storage cost of gradient statistics.  Choosing an overly small block size results in small workload for each thread and leads to inefficient parallelization.  On the other hand, overly large blocks result in cache misses, as the gradient statistics do not fit into the CPU cache.  A good choice of block size balances these two factors.  We compared various choices of block size on two data sets.  The results are given in Fig. 9.  This result validates our discussion and\n\nshows that choosing $2^{16}$ examples per block balances the\ncache property and parallelization."
        },
        {
            "text": "For approximate algorithms, we solve the problem by choosing a correct block size.  We define the block size to be maximum number of examples in contained in a block, as this reflects the cache storage cost of gradient statistics.  Choosing an overly small block size results in small workload for each thread and leads to inefficient parallelization.  On the other hand, overly large blocks result in cache misses, as the gradient statistics do not fit into the CPU cache.  A good choice of block size balances these two factors.  We compared various choices of block size on two data sets.  The results are given in Fig. 9.  This result validates our discussion and",
            "page": 6,
            "x": 314,
            "y": 604,
            "width": 245,
            "height": 115,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-cache",
            "chunk_id": "1925cb0d-4ff4-4ce3-9407-763b6da49b9f",
            "group_text": "## 4.2  Cache-aware Access\n\nWhile the proposed block structure helps optimize the computation complexity of split finding, the new algorithm requires indirect fetches of gradient statistics by row index, since these values are accessed in order of feature. This is a non-continuous memory access. A naive implementation of split enumeration introduces immediate read/write dependency between the accumulation and the non-continuous memory fetch operation (see Fig. 8). This slows down split finding when the gradient statistics do not fit into CPU cache and cache miss occur.\n\nFor the exact greedy algorithm, we can alleviate the problem by a cache-aware prefetching algorithm.  Specifically, we allocate an internal buffer in each thread, fetch the gradient statistics into it, and then perform accumulation in a mini-batch manner.  This prefetching changes the direct read/write dependency to a longer dependency and helps to reduce the runtime overhead when number of rows in the is large.  Figure 7 gives the comparison of cache-aware vs.\n\nnon cache-aware algorithm on the the Higgs and the All-\nstate dataset. We find that cache-aware implementation of\nthe exact greedy algorithm runs twice as fast as the naive\nversion when the dataset is large.\n\nFor approximate algorithms, we solve the problem by choosing a correct block size.  We define the block size to be maximum number of examples in contained in a block, as this reflects the cache storage cost of gradient statistics.  Choosing an overly small block size results in small workload for each thread and leads to inefficient parallelization.  On the other hand, overly large blocks result in cache misses, as the gradient statistics do not fit into the CPU cache.  A good choice of block size balances these two factors.  We compared various choices of block size on two data sets.  The results are given in Fig. 9.  This result validates our discussion and\n\nshows that choosing $2^{16}$ examples per block balances the\ncache property and parallelization."
        },
        {
            "text": "shows that choosing $2^{16}$ examples per block balances the\ncache property and parallelization.",
            "page": 7,
            "x": 49,
            "y": 179,
            "width": 246,
            "height": 24,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-cache",
            "chunk_id": "30a9c773-7a2e-405b-a13f-87b769a0d8c5",
            "group_text": "## 4.2  Cache-aware Access\n\nWhile the proposed block structure helps optimize the computation complexity of split finding, the new algorithm requires indirect fetches of gradient statistics by row index, since these values are accessed in order of feature. This is a non-continuous memory access. A naive implementation of split enumeration introduces immediate read/write dependency between the accumulation and the non-continuous memory fetch operation (see Fig. 8). This slows down split finding when the gradient statistics do not fit into CPU cache and cache miss occur.\n\nFor the exact greedy algorithm, we can alleviate the problem by a cache-aware prefetching algorithm.  Specifically, we allocate an internal buffer in each thread, fetch the gradient statistics into it, and then perform accumulation in a mini-batch manner.  This prefetching changes the direct read/write dependency to a longer dependency and helps to reduce the runtime overhead when number of rows in the is large.  Figure 7 gives the comparison of cache-aware vs.\n\nnon cache-aware algorithm on the the Higgs and the All-\nstate dataset. We find that cache-aware implementation of\nthe exact greedy algorithm runs twice as fast as the naive\nversion when the dataset is large.\n\nFor approximate algorithms, we solve the problem by choosing a correct block size.  We define the block size to be maximum number of examples in contained in a block, as this reflects the cache storage cost of gradient statistics.  Choosing an overly small block size results in small workload for each thread and leads to inefficient parallelization.  On the other hand, overly large blocks result in cache misses, as the gradient statistics do not fit into the CPU cache.  A good choice of block size balances these two factors.  We compared various choices of block size on two data sets.  The results are given in Fig. 9.  This result validates our discussion and\n\nshows that choosing $2^{16}$ examples per block balances the\ncache property and parallelization."
        },
        {
            "text": "### 4.3  Blocks for Out-of-core Computation\n\nOne goal of our system is to fully utilize a machine\u2019s resources to achieve scalable learning. Besides processors and memory, it is important to utilize disk space to handle data that does not fit into main memory. To enable out-of-core computation, we divide the data into multiple blocks and store each block on disk. During computation, it is important to use an independent thread to pre-fetch the block into a main memory buffer, so computation can happen in concurrence with disk reading. However, this does not entirely solve the problem since the disk reading takes most of the computation time. It is important to reduce the overhead and increase the throughput of disk IO. We mainly use two techniques to improve the out-of-core computation.",
            "page": 7,
            "x": 50,
            "y": 206,
            "width": 246,
            "height": 151,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "13-blocks",
            "chunk_id": "bad167cf-ef2c-4355-abfb-75c7d4b112d5",
            "group_text": "### 4.3  Blocks for Out-of-core Computation\n\nOne goal of our system is to fully utilize a machine\u2019s resources to achieve scalable learning. Besides processors and memory, it is important to utilize disk space to handle data that does not fit into main memory. To enable out-of-core computation, we divide the data into multiple blocks and store each block on disk. During computation, it is important to use an independent thread to pre-fetch the block into a main memory buffer, so computation can happen in concurrence with disk reading. However, this does not entirely solve the problem since the disk reading takes most of the computation time. It is important to reduce the overhead and increase the throughput of disk IO. We mainly use two techniques to improve the out-of-core computation.\n\n**Block Compression** The first technique we use is block compression. The block is compressed by columns, and decompressed on the fly by an independent thread when loading into main memory. This helps to trade some of the computation in decompression with the disk reading cost. We use a general purpose compression algorithm for compressing the features values. For the row index, we substract the row index by the begining index of the block and use a 16bit integer to store each offset. This requires $2^{16}$ examples per block, which is confirmed to be a good setting. In most of the dataset we tested, we achieve roughly a 26% to 29% compression ratio.\n\n**Block Sharding** The second technique is to shard the data\nonto multiple disks in an alternative manner. A pre-fetcher\nthread is assigned to each disk and fetches the data into an\nin-memory buffer. The training thread then alternatively\nreads the data from each buffer. This helps to increase the\nthroughput of disk reading when multiple disks are available."
        },
        {
            "text": "**Block Compression** The first technique we use is block compression. The block is compressed by columns, and decompressed on the fly by an independent thread when loading into main memory. This helps to trade some of the computation in decompression with the disk reading cost. We use a general purpose compression algorithm for compressing the features values. For the row index, we substract the row index by the begining index of the block and use a 16bit integer to store each offset. This requires $2^{16}$ examples per block, which is confirmed to be a good setting. In most of the dataset we tested, we achieve roughly a 26% to 29% compression ratio.",
            "page": 7,
            "x": 50,
            "y": 358,
            "width": 245,
            "height": 125,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "13-blocks",
            "chunk_id": "902c6834-6ee6-4965-b7a3-32a9f416dc5a",
            "group_text": "### 4.3  Blocks for Out-of-core Computation\n\nOne goal of our system is to fully utilize a machine\u2019s resources to achieve scalable learning. Besides processors and memory, it is important to utilize disk space to handle data that does not fit into main memory. To enable out-of-core computation, we divide the data into multiple blocks and store each block on disk. During computation, it is important to use an independent thread to pre-fetch the block into a main memory buffer, so computation can happen in concurrence with disk reading. However, this does not entirely solve the problem since the disk reading takes most of the computation time. It is important to reduce the overhead and increase the throughput of disk IO. We mainly use two techniques to improve the out-of-core computation.\n\n**Block Compression** The first technique we use is block compression. The block is compressed by columns, and decompressed on the fly by an independent thread when loading into main memory. This helps to trade some of the computation in decompression with the disk reading cost. We use a general purpose compression algorithm for compressing the features values. For the row index, we substract the row index by the begining index of the block and use a 16bit integer to store each offset. This requires $2^{16}$ examples per block, which is confirmed to be a good setting. In most of the dataset we tested, we achieve roughly a 26% to 29% compression ratio.\n\n**Block Sharding** The second technique is to shard the data\nonto multiple disks in an alternative manner. A pre-fetcher\nthread is assigned to each disk and fetches the data into an\nin-memory buffer. The training thread then alternatively\nreads the data from each buffer. This helps to increase the\nthroughput of disk reading when multiple disks are available."
        },
        {
            "text": "**Block Sharding** The second technique is to shard the data\nonto multiple disks in an alternative manner. A pre-fetcher\nthread is assigned to each disk and fetches the data into an\nin-memory buffer. The training thread then alternatively\nreads the data from each buffer. This helps to increase the\nthroughput of disk reading when multiple disks are available.",
            "page": 7,
            "x": 51,
            "y": 484,
            "width": 244,
            "height": 65,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "13-blocks",
            "chunk_id": "426fe202-8cc0-4506-a500-2eea0f9dadd7",
            "group_text": "### 4.3  Blocks for Out-of-core Computation\n\nOne goal of our system is to fully utilize a machine\u2019s resources to achieve scalable learning. Besides processors and memory, it is important to utilize disk space to handle data that does not fit into main memory. To enable out-of-core computation, we divide the data into multiple blocks and store each block on disk. During computation, it is important to use an independent thread to pre-fetch the block into a main memory buffer, so computation can happen in concurrence with disk reading. However, this does not entirely solve the problem since the disk reading takes most of the computation time. It is important to reduce the overhead and increase the throughput of disk IO. We mainly use two techniques to improve the out-of-core computation.\n\n**Block Compression** The first technique we use is block compression. The block is compressed by columns, and decompressed on the fly by an independent thread when loading into main memory. This helps to trade some of the computation in decompression with the disk reading cost. We use a general purpose compression algorithm for compressing the features values. For the row index, we substract the row index by the begining index of the block and use a 16bit integer to store each offset. This requires $2^{16}$ examples per block, which is confirmed to be a good setting. In most of the dataset we tested, we achieve roughly a 26% to 29% compression ratio.\n\n**Block Sharding** The second technique is to shard the data\nonto multiple disks in an alternative manner. A pre-fetcher\nthread is assigned to each disk and fetches the data into an\nin-memory buffer. The training thread then alternatively\nreads the data from each buffer. This helps to increase the\nthroughput of disk reading when multiple disks are available."
        },
        {
            "text": "5.  RELATED WORKS\n\n    Our system implements gradient boosting [10], which performs additive optimization in functional space.  Gradient tree boosting has been successfully used in classification [12], learning to rank [5], structured prediction [8] as well as other fields.  XGBoost incorporates a regularized model to prevent overfitting.  This this resembles previous work on regularized greedy forest [25], but simplifies the objective and algorithm for parallelization.  Column sampling is a simple but effective technique borrowed from RandomForest [4].  While sparsity-aware learning is essential in other types of models such as linear models [9], few works on tree learning have considered this topic in a principled way.  The algorithm proposed in this paper is the first unified approach to handle all kinds of sparsity patterns.",
            "page": 7,
            "x": 50,
            "y": 557,
            "width": 244,
            "height": 163,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "14-related",
            "chunk_id": "707ba0c3-671b-441d-bb6b-2c8d78fd42f4",
            "group_text": "5.  RELATED WORKS\n\n    Our system implements gradient boosting [10], which performs additive optimization in functional space.  Gradient tree boosting has been successfully used in classification [12], learning to rank [5], structured prediction [8] as well as other fields.  XGBoost incorporates a regularized model to prevent overfitting.  This this resembles previous work on regularized greedy forest [25], but simplifies the objective and algorithm for parallelization.  Column sampling is a simple but effective technique borrowed from RandomForest [4].  While sparsity-aware learning is essential in other types of models such as linear models [9], few works on tree learning have considered this topic in a principled way.  The algorithm proposed in this paper is the first unified approach to handle all kinds of sparsity patterns.\n\nThere are several existing works on parallelizing tree learning [22, 19]. Most of these algorithms fall into the approximate framework described in this paper. Notably, it is also possible to partition data by columns [23] and apply the exact greedy algorithm. This is also supported in our framework, and the techniques such as cache-aware prefetching can be used to benefit this type of algorithm. While most existing works focus on the algorithmic aspect of parallelization, our work improves in two unexplored system directions: out-of-core computation and cache-aware learning. This gives us insights on how the system and the algorithm can be jointly optimized and provides an end-to-end system that can handle large scale problems with very limited computing resources. We also summarize the comparison between our system and existing opensource implementations in Table 1.\n\nQuantile summary (without weights) is a classical problem in the database community [14, 24]. However, the approximate tree boosting algorithm reveals a more general problem \u2013 finding quantiles on weighted data. To the best of our knowledge, the weighted quantile sketch proposed in this paper is the first method to solve this problem. The weighted quantile summary is also not specific to the tree learning and can benefit other applications in data science and machine learning in the future."
        },
        {
            "text": "There are several existing works on parallelizing tree learning [22, 19]. Most of these algorithms fall into the approximate framework described in this paper. Notably, it is also possible to partition data by columns [23] and apply the exact greedy algorithm. This is also supported in our framework, and the techniques such as cache-aware prefetching can be used to benefit this type of algorithm. While most existing works focus on the algorithmic aspect of parallelization, our work improves in two unexplored system directions: out-of-core computation and cache-aware learning. This gives us insights on how the system and the algorithm can be jointly optimized and provides an end-to-end system that can handle large scale problems with very limited computing resources. We also summarize the comparison between our system and existing opensource implementations in Table 1.",
            "page": 7,
            "x": 314,
            "y": 179,
            "width": 244,
            "height": 168,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "14-related",
            "chunk_id": "842ca568-927a-45c7-a461-08a2c3cec06f",
            "group_text": "5.  RELATED WORKS\n\n    Our system implements gradient boosting [10], which performs additive optimization in functional space.  Gradient tree boosting has been successfully used in classification [12], learning to rank [5], structured prediction [8] as well as other fields.  XGBoost incorporates a regularized model to prevent overfitting.  This this resembles previous work on regularized greedy forest [25], but simplifies the objective and algorithm for parallelization.  Column sampling is a simple but effective technique borrowed from RandomForest [4].  While sparsity-aware learning is essential in other types of models such as linear models [9], few works on tree learning have considered this topic in a principled way.  The algorithm proposed in this paper is the first unified approach to handle all kinds of sparsity patterns.\n\nThere are several existing works on parallelizing tree learning [22, 19]. Most of these algorithms fall into the approximate framework described in this paper. Notably, it is also possible to partition data by columns [23] and apply the exact greedy algorithm. This is also supported in our framework, and the techniques such as cache-aware prefetching can be used to benefit this type of algorithm. While most existing works focus on the algorithmic aspect of parallelization, our work improves in two unexplored system directions: out-of-core computation and cache-aware learning. This gives us insights on how the system and the algorithm can be jointly optimized and provides an end-to-end system that can handle large scale problems with very limited computing resources. We also summarize the comparison between our system and existing opensource implementations in Table 1.\n\nQuantile summary (without weights) is a classical problem in the database community [14, 24]. However, the approximate tree boosting algorithm reveals a more general problem \u2013 finding quantiles on weighted data. To the best of our knowledge, the weighted quantile sketch proposed in this paper is the first method to solve this problem. The weighted quantile summary is also not specific to the tree learning and can benefit other applications in data science and machine learning in the future."
        },
        {
            "text": "Quantile summary (without weights) is a classical problem in the database community [14, 24]. However, the approximate tree boosting algorithm reveals a more general problem \u2013 finding quantiles on weighted data. To the best of our knowledge, the weighted quantile sketch proposed in this paper is the first method to solve this problem. The weighted quantile summary is also not specific to the tree learning and can benefit other applications in data science and machine learning in the future.",
            "page": 7,
            "x": 314,
            "y": 347,
            "width": 244,
            "height": 95,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "14-related",
            "chunk_id": "001cd85d-2ab0-4a04-ba4b-77cf51ee7362",
            "group_text": "5.  RELATED WORKS\n\n    Our system implements gradient boosting [10], which performs additive optimization in functional space.  Gradient tree boosting has been successfully used in classification [12], learning to rank [5], structured prediction [8] as well as other fields.  XGBoost incorporates a regularized model to prevent overfitting.  This this resembles previous work on regularized greedy forest [25], but simplifies the objective and algorithm for parallelization.  Column sampling is a simple but effective technique borrowed from RandomForest [4].  While sparsity-aware learning is essential in other types of models such as linear models [9], few works on tree learning have considered this topic in a principled way.  The algorithm proposed in this paper is the first unified approach to handle all kinds of sparsity patterns.\n\nThere are several existing works on parallelizing tree learning [22, 19]. Most of these algorithms fall into the approximate framework described in this paper. Notably, it is also possible to partition data by columns [23] and apply the exact greedy algorithm. This is also supported in our framework, and the techniques such as cache-aware prefetching can be used to benefit this type of algorithm. While most existing works focus on the algorithmic aspect of parallelization, our work improves in two unexplored system directions: out-of-core computation and cache-aware learning. This gives us insights on how the system and the algorithm can be jointly optimized and provides an end-to-end system that can handle large scale problems with very limited computing resources. We also summarize the comparison between our system and existing opensource implementations in Table 1.\n\nQuantile summary (without weights) is a classical problem in the database community [14, 24]. However, the approximate tree boosting algorithm reveals a more general problem \u2013 finding quantiles on weighted data. To the best of our knowledge, the weighted quantile sketch proposed in this paper is the first method to solve this problem. The weighted quantile summary is also not specific to the tree learning and can benefit other applications in data science and machine learning in the future."
        },
        {
            "text": "6.  END TO END EVALUATIONS\n\n6.1  System Implementation\n\n    We implemented XGBoost as an open source package\\textsuperscript{5}. The package is portable and reusable. It supports various weighted classification and rank objective functions, as well as user defined objective function. It is available in popular languages such as python, R, Julia and integrates naturally with language native data science pipelines such as scikit-learn. The distributed version is built on top of the rabit library\\textsuperscript{6} for allreduce. The portability of XGBoost makes it available in many ecosystems, instead of only being tied to a specific platform. The distributed XGBoost runs natively on Hadoop, MPI Sun Grid engine. Recently, we also enable distributed XGBoost on jvm bigdata stacks such as Flink and Spark. The distributed version has also been integrated into cloud platform Tianchi\\textsuperscript{7} of Alibaba. We believe that there will be more integrations in the future.",
            "page": 7,
            "x": 313,
            "y": 457,
            "width": 244,
            "height": 197,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "15-end",
            "chunk_id": "2d0f857f-1ef3-42ec-9c4d-093e8d294e8e",
            "group_text": "6.  END TO END EVALUATIONS\n\n6.1  System Implementation\n\n    We implemented XGBoost as an open source package\\textsuperscript{5}. The package is portable and reusable. It supports various weighted classification and rank objective functions, as well as user defined objective function. It is available in popular languages such as python, R, Julia and integrates naturally with language native data science pipelines such as scikit-learn. The distributed version is built on top of the rabit library\\textsuperscript{6} for allreduce. The portability of XGBoost makes it available in many ecosystems, instead of only being tied to a specific platform. The distributed XGBoost runs natively on Hadoop, MPI Sun Grid engine. Recently, we also enable distributed XGBoost on jvm bigdata stacks such as Flink and Spark. The distributed version has also been integrated into cloud platform Tianchi\\textsuperscript{7} of Alibaba. We believe that there will be more integrations in the future."
        },
        {
            "text": "## 6.2   Dataset and Setup\n\n\u2075https://github.com/dmlc/xgboost  \n\u2076https://github.com/dmlc/rabit  \n\u2077https://tianchi.aliyun.com",
            "page": 7,
            "x": 313,
            "y": 662,
            "width": 146,
            "height": 59,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "16-dataset",
            "chunk_id": "fd16c5e7-35dd-4034-b00b-0a97599f08a7",
            "group_text": "## 6.2   Dataset and Setup\n\n\u2075https://github.com/dmlc/xgboost  \n\u2076https://github.com/dmlc/rabit  \n\u2077https://tianchi.aliyun.com\n\nWe used four datasets in our experiments. A summary of these datasets is given in Table 2. In some of the experiments, we use a randomly selected subset of the data either due to slow baselines or to demonstrate the performance of the algorithm with varying dataset size. We use a suffix to denote the size in these cases. For example Allstate-10K means a subset of the Allstate dataset with 10K instances.\n\nThe first dataset we use is the Allstate insurance claim\ndataset\\textsuperscript{8}. The task is to predict the likelihood and cost of\nan insurance claim given different risk factors. In the experi-\nment, we simplified the task to only predict the likelihood\nof an insurance claim. This dataset is used to evaluate the\nimpact of sparsity-aware algorithm in Sec. 3.4. Most of the\nsparse features in this data come from one-hot encoding. We\nrandomly select 10M instances as training set and use the\nrest as evaluation set.\n\nThe second dataset is the Higgs boson dataset\\textsuperscript{9} from high\nenergy physics. The data was produced using Monte Carlo\nsimulations of physics events. It contains 21 kinematic prop-\nerties measured by the particle detectors in the accelerator.\nIt also contains seven additional derived physics quantities\nof the particles. The task is to classify whether an event\ncorresponds to the Higgs boson. We randomly select 10M\ninstances as training set and use the rest as evaluation set.\n\nThe third dataset is the Yahoo! learning to rank challenge\ndataset [6], which is one of the most commonly used bench-\nmarks in learning to rank algorithms. The dataset contains\n20K web search queries, with each query corresponding to a\nlist of around 22 documents. The task is to rank the doc-\numents according to relevance of the query. We use the official\ntrain test split in our experiment.\n\nThe last dataset is the criteo terabyte click log dataset\\textsuperscript{10}.\nWe use this dataset to evaluate the scaling property of the\nsystem in the out-of-core and the distributed settings. The\ndata contains 13 integer features and 26 ID features of user,\nitem and advertiser information. Since a tree based model\nis better at handling continuous features, we preprocess the\ndata by calculating the statistics of average CTR and count\nof ID features on the first ten days, replacing the ID fea-\ntures by the corresponding count statistics during the next\nten days for training. The training set after preprocessing\ncontains 1.7 billion instances with 67 features (13 integer, 26\naverage CTR statistics and 26 counts). The entire dataset\nis more than one terabyte in LibSVM format.\n\nWe use the first three datasets for the single machine parallel setting, and the last dataset for the distributed and out-of-core settings. All the single machine experiments are conducted on a Dell PowerEdge R420 with two eight-core Intel Xeon (E5-2470) (2.3GHz) and 64GB of memory. If not specified, all the experiments are run using all the avail-\n\n\u2078https://www.kaggle.com/c/ClaimPredictionChallenge  \n\u2079https://archive.ics.uci.edu/ml/datasets/HIGGS  \n\u00b9\u2070http://labs.criteo.com/downloads/download-terabyte-click-logs/\n\nable cores in the machine.  The machine settings of the dis-\ntributed and the out-of-core experiments will be described in\nthe corresponding section.  In all the experiments, we boost\ntrees with a common setting of maximum depth equals 8,\nshrinkage equals 0.1 and no column subsampling unless ex-\nplicitly specified.  We can find similar results when we use\nother settings of maximum depth."
        },
        {
            "text": "We used four datasets in our experiments. A summary of these datasets is given in Table 2. In some of the experiments, we use a randomly selected subset of the data either due to slow baselines or to demonstrate the performance of the algorithm with varying dataset size. We use a suffix to denote the size in these cases. For example Allstate-10K means a subset of the Allstate dataset with 10K instances.",
            "page": 8,
            "x": 51,
            "y": 148,
            "width": 244,
            "height": 73,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "16-dataset",
            "chunk_id": "5e69a77b-ae9e-4c75-8409-bdb67e02f7ae",
            "group_text": "## 6.2   Dataset and Setup\n\n\u2075https://github.com/dmlc/xgboost  \n\u2076https://github.com/dmlc/rabit  \n\u2077https://tianchi.aliyun.com\n\nWe used four datasets in our experiments. A summary of these datasets is given in Table 2. In some of the experiments, we use a randomly selected subset of the data either due to slow baselines or to demonstrate the performance of the algorithm with varying dataset size. We use a suffix to denote the size in these cases. For example Allstate-10K means a subset of the Allstate dataset with 10K instances.\n\nThe first dataset we use is the Allstate insurance claim\ndataset\\textsuperscript{8}. The task is to predict the likelihood and cost of\nan insurance claim given different risk factors. In the experi-\nment, we simplified the task to only predict the likelihood\nof an insurance claim. This dataset is used to evaluate the\nimpact of sparsity-aware algorithm in Sec. 3.4. Most of the\nsparse features in this data come from one-hot encoding. We\nrandomly select 10M instances as training set and use the\nrest as evaluation set.\n\nThe second dataset is the Higgs boson dataset\\textsuperscript{9} from high\nenergy physics. The data was produced using Monte Carlo\nsimulations of physics events. It contains 21 kinematic prop-\nerties measured by the particle detectors in the accelerator.\nIt also contains seven additional derived physics quantities\nof the particles. The task is to classify whether an event\ncorresponds to the Higgs boson. We randomly select 10M\ninstances as training set and use the rest as evaluation set.\n\nThe third dataset is the Yahoo! learning to rank challenge\ndataset [6], which is one of the most commonly used bench-\nmarks in learning to rank algorithms. The dataset contains\n20K web search queries, with each query corresponding to a\nlist of around 22 documents. The task is to rank the doc-\numents according to relevance of the query. We use the official\ntrain test split in our experiment.\n\nThe last dataset is the criteo terabyte click log dataset\\textsuperscript{10}.\nWe use this dataset to evaluate the scaling property of the\nsystem in the out-of-core and the distributed settings. The\ndata contains 13 integer features and 26 ID features of user,\nitem and advertiser information. Since a tree based model\nis better at handling continuous features, we preprocess the\ndata by calculating the statistics of average CTR and count\nof ID features on the first ten days, replacing the ID fea-\ntures by the corresponding count statistics during the next\nten days for training. The training set after preprocessing\ncontains 1.7 billion instances with 67 features (13 integer, 26\naverage CTR statistics and 26 counts). The entire dataset\nis more than one terabyte in LibSVM format.\n\nWe use the first three datasets for the single machine parallel setting, and the last dataset for the distributed and out-of-core settings. All the single machine experiments are conducted on a Dell PowerEdge R420 with two eight-core Intel Xeon (E5-2470) (2.3GHz) and 64GB of memory. If not specified, all the experiments are run using all the avail-\n\n\u2078https://www.kaggle.com/c/ClaimPredictionChallenge  \n\u2079https://archive.ics.uci.edu/ml/datasets/HIGGS  \n\u00b9\u2070http://labs.criteo.com/downloads/download-terabyte-click-logs/\n\nable cores in the machine.  The machine settings of the dis-\ntributed and the out-of-core experiments will be described in\nthe corresponding section.  In all the experiments, we boost\ntrees with a common setting of maximum depth equals 8,\nshrinkage equals 0.1 and no column subsampling unless ex-\nplicitly specified.  We can find similar results when we use\nother settings of maximum depth."
        },
        {
            "text": "The first dataset we use is the Allstate insurance claim\ndataset\\textsuperscript{8}. The task is to predict the likelihood and cost of\nan insurance claim given different risk factors. In the experi-\nment, we simplified the task to only predict the likelihood\nof an insurance claim. This dataset is used to evaluate the\nimpact of sparsity-aware algorithm in Sec. 3.4. Most of the\nsparse features in this data come from one-hot encoding. We\nrandomly select 10M instances as training set and use the\nrest as evaluation set.",
            "page": 8,
            "x": 51,
            "y": 222,
            "width": 243,
            "height": 94,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "16-dataset",
            "chunk_id": "de216b45-b923-47b6-82ce-8bbec290c221",
            "group_text": "## 6.2   Dataset and Setup\n\n\u2075https://github.com/dmlc/xgboost  \n\u2076https://github.com/dmlc/rabit  \n\u2077https://tianchi.aliyun.com\n\nWe used four datasets in our experiments. A summary of these datasets is given in Table 2. In some of the experiments, we use a randomly selected subset of the data either due to slow baselines or to demonstrate the performance of the algorithm with varying dataset size. We use a suffix to denote the size in these cases. For example Allstate-10K means a subset of the Allstate dataset with 10K instances.\n\nThe first dataset we use is the Allstate insurance claim\ndataset\\textsuperscript{8}. The task is to predict the likelihood and cost of\nan insurance claim given different risk factors. In the experi-\nment, we simplified the task to only predict the likelihood\nof an insurance claim. This dataset is used to evaluate the\nimpact of sparsity-aware algorithm in Sec. 3.4. Most of the\nsparse features in this data come from one-hot encoding. We\nrandomly select 10M instances as training set and use the\nrest as evaluation set.\n\nThe second dataset is the Higgs boson dataset\\textsuperscript{9} from high\nenergy physics. The data was produced using Monte Carlo\nsimulations of physics events. It contains 21 kinematic prop-\nerties measured by the particle detectors in the accelerator.\nIt also contains seven additional derived physics quantities\nof the particles. The task is to classify whether an event\ncorresponds to the Higgs boson. We randomly select 10M\ninstances as training set and use the rest as evaluation set.\n\nThe third dataset is the Yahoo! learning to rank challenge\ndataset [6], which is one of the most commonly used bench-\nmarks in learning to rank algorithms. The dataset contains\n20K web search queries, with each query corresponding to a\nlist of around 22 documents. The task is to rank the doc-\numents according to relevance of the query. We use the official\ntrain test split in our experiment.\n\nThe last dataset is the criteo terabyte click log dataset\\textsuperscript{10}.\nWe use this dataset to evaluate the scaling property of the\nsystem in the out-of-core and the distributed settings. The\ndata contains 13 integer features and 26 ID features of user,\nitem and advertiser information. Since a tree based model\nis better at handling continuous features, we preprocess the\ndata by calculating the statistics of average CTR and count\nof ID features on the first ten days, replacing the ID fea-\ntures by the corresponding count statistics during the next\nten days for training. The training set after preprocessing\ncontains 1.7 billion instances with 67 features (13 integer, 26\naverage CTR statistics and 26 counts). The entire dataset\nis more than one terabyte in LibSVM format.\n\nWe use the first three datasets for the single machine parallel setting, and the last dataset for the distributed and out-of-core settings. All the single machine experiments are conducted on a Dell PowerEdge R420 with two eight-core Intel Xeon (E5-2470) (2.3GHz) and 64GB of memory. If not specified, all the experiments are run using all the avail-\n\n\u2078https://www.kaggle.com/c/ClaimPredictionChallenge  \n\u2079https://archive.ics.uci.edu/ml/datasets/HIGGS  \n\u00b9\u2070http://labs.criteo.com/downloads/download-terabyte-click-logs/\n\nable cores in the machine.  The machine settings of the dis-\ntributed and the out-of-core experiments will be described in\nthe corresponding section.  In all the experiments, we boost\ntrees with a common setting of maximum depth equals 8,\nshrinkage equals 0.1 and no column subsampling unless ex-\nplicitly specified.  We can find similar results when we use\nother settings of maximum depth."
        },
        {
            "text": "The second dataset is the Higgs boson dataset\\textsuperscript{9} from high\nenergy physics. The data was produced using Monte Carlo\nsimulations of physics events. It contains 21 kinematic prop-\nerties measured by the particle detectors in the accelerator.\nIt also contains seven additional derived physics quantities\nof the particles. The task is to classify whether an event\ncorresponds to the Higgs boson. We randomly select 10M\ninstances as training set and use the rest as evaluation set.",
            "page": 8,
            "x": 52,
            "y": 316,
            "width": 242,
            "height": 83,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "16-dataset",
            "chunk_id": "4a755c0f-c1c7-4ed5-9651-a7aca0073b26",
            "group_text": "## 6.2   Dataset and Setup\n\n\u2075https://github.com/dmlc/xgboost  \n\u2076https://github.com/dmlc/rabit  \n\u2077https://tianchi.aliyun.com\n\nWe used four datasets in our experiments. A summary of these datasets is given in Table 2. In some of the experiments, we use a randomly selected subset of the data either due to slow baselines or to demonstrate the performance of the algorithm with varying dataset size. We use a suffix to denote the size in these cases. For example Allstate-10K means a subset of the Allstate dataset with 10K instances.\n\nThe first dataset we use is the Allstate insurance claim\ndataset\\textsuperscript{8}. The task is to predict the likelihood and cost of\nan insurance claim given different risk factors. In the experi-\nment, we simplified the task to only predict the likelihood\nof an insurance claim. This dataset is used to evaluate the\nimpact of sparsity-aware algorithm in Sec. 3.4. Most of the\nsparse features in this data come from one-hot encoding. We\nrandomly select 10M instances as training set and use the\nrest as evaluation set.\n\nThe second dataset is the Higgs boson dataset\\textsuperscript{9} from high\nenergy physics. The data was produced using Monte Carlo\nsimulations of physics events. It contains 21 kinematic prop-\nerties measured by the particle detectors in the accelerator.\nIt also contains seven additional derived physics quantities\nof the particles. The task is to classify whether an event\ncorresponds to the Higgs boson. We randomly select 10M\ninstances as training set and use the rest as evaluation set.\n\nThe third dataset is the Yahoo! learning to rank challenge\ndataset [6], which is one of the most commonly used bench-\nmarks in learning to rank algorithms. The dataset contains\n20K web search queries, with each query corresponding to a\nlist of around 22 documents. The task is to rank the doc-\numents according to relevance of the query. We use the official\ntrain test split in our experiment.\n\nThe last dataset is the criteo terabyte click log dataset\\textsuperscript{10}.\nWe use this dataset to evaluate the scaling property of the\nsystem in the out-of-core and the distributed settings. The\ndata contains 13 integer features and 26 ID features of user,\nitem and advertiser information. Since a tree based model\nis better at handling continuous features, we preprocess the\ndata by calculating the statistics of average CTR and count\nof ID features on the first ten days, replacing the ID fea-\ntures by the corresponding count statistics during the next\nten days for training. The training set after preprocessing\ncontains 1.7 billion instances with 67 features (13 integer, 26\naverage CTR statistics and 26 counts). The entire dataset\nis more than one terabyte in LibSVM format.\n\nWe use the first three datasets for the single machine parallel setting, and the last dataset for the distributed and out-of-core settings. All the single machine experiments are conducted on a Dell PowerEdge R420 with two eight-core Intel Xeon (E5-2470) (2.3GHz) and 64GB of memory. If not specified, all the experiments are run using all the avail-\n\n\u2078https://www.kaggle.com/c/ClaimPredictionChallenge  \n\u2079https://archive.ics.uci.edu/ml/datasets/HIGGS  \n\u00b9\u2070http://labs.criteo.com/downloads/download-terabyte-click-logs/\n\nable cores in the machine.  The machine settings of the dis-\ntributed and the out-of-core experiments will be described in\nthe corresponding section.  In all the experiments, we boost\ntrees with a common setting of maximum depth equals 8,\nshrinkage equals 0.1 and no column subsampling unless ex-\nplicitly specified.  We can find similar results when we use\nother settings of maximum depth."
        },
        {
            "text": "The third dataset is the Yahoo! learning to rank challenge\ndataset [6], which is one of the most commonly used bench-\nmarks in learning to rank algorithms. The dataset contains\n20K web search queries, with each query corresponding to a\nlist of around 22 documents. The task is to rank the doc-\numents according to relevance of the query. We use the official\ntrain test split in our experiment.",
            "page": 8,
            "x": 52,
            "y": 398,
            "width": 241,
            "height": 74,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "16-dataset",
            "chunk_id": "080396fe-91ff-4700-a3ec-6314589cf1cb",
            "group_text": "## 6.2   Dataset and Setup\n\n\u2075https://github.com/dmlc/xgboost  \n\u2076https://github.com/dmlc/rabit  \n\u2077https://tianchi.aliyun.com\n\nWe used four datasets in our experiments. A summary of these datasets is given in Table 2. In some of the experiments, we use a randomly selected subset of the data either due to slow baselines or to demonstrate the performance of the algorithm with varying dataset size. We use a suffix to denote the size in these cases. For example Allstate-10K means a subset of the Allstate dataset with 10K instances.\n\nThe first dataset we use is the Allstate insurance claim\ndataset\\textsuperscript{8}. The task is to predict the likelihood and cost of\nan insurance claim given different risk factors. In the experi-\nment, we simplified the task to only predict the likelihood\nof an insurance claim. This dataset is used to evaluate the\nimpact of sparsity-aware algorithm in Sec. 3.4. Most of the\nsparse features in this data come from one-hot encoding. We\nrandomly select 10M instances as training set and use the\nrest as evaluation set.\n\nThe second dataset is the Higgs boson dataset\\textsuperscript{9} from high\nenergy physics. The data was produced using Monte Carlo\nsimulations of physics events. It contains 21 kinematic prop-\nerties measured by the particle detectors in the accelerator.\nIt also contains seven additional derived physics quantities\nof the particles. The task is to classify whether an event\ncorresponds to the Higgs boson. We randomly select 10M\ninstances as training set and use the rest as evaluation set.\n\nThe third dataset is the Yahoo! learning to rank challenge\ndataset [6], which is one of the most commonly used bench-\nmarks in learning to rank algorithms. The dataset contains\n20K web search queries, with each query corresponding to a\nlist of around 22 documents. The task is to rank the doc-\numents according to relevance of the query. We use the official\ntrain test split in our experiment.\n\nThe last dataset is the criteo terabyte click log dataset\\textsuperscript{10}.\nWe use this dataset to evaluate the scaling property of the\nsystem in the out-of-core and the distributed settings. The\ndata contains 13 integer features and 26 ID features of user,\nitem and advertiser information. Since a tree based model\nis better at handling continuous features, we preprocess the\ndata by calculating the statistics of average CTR and count\nof ID features on the first ten days, replacing the ID fea-\ntures by the corresponding count statistics during the next\nten days for training. The training set after preprocessing\ncontains 1.7 billion instances with 67 features (13 integer, 26\naverage CTR statistics and 26 counts). The entire dataset\nis more than one terabyte in LibSVM format.\n\nWe use the first three datasets for the single machine parallel setting, and the last dataset for the distributed and out-of-core settings. All the single machine experiments are conducted on a Dell PowerEdge R420 with two eight-core Intel Xeon (E5-2470) (2.3GHz) and 64GB of memory. If not specified, all the experiments are run using all the avail-\n\n\u2078https://www.kaggle.com/c/ClaimPredictionChallenge  \n\u2079https://archive.ics.uci.edu/ml/datasets/HIGGS  \n\u00b9\u2070http://labs.criteo.com/downloads/download-terabyte-click-logs/\n\nable cores in the machine.  The machine settings of the dis-\ntributed and the out-of-core experiments will be described in\nthe corresponding section.  In all the experiments, we boost\ntrees with a common setting of maximum depth equals 8,\nshrinkage equals 0.1 and no column subsampling unless ex-\nplicitly specified.  We can find similar results when we use\nother settings of maximum depth."
        },
        {
            "text": "The last dataset is the criteo terabyte click log dataset\\textsuperscript{10}.\nWe use this dataset to evaluate the scaling property of the\nsystem in the out-of-core and the distributed settings. The\ndata contains 13 integer features and 26 ID features of user,\nitem and advertiser information. Since a tree based model\nis better at handling continuous features, we preprocess the\ndata by calculating the statistics of average CTR and count\nof ID features on the first ten days, replacing the ID fea-\ntures by the corresponding count statistics during the next\nten days for training. The training set after preprocessing\ncontains 1.7 billion instances with 67 features (13 integer, 26\naverage CTR statistics and 26 counts). The entire dataset\nis more than one terabyte in LibSVM format.",
            "page": 8,
            "x": 52,
            "y": 473,
            "width": 241,
            "height": 136,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "16-dataset",
            "chunk_id": "3e014888-a7e5-42b1-818b-8b3fddd81966",
            "group_text": "## 6.2   Dataset and Setup\n\n\u2075https://github.com/dmlc/xgboost  \n\u2076https://github.com/dmlc/rabit  \n\u2077https://tianchi.aliyun.com\n\nWe used four datasets in our experiments. A summary of these datasets is given in Table 2. In some of the experiments, we use a randomly selected subset of the data either due to slow baselines or to demonstrate the performance of the algorithm with varying dataset size. We use a suffix to denote the size in these cases. For example Allstate-10K means a subset of the Allstate dataset with 10K instances.\n\nThe first dataset we use is the Allstate insurance claim\ndataset\\textsuperscript{8}. The task is to predict the likelihood and cost of\nan insurance claim given different risk factors. In the experi-\nment, we simplified the task to only predict the likelihood\nof an insurance claim. This dataset is used to evaluate the\nimpact of sparsity-aware algorithm in Sec. 3.4. Most of the\nsparse features in this data come from one-hot encoding. We\nrandomly select 10M instances as training set and use the\nrest as evaluation set.\n\nThe second dataset is the Higgs boson dataset\\textsuperscript{9} from high\nenergy physics. The data was produced using Monte Carlo\nsimulations of physics events. It contains 21 kinematic prop-\nerties measured by the particle detectors in the accelerator.\nIt also contains seven additional derived physics quantities\nof the particles. The task is to classify whether an event\ncorresponds to the Higgs boson. We randomly select 10M\ninstances as training set and use the rest as evaluation set.\n\nThe third dataset is the Yahoo! learning to rank challenge\ndataset [6], which is one of the most commonly used bench-\nmarks in learning to rank algorithms. The dataset contains\n20K web search queries, with each query corresponding to a\nlist of around 22 documents. The task is to rank the doc-\numents according to relevance of the query. We use the official\ntrain test split in our experiment.\n\nThe last dataset is the criteo terabyte click log dataset\\textsuperscript{10}.\nWe use this dataset to evaluate the scaling property of the\nsystem in the out-of-core and the distributed settings. The\ndata contains 13 integer features and 26 ID features of user,\nitem and advertiser information. Since a tree based model\nis better at handling continuous features, we preprocess the\ndata by calculating the statistics of average CTR and count\nof ID features on the first ten days, replacing the ID fea-\ntures by the corresponding count statistics during the next\nten days for training. The training set after preprocessing\ncontains 1.7 billion instances with 67 features (13 integer, 26\naverage CTR statistics and 26 counts). The entire dataset\nis more than one terabyte in LibSVM format.\n\nWe use the first three datasets for the single machine parallel setting, and the last dataset for the distributed and out-of-core settings. All the single machine experiments are conducted on a Dell PowerEdge R420 with two eight-core Intel Xeon (E5-2470) (2.3GHz) and 64GB of memory. If not specified, all the experiments are run using all the avail-\n\n\u2078https://www.kaggle.com/c/ClaimPredictionChallenge  \n\u2079https://archive.ics.uci.edu/ml/datasets/HIGGS  \n\u00b9\u2070http://labs.criteo.com/downloads/download-terabyte-click-logs/\n\nable cores in the machine.  The machine settings of the dis-\ntributed and the out-of-core experiments will be described in\nthe corresponding section.  In all the experiments, we boost\ntrees with a common setting of maximum depth equals 8,\nshrinkage equals 0.1 and no column subsampling unless ex-\nplicitly specified.  We can find similar results when we use\nother settings of maximum depth."
        },
        {
            "text": "We use the first three datasets for the single machine parallel setting, and the last dataset for the distributed and out-of-core settings. All the single machine experiments are conducted on a Dell PowerEdge R420 with two eight-core Intel Xeon (E5-2470) (2.3GHz) and 64GB of memory. If not specified, all the experiments are run using all the avail-",
            "page": 8,
            "x": 52,
            "y": 609,
            "width": 241,
            "height": 63,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "16-dataset",
            "chunk_id": "2fcfdc95-3b81-492f-b6f7-72cbac417dd8",
            "group_text": "## 6.2   Dataset and Setup\n\n\u2075https://github.com/dmlc/xgboost  \n\u2076https://github.com/dmlc/rabit  \n\u2077https://tianchi.aliyun.com\n\nWe used four datasets in our experiments. A summary of these datasets is given in Table 2. In some of the experiments, we use a randomly selected subset of the data either due to slow baselines or to demonstrate the performance of the algorithm with varying dataset size. We use a suffix to denote the size in these cases. For example Allstate-10K means a subset of the Allstate dataset with 10K instances.\n\nThe first dataset we use is the Allstate insurance claim\ndataset\\textsuperscript{8}. The task is to predict the likelihood and cost of\nan insurance claim given different risk factors. In the experi-\nment, we simplified the task to only predict the likelihood\nof an insurance claim. This dataset is used to evaluate the\nimpact of sparsity-aware algorithm in Sec. 3.4. Most of the\nsparse features in this data come from one-hot encoding. We\nrandomly select 10M instances as training set and use the\nrest as evaluation set.\n\nThe second dataset is the Higgs boson dataset\\textsuperscript{9} from high\nenergy physics. The data was produced using Monte Carlo\nsimulations of physics events. It contains 21 kinematic prop-\nerties measured by the particle detectors in the accelerator.\nIt also contains seven additional derived physics quantities\nof the particles. The task is to classify whether an event\ncorresponds to the Higgs boson. We randomly select 10M\ninstances as training set and use the rest as evaluation set.\n\nThe third dataset is the Yahoo! learning to rank challenge\ndataset [6], which is one of the most commonly used bench-\nmarks in learning to rank algorithms. The dataset contains\n20K web search queries, with each query corresponding to a\nlist of around 22 documents. The task is to rank the doc-\numents according to relevance of the query. We use the official\ntrain test split in our experiment.\n\nThe last dataset is the criteo terabyte click log dataset\\textsuperscript{10}.\nWe use this dataset to evaluate the scaling property of the\nsystem in the out-of-core and the distributed settings. The\ndata contains 13 integer features and 26 ID features of user,\nitem and advertiser information. Since a tree based model\nis better at handling continuous features, we preprocess the\ndata by calculating the statistics of average CTR and count\nof ID features on the first ten days, replacing the ID fea-\ntures by the corresponding count statistics during the next\nten days for training. The training set after preprocessing\ncontains 1.7 billion instances with 67 features (13 integer, 26\naverage CTR statistics and 26 counts). The entire dataset\nis more than one terabyte in LibSVM format.\n\nWe use the first three datasets for the single machine parallel setting, and the last dataset for the distributed and out-of-core settings. All the single machine experiments are conducted on a Dell PowerEdge R420 with two eight-core Intel Xeon (E5-2470) (2.3GHz) and 64GB of memory. If not specified, all the experiments are run using all the avail-\n\n\u2078https://www.kaggle.com/c/ClaimPredictionChallenge  \n\u2079https://archive.ics.uci.edu/ml/datasets/HIGGS  \n\u00b9\u2070http://labs.criteo.com/downloads/download-terabyte-click-logs/\n\nable cores in the machine.  The machine settings of the dis-\ntributed and the out-of-core experiments will be described in\nthe corresponding section.  In all the experiments, we boost\ntrees with a common setting of maximum depth equals 8,\nshrinkage equals 0.1 and no column subsampling unless ex-\nplicitly specified.  We can find similar results when we use\nother settings of maximum depth."
        },
        {
            "text": "\u2078https://www.kaggle.com/c/ClaimPredictionChallenge  \n\u2079https://archive.ics.uci.edu/ml/datasets/HIGGS  \n\u00b9\u2070http://labs.criteo.com/downloads/download-terabyte-click-logs/",
            "page": 8,
            "x": 51,
            "y": 678,
            "width": 224,
            "height": 41,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "16-dataset",
            "chunk_id": "1c5ed86b-6c2c-42c6-88bd-1bf7365b1186",
            "group_text": "## 6.2   Dataset and Setup\n\n\u2075https://github.com/dmlc/xgboost  \n\u2076https://github.com/dmlc/rabit  \n\u2077https://tianchi.aliyun.com\n\nWe used four datasets in our experiments. A summary of these datasets is given in Table 2. In some of the experiments, we use a randomly selected subset of the data either due to slow baselines or to demonstrate the performance of the algorithm with varying dataset size. We use a suffix to denote the size in these cases. For example Allstate-10K means a subset of the Allstate dataset with 10K instances.\n\nThe first dataset we use is the Allstate insurance claim\ndataset\\textsuperscript{8}. The task is to predict the likelihood and cost of\nan insurance claim given different risk factors. In the experi-\nment, we simplified the task to only predict the likelihood\nof an insurance claim. This dataset is used to evaluate the\nimpact of sparsity-aware algorithm in Sec. 3.4. Most of the\nsparse features in this data come from one-hot encoding. We\nrandomly select 10M instances as training set and use the\nrest as evaluation set.\n\nThe second dataset is the Higgs boson dataset\\textsuperscript{9} from high\nenergy physics. The data was produced using Monte Carlo\nsimulations of physics events. It contains 21 kinematic prop-\nerties measured by the particle detectors in the accelerator.\nIt also contains seven additional derived physics quantities\nof the particles. The task is to classify whether an event\ncorresponds to the Higgs boson. We randomly select 10M\ninstances as training set and use the rest as evaluation set.\n\nThe third dataset is the Yahoo! learning to rank challenge\ndataset [6], which is one of the most commonly used bench-\nmarks in learning to rank algorithms. The dataset contains\n20K web search queries, with each query corresponding to a\nlist of around 22 documents. The task is to rank the doc-\numents according to relevance of the query. We use the official\ntrain test split in our experiment.\n\nThe last dataset is the criteo terabyte click log dataset\\textsuperscript{10}.\nWe use this dataset to evaluate the scaling property of the\nsystem in the out-of-core and the distributed settings. The\ndata contains 13 integer features and 26 ID features of user,\nitem and advertiser information. Since a tree based model\nis better at handling continuous features, we preprocess the\ndata by calculating the statistics of average CTR and count\nof ID features on the first ten days, replacing the ID fea-\ntures by the corresponding count statistics during the next\nten days for training. The training set after preprocessing\ncontains 1.7 billion instances with 67 features (13 integer, 26\naverage CTR statistics and 26 counts). The entire dataset\nis more than one terabyte in LibSVM format.\n\nWe use the first three datasets for the single machine parallel setting, and the last dataset for the distributed and out-of-core settings. All the single machine experiments are conducted on a Dell PowerEdge R420 with two eight-core Intel Xeon (E5-2470) (2.3GHz) and 64GB of memory. If not specified, all the experiments are run using all the avail-\n\n\u2078https://www.kaggle.com/c/ClaimPredictionChallenge  \n\u2079https://archive.ics.uci.edu/ml/datasets/HIGGS  \n\u00b9\u2070http://labs.criteo.com/downloads/download-terabyte-click-logs/\n\nable cores in the machine.  The machine settings of the dis-\ntributed and the out-of-core experiments will be described in\nthe corresponding section.  In all the experiments, we boost\ntrees with a common setting of maximum depth equals 8,\nshrinkage equals 0.1 and no column subsampling unless ex-\nplicitly specified.  We can find similar results when we use\nother settings of maximum depth."
        },
        {
            "text": "able cores in the machine.  The machine settings of the dis-\ntributed and the out-of-core experiments will be described in\nthe corresponding section.  In all the experiments, we boost\ntrees with a common setting of maximum depth equals 8,\nshrinkage equals 0.1 and no column subsampling unless ex-\nplicitly specified.  We can find similar results when we use\nother settings of maximum depth.",
            "page": 8,
            "x": 315,
            "y": 403,
            "width": 241,
            "height": 74,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "16-dataset",
            "chunk_id": "4dc234fc-8403-4083-ad26-5ada7aa2295f",
            "group_text": "## 6.2   Dataset and Setup\n\n\u2075https://github.com/dmlc/xgboost  \n\u2076https://github.com/dmlc/rabit  \n\u2077https://tianchi.aliyun.com\n\nWe used four datasets in our experiments. A summary of these datasets is given in Table 2. In some of the experiments, we use a randomly selected subset of the data either due to slow baselines or to demonstrate the performance of the algorithm with varying dataset size. We use a suffix to denote the size in these cases. For example Allstate-10K means a subset of the Allstate dataset with 10K instances.\n\nThe first dataset we use is the Allstate insurance claim\ndataset\\textsuperscript{8}. The task is to predict the likelihood and cost of\nan insurance claim given different risk factors. In the experi-\nment, we simplified the task to only predict the likelihood\nof an insurance claim. This dataset is used to evaluate the\nimpact of sparsity-aware algorithm in Sec. 3.4. Most of the\nsparse features in this data come from one-hot encoding. We\nrandomly select 10M instances as training set and use the\nrest as evaluation set.\n\nThe second dataset is the Higgs boson dataset\\textsuperscript{9} from high\nenergy physics. The data was produced using Monte Carlo\nsimulations of physics events. It contains 21 kinematic prop-\nerties measured by the particle detectors in the accelerator.\nIt also contains seven additional derived physics quantities\nof the particles. The task is to classify whether an event\ncorresponds to the Higgs boson. We randomly select 10M\ninstances as training set and use the rest as evaluation set.\n\nThe third dataset is the Yahoo! learning to rank challenge\ndataset [6], which is one of the most commonly used bench-\nmarks in learning to rank algorithms. The dataset contains\n20K web search queries, with each query corresponding to a\nlist of around 22 documents. The task is to rank the doc-\numents according to relevance of the query. We use the official\ntrain test split in our experiment.\n\nThe last dataset is the criteo terabyte click log dataset\\textsuperscript{10}.\nWe use this dataset to evaluate the scaling property of the\nsystem in the out-of-core and the distributed settings. The\ndata contains 13 integer features and 26 ID features of user,\nitem and advertiser information. Since a tree based model\nis better at handling continuous features, we preprocess the\ndata by calculating the statistics of average CTR and count\nof ID features on the first ten days, replacing the ID fea-\ntures by the corresponding count statistics during the next\nten days for training. The training set after preprocessing\ncontains 1.7 billion instances with 67 features (13 integer, 26\naverage CTR statistics and 26 counts). The entire dataset\nis more than one terabyte in LibSVM format.\n\nWe use the first three datasets for the single machine parallel setting, and the last dataset for the distributed and out-of-core settings. All the single machine experiments are conducted on a Dell PowerEdge R420 with two eight-core Intel Xeon (E5-2470) (2.3GHz) and 64GB of memory. If not specified, all the experiments are run using all the avail-\n\n\u2078https://www.kaggle.com/c/ClaimPredictionChallenge  \n\u2079https://archive.ics.uci.edu/ml/datasets/HIGGS  \n\u00b9\u2070http://labs.criteo.com/downloads/download-terabyte-click-logs/\n\nable cores in the machine.  The machine settings of the dis-\ntributed and the out-of-core experiments will be described in\nthe corresponding section.  In all the experiments, we boost\ntrees with a common setting of maximum depth equals 8,\nshrinkage equals 0.1 and no column subsampling unless ex-\nplicitly specified.  We can find similar results when we use\nother settings of maximum depth."
        },
        {
            "text": "6.3 Classification\n\nIn this section, we evaluate the performance of XGBoost\non a single machine using the exact greedy algorithm on\nHiggs-1M data, by comparing it against two other commonly\nused exact greedy tree boosting implementations. Since\nscikit-learn only handles non-sparse input, we choose the\ndense Higgs dataset for a fair comparison. We use the 1M\nsubset to make scikit-learn finish running in reasonable time.\nAmong the methods in comparison, R\u2019s GBM uses a greedy\napproach that only expands one branch of a tree, which\nmakes it faster but can result in lower accuracy, while both\nscikit-learn and XGBoost learn a full tree. The results are\nshown in Table 3. Both XGBoost and scikit-learn give better\nperformance than R\u2019s GBM, while XGBoost runs more than\n10x faster than scikit-learn. In this experiment, we also find\ncolumn subsamples gives slightly worse performance than\nusing all the features. This could due to the fact that there\nare few important features in this dataset and we can benefit\nfrom greedily select from all the features.",
            "page": 8,
            "x": 315,
            "y": 483,
            "width": 242,
            "height": 204,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "17-classification",
            "chunk_id": "54267701-b54c-4380-be86-1470f342d217",
            "group_text": "6.3 Classification\n\nIn this section, we evaluate the performance of XGBoost\non a single machine using the exact greedy algorithm on\nHiggs-1M data, by comparing it against two other commonly\nused exact greedy tree boosting implementations. Since\nscikit-learn only handles non-sparse input, we choose the\ndense Higgs dataset for a fair comparison. We use the 1M\nsubset to make scikit-learn finish running in reasonable time.\nAmong the methods in comparison, R\u2019s GBM uses a greedy\napproach that only expands one branch of a tree, which\nmakes it faster but can result in lower accuracy, while both\nscikit-learn and XGBoost learn a full tree. The results are\nshown in Table 3. Both XGBoost and scikit-learn give better\nperformance than R\u2019s GBM, while XGBoost runs more than\n10x faster than scikit-learn. In this experiment, we also find\ncolumn subsamples gives slightly worse performance than\nusing all the features. This could due to the fact that there\nare few important features in this dataset and we can benefit\nfrom greedily select from all the features.\n\n**6.4  Learning to Rank**  \nWe next evaluate the performance of XGBoost on the\n\nFigure 11: Comparison of out-of-core methods on different subsets of criteo data. The missing data points are due to out of disk space. We can find that basic algorithm can only handle 200M examples. Adding compression gives 3x speedup, and sharding into two disks gives another 2x speedup. The system runs out of file cache start from 400M examples. The algorithm really has to rely on disk after this point. The compression+shard method has a less dramatic slowdown when running out of file cache, and exhibits a linear trend afterwards.\n\nlearning to rank problem. We compare against pGBRT [22],\nthe best previously pubished system on this task. XGBoost\nruns exact greedy algorithm, while pGBRT only support an\napproximate algorithm. The results are shown in Table 4\nand Fig. 10. We find that XGBoost runs faster. Interest-\ningly, subsampling columns not only reduces running time,\nand but also gives a bit higher performance for this prob-\nlem. This could due to the fact that the subsampling helps\nprevent overfitting, which is observed by many of the users."
        },
        {
            "text": "**6.4  Learning to Rank**  \nWe next evaluate the performance of XGBoost on the",
            "page": 8,
            "x": 315,
            "y": 694,
            "width": 241,
            "height": 26,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "17-classification",
            "chunk_id": "8e371de0-eb02-4ac8-a4ea-dab47a9903ae",
            "group_text": "6.3 Classification\n\nIn this section, we evaluate the performance of XGBoost\non a single machine using the exact greedy algorithm on\nHiggs-1M data, by comparing it against two other commonly\nused exact greedy tree boosting implementations. Since\nscikit-learn only handles non-sparse input, we choose the\ndense Higgs dataset for a fair comparison. We use the 1M\nsubset to make scikit-learn finish running in reasonable time.\nAmong the methods in comparison, R\u2019s GBM uses a greedy\napproach that only expands one branch of a tree, which\nmakes it faster but can result in lower accuracy, while both\nscikit-learn and XGBoost learn a full tree. The results are\nshown in Table 3. Both XGBoost and scikit-learn give better\nperformance than R\u2019s GBM, while XGBoost runs more than\n10x faster than scikit-learn. In this experiment, we also find\ncolumn subsamples gives slightly worse performance than\nusing all the features. This could due to the fact that there\nare few important features in this dataset and we can benefit\nfrom greedily select from all the features.\n\n**6.4  Learning to Rank**  \nWe next evaluate the performance of XGBoost on the\n\nFigure 11: Comparison of out-of-core methods on different subsets of criteo data. The missing data points are due to out of disk space. We can find that basic algorithm can only handle 200M examples. Adding compression gives 3x speedup, and sharding into two disks gives another 2x speedup. The system runs out of file cache start from 400M examples. The algorithm really has to rely on disk after this point. The compression+shard method has a less dramatic slowdown when running out of file cache, and exhibits a linear trend afterwards.\n\nlearning to rank problem. We compare against pGBRT [22],\nthe best previously pubished system on this task. XGBoost\nruns exact greedy algorithm, while pGBRT only support an\napproximate algorithm. The results are shown in Table 4\nand Fig. 10. We find that XGBoost runs faster. Interest-\ningly, subsampling columns not only reduces running time,\nand but also gives a bit higher performance for this prob-\nlem. This could due to the fact that the subsampling helps\nprevent overfitting, which is observed by many of the users."
        },
        {
            "text": "Figure 11: Comparison of out-of-core methods on different subsets of criteo data. The missing data points are due to out of disk space. We can find that basic algorithm can only handle 200M examples. Adding compression gives 3x speedup, and sharding into two disks gives another 2x speedup. The system runs out of file cache start from 400M examples. The algorithm really has to rely on disk after this point. The compression+shard method has a less dramatic slowdown when running out of file cache, and exhibits a linear trend afterwards.",
            "page": 9,
            "x": 50,
            "y": 212,
            "width": 246,
            "height": 117,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "17-classification",
            "chunk_id": "52943a84-660d-42fa-a7e1-56ee4fbd96f7",
            "group_text": "6.3 Classification\n\nIn this section, we evaluate the performance of XGBoost\non a single machine using the exact greedy algorithm on\nHiggs-1M data, by comparing it against two other commonly\nused exact greedy tree boosting implementations. Since\nscikit-learn only handles non-sparse input, we choose the\ndense Higgs dataset for a fair comparison. We use the 1M\nsubset to make scikit-learn finish running in reasonable time.\nAmong the methods in comparison, R\u2019s GBM uses a greedy\napproach that only expands one branch of a tree, which\nmakes it faster but can result in lower accuracy, while both\nscikit-learn and XGBoost learn a full tree. The results are\nshown in Table 3. Both XGBoost and scikit-learn give better\nperformance than R\u2019s GBM, while XGBoost runs more than\n10x faster than scikit-learn. In this experiment, we also find\ncolumn subsamples gives slightly worse performance than\nusing all the features. This could due to the fact that there\nare few important features in this dataset and we can benefit\nfrom greedily select from all the features.\n\n**6.4  Learning to Rank**  \nWe next evaluate the performance of XGBoost on the\n\nFigure 11: Comparison of out-of-core methods on different subsets of criteo data. The missing data points are due to out of disk space. We can find that basic algorithm can only handle 200M examples. Adding compression gives 3x speedup, and sharding into two disks gives another 2x speedup. The system runs out of file cache start from 400M examples. The algorithm really has to rely on disk after this point. The compression+shard method has a less dramatic slowdown when running out of file cache, and exhibits a linear trend afterwards.\n\nlearning to rank problem. We compare against pGBRT [22],\nthe best previously pubished system on this task. XGBoost\nruns exact greedy algorithm, while pGBRT only support an\napproximate algorithm. The results are shown in Table 4\nand Fig. 10. We find that XGBoost runs faster. Interest-\ningly, subsampling columns not only reduces running time,\nand but also gives a bit higher performance for this prob-\nlem. This could due to the fact that the subsampling helps\nprevent overfitting, which is observed by many of the users."
        },
        {
            "text": "learning to rank problem. We compare against pGBRT [22],\nthe best previously pubished system on this task. XGBoost\nruns exact greedy algorithm, while pGBRT only support an\napproximate algorithm. The results are shown in Table 4\nand Fig. 10. We find that XGBoost runs faster. Interest-\ningly, subsampling columns not only reduces running time,\nand but also gives a bit higher performance for this prob-\nlem. This could due to the fact that the subsampling helps\nprevent overfitting, which is observed by many of the users.",
            "page": 9,
            "x": 50,
            "y": 337,
            "width": 244,
            "height": 96,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "17-classification",
            "chunk_id": "581c4084-4f08-4bf6-a74a-6de379297199",
            "group_text": "6.3 Classification\n\nIn this section, we evaluate the performance of XGBoost\non a single machine using the exact greedy algorithm on\nHiggs-1M data, by comparing it against two other commonly\nused exact greedy tree boosting implementations. Since\nscikit-learn only handles non-sparse input, we choose the\ndense Higgs dataset for a fair comparison. We use the 1M\nsubset to make scikit-learn finish running in reasonable time.\nAmong the methods in comparison, R\u2019s GBM uses a greedy\napproach that only expands one branch of a tree, which\nmakes it faster but can result in lower accuracy, while both\nscikit-learn and XGBoost learn a full tree. The results are\nshown in Table 3. Both XGBoost and scikit-learn give better\nperformance than R\u2019s GBM, while XGBoost runs more than\n10x faster than scikit-learn. In this experiment, we also find\ncolumn subsamples gives slightly worse performance than\nusing all the features. This could due to the fact that there\nare few important features in this dataset and we can benefit\nfrom greedily select from all the features.\n\n**6.4  Learning to Rank**  \nWe next evaluate the performance of XGBoost on the\n\nFigure 11: Comparison of out-of-core methods on different subsets of criteo data. The missing data points are due to out of disk space. We can find that basic algorithm can only handle 200M examples. Adding compression gives 3x speedup, and sharding into two disks gives another 2x speedup. The system runs out of file cache start from 400M examples. The algorithm really has to rely on disk after this point. The compression+shard method has a less dramatic slowdown when running out of file cache, and exhibits a linear trend afterwards.\n\nlearning to rank problem. We compare against pGBRT [22],\nthe best previously pubished system on this task. XGBoost\nruns exact greedy algorithm, while pGBRT only support an\napproximate algorithm. The results are shown in Table 4\nand Fig. 10. We find that XGBoost runs faster. Interest-\ningly, subsampling columns not only reduces running time,\nand but also gives a bit higher performance for this prob-\nlem. This could due to the fact that the subsampling helps\nprevent overfitting, which is observed by many of the users."
        },
        {
            "text": "6.5  Out-of-core Experiment\n\nWe also evaluate our system in the out-of-core setting on the criteo data. We conducted the experiment on one AWS c3.8xlarge machine (32 vcores, two 320 GB SSD, 60 GB RAM). The results are shown in Figure 11. We can find that compression helps to speed up computation by factor of three, and sharding into two disks further gives 2x speedup. For this type of experiment, it is important to use a very large dataset to drain the system file cache for a real out-of-core setting. This is indeed our setup. We can observe a transition point when the system runs out of file cache. Note that the transition in the final method is less dramatic. This is due to larger disk throughput and better utilization of computation resources. Our final method is able to process 1.7 billion examples on a single machine.",
            "page": 9,
            "x": 51,
            "y": 437,
            "width": 244,
            "height": 163,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "18-out",
            "chunk_id": "9d4a5a3d-15ee-430d-a6ad-bcd9f1ee475b",
            "group_text": "6.5  Out-of-core Experiment\n\nWe also evaluate our system in the out-of-core setting on the criteo data. We conducted the experiment on one AWS c3.8xlarge machine (32 vcores, two 320 GB SSD, 60 GB RAM). The results are shown in Figure 11. We can find that compression helps to speed up computation by factor of three, and sharding into two disks further gives 2x speedup. For this type of experiment, it is important to use a very large dataset to drain the system file cache for a real out-of-core setting. This is indeed our setup. We can observe a transition point when the system runs out of file cache. Note that the transition in the final method is less dramatic. This is due to larger disk throughput and better utilization of computation resources. Our final method is able to process 1.7 billion examples on a single machine."
        },
        {
            "text": "## 6.6  Distributed Experiment\n\nFinally, we evaluate the system in the distributed setting.\nWe set up a YARN cluster on EC2 with m3.2xlarge machines, which is a very common choice for clusters. Each machine contains 8 virtual cores, 30GB of RAM and two 80GB SSD local disks. The dataset is stored on AWS S3 instead of HDFS to avoid purchasing persistent storage.\n\nWe first compare our system against two production-level distributed systems: Spark MLLib [18] and H2O 11. We use",
            "page": 9,
            "x": 51,
            "y": 603,
            "width": 244,
            "height": 101,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "19-distributed",
            "chunk_id": "6646402d-4241-4027-a9d7-162fa20083f1",
            "group_text": "## 6.6  Distributed Experiment\n\nFinally, we evaluate the system in the distributed setting.\nWe set up a YARN cluster on EC2 with m3.2xlarge machines, which is a very common choice for clusters. Each machine contains 8 virtual cores, 30GB of RAM and two 80GB SSD local disks. The dataset is stored on AWS S3 instead of HDFS to avoid purchasing persistent storage.\n\nWe first compare our system against two production-level distributed systems: Spark MLLib [18] and H2O 11. We use\n\nFigure 12:  Comparison of different distributed systems on 32 EC2 nodes for 10 iterations on different subset of criteo data. XGBoost runs more 10x than spark per iteration and 2.2x as H2O\u2019s optimized version (However, H2O is slow in loading the data, getting worse end-to-end time). Note that spark suffers from drastic slow down when running out of memory. XGBoost runs faster and scales smoothly to the full 1.7 billion examples with given resources by utilizing out-of-core computation."
        },
        {
            "text": "Figure 12:  Comparison of different distributed systems on 32 EC2 nodes for 10 iterations on different subset of criteo data. XGBoost runs more 10x than spark per iteration and 2.2x as H2O\u2019s optimized version (However, H2O is slow in loading the data, getting worse end-to-end time). Note that spark suffers from drastic slow down when running out of memory. XGBoost runs faster and scales smoothly to the full 1.7 billion examples with given resources by utilizing out-of-core computation.",
            "page": 9,
            "x": 314,
            "y": 400,
            "width": 244,
            "height": 105,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "19-distributed",
            "chunk_id": "deea756e-609a-448d-8fe5-67ab873fce30",
            "group_text": "## 6.6  Distributed Experiment\n\nFinally, we evaluate the system in the distributed setting.\nWe set up a YARN cluster on EC2 with m3.2xlarge machines, which is a very common choice for clusters. Each machine contains 8 virtual cores, 30GB of RAM and two 80GB SSD local disks. The dataset is stored on AWS S3 instead of HDFS to avoid purchasing persistent storage.\n\nWe first compare our system against two production-level distributed systems: Spark MLLib [18] and H2O 11. We use\n\nFigure 12:  Comparison of different distributed systems on 32 EC2 nodes for 10 iterations on different subset of criteo data. XGBoost runs more 10x than spark per iteration and 2.2x as H2O\u2019s optimized version (However, H2O is slow in loading the data, getting worse end-to-end time). Note that spark suffers from drastic slow down when running out of memory. XGBoost runs faster and scales smoothly to the full 1.7 billion examples with given resources by utilizing out-of-core computation."
        },
        {
            "text": "32 m3.2xlarge machines and test the performance of the sys-\ntems with various input size. Both of the baseline systems\nare in-memory analytics frameworks that need to store the\ndata in RAM, while XGBoost can switch to out-of-core set-\nting when it runs out of memory. The results are shown\nin Fig. 12. We can find that XGBoost runs faster than the\nbaseline systems. More importantly, it is able to take ad-\nvantage of out-of-core computing and smoothly scale to all\n1.7 billion examples with the given limited computing re-\nsources. The baseline systems are only able to handle sub-\nset of the data with the given resources. This experiment\nshows the advantage to bring all the system improvement\ntogether and solve a real-world scale problem. We also eval-\nuate the scaling property of XGBoost by varying the number\nof machines. The results are shown in Fig. 13. We can find\nXGBoost\u2019s performance scales linearly as we add more ma-\nchines. Importantly, XGBoost is able to handle the entire\n1.7 billion data with only four machines. This shows the\nsystem\u2019s potential to handle even larger data.",
            "page": 9,
            "x": 314,
            "y": 519,
            "width": 243,
            "height": 199,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "20-m3",
            "chunk_id": "06bb63e1-4a79-4e91-a83a-e6b71c440226",
            "group_text": "32 m3.2xlarge machines and test the performance of the sys-\ntems with various input size. Both of the baseline systems\nare in-memory analytics frameworks that need to store the\ndata in RAM, while XGBoost can switch to out-of-core set-\nting when it runs out of memory. The results are shown\nin Fig. 12. We can find that XGBoost runs faster than the\nbaseline systems. More importantly, it is able to take ad-\nvantage of out-of-core computing and smoothly scale to all\n1.7 billion examples with the given limited computing re-\nsources. The baseline systems are only able to handle sub-\nset of the data with the given resources. This experiment\nshows the advantage to bring all the system improvement\ntogether and solve a real-world scale problem. We also eval-\nuate the scaling property of XGBoost by varying the number\nof machines. The results are shown in Fig. 13. We can find\nXGBoost\u2019s performance scales linearly as we add more ma-\nchines. Importantly, XGBoost is able to handle the entire\n1.7 billion data with only four machines. This shows the\nsystem\u2019s potential to handle even larger data."
        },
        {
            "text": "7.  CONCLUSION\n\n    In this paper, we described the lessons we learnt when\nbuilding XGBoost, a scalable tree boosting system that is\nwidely used by data scientists and provides state-of-the-art\nresults on many problems. We proposed a novel sparsity\naware algorithm for handling sparse data and a theoretically\njustified weighted quantile sketch for approximate learning.\nOur experience shows that cache access patterns, data com-\npression and sharding are essential elements for building a\nscalable end-to-end system for tree boosting. These lessons\ncan be applied to other machine learning systems as well.\nBy combining these insights, XGBoost is able to solve real-\nworld scale problems using a minimal amount of resources.",
            "page": 10,
            "x": 50,
            "y": 297,
            "width": 245,
            "height": 143,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "21-conclusion",
            "chunk_id": "dbc47c66-8d78-4f18-9af3-1b74d892d6a2",
            "group_text": "7.  CONCLUSION\n\n    In this paper, we described the lessons we learnt when\nbuilding XGBoost, a scalable tree boosting system that is\nwidely used by data scientists and provides state-of-the-art\nresults on many problems. We proposed a novel sparsity\naware algorithm for handling sparse data and a theoretically\njustified weighted quantile sketch for approximate learning.\nOur experience shows that cache access patterns, data com-\npression and sharding are essential elements for building a\nscalable end-to-end system for tree boosting. These lessons\ncan be applied to other machine learning systems as well.\nBy combining these insights, XGBoost is able to solve real-\nworld scale problems using a minimal amount of resources.\n\n# Acknowledgments\nWe would like to thank Tyler B. Johnson, Marco Tulio Ribeiro, Sameer Singh, Arvind Krishnamurthy for their valuable feedback. We also sincerely thank Tong He, Bing Xu, Michael Benesty, Yuan Tang, Hongliang Liu, Qiang Kou, Nan Zhu and all other contributors in the XGBoost community. This work was supported in part by ONR (PECASE) N000141010672, NSF IIS 1258741 and the TerraSwarm Research Center sponsored by MARCO and DARPA.\n\n8. **REFERENCES**\n1. R. Bekkerman. The present and the future of the kdd cup\n   competition: an outsider\u2019s perspective.\n2. R. Bekkerman, M. Bilenko, and J. Langford. _Scaling Up\n   Machine Learning: Parallel and Distributed Approaches_.\n   Cambridge University Press, New York, NY, USA, 2011.\n3. J. Bennett and S. Lanning. The netflix prize. In  \n   _Proceedings of the KDD Cup Workshop 2007_, pages 3\u20136,\n   New York, Aug. 2007.\n4. L. Breiman. Random forests. _Maching Learning_,\n   45(1):5\u201332, Oct. 2001.\n5. C. Burges. From ranknet to lambdarank to lambdamart:\n   An overview. _Learning_, 11:23\u2013581, 2010.\n6. O. Chapelle and Y. Chang. Yahoo! Learning to Rank\n   Challenge Overview. _Journal of Machine Learning\n   Research - W & CP_, 14:1\u201324, 2011.\n7. T. Chen, H. Li, Q. Yang, and Y. Yu. General functional\n   matrix factorization using gradient boosting. In _Proceeding_\n\n- [8] T. Chen, S. Singh, B. Taskar, and C. Guestrin. Efficient second-order gradient boosting for conditional random fields. In *Proceedings of 18th Artificial Intelligence and Statistics Conference (AISTATS\u201915)*, volume 1, 2015.\n- [9] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. LIBLINEAR: A library for large linear classification. *Journal of Machine Learning Research*, 9:1871\u20131874, 2008.\n- [10] J. Friedman. Greedy function approximation: a gradient boosting machine. *Annals of Statistics*, 29(5):1189\u20131232, 2001.\n- [11] J. Friedman. Stochastic gradient boosting. *Computational Statistics & Data Analysis*, 38(4):367\u2013378, 2002.\n- [12] J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: a statistical view of boosting. *Annals of Statistics*, 28(2):337\u2013407, 2000.\n- [13] J. H. Friedman and B. E. Popescu. Importance sampled learning ensembles, 2003.\n- [14] M. Greenwald and S. Khanna. Space-efficient online computation of quantile summaries. In *Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data*, pages 58\u201366, 2001.\n- [15] X. He, J. Pan, O. Jin, T. Xu, B. Liu, T. Xu, Y. Shi, A. Atallah, R. Herbrich, S. Bowers, and J. Q. n. Candela. Practical lessons from predicting clicks on ads at facebook. In *Proceedings of the Eighth International Workshop on Data Mining for Online Advertising*, ADKDD\u201914, 2014.\n- [16] P. Li. Robust Logitboost and adaptive base class (ABC) Logitboost. In *Proceedings of the Twenty-Sixth Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI\u201910)*, pages 302\u2013311, 2010.\n- [17] P. Li, Q. Wu, and C. J. Burges. Mcrank: Learning to rank using multiple classification and gradient boosting. In *Advances in Neural Information Processing Systems 20*, pages 897\u2013904. 2008.\n- [18] X. Meng, J. Bradley, B. Yavuz, E. Sparks, S. Venkataraman, D. Liu, J. Freeman, D. Tsai, M. Amde, S. Owen, D. Xin, R. Xin, M. J. Franklin, R. Zadeh, M. Zaharia, and A. Talwalkar. MLlib: Machine learning in apache spark. *Journal of Machine Learning Research*, 17(34):1\u20137, 2016.\n- [19] B. Panda, J. S. Herbach, S. Basu, and R. J. Bayardo. Planet: Massively parallel learning of tree ensembles with mapreduce. *Proceeding of VLDB Endowment*, 2(2):1426\u20131437, Aug. 2009.\n- [20] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. *Journal of Machine Learning Research*, 12:2825\u20132830, 2011.\n- [21] G. Ridgeway. *Generalized Boosted Models: A guide to the gbm package*.\n- [22] S. Tyree, K. Weinberger, K. Agrawal, and J. Paykin. Parallel boosted regression trees for web search ranking. In *Proceedings of the 20th international conference on World wide web*, pages 387\u2013396. ACM, 2011.\n- [23] J. Ye, J.-H. Chow, J. Chen, and Z. Zheng. Stochastic gradient boosted distributed decision trees. In *Proceedings of the 18th ACM Conference on Information and Knowledge Management*, CIKM \u201909.\n- [24] Q. Zhang and W. Wang. A fast algorithm for approximate quantiles in high speed data streams. In *Proceedings of the 19th International Conference on Scientific and Statistical Database Management*, 2007.\n- [25] T. Zhang and R. Johnson. Learning nonlinear functions using regularized greedy forest. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 36(5), 2014."
        },
        {
            "text": "# Acknowledgments\nWe would like to thank Tyler B. Johnson, Marco Tulio Ribeiro, Sameer Singh, Arvind Krishnamurthy for their valuable feedback. We also sincerely thank Tong He, Bing Xu, Michael Benesty, Yuan Tang, Hongliang Liu, Qiang Kou, Nan Zhu and all other contributors in the XGBoost community. This work was supported in part by ONR (PECASE) N000141010672, NSF IIS 1258741 and the TerraSwarm Research Center sponsored by MARCO and DARPA.",
            "page": 10,
            "x": 50,
            "y": 450,
            "width": 244,
            "height": 89,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "21-conclusion",
            "chunk_id": "045c29ac-03c6-4301-8456-6fe75cd61dec",
            "group_text": "7.  CONCLUSION\n\n    In this paper, we described the lessons we learnt when\nbuilding XGBoost, a scalable tree boosting system that is\nwidely used by data scientists and provides state-of-the-art\nresults on many problems. We proposed a novel sparsity\naware algorithm for handling sparse data and a theoretically\njustified weighted quantile sketch for approximate learning.\nOur experience shows that cache access patterns, data com-\npression and sharding are essential elements for building a\nscalable end-to-end system for tree boosting. These lessons\ncan be applied to other machine learning systems as well.\nBy combining these insights, XGBoost is able to solve real-\nworld scale problems using a minimal amount of resources.\n\n# Acknowledgments\nWe would like to thank Tyler B. Johnson, Marco Tulio Ribeiro, Sameer Singh, Arvind Krishnamurthy for their valuable feedback. We also sincerely thank Tong He, Bing Xu, Michael Benesty, Yuan Tang, Hongliang Liu, Qiang Kou, Nan Zhu and all other contributors in the XGBoost community. This work was supported in part by ONR (PECASE) N000141010672, NSF IIS 1258741 and the TerraSwarm Research Center sponsored by MARCO and DARPA.\n\n8. **REFERENCES**\n1. R. Bekkerman. The present and the future of the kdd cup\n   competition: an outsider\u2019s perspective.\n2. R. Bekkerman, M. Bilenko, and J. Langford. _Scaling Up\n   Machine Learning: Parallel and Distributed Approaches_.\n   Cambridge University Press, New York, NY, USA, 2011.\n3. J. Bennett and S. Lanning. The netflix prize. In  \n   _Proceedings of the KDD Cup Workshop 2007_, pages 3\u20136,\n   New York, Aug. 2007.\n4. L. Breiman. Random forests. _Maching Learning_,\n   45(1):5\u201332, Oct. 2001.\n5. C. Burges. From ranknet to lambdarank to lambdamart:\n   An overview. _Learning_, 11:23\u2013581, 2010.\n6. O. Chapelle and Y. Chang. Yahoo! Learning to Rank\n   Challenge Overview. _Journal of Machine Learning\n   Research - W & CP_, 14:1\u201324, 2011.\n7. T. Chen, H. Li, Q. Yang, and Y. Yu. General functional\n   matrix factorization using gradient boosting. In _Proceeding_\n\n- [8] T. Chen, S. Singh, B. Taskar, and C. Guestrin. Efficient second-order gradient boosting for conditional random fields. In *Proceedings of 18th Artificial Intelligence and Statistics Conference (AISTATS\u201915)*, volume 1, 2015.\n- [9] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. LIBLINEAR: A library for large linear classification. *Journal of Machine Learning Research*, 9:1871\u20131874, 2008.\n- [10] J. Friedman. Greedy function approximation: a gradient boosting machine. *Annals of Statistics*, 29(5):1189\u20131232, 2001.\n- [11] J. Friedman. Stochastic gradient boosting. *Computational Statistics & Data Analysis*, 38(4):367\u2013378, 2002.\n- [12] J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: a statistical view of boosting. *Annals of Statistics*, 28(2):337\u2013407, 2000.\n- [13] J. H. Friedman and B. E. Popescu. Importance sampled learning ensembles, 2003.\n- [14] M. Greenwald and S. Khanna. Space-efficient online computation of quantile summaries. In *Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data*, pages 58\u201366, 2001.\n- [15] X. He, J. Pan, O. Jin, T. Xu, B. Liu, T. Xu, Y. Shi, A. Atallah, R. Herbrich, S. Bowers, and J. Q. n. Candela. Practical lessons from predicting clicks on ads at facebook. In *Proceedings of the Eighth International Workshop on Data Mining for Online Advertising*, ADKDD\u201914, 2014.\n- [16] P. Li. Robust Logitboost and adaptive base class (ABC) Logitboost. In *Proceedings of the Twenty-Sixth Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI\u201910)*, pages 302\u2013311, 2010.\n- [17] P. Li, Q. Wu, and C. J. Burges. Mcrank: Learning to rank using multiple classification and gradient boosting. In *Advances in Neural Information Processing Systems 20*, pages 897\u2013904. 2008.\n- [18] X. Meng, J. Bradley, B. Yavuz, E. Sparks, S. Venkataraman, D. Liu, J. Freeman, D. Tsai, M. Amde, S. Owen, D. Xin, R. Xin, M. J. Franklin, R. Zadeh, M. Zaharia, and A. Talwalkar. MLlib: Machine learning in apache spark. *Journal of Machine Learning Research*, 17(34):1\u20137, 2016.\n- [19] B. Panda, J. S. Herbach, S. Basu, and R. J. Bayardo. Planet: Massively parallel learning of tree ensembles with mapreduce. *Proceeding of VLDB Endowment*, 2(2):1426\u20131437, Aug. 2009.\n- [20] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. *Journal of Machine Learning Research*, 12:2825\u20132830, 2011.\n- [21] G. Ridgeway. *Generalized Boosted Models: A guide to the gbm package*.\n- [22] S. Tyree, K. Weinberger, K. Agrawal, and J. Paykin. Parallel boosted regression trees for web search ranking. In *Proceedings of the 20th international conference on World wide web*, pages 387\u2013396. ACM, 2011.\n- [23] J. Ye, J.-H. Chow, J. Chen, and Z. Zheng. Stochastic gradient boosted distributed decision trees. In *Proceedings of the 18th ACM Conference on Information and Knowledge Management*, CIKM \u201909.\n- [24] Q. Zhang and W. Wang. A fast algorithm for approximate quantiles in high speed data streams. In *Proceedings of the 19th International Conference on Scientific and Statistical Database Management*, 2007.\n- [25] T. Zhang and R. Johnson. Learning nonlinear functions using regularized greedy forest. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 36(5), 2014."
        },
        {
            "text": "8. **REFERENCES**\n1. R. Bekkerman. The present and the future of the kdd cup\n   competition: an outsider\u2019s perspective.\n2. R. Bekkerman, M. Bilenko, and J. Langford. _Scaling Up\n   Machine Learning: Parallel and Distributed Approaches_.\n   Cambridge University Press, New York, NY, USA, 2011.\n3. J. Bennett and S. Lanning. The netflix prize. In  \n   _Proceedings of the KDD Cup Workshop 2007_, pages 3\u20136,\n   New York, Aug. 2007.\n4. L. Breiman. Random forests. _Maching Learning_,\n   45(1):5\u201332, Oct. 2001.\n5. C. Burges. From ranknet to lambdarank to lambdamart:\n   An overview. _Learning_, 11:23\u2013581, 2010.\n6. O. Chapelle and Y. Chang. Yahoo! Learning to Rank\n   Challenge Overview. _Journal of Machine Learning\n   Research - W & CP_, 14:1\u201324, 2011.\n7. T. Chen, H. Li, Q. Yang, and Y. Yu. General functional\n   matrix factorization using gradient boosting. In _Proceeding_",
            "page": 10,
            "x": 51,
            "y": 548,
            "width": 242,
            "height": 174,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "21-conclusion",
            "chunk_id": "d4cb59f7-3b2c-4825-a99d-5fc7137991a5",
            "group_text": "7.  CONCLUSION\n\n    In this paper, we described the lessons we learnt when\nbuilding XGBoost, a scalable tree boosting system that is\nwidely used by data scientists and provides state-of-the-art\nresults on many problems. We proposed a novel sparsity\naware algorithm for handling sparse data and a theoretically\njustified weighted quantile sketch for approximate learning.\nOur experience shows that cache access patterns, data com-\npression and sharding are essential elements for building a\nscalable end-to-end system for tree boosting. These lessons\ncan be applied to other machine learning systems as well.\nBy combining these insights, XGBoost is able to solve real-\nworld scale problems using a minimal amount of resources.\n\n# Acknowledgments\nWe would like to thank Tyler B. Johnson, Marco Tulio Ribeiro, Sameer Singh, Arvind Krishnamurthy for their valuable feedback. We also sincerely thank Tong He, Bing Xu, Michael Benesty, Yuan Tang, Hongliang Liu, Qiang Kou, Nan Zhu and all other contributors in the XGBoost community. This work was supported in part by ONR (PECASE) N000141010672, NSF IIS 1258741 and the TerraSwarm Research Center sponsored by MARCO and DARPA.\n\n8. **REFERENCES**\n1. R. Bekkerman. The present and the future of the kdd cup\n   competition: an outsider\u2019s perspective.\n2. R. Bekkerman, M. Bilenko, and J. Langford. _Scaling Up\n   Machine Learning: Parallel and Distributed Approaches_.\n   Cambridge University Press, New York, NY, USA, 2011.\n3. J. Bennett and S. Lanning. The netflix prize. In  \n   _Proceedings of the KDD Cup Workshop 2007_, pages 3\u20136,\n   New York, Aug. 2007.\n4. L. Breiman. Random forests. _Maching Learning_,\n   45(1):5\u201332, Oct. 2001.\n5. C. Burges. From ranknet to lambdarank to lambdamart:\n   An overview. _Learning_, 11:23\u2013581, 2010.\n6. O. Chapelle and Y. Chang. Yahoo! Learning to Rank\n   Challenge Overview. _Journal of Machine Learning\n   Research - W & CP_, 14:1\u201324, 2011.\n7. T. Chen, H. Li, Q. Yang, and Y. Yu. General functional\n   matrix factorization using gradient boosting. In _Proceeding_\n\n- [8] T. Chen, S. Singh, B. Taskar, and C. Guestrin. Efficient second-order gradient boosting for conditional random fields. In *Proceedings of 18th Artificial Intelligence and Statistics Conference (AISTATS\u201915)*, volume 1, 2015.\n- [9] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. LIBLINEAR: A library for large linear classification. *Journal of Machine Learning Research*, 9:1871\u20131874, 2008.\n- [10] J. Friedman. Greedy function approximation: a gradient boosting machine. *Annals of Statistics*, 29(5):1189\u20131232, 2001.\n- [11] J. Friedman. Stochastic gradient boosting. *Computational Statistics & Data Analysis*, 38(4):367\u2013378, 2002.\n- [12] J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: a statistical view of boosting. *Annals of Statistics*, 28(2):337\u2013407, 2000.\n- [13] J. H. Friedman and B. E. Popescu. Importance sampled learning ensembles, 2003.\n- [14] M. Greenwald and S. Khanna. Space-efficient online computation of quantile summaries. In *Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data*, pages 58\u201366, 2001.\n- [15] X. He, J. Pan, O. Jin, T. Xu, B. Liu, T. Xu, Y. Shi, A. Atallah, R. Herbrich, S. Bowers, and J. Q. n. Candela. Practical lessons from predicting clicks on ads at facebook. In *Proceedings of the Eighth International Workshop on Data Mining for Online Advertising*, ADKDD\u201914, 2014.\n- [16] P. Li. Robust Logitboost and adaptive base class (ABC) Logitboost. In *Proceedings of the Twenty-Sixth Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI\u201910)*, pages 302\u2013311, 2010.\n- [17] P. Li, Q. Wu, and C. J. Burges. Mcrank: Learning to rank using multiple classification and gradient boosting. In *Advances in Neural Information Processing Systems 20*, pages 897\u2013904. 2008.\n- [18] X. Meng, J. Bradley, B. Yavuz, E. Sparks, S. Venkataraman, D. Liu, J. Freeman, D. Tsai, M. Amde, S. Owen, D. Xin, R. Xin, M. J. Franklin, R. Zadeh, M. Zaharia, and A. Talwalkar. MLlib: Machine learning in apache spark. *Journal of Machine Learning Research*, 17(34):1\u20137, 2016.\n- [19] B. Panda, J. S. Herbach, S. Basu, and R. J. Bayardo. Planet: Massively parallel learning of tree ensembles with mapreduce. *Proceeding of VLDB Endowment*, 2(2):1426\u20131437, Aug. 2009.\n- [20] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. *Journal of Machine Learning Research*, 12:2825\u20132830, 2011.\n- [21] G. Ridgeway. *Generalized Boosted Models: A guide to the gbm package*.\n- [22] S. Tyree, K. Weinberger, K. Agrawal, and J. Paykin. Parallel boosted regression trees for web search ranking. In *Proceedings of the 20th international conference on World wide web*, pages 387\u2013396. ACM, 2011.\n- [23] J. Ye, J.-H. Chow, J. Chen, and Z. Zheng. Stochastic gradient boosted distributed decision trees. In *Proceedings of the 18th ACM Conference on Information and Knowledge Management*, CIKM \u201909.\n- [24] Q. Zhang and W. Wang. A fast algorithm for approximate quantiles in high speed data streams. In *Proceedings of the 19th International Conference on Scientific and Statistical Database Management*, 2007.\n- [25] T. Zhang and R. Johnson. Learning nonlinear functions using regularized greedy forest. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 36(5), 2014."
        },
        {
            "text": "- [8] T. Chen, S. Singh, B. Taskar, and C. Guestrin. Efficient second-order gradient boosting for conditional random fields. In *Proceedings of 18th Artificial Intelligence and Statistics Conference (AISTATS\u201915)*, volume 1, 2015.\n- [9] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. LIBLINEAR: A library for large linear classification. *Journal of Machine Learning Research*, 9:1871\u20131874, 2008.\n- [10] J. Friedman. Greedy function approximation: a gradient boosting machine. *Annals of Statistics*, 29(5):1189\u20131232, 2001.\n- [11] J. Friedman. Stochastic gradient boosting. *Computational Statistics & Data Analysis*, 38(4):367\u2013378, 2002.\n- [12] J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: a statistical view of boosting. *Annals of Statistics*, 28(2):337\u2013407, 2000.\n- [13] J. H. Friedman and B. E. Popescu. Importance sampled learning ensembles, 2003.\n- [14] M. Greenwald and S. Khanna. Space-efficient online computation of quantile summaries. In *Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data*, pages 58\u201366, 2001.\n- [15] X. He, J. Pan, O. Jin, T. Xu, B. Liu, T. Xu, Y. Shi, A. Atallah, R. Herbrich, S. Bowers, and J. Q. n. Candela. Practical lessons from predicting clicks on ads at facebook. In *Proceedings of the Eighth International Workshop on Data Mining for Online Advertising*, ADKDD\u201914, 2014.\n- [16] P. Li. Robust Logitboost and adaptive base class (ABC) Logitboost. In *Proceedings of the Twenty-Sixth Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI\u201910)*, pages 302\u2013311, 2010.\n- [17] P. Li, Q. Wu, and C. J. Burges. Mcrank: Learning to rank using multiple classification and gradient boosting. In *Advances in Neural Information Processing Systems 20*, pages 897\u2013904. 2008.\n- [18] X. Meng, J. Bradley, B. Yavuz, E. Sparks, S. Venkataraman, D. Liu, J. Freeman, D. Tsai, M. Amde, S. Owen, D. Xin, R. Xin, M. J. Franklin, R. Zadeh, M. Zaharia, and A. Talwalkar. MLlib: Machine learning in apache spark. *Journal of Machine Learning Research*, 17(34):1\u20137, 2016.\n- [19] B. Panda, J. S. Herbach, S. Basu, and R. J. Bayardo. Planet: Massively parallel learning of tree ensembles with mapreduce. *Proceeding of VLDB Endowment*, 2(2):1426\u20131437, Aug. 2009.\n- [20] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. *Journal of Machine Learning Research*, 12:2825\u20132830, 2011.\n- [21] G. Ridgeway. *Generalized Boosted Models: A guide to the gbm package*.\n- [22] S. Tyree, K. Weinberger, K. Agrawal, and J. Paykin. Parallel boosted regression trees for web search ranking. In *Proceedings of the 20th international conference on World wide web*, pages 387\u2013396. ACM, 2011.\n- [23] J. Ye, J.-H. Chow, J. Chen, and Z. Zheng. Stochastic gradient boosted distributed decision trees. In *Proceedings of the 18th ACM Conference on Information and Knowledge Management*, CIKM \u201909.\n- [24] Q. Zhang and W. Wang. A fast algorithm for approximate quantiles in high speed data streams. In *Proceedings of the 19th International Conference on Scientific and Statistical Database Management*, 2007.\n- [25] T. Zhang and R. Johnson. Learning nonlinear functions using regularized greedy forest. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 36(5), 2014.",
            "page": 10,
            "x": 313,
            "y": 55,
            "width": 246,
            "height": 651,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "21-conclusion",
            "chunk_id": "00d0c871-8360-4947-a6e2-ffbf82c03ef8",
            "group_text": "7.  CONCLUSION\n\n    In this paper, we described the lessons we learnt when\nbuilding XGBoost, a scalable tree boosting system that is\nwidely used by data scientists and provides state-of-the-art\nresults on many problems. We proposed a novel sparsity\naware algorithm for handling sparse data and a theoretically\njustified weighted quantile sketch for approximate learning.\nOur experience shows that cache access patterns, data com-\npression and sharding are essential elements for building a\nscalable end-to-end system for tree boosting. These lessons\ncan be applied to other machine learning systems as well.\nBy combining these insights, XGBoost is able to solve real-\nworld scale problems using a minimal amount of resources.\n\n# Acknowledgments\nWe would like to thank Tyler B. Johnson, Marco Tulio Ribeiro, Sameer Singh, Arvind Krishnamurthy for their valuable feedback. We also sincerely thank Tong He, Bing Xu, Michael Benesty, Yuan Tang, Hongliang Liu, Qiang Kou, Nan Zhu and all other contributors in the XGBoost community. This work was supported in part by ONR (PECASE) N000141010672, NSF IIS 1258741 and the TerraSwarm Research Center sponsored by MARCO and DARPA.\n\n8. **REFERENCES**\n1. R. Bekkerman. The present and the future of the kdd cup\n   competition: an outsider\u2019s perspective.\n2. R. Bekkerman, M. Bilenko, and J. Langford. _Scaling Up\n   Machine Learning: Parallel and Distributed Approaches_.\n   Cambridge University Press, New York, NY, USA, 2011.\n3. J. Bennett and S. Lanning. The netflix prize. In  \n   _Proceedings of the KDD Cup Workshop 2007_, pages 3\u20136,\n   New York, Aug. 2007.\n4. L. Breiman. Random forests. _Maching Learning_,\n   45(1):5\u201332, Oct. 2001.\n5. C. Burges. From ranknet to lambdarank to lambdamart:\n   An overview. _Learning_, 11:23\u2013581, 2010.\n6. O. Chapelle and Y. Chang. Yahoo! Learning to Rank\n   Challenge Overview. _Journal of Machine Learning\n   Research - W & CP_, 14:1\u201324, 2011.\n7. T. Chen, H. Li, Q. Yang, and Y. Yu. General functional\n   matrix factorization using gradient boosting. In _Proceeding_\n\n- [8] T. Chen, S. Singh, B. Taskar, and C. Guestrin. Efficient second-order gradient boosting for conditional random fields. In *Proceedings of 18th Artificial Intelligence and Statistics Conference (AISTATS\u201915)*, volume 1, 2015.\n- [9] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. LIBLINEAR: A library for large linear classification. *Journal of Machine Learning Research*, 9:1871\u20131874, 2008.\n- [10] J. Friedman. Greedy function approximation: a gradient boosting machine. *Annals of Statistics*, 29(5):1189\u20131232, 2001.\n- [11] J. Friedman. Stochastic gradient boosting. *Computational Statistics & Data Analysis*, 38(4):367\u2013378, 2002.\n- [12] J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: a statistical view of boosting. *Annals of Statistics*, 28(2):337\u2013407, 2000.\n- [13] J. H. Friedman and B. E. Popescu. Importance sampled learning ensembles, 2003.\n- [14] M. Greenwald and S. Khanna. Space-efficient online computation of quantile summaries. In *Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data*, pages 58\u201366, 2001.\n- [15] X. He, J. Pan, O. Jin, T. Xu, B. Liu, T. Xu, Y. Shi, A. Atallah, R. Herbrich, S. Bowers, and J. Q. n. Candela. Practical lessons from predicting clicks on ads at facebook. In *Proceedings of the Eighth International Workshop on Data Mining for Online Advertising*, ADKDD\u201914, 2014.\n- [16] P. Li. Robust Logitboost and adaptive base class (ABC) Logitboost. In *Proceedings of the Twenty-Sixth Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI\u201910)*, pages 302\u2013311, 2010.\n- [17] P. Li, Q. Wu, and C. J. Burges. Mcrank: Learning to rank using multiple classification and gradient boosting. In *Advances in Neural Information Processing Systems 20*, pages 897\u2013904. 2008.\n- [18] X. Meng, J. Bradley, B. Yavuz, E. Sparks, S. Venkataraman, D. Liu, J. Freeman, D. Tsai, M. Amde, S. Owen, D. Xin, R. Xin, M. J. Franklin, R. Zadeh, M. Zaharia, and A. Talwalkar. MLlib: Machine learning in apache spark. *Journal of Machine Learning Research*, 17(34):1\u20137, 2016.\n- [19] B. Panda, J. S. Herbach, S. Basu, and R. J. Bayardo. Planet: Massively parallel learning of tree ensembles with mapreduce. *Proceeding of VLDB Endowment*, 2(2):1426\u20131437, Aug. 2009.\n- [20] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. *Journal of Machine Learning Research*, 12:2825\u20132830, 2011.\n- [21] G. Ridgeway. *Generalized Boosted Models: A guide to the gbm package*.\n- [22] S. Tyree, K. Weinberger, K. Agrawal, and J. Paykin. Parallel boosted regression trees for web search ranking. In *Proceedings of the 20th international conference on World wide web*, pages 387\u2013396. ACM, 2011.\n- [23] J. Ye, J.-H. Chow, J. Chen, and Z. Zheng. Stochastic gradient boosted distributed decision trees. In *Proceedings of the 18th ACM Conference on Information and Knowledge Management*, CIKM \u201909.\n- [24] Q. Zhang and W. Wang. A fast algorithm for approximate quantiles in high speed data streams. In *Proceedings of the 19th International Conference on Scientific and Statistical Database Management*, 2007.\n- [25] T. Zhang and R. Johnson. Learning nonlinear functions using regularized greedy forest. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 36(5), 2014."
        },
        {
            "text": "APPENDIX",
            "page": 11,
            "x": 51,
            "y": 50,
            "width": 68,
            "height": 17,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "b60e7360-a320-49c1-ab77-dcb76c6d2265",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "A.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:",
            "page": 11,
            "x": 49,
            "y": 69,
            "width": 246,
            "height": 88,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "66fefadc-235b-44d1-9ba3-3bd939842afe",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.",
            "page": 11,
            "x": 61,
            "y": 160,
            "width": 234,
            "height": 60,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "57eccfe6-2b9c-4c7d-8733-8e61f911bc99",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "A quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].",
            "page": 11,
            "x": 51,
            "y": 222,
            "width": 244,
            "height": 28,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "55be1600-235f-47dd-a887-9ade84553e88",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "In order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.",
            "page": 11,
            "x": 51,
            "y": 250,
            "width": 244,
            "height": 81,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "ebc38789-9020-4cae-bc6b-3d2a601d4ce5",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "A.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$",
            "page": 11,
            "x": 51,
            "y": 337,
            "width": 244,
            "height": 61,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "81ce599a-8a91-4fdf-8685-4d81f7bf2ec7",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$",
            "page": 11,
            "x": 127,
            "y": 404,
            "width": 166,
            "height": 25,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "ce06c4e0-366a-49fd-8dc6-9ed05379d5e2",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$",
            "page": 11,
            "x": 129,
            "y": 435,
            "width": 165,
            "height": 26,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "7d04dde6-dd39-400b-8361-eb700885b7d5",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "We should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as",
            "page": 11,
            "x": 51,
            "y": 462,
            "width": 243,
            "height": 38,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "88cdad9a-6155-4ec6-a2bc-b55f4c15930b",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$",
            "page": 11,
            "x": 95,
            "y": 506,
            "width": 199,
            "height": 26,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "0658e0a1-2f2c-489d-9fcb-4dce67d170af",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "Finally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set",
            "page": 11,
            "x": 52,
            "y": 537,
            "width": 242,
            "height": 20,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "0c58c013-7fc2-47d0-bbb9-4373250329e9",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$",
            "page": 11,
            "x": 137,
            "y": 563,
            "width": 157,
            "height": 25,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "b2f2b21e-2be5-4a97-ac24-aa2838211daf",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "Our task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:",
            "page": 11,
            "x": 52,
            "y": 595,
            "width": 243,
            "height": 37,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "d29f6069-f6f7-4e64-bade-35510964eaa4",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "Definition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:",
            "page": 11,
            "x": 52,
            "y": 641,
            "width": 261,
            "height": 56,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "ed254942-a88c-4f49-a6e3-e0e16b127a73",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$",
            "page": 11,
            "x": 110,
            "y": 704,
            "width": 125,
            "height": 19,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "630374c4-7f9e-4455-983b-abc9cb57caa0",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$",
            "page": 11,
            "x": 315,
            "y": 54,
            "width": 244,
            "height": 35,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "104ba3b1-7450-483c-bd08-9bf99250c0ab",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "the equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints",
            "page": 11,
            "x": 314,
            "y": 90,
            "width": 248,
            "height": 41,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "faf36f49-b313-49fb-be9a-6e222738d396",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $",
            "page": 11,
            "x": 316,
            "y": 135,
            "width": 240,
            "height": 23,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "43a09780-399b-4f9d-9184-7760c13cae61",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "Since these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.",
            "page": 11,
            "x": 315,
            "y": 161,
            "width": 242,
            "height": 30,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "a00de5f7-b1ca-4326-ab20-e7be7702dbdb",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "Definition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:",
            "page": 11,
            "x": 315,
            "y": 196,
            "width": 242,
            "height": 57,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "8635d66b-a43b-4f30-b3f1-72f9365398b1",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$",
            "page": 11,
            "x": 370,
            "y": 258,
            "width": 186,
            "height": 16,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "fc3e073a-2c67-410b-8a6e-7f0034eb37d3",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "When $y > x_k$:",
            "page": 11,
            "x": 317,
            "y": 275,
            "width": 54,
            "height": 11,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "306d7cda-4098-437f-ae1b-95d5270aa4a7",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$",
            "page": 11,
            "x": 348,
            "y": 289,
            "width": 208,
            "height": 15,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "226b1a3c-eacf-4e9c-8393-40ad24d2b371",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "When $y \\in (x_i, x_{i+1})$ for some $i$:",
            "page": 11,
            "x": 317,
            "y": 306,
            "width": 121,
            "height": 12,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "face9b99-6bf0-41f7-836d-fbc5b744342b",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$",
            "page": 11,
            "x": 376,
            "y": 322,
            "width": 180,
            "height": 40,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "025f3669-0c60-49fd-81e8-2a273a83d30c",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "Lemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*",
            "page": 11,
            "x": 315,
            "y": 368,
            "width": 241,
            "height": 28,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "50531906-9b60-4ba7-9521-7b4438a21fc9",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$",
            "page": 11,
            "x": 334,
            "y": 399,
            "width": 222,
            "height": 15,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "1c62ded0-2c2e-4907-a516-9a13f4f489e9",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$",
            "page": 11,
            "x": 315,
            "y": 419,
            "width": 241,
            "height": 22,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "15d6dfbd-7483-4981-8f3e-084dd5f11c24",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "Proof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:",
            "page": 11,
            "x": 315,
            "y": 442,
            "width": 241,
            "height": 20,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "dc9a4c74-c415-4a08-96f1-12b03ead0230",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$",
            "page": 11,
            "x": 334,
            "y": 467,
            "width": 203,
            "height": 15,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "5ec2471b-f333-43db-b45f-ff3456634ba3",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$",
            "page": 11,
            "x": 316,
            "y": 485,
            "width": 239,
            "height": 24,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "39b7245d-3cdf-4d69-ab29-0d6da5651e94",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$",
            "page": 11,
            "x": 343,
            "y": 512,
            "width": 185,
            "height": 16,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "928c0ec5-f3af-4939-a3af-73e7cb43552e",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$",
            "page": 11,
            "x": 342,
            "y": 532,
            "width": 189,
            "height": 26,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "988f518d-1ff2-462d-a7ff-ef9c08cff5f9",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "Using these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.",
            "page": 11,
            "x": 315,
            "y": 559,
            "width": 241,
            "height": 63,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "e6fdbeed-6aa3-45a7-b98a-da1f998da82e",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "Definition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$",
            "page": 11,
            "x": 316,
            "y": 627,
            "width": 240,
            "height": 28,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "4f25e1e9-aabf-4438-bb6f-6ef9877e567d",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$",
            "page": 11,
            "x": 375,
            "y": 658,
            "width": 182,
            "height": 16,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "16e69f43-67eb-4dd0-8395-c3c0205d3c95",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.",
            "page": 11,
            "x": 315,
            "y": 678,
            "width": 242,
            "height": 42,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "d5d554b8-5c99-44cb-9e86-7c482680d8dd",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "Lemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $",
            "page": 12,
            "x": 49,
            "y": 53,
            "width": 249,
            "height": 60,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "a2f0e0a3-d9a0-4ca8-9cf4-7df3f11f0c14",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$",
            "page": 12,
            "x": 50,
            "y": 121,
            "width": 256,
            "height": 54,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "a76f33f8-8701-4bb0-9022-2b7eb4543e8e",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.",
            "page": 12,
            "x": 50,
            "y": 179,
            "width": 245,
            "height": 75,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "6f1abbc2-1c44-48fb-b664-1957e224ee84",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$",
            "page": 12,
            "x": 50,
            "y": 256,
            "width": 244,
            "height": 74,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "d64e7669-95a8-40b6-8bff-ea7fa83a3c59",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "The constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.",
            "page": 12,
            "x": 51,
            "y": 329,
            "width": 243,
            "height": 34,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "96280679-342b-4722-b858-7003e8aa0e5a",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)",
            "page": 12,
            "x": 51,
            "y": 366,
            "width": 243,
            "height": 83,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "14ddda12-fb07-4596-9b8e-1864dfb74223",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "The points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)",
            "page": 12,
            "x": 51,
            "y": 450,
            "width": 244,
            "height": 161,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "3720f1a5-8d13-43dd-9caf-4c4961faf674",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "and the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.",
            "page": 12,
            "x": 51,
            "y": 616,
            "width": 244,
            "height": 29,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "a34c77b3-92da-4b4d-83d2-777e4cd704ab",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "Lemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $",
            "page": 12,
            "x": 62,
            "y": 649,
            "width": 232,
            "height": 61,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "ea967bdd-18d0-40c1-a394-94d16428255b",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "Algorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend",
            "page": 12,
            "x": 316,
            "y": 55,
            "width": 240,
            "height": 143,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "f9df89a5-cb46-4fba-b970-3e5fec908bf7",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "This can be obtained by straight-forward application of Definition A.2.",
            "page": 12,
            "x": 314,
            "y": 222,
            "width": 243,
            "height": 21,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "ec9afa74-c8c0-4e1b-9ad8-ed83907a375f",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "Theorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$",
            "page": 12,
            "x": 315,
            "y": 247,
            "width": 253,
            "height": 92,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "e1a01117-f16e-4971-9b74-a51f07777f6a",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "Here the first inequality is due to Lemma A.3. [ ]",
            "page": 12,
            "x": 314,
            "y": 341,
            "width": 195,
            "height": 16,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "a72d1495-ff4a-4453-9931-37d1ed856e23",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "A.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.",
            "page": 12,
            "x": 314,
            "y": 360,
            "width": 243,
            "height": 60,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "c87cbc3c-3534-4279-a9c5-7728c68a9e39",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $",
            "page": 12,
            "x": 316,
            "y": 423,
            "width": 240,
            "height": 66,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "e6ead0df-fdd7-490a-852d-3d5e1058b912",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "Proof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$",
            "page": 12,
            "x": 322,
            "y": 492,
            "width": 236,
            "height": 226,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "e10235f9-6968-4a55-a295-8d96200483ff",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$",
            "page": 13,
            "x": 60,
            "y": 54,
            "width": 274,
            "height": 105,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "282c2a91-57da-416e-a9f5-350ad4333d72",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$",
            "page": 13,
            "x": 59,
            "y": 162,
            "width": 232,
            "height": 133,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "b9965b5c-7058-44a8-9261-cd946ba77ebf",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "Now we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that",
            "page": 13,
            "x": 49,
            "y": 299,
            "width": 275,
            "height": 67,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "b1cdf10a-8823-4c02-a886-9cd1de6d79ca",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$",
            "page": 13,
            "x": 124,
            "y": 370,
            "width": 94,
            "height": 27,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "b226535a-f074-4bb4-a6d2-7f6de0baf6b3",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "The definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.",
            "page": 13,
            "x": 51,
            "y": 400,
            "width": 244,
            "height": 58,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "9283b23c-32f4-478b-9340-af61e5cea15c",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "Theorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.",
            "page": 13,
            "x": 51,
            "y": 463,
            "width": 243,
            "height": 32,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "0707e17e-745b-407b-974c-50b9c0f3ff11",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "Proof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have",
            "page": 13,
            "x": 52,
            "y": 498,
            "width": 241,
            "height": 21,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "91ea7d4a-f191-4ef1-b1eb-cb8d6882b8f5",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$",
            "page": 13,
            "x": 96,
            "y": 523,
            "width": 155,
            "height": 44,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "15eed6aa-ac40-49e7-b906-e70b461f725f",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "Combining these inequalities gives",
            "page": 13,
            "x": 51,
            "y": 569,
            "width": 131,
            "height": 12,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "a186adbd-d8ca-4ba2-b5f3-e2a964c63cd2",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        },
        {
            "text": "$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$",
            "page": 13,
            "x": 59,
            "y": 585,
            "width": 225,
            "height": 52,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-appendix",
            "chunk_id": "4b67cf2d-3044-41d0-b441-264f3bad262f",
            "group_text": "APPENDIX\n\nA.  WEIGHTED QUANTILE SKETCH  \n    In this section, we introduce the weighted quantile sketch algo-\nrithm. Approximate answer of quantile queries is for many real-\nworld applications. One classical approach to this problem is GK\nalgorithm [14] and extensions based on the GK framework [24].\nThe main component of these algorithms is a data structure called\nquantile summary, that is able to answer quantile queries with\nrelative accuracy of \u03f5. Two operations are defined for a quantile\nsummary:\n\n- \u2022 A merge operation that combines two summaries with approximation error $\\epsilon_1$ and $\\epsilon_2$ together and create a merged summary with approximation error $\\max(\\epsilon_1, \\epsilon_2)$.\n\n- \u2022 A prune operation that reduces the number of elements in the summary to $b+1$ and changes approximation error from $\\epsilon$ to $\\epsilon + \\frac{1}{b}$.\n\nA quantile summary with merge and prune operations forms basic\nbuilding blocks of the distributed and streaming quantile comput-\ning algorithms [24].\n\nIn order to use quantile computation for approximate tree boosting, we need to find quantiles on weighted data. This more general problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the new algorithm contains merge and prune operations with *the same guarantee* as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data efficiently.\n\nA.1  Formalization and Definitions\n\n    Given an input multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots (x_n, w_n)\\}$ such that $w_i \\in [0, +\\infty), x_i \\in \\mathcal{X}$. Each $x_i$ corresponds to a position of the point and $w_i$ is the weight of the point. Assume we have a total order $<$ defined on $\\mathcal{X}$. Let us define two rank functions $r_{\\mathcal{D}}^{-}, r_{\\mathcal{D}}^{+}: \\mathcal{X} \\rightarrow [0, +\\infty)$\n\n$r_{\\overline{\\mathcal{D}}}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x < y} w \\qquad\\qquad\\qquad (10)$\n\n$r_{\\mathcal{D}}^{+}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x \\leq y} w \\qquad (11)$\n\nWe should note that since $\\mathcal{D}$ is defined to be a *multiset* of the points. It can contain multiple record with exactly same position $x$ and weight $w$. We also define another weight function $\\omega_\\mathcal{D} : \\mathcal{X} \\to [0, +\\infty)$ as\n\n$\\omega_{\\mathcal{D}}(y) = r_{\\mathcal{D}}^{+}(y) - r_{\\mathcal{D}}^{-}(y) = \\sum_{(x, w) \\in \\mathcal{D}, x = y} w.$\n\nFinally, we also define the weight of multi-set $D$ to be the sum of\nweights of all the points in the set\n\n$\\omega'(\\mathcal{D}) = \\sum_{(x, w) \\in \\mathcal{D}} w \\hspace{4cm} (13)$\n\nOur task is given a series of input $\\mathcal{D}$, to estimate $r^+(y)$ and $r^-(y)$ for $y \\in \\mathcal{X}$ as well as finding points with specific rank. Given these notations, we define quantile summary of weighted examples as follows:\n\nDefinition A.1. *Quantile Summary of Weighted Data*  \nA quantile summary for $\\mathcal{D}$ is defined to be tuple $Q(\\mathcal{D}) = (S, \\overrightarrow{r}^+_{\\mathcal{D}}, \\overrightarrow{r}^-_{\\mathcal{D}}, \\overrightarrow{\\omega}_{\\mathcal{D}})$ where $S = \\{x_1, x_2, \\cdots, x_k\\}$ is selected from the points in $\\mathcal{D}$ (i.e. $x_i \\in \\{x|(x, w) \\in \\mathcal{D}\\}$) with the following properties:  \n1) $x_i < x_{i+1}$ for all $i$, and $x_1$ and $x_k$ are minimum and maximum point in $\\mathcal{D}$:\n\n$x_1 = \\min_{(x, w) \\in \\mathcal{D}} x,\\quad x_k = \\max_{(x, w) \\in \\mathcal{D}} x$\n\n$2)\\ \\vec{r}_D^+,\\ \\vec{r}_D^-,\\ \\text{and}\\ \\vec{\\omega}_D\\ \\text{are functions in}\\ S \\rightarrow [0, +\\infty),\\ \\text{that satisfies}$\n$\\vec{r}_D^-(x_i) \\leq r_D^-(x_i),\\quad r_D^+(x_i) \\geq r_D^+(x_i),\\quad \\vec{\\omega}_D(x_i) \\leq \\omega_D(x_i), \\tag{14}$\n\nthe equality sign holds for maximum and minimum point ($\\vec{r}_D^-(x_i) = \\vec{r}_D^-(x_i)$, $\\vec{r}_D^+(x_i) = \\vec{r}_D^+(x_i)$ and $\\tilde{\\omega}_D(x_i) = \\omega_D(x_i)$ for $i \\in \\{1, k\\}$).\nFinally, the function value must also satisfy the following constraints\n\n$ \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) \\leq \\vec{r}_D^-(x_{i+1}), \\quad \\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) $\n  \n$ (15) $\n\nSince these functions are only defined on $S$, it is suffice to use $4k$ record to store the summary. Specifically, we need to remember each $x_i$ and the corresponding function values of each $x_i$.\n\nDefinition A.2. *Extension of Function Domains*  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}_\\mathcal{D}^+, \\tilde{r}_\\mathcal{D}^-, \\tilde{\\omega}_\\mathcal{D})$ defined in Definition A.1, the domain of $\\tilde{r}_\\mathcal{D}^+$, $\\tilde{r}_\\mathcal{D}^-$ and $\\tilde{\\omega}_\\mathcal{D}$ were defined only in $S$. We extend the definition of these functions to $\\mathcal{X} \\to [0, +\\infty)$ as follows  \nWhen $y < x_1$:\n\n$\\bar{r}_D^-(y) = 0, \\quad \\bar{r}_D^+(y) = 0, \\quad \\bar{\\omega}_D(y) = 0 \\qquad\\qquad (16)$\n\nWhen $y > x_k$:\n\n$\\vec{r}_D^-(y) = \\vec{r}_D^+(x_k),\\ \\vec{r}_D^+(y) = \\vec{r}_D^+(x_k),\\ \\bar{\\omega}_D(y) = 0 \\qquad (17)$\n\nWhen $y \\in (x_i, x_{i+1})$ for some $i$:\n\n$\n\\vec{r}_D^-(y) = \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i), \\\\\n\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}), \\\\\n\\vec{\\omega}_D(y) = 0\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (18)\n$\n\nLemma A.1. *Extended Constraint*  \n*The extended definition of* $\\vec{r}_D^-$, $\\vec{r}_D^+$, $\\vec{\\omega}_D$ *satisfies the following constraints*\n\n$\\bar{r}_D^-(y) \\leq \\bar{r}_D(y),\\quad \\bar{r}_D^+(y) \\geq \\bar{r}_D(y),\\quad \\bar{\\omega}_D(y) \\leq \\omega_D(y) \\qquad (19)$\n\n$\\vec{r}_D^-(y) + \\vec{\\omega}_D(y) \\leq \\vec{r}_D^-(x), \\qquad \\vec{r}_D^+(y) \\leq \\vec{r}_D^+(x) - \\vec{\\omega}_D(x), \\text{ for all } y < x \\tag{20}$\n\nProof. The only non-trivial part is to prove the case when\n$y \\in (x_i, x_{i+1})$:\n\n$\\vec{r}_{\\mathcal{D}}(y) = \\vec{r}_{\\mathcal{D}}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(x_i) + \\omega_{\\mathcal{D}}(x_i) \\leq \\vec{r}_{\\mathcal{D}}(y)$\n\n$\\vec{r}_D^{\\top}(y) = \\vec{r}_D^{\\top}(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) \\geq r_D^{\\top}(x_{i+1}) - \\omega_D(x_{i+1}) \\geq r_D^{\\top}(y)$\n\n$\\vec{r}_D^+(x_i) \\leq \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) = \\vec{r}_D^+(y) - \\vec{\\omega}_D(y)$\n\n$\\vec{r}_D^-(y) + \\tilde{\\omega}_D(y) = \\vec{r}_D^-(x_i) + \\tilde{\\omega}_D(x_i) + 0 \\leq \\vec{r}_D^-(x_{i+1})$\n$\\vec{r}_D^+(y) = \\vec{r}_D^+(x_{i+1}) - \\tilde{\\omega}_D(x_{i+1})$\n\nUsing these facts and transitivity of < relation, we can prove Eq. (20) \u25a1\n\nWe should note that the extension is based on the ground case defined in $S$, and we do not require extra space to store the summary in order to use the extended definition. We are now ready to introduce the definition of $\\epsilon$-approximate quantile summary.\n\nDefinition A.3. $\\epsilon$-Approximate Quantile Summary  \nGiven a quantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_\\mathcal{D}, \\tilde{r}^-_\\mathcal{D}, \\tilde{\\omega}_\\mathcal{D})$, we call it is  \n$\\epsilon$-approximate summary if for any $y \\in \\mathcal{X}$\n\n$\\vec{r}_{\\mathcal{D}}^{+}(y) - \\vec{r}_{\\mathcal{D}}^{-}(y) - \\bar{\\omega}_{\\mathcal{D}}(y) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2cm} (21)$\n\n*We use this definition since we know that* $r^{-}(y) \\in [\\bar{r}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y) - \\hat{\\omega}_{\\mathcal{D}}(y)]$ *and* $r^{+}(y) \\in [\\bar{r}_{\\mathcal{D}}(y) + \\hat{\\omega}_{\\mathcal{D}}(y), \\bar{r}_{\\mathcal{D}}^{+}(y)]$. *Eq. (21) means the we can get estimation of* $r^{+}(y)$ *and* $r^{-}(y)$ *by error of at most* $\\epsilon \\omega(\\mathcal{D})$.\n\nLemma A.2. *Quantile summary* $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ *is an* $\\epsilon$-*approximate summary if and only if the following two condition holds*\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_i) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (22) $\n\n$ \\vec{r}_{\\mathcal{D}}^+(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^-(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_i) \\leq \\epsilon \\omega(\\mathcal{D}) \\hspace{2em} (23) $\n\n$ \\text{Proof. \\textit{The key is again consider} } y \\in (x_i, x_{i+1}) $\n$$\n\\vec{r}_D^+(y) - \\vec{r}_D^-(y) - \\vec{\\omega}_D(y) = [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})] - [\\vec{r}_D^+(x_i) + \\vec{\\omega}_D(x_i)] - 0\n$$\n$\\textit{This means the condition in Eq. (23) plus Eq.(22) can give us Eq. (21) } \\quad \\square$\n\n**Property of Extended Function** In this section, we have introduced the extension of function $\\vec{r}_D^+, \\vec{r}_D^-, \\tilde{\\omega}_D$ to $\\mathcal{X} \\rightarrow [0, +\\infty)$. The key theme discussed in this section is the relation of _constraints on the original function and constraints on the extended function_. Lemma A.1 and  A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections.\n\n## A.2  Construction of Initial Summary\n\nGiven a small multi-set $\\mathcal{D} = \\{(x_1, w_1), (x_2, w_2), \\cdots, (x_n, w_n)\\}$, we can construct initial summary $Q(\\mathcal{D}) = \\{S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}\\}$, with $S$ to the set of all values in $\\mathcal{D}$ ($S = \\{x|(x, w) \\in \\mathcal{D}\\}$), and $\\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}}$ defined to be\n\n$\\vec{r}_{\\mathcal{D}}^+(x) = r_{\\mathcal{D}}^+(x), \\quad \\vec{r}_{\\mathcal{D}}^-(x) = r_{\\mathcal{D}}^-(x), \\quad \\vec{\\omega}_{\\mathcal{D}}(x) = \\omega_{\\mathcal{D}}(x)$ for $x \\in S$\n\nThe constructed summary is 0-approximate summary, since it can\nanswer all the queries accurately. The constructed summary can\nbe feed into future operations described in the latter sections.\n\n### A.3  Merge Operation\nIn this section, we define how we can merge the two summaries together. Assume we have $Q(\\mathcal{D}_1) = (S_1, \\vec{r}_{\\mathcal{D}_1}^+, \\vec{r}_{\\mathcal{D}_1}^-, \\vec{\\omega}_{\\mathcal{D}_1})$ and $Q(\\mathcal{D}_2) = (S_2, \\vec{r}_{\\mathcal{D}_2}^+, \\vec{r}_{\\mathcal{D}_2}^-, \\vec{\\omega}_{\\mathcal{D}_2})$ quantile summary of two dataset $D_1$ and $D_2$. Let $D = D_1 \\cup D_2$, and define the merged summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^+, \\vec{r}_{\\mathcal{D}}^-, \\vec{\\omega}_{\\mathcal{D}})$ as follows.\n\n$S = \\{x_1, x_2, \\cdots, x_k\\}, x_i \\in S_1 \\text{ or } x_i \\in S_2$  (25)\n\nThe points in $S$ are combination of points in $S_1$ and $S_2$. And the function $\\vec{r}_D^+$, $\\vec{r}_D^-$, $\\omega_D$ are defined to be\n\n$\\vec{r}_D^+(x_i) = \\vec{r}_{D_1}^+(x_i) + \\vec{r}_{D_2}^+(x_i)$  \\hfill (26)\n\n$\\vec{r}_D^-(x_i) = \\vec{r}_{D_1}^-(x_i) + \\vec{r}_{D_2}^-(x_i)$  \\hfill (27)\n\n$\\omega_D(x_i) = \\omega_{D_1}(x_i) + \\omega_{D_2}(x_i)$  \\hfill (28)\n\nHere we use functions defined on $S \\rightarrow [0, +\\infty)$ on the left sides of equalities and use the extended function definitions on the right sides.\n\nDue to additive nature of $r^+$, $r^-$ and $\\omega$, which can be formally written as\n\n$\\vec{r}_D^-(y) = \\vec{r}_{D_1}^-(y) + \\vec{r}_{D_2}^-(y),$\n\n$\\vec{r}_D^+(y) = \\vec{r}_{D_1}^+(y) + \\vec{r}_{D_2}^+(y),$\n\n$\\omega_D(y) = \\omega_{D_1}(y) + \\omega_{D_2}(y),$  \\hfill (29)\n\nand the extended constraint property in Lemma A.1, we can verify\nthat $Q(\\mathcal{D})$ satisfies all the constraints in Definition A.1. Therefore\nit is a valid quantile summary.\n\nLemma A.3. *The combined quantile summary satisfies*\n\n$ \\tilde{r}_{\\mathcal{D}}^{-}(y) = \\tilde{r}_{\\mathcal{D}_1}^{-}(y) + \\tilde{r}_{\\mathcal{D}_2}^{-}(y) \\hspace{3cm} (30) $\n\n$ \\tilde{r}_{\\mathcal{D}}^{+}(y) = \\tilde{r}_{\\mathcal{D}_1}^{+}(y) + \\tilde{r}_{\\mathcal{D}_2}^{+}(y) \\hspace{3cm} (31) $\n\n$ \\tilde{\\omega}_{\\mathcal{D}}(y) = \\tilde{\\omega}_{\\mathcal{D}_1}(y) + \\tilde{\\omega}_{\\mathcal{D}_2}(y) \\hspace{3cm} (32) $\n\nAlgorithm 4: Query Function $g(Q, d)$  \nInput: $d: 0 \\leq d \\leq \\omega(\\mathcal{D})$  \nInput: $Q(\\mathcal{D}) = (S, \\vec{r}_D^-, \\vec{r}_D^+, \\vec{\\omega}_D)$ where  \n\\hspace{1cm} $S = x_1, x_2, \\ldots, x_k$  \nif $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ then return $x_1$ ;  \nif $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ then return $x_k$ ;  \nFind $i$ such that  \n$\\frac{1}{2} [\\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)] \\leq d < \\frac{1}{2} [\\vec{r}_D^-(x_{i+1}) + \\vec{r}_D^+(x_{i+1})]$  \nif $2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$ then  \n\\hspace{1cm} return $x_i$  \nelse  \n\\hspace{1cm} return $x_{i+1}$  \nend\n\nThis can be obtained by straight-forward application of Definition A.2.\n\nTheorem A.1. If $Q(\\mathcal{D}_1)$ is $\\epsilon_1$-approximate summary, and $Q(\\mathcal{D}_2)$ is $\\epsilon_2$-approximate summary. Then the merged summary $Q(\\mathcal{D})$ is $\\max(\\epsilon_1, \\epsilon_2)$-approximate summary.\n\nProof. For any $y \\in \\mathcal{X}$, we have\n\n$ \\vec{r}_{\\mathcal{D}}^+(y) - \\vec{r}_{\\mathcal{D}}^-(y) - \\vec{\\omega}_{\\mathcal{D}}(y) $\n\n$= [\\vec{r}_{\\mathcal{D}_1}^+(y) + \\vec{r}_{\\mathcal{D}_2}^+(y)] - [\\vec{r}_{\\mathcal{D}_1}^-(y) + \\vec{r}_{\\mathcal{D}_2}^-(y)] - [\\vec{\\omega}_{\\mathcal{D}_1}(y) + \\vec{\\omega}_{\\mathcal{D}_2}(y)]$\n\n$\\leq \\epsilon_1 \\omega(\\mathcal{D}_1) + \\epsilon_2 \\omega(\\mathcal{D}_2) \\leq \\max(\\epsilon_1, \\epsilon_2) \\omega(\\mathcal{D}_1 \\cup \\mathcal{D}_2)$\n\nHere the first inequality is due to Lemma A.3. [ ]\n\nA.4  Prune Operation  \n    Before we start discussing the prune operation, we first introduce a query function $g(Q, d)$. The definition of function is shown in Algorithm 4. For a given rank $d$, the function returns a $x$ whose rank is close to $d$. This property is formally described in the following Lemma.\n\n$ \\text{Lemma A.4. \\textit{For a given $\\epsilon$-approximate summary $Q(\\mathcal{D}) = (S, \\vec{r}_{\\mathcal{D}}^{+}, \\vec{r}_{\\mathcal{D}}^{-}, \\vec{\\omega}_{\\mathcal{D}})$, $x^* = g(Q, d)$ satisfies the following property}} $\n\n$ d \\geq \\vec{r}_{\\mathcal{D}}^{+}(x^*) - \\vec{\\omega}_{\\mathcal{D}}(x^*) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) $\n\n$ d \\leq \\vec{r}_{\\mathcal{D}}^{-}(x^*) + \\vec{\\omega}_{\\mathcal{D}}(x^*) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) \\hspace{2cm} (33) $\n\nProof. We need to discuss four possible cases\n\n- \u2022 $d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]$ and $x^* = x_1$. Note that the rank information for $x_1$ is accurate ($\\tilde{\\omega}_D(x_1) = \\vec{r}_D^+(x_1) = \\omega(x_1)$, $\\vec{r}_D^-(x_1) = 0$), we have\n\n  $$\n  d \\geq 0 - \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_1) - \\tilde{\\omega}_D(x_1) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n  $$\n  d < \\frac{1}{2} [\\vec{r}_D^-(x_1) + \\vec{r}_D^+(x_1)]\n  $$\n\n  $$\n  \\leq \\vec{r}_D^+(x_1) + \\vec{r}_D^-(x_1)\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_1) + \\tilde{\\omega}_D^-(x_1)\n  $$\n\n- \u2022 $d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]$ and $x^* = x_k$, then\n\n  $$\n  d \\geq \\frac{1}{2} [\\vec{r}_D^-(x_k) + \\vec{r}_D^+(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} [\\vec{r}_D^+(x_k) - \\vec{r}_D^-(x_k)]\n  $$\n\n  $$\n  = \\vec{r}_D^+(x_k) - \\frac{1}{2} \\omega(x_k)\n  $$\n\n  $$\n  d < \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D}) = \\vec{r}_D^+(x_k) + \\tilde{\\omega}_D(x_k) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})\n  $$\n\n$x^* = x_i$ in the general case, then\n\n$2d < \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i) + \\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1})$\n\n$= 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + [\\vec{r}_D^+(x_{i+1}) - \\vec{\\omega}_D(x_{i+1}) - \\vec{r}_D^-(x_i) - \\vec{\\omega}_D(x_i)]$\n\n$\\leq 2[\\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\epsilon \\omega(D)$\n\n$2d \\geq \\vec{r}_D^-(x_i) + \\vec{r}_D^+(x_i)$\n\n$= 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - [\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i) - \\vec{r}_D^-(x_i) + \\vec{\\omega}_D(x_i)] + \\vec{\\omega}_D(x_i)$\n\n$\\geq 2[\\vec{r}_D^+(x_i) - \\vec{\\omega}_D(x_i)] - \\epsilon \\omega(D) + 0$\n\n- \u2022 $x^* = x_{i+1}$ in the general case\n  $$\n  \\begin{aligned}\n  2d &\\geq \\vec{r}_{\\mathcal{D}}^{-}(x_i) + \\vec{\\omega}_{\\mathcal{D}}(x_i) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad - [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_i) - \\vec{\\omega}_{\\mathcal{D}}(x_i)] \\\\\n     &\\geq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] - \\epsilon \\omega(\\mathcal{D}) \\\\\n  2d &\\leq \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1}) + \\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) \\\\\n     &= 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] \\\\\n     &\\quad + [\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) - \\vec{\\omega}_{\\mathcal{D}}(x_{i+1}) - \\vec{r}_{\\mathcal{D}}^{-}(x_{i+1})] \\\\\n     &\\leq 2[\\vec{r}_{\\mathcal{D}}^{+}(x_{i+1}) + \\vec{\\omega}_{\\mathcal{D}}(x_{i+1})] + \\epsilon \\omega(\\mathcal{D}) - 0\n  \\end{aligned}\n  $$\n\nNow we are ready to introduce the prune operation. Given a\nquantile summary $Q(\\mathcal{D}) = (S, \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S = \\{x_1, x_2, \\cdots, x_k\\}$\nelements, and a memory budget $b$. The prune operation creates\nanother summary $Q'(\\mathcal{D}) = (S', \\tilde{r}^+_D, \\tilde{r}^-_D, \\tilde{\\omega}_D)$ with $S' = \\{x'_1, x'_2, \\cdots, x'_{b+1}\\}$,\nwhere $x'_i$ are selected by query the original summary such that\n\n$x_i' = g\\left(Q, \\frac{i-1}{b} \\omega(\\mathcal{D})\\right).$\n\nThe definition of $\\tilde{r}_{D}^{+},\\tilde{r}_{D}^{-},\\tilde{\\omega}_{D}$ in $Q'$ is copied from original summary $Q$, by restricting input domain from $S$ to $S'$. There could be duplicated entries in the $S'$. These duplicated entries can be safely removed to further reduce the memory cost. Since all the elements in $Q'$ comes from $Q$, we can verify that $Q'$ satisfies all the constraints in Definition A.1 and is a valid quantile summary.\n\nTheorem A.2. Let $Q'(\\mathcal{D})$ be the summary pruned from an\n$\\epsilon$-approximate quantile summary $Q(\\mathcal{D})$ with $b$ memory budget.\nThen $Q'(\\mathcal{D})$ is a $(\\epsilon + \\frac{1}{b})$-approximate summary.\n\nProof. We only need to prove the property in Eq.\u00a0(23) for $Q'$. Using Lemma\u00a0A.4, we have\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) + \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\geq \\vec{r}_{\\mathcal{D}}^{+}(x_i') - \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\n$\n\\frac{i-1}{b}\\,\\omega(\\mathcal{D}) - \\frac{\\epsilon}{2}\\,\\omega'(\\mathcal{D}) \\leq \\vec{r}_{\\mathcal{D}}^{-}(x_i') + \\tilde{\\omega}_{\\mathcal{D}}(x_i')\n$\n\nCombining these inequalities gives\n\n$ \\vec{r}_D^+(x_{i+1}^\\prime) - \\vec{\\omega}_D(x_{i+1}^\\prime) - \\vec{r}_D^-(x_i^\\prime) - \\vec{\\omega}_D(x_i^\\prime) $\n\n$\\leq [\\frac{i}{b} \\omega(\\mathcal{D}) + \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] - [\\frac{i-1}{b} \\omega(\\mathcal{D}) - \\frac{\\epsilon}{2} \\omega(\\mathcal{D})] = (\\frac{1}{b} + \\epsilon)\\omega(\\mathcal{D})$\n\n$\\square$"
        }
    ]
}