{
    "annotations": [
        {
            "text": "Latent Dirichlet Allocation",
            "page": 1,
            "x": 221,
            "y": 98,
            "width": 168,
            "height": 18,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "0-title",
            "chunk_id": "9b806f09-8e49-4790-abae-02b709d67574",
            "group_text": "Latent Dirichlet Allocation\n\n**David M. Blei**  \n*Computer Science Division*  \n*University of California*  \n*Berkeley, CA 94720, USA*\n\nBLEI@CS.BERKELEY.EDU\n\nANG@CS.STANFORD.EDU\n\n**Andrew Y. Ng**  \n*Computer Science Department*  \n*Stanford University*  \n*Stanford, CA 94305, USA*\n\n**Michael I. Jordan**  \n*Computer Science Division and Department of Statistics*  \n*University of California*  \n*Berkeley, CA 94720, USA*\n\n**Editor:** John Lafferty"
        },
        {
            "text": "**David M. Blei**  \n*Computer Science Division*  \n*University of California*  \n*Berkeley, CA 94720, USA*",
            "page": 1,
            "x": 87,
            "y": 131,
            "width": 113,
            "height": 51,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "0-title",
            "chunk_id": "01620bd0-585f-4af8-a6f9-fccc55b18c7c",
            "group_text": "Latent Dirichlet Allocation\n\n**David M. Blei**  \n*Computer Science Division*  \n*University of California*  \n*Berkeley, CA 94720, USA*\n\nBLEI@CS.BERKELEY.EDU\n\nANG@CS.STANFORD.EDU\n\n**Andrew Y. Ng**  \n*Computer Science Department*  \n*Stanford University*  \n*Stanford, CA 94305, USA*\n\n**Michael I. Jordan**  \n*Computer Science Division and Department of Statistics*  \n*University of California*  \n*Berkeley, CA 94720, USA*\n\n**Editor:** John Lafferty"
        },
        {
            "text": "BLEI@CS.BERKELEY.EDU",
            "page": 1,
            "x": 411,
            "y": 134,
            "width": 111,
            "height": 13,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "0-title",
            "chunk_id": "c4ab93e9-eccd-474f-a3d8-22b81d3d5ae9",
            "group_text": "Latent Dirichlet Allocation\n\n**David M. Blei**  \n*Computer Science Division*  \n*University of California*  \n*Berkeley, CA 94720, USA*\n\nBLEI@CS.BERKELEY.EDU\n\nANG@CS.STANFORD.EDU\n\n**Andrew Y. Ng**  \n*Computer Science Department*  \n*Stanford University*  \n*Stanford, CA 94305, USA*\n\n**Michael I. Jordan**  \n*Computer Science Division and Department of Statistics*  \n*University of California*  \n*Berkeley, CA 94720, USA*\n\n**Editor:** John Lafferty"
        },
        {
            "text": "ANG@CS.STANFORD.EDU",
            "page": 1,
            "x": 412,
            "y": 188,
            "width": 110,
            "height": 11,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "0-title",
            "chunk_id": "7908a837-ff77-4b27-b264-eae3a4e2f8e5",
            "group_text": "Latent Dirichlet Allocation\n\n**David M. Blei**  \n*Computer Science Division*  \n*University of California*  \n*Berkeley, CA 94720, USA*\n\nBLEI@CS.BERKELEY.EDU\n\nANG@CS.STANFORD.EDU\n\n**Andrew Y. Ng**  \n*Computer Science Department*  \n*Stanford University*  \n*Stanford, CA 94305, USA*\n\n**Michael I. Jordan**  \n*Computer Science Division and Department of Statistics*  \n*University of California*  \n*Berkeley, CA 94720, USA*\n\n**Editor:** John Lafferty"
        },
        {
            "text": "**Andrew Y. Ng**  \n*Computer Science Department*  \n*Stanford University*  \n*Stanford, CA 94305, USA*",
            "page": 1,
            "x": 88,
            "y": 187,
            "width": 126,
            "height": 48,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "0-title",
            "chunk_id": "bac5a3b0-7398-4e34-8bd4-6e5f62c7d2bf",
            "group_text": "Latent Dirichlet Allocation\n\n**David M. Blei**  \n*Computer Science Division*  \n*University of California*  \n*Berkeley, CA 94720, USA*\n\nBLEI@CS.BERKELEY.EDU\n\nANG@CS.STANFORD.EDU\n\n**Andrew Y. Ng**  \n*Computer Science Department*  \n*Stanford University*  \n*Stanford, CA 94305, USA*\n\n**Michael I. Jordan**  \n*Computer Science Division and Department of Statistics*  \n*University of California*  \n*Berkeley, CA 94720, USA*\n\n**Editor:** John Lafferty"
        },
        {
            "text": "**Michael I. Jordan**  \n*Computer Science Division and Department of Statistics*  \n*University of California*  \n*Berkeley, CA 94720, USA*",
            "page": 1,
            "x": 88,
            "y": 242,
            "width": 227,
            "height": 53,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "0-title",
            "chunk_id": "c447f01a-973b-4b89-98d2-2d811e4eedcb",
            "group_text": "Latent Dirichlet Allocation\n\n**David M. Blei**  \n*Computer Science Division*  \n*University of California*  \n*Berkeley, CA 94720, USA*\n\nBLEI@CS.BERKELEY.EDU\n\nANG@CS.STANFORD.EDU\n\n**Andrew Y. Ng**  \n*Computer Science Department*  \n*Stanford University*  \n*Stanford, CA 94305, USA*\n\n**Michael I. Jordan**  \n*Computer Science Division and Department of Statistics*  \n*University of California*  \n*Berkeley, CA 94720, USA*\n\n**Editor:** John Lafferty"
        },
        {
            "text": "**Editor:** John Lafferty",
            "page": 1,
            "x": 88,
            "y": 318,
            "width": 90,
            "height": 13,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "0-title",
            "chunk_id": "2bce39d4-c075-4bce-bbaf-3cbb77dd686d",
            "group_text": "Latent Dirichlet Allocation\n\n**David M. Blei**  \n*Computer Science Division*  \n*University of California*  \n*Berkeley, CA 94720, USA*\n\nBLEI@CS.BERKELEY.EDU\n\nANG@CS.STANFORD.EDU\n\n**Andrew Y. Ng**  \n*Computer Science Department*  \n*Stanford University*  \n*Stanford, CA 94305, USA*\n\n**Michael I. Jordan**  \n*Computer Science Division and Department of Statistics*  \n*University of California*  \n*Berkeley, CA 94720, USA*\n\n**Editor:** John Lafferty"
        },
        {
            "text": "# Abstract\n\nWe describe _latent Dirichlet allocation_ (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.",
            "page": 1,
            "x": 107,
            "y": 354,
            "width": 396,
            "height": 127,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "1-abstract",
            "chunk_id": "f1a43807-11de-4911-a383-14ed88f26e1f",
            "group_text": "# Abstract\n\nWe describe _latent Dirichlet allocation_ (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model."
        },
        {
            "text": "1. Introduction\n\nIn this paper we consider the problem of modeling text corpora and other collections of discrete data. The goal is to find short descriptions of the members of a collection that enable efficient processing of large collections while preserving the essential statistical relationships that are useful for basic tasks such as classification, novelty detection, summarization, and similarity and relevance judgments.",
            "page": 1,
            "x": 87,
            "y": 502,
            "width": 434,
            "height": 92,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "14e069aa-47a9-43c4-9f55-de5222a5d4fa",
            "group_text": "1. Introduction\n\nIn this paper we consider the problem of modeling text corpora and other collections of discrete data. The goal is to find short descriptions of the members of a collection that enable efficient processing of large collections while preserving the essential statistical relationships that are useful for basic tasks such as classification, novelty detection, summarization, and similarity and relevance judgments.\n\nSignificant progress has been made on this problem by researchers in the field of information retrieval (IR) (Baeza-Yates and Ribeiro-Neto, 1999).  The basic methodology proposed by IR researchers for text corpora\u2014a methodology successfully deployed in modern Internet search engines\u2014reduces each document in the corpus to a vector of real numbers, each of which represents ratios of counts.  In the popular _tf-idf_ scheme (Salton and McGill, 1983), a basic vocabulary of \u201cwords\u201d or \u201cterms\u201d is chosen, and, for each document in the corpus, a count is formed of the number of occurrences of each word.  After suitable normalization, this term frequency count is compared to an inverse document frequency count, which measures the number of occurrences of a\n\nword in the entire corpus (generally on a log scale, and again suitably normalized). The end result\nis a term-by-document matrix $X$ whose columns contain the *tf-idf* values for each of the documents\nin the corpus. Thus the *tf-idf* scheme reduces documents of arbitrary length to fixed-length lists of\nnumbers.\n\nWhile the tf-idf reduction has some appealing features\u2014notably in its basic identification of sets\nof words that are discriminative for documents in the collection\u2014the approach also provides a rela-\ntively small amount of reduction in description length and reveals little in the way of inter- or intra-\ndocument statistical structure. To address these shortcomings, IR researchers have proposed several\nother dimensionality reduction techniques, most notably *latent semantic indexing (LSI)* (Deerwester\net al., 1990). LSI uses a singular value decomposition of the $X$ matrix to identify a linear subspace\nin the space of *tf-idf* features that captures most of the variance in the collection. This approach can\nachieve significant compression in large collections. Furthermore, Deerwester et al. argue that the\nderived features of LSI, which are linear combinations of the original *tf-idf* features, can capture\nsome aspects of basic linguistic notions such as synonymy and polysemy.\n\nTo substantiate the claims regarding LSI, and to study its relative strengths and weaknesses, it is\nuseful to develop a generative probabilistic model of text corpora and to study the ability of LSI to\nrecover aspects of the generative model from data (Papadimitriou et al., 1998). Given a generative\nmodel of text, however, it is not clear why one should adopt the LSI methodology\u2014one can attempt\nto proceed more directly, fitting the model to data using maximum likelihood or Bayesian methods.\n\nA significant step forward in this regard was made by Hofmann (1999), who presented the *probabilistic LSI (pLSI)* model, also known as the *aspect model*, as an alternative to LSI. The pLSI approach, which we describe in detail in Section 4.3, models each word in a document as a sample from a mixture model, where the mixture components are multinomial random variables that can be viewed as representations of \u201ctopics.\u201d Thus each word is generated from a single topic, and different words in a document may be generated from different topics. Each document is represented as a list of mixing proportions for these mixture components and thereby reduced to a probability distribution on a fixed set of topics. This distribution is the \u201creduced description\u201d associated with the document.\n\nWhile Hofmann\u2019s work is a useful step toward probabilistic modeling of text, it is incomplete\nin that it provides no probabilistic model at the level of documents.  In pLSI, each document is\nrepresented as a list of numbers (the mixing proportions for topics), and there is no generative\nprobabilistic model for these numbers.  This leads to several problems: (1) the number of parameters in the model grows linearly with the size of the corpus, which leads to serious problems with\noverfitting, and (2) it is not clear how to assign probability to a document outside of the training set.\n\nTo see how to proceed beyond pLSI, let us consider the fundamental probabilistic assumptions\nunderlying the class of dimensionality reduction methods that includes LSI and pLSI. All of these\nmethods are based on the \u201cbag-of-words\u201d assumption\u2014that the order of words in a document can\nbe neglected. In the language of probability theory, this is an assumption of *exchangeability* for the\nwords in a document (Aldous, 1985). Moreover, although less often stated formally, these methods\nalso assume that documents are exchangeable; the specific ordering of the documents in a corpus\ncan also be neglected.\n\nA classic representation theorem due to de Finetti (1990) establishes that any collection of exchangeable random variables has a representation as a mixture distribution\u2014in general an infinite mixture. Thus, if we wish to consider exchangeable representations for documents and words, we need to consider mixture models that capture the exchangeability of both words and documents.\n\nThis line of thinking leads to the *latent Dirichlet allocation (LDA)* model that we present in the current paper.\n\nIt is important to emphasize that an assumption of exchangeability is not equivalent to an as-\nsumption that the random variables are independent and identically distributed. Rather, exchange-\nability essentially can be interpreted as meaning \u201c*conditionally* independent and identically dis-\ntributed,\u201d where the conditioning is with respect to an underlying latent parameter of a probability\ndistribution. Conditionally, the joint distribution of the random variables is simple and factored\nwhile marginally over the latent parameter, the joint distribution can be quite complex. Thus, while\nan assumption of exchangeability is clearly a major simplifying assumption in the domain of text\nmodeling, and its principal justification is that it leads to methods that are computationally efficient,\nthe exchangeability assumptions do not necessarily lead to methods that are restricted to simple\nfrequency counts or linear operations. We aim to demonstrate in the current paper that, by taking\nthe de Finetti theorem seriously, we can capture significant intra-document statistical structure via\nthe mixing distribution.\n\nIt is also worth noting that there are a large number of generalizations of the basic notion of\nexchangeability, including various forms of partial exchangeability, and that representation theo-\nrems are available for these cases as well (Diaconis, 1988). Thus, while the work that we discuss in\nthe current paper focuses on simple \u201cbag-of-words\u201d models, which lead to mixture distributions for\nsingle words (unigrams), our methods are also applicable to richer models that involve mixtures for\nlarger structural units such as *n*-grams or paragraphs.\n\nThe paper is organized as follows. In Section 2 we introduce basic notation and terminology.\nThe LDA model is presented in Section 3 and is compared to related latent variable models in\nSection 4. We discuss inference and parameter estimation for LDA in Section 5. An illustrative\nexample of fitting LDA to data is provided in Section 6. Empirical results in text modeling, text\nclassification and collaborative filtering are presented in Section 7. Finally, Section 8 presents our\nconclusions."
        },
        {
            "text": "Significant progress has been made on this problem by researchers in the field of information retrieval (IR) (Baeza-Yates and Ribeiro-Neto, 1999).  The basic methodology proposed by IR researchers for text corpora\u2014a methodology successfully deployed in modern Internet search engines\u2014reduces each document in the corpus to a vector of real numbers, each of which represents ratios of counts.  In the popular _tf-idf_ scheme (Salton and McGill, 1983), a basic vocabulary of \u201cwords\u201d or \u201cterms\u201d is chosen, and, for each document in the corpus, a count is formed of the number of occurrences of each word.  After suitable normalization, this term frequency count is compared to an inverse document frequency count, which measures the number of occurrences of a",
            "page": 1,
            "x": 88,
            "y": 598,
            "width": 434,
            "height": 108,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "2b03095d-746d-41c8-8bbb-642f57359140",
            "group_text": "1. Introduction\n\nIn this paper we consider the problem of modeling text corpora and other collections of discrete data. The goal is to find short descriptions of the members of a collection that enable efficient processing of large collections while preserving the essential statistical relationships that are useful for basic tasks such as classification, novelty detection, summarization, and similarity and relevance judgments.\n\nSignificant progress has been made on this problem by researchers in the field of information retrieval (IR) (Baeza-Yates and Ribeiro-Neto, 1999).  The basic methodology proposed by IR researchers for text corpora\u2014a methodology successfully deployed in modern Internet search engines\u2014reduces each document in the corpus to a vector of real numbers, each of which represents ratios of counts.  In the popular _tf-idf_ scheme (Salton and McGill, 1983), a basic vocabulary of \u201cwords\u201d or \u201cterms\u201d is chosen, and, for each document in the corpus, a count is formed of the number of occurrences of each word.  After suitable normalization, this term frequency count is compared to an inverse document frequency count, which measures the number of occurrences of a\n\nword in the entire corpus (generally on a log scale, and again suitably normalized). The end result\nis a term-by-document matrix $X$ whose columns contain the *tf-idf* values for each of the documents\nin the corpus. Thus the *tf-idf* scheme reduces documents of arbitrary length to fixed-length lists of\nnumbers.\n\nWhile the tf-idf reduction has some appealing features\u2014notably in its basic identification of sets\nof words that are discriminative for documents in the collection\u2014the approach also provides a rela-\ntively small amount of reduction in description length and reveals little in the way of inter- or intra-\ndocument statistical structure. To address these shortcomings, IR researchers have proposed several\nother dimensionality reduction techniques, most notably *latent semantic indexing (LSI)* (Deerwester\net al., 1990). LSI uses a singular value decomposition of the $X$ matrix to identify a linear subspace\nin the space of *tf-idf* features that captures most of the variance in the collection. This approach can\nachieve significant compression in large collections. Furthermore, Deerwester et al. argue that the\nderived features of LSI, which are linear combinations of the original *tf-idf* features, can capture\nsome aspects of basic linguistic notions such as synonymy and polysemy.\n\nTo substantiate the claims regarding LSI, and to study its relative strengths and weaknesses, it is\nuseful to develop a generative probabilistic model of text corpora and to study the ability of LSI to\nrecover aspects of the generative model from data (Papadimitriou et al., 1998). Given a generative\nmodel of text, however, it is not clear why one should adopt the LSI methodology\u2014one can attempt\nto proceed more directly, fitting the model to data using maximum likelihood or Bayesian methods.\n\nA significant step forward in this regard was made by Hofmann (1999), who presented the *probabilistic LSI (pLSI)* model, also known as the *aspect model*, as an alternative to LSI. The pLSI approach, which we describe in detail in Section 4.3, models each word in a document as a sample from a mixture model, where the mixture components are multinomial random variables that can be viewed as representations of \u201ctopics.\u201d Thus each word is generated from a single topic, and different words in a document may be generated from different topics. Each document is represented as a list of mixing proportions for these mixture components and thereby reduced to a probability distribution on a fixed set of topics. This distribution is the \u201creduced description\u201d associated with the document.\n\nWhile Hofmann\u2019s work is a useful step toward probabilistic modeling of text, it is incomplete\nin that it provides no probabilistic model at the level of documents.  In pLSI, each document is\nrepresented as a list of numbers (the mixing proportions for topics), and there is no generative\nprobabilistic model for these numbers.  This leads to several problems: (1) the number of parameters in the model grows linearly with the size of the corpus, which leads to serious problems with\noverfitting, and (2) it is not clear how to assign probability to a document outside of the training set.\n\nTo see how to proceed beyond pLSI, let us consider the fundamental probabilistic assumptions\nunderlying the class of dimensionality reduction methods that includes LSI and pLSI. All of these\nmethods are based on the \u201cbag-of-words\u201d assumption\u2014that the order of words in a document can\nbe neglected. In the language of probability theory, this is an assumption of *exchangeability* for the\nwords in a document (Aldous, 1985). Moreover, although less often stated formally, these methods\nalso assume that documents are exchangeable; the specific ordering of the documents in a corpus\ncan also be neglected.\n\nA classic representation theorem due to de Finetti (1990) establishes that any collection of exchangeable random variables has a representation as a mixture distribution\u2014in general an infinite mixture. Thus, if we wish to consider exchangeable representations for documents and words, we need to consider mixture models that capture the exchangeability of both words and documents.\n\nThis line of thinking leads to the *latent Dirichlet allocation (LDA)* model that we present in the current paper.\n\nIt is important to emphasize that an assumption of exchangeability is not equivalent to an as-\nsumption that the random variables are independent and identically distributed. Rather, exchange-\nability essentially can be interpreted as meaning \u201c*conditionally* independent and identically dis-\ntributed,\u201d where the conditioning is with respect to an underlying latent parameter of a probability\ndistribution. Conditionally, the joint distribution of the random variables is simple and factored\nwhile marginally over the latent parameter, the joint distribution can be quite complex. Thus, while\nan assumption of exchangeability is clearly a major simplifying assumption in the domain of text\nmodeling, and its principal justification is that it leads to methods that are computationally efficient,\nthe exchangeability assumptions do not necessarily lead to methods that are restricted to simple\nfrequency counts or linear operations. We aim to demonstrate in the current paper that, by taking\nthe de Finetti theorem seriously, we can capture significant intra-document statistical structure via\nthe mixing distribution.\n\nIt is also worth noting that there are a large number of generalizations of the basic notion of\nexchangeability, including various forms of partial exchangeability, and that representation theo-\nrems are available for these cases as well (Diaconis, 1988). Thus, while the work that we discuss in\nthe current paper focuses on simple \u201cbag-of-words\u201d models, which lead to mixture distributions for\nsingle words (unigrams), our methods are also applicable to richer models that involve mixtures for\nlarger structural units such as *n*-grams or paragraphs.\n\nThe paper is organized as follows. In Section 2 we introduce basic notation and terminology.\nThe LDA model is presented in Section 3 and is compared to related latent variable models in\nSection 4. We discuss inference and parameter estimation for LDA in Section 5. An illustrative\nexample of fitting LDA to data is provided in Section 6. Empirical results in text modeling, text\nclassification and collaborative filtering are presented in Section 7. Finally, Section 8 presents our\nconclusions."
        },
        {
            "text": "word in the entire corpus (generally on a log scale, and again suitably normalized). The end result\nis a term-by-document matrix $X$ whose columns contain the *tf-idf* values for each of the documents\nin the corpus. Thus the *tf-idf* scheme reduces documents of arbitrary length to fixed-length lists of\nnumbers.",
            "page": 2,
            "x": 86,
            "y": 91,
            "width": 437,
            "height": 54,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "4f260292-9702-499b-9a48-32de74f8c503",
            "group_text": "1. Introduction\n\nIn this paper we consider the problem of modeling text corpora and other collections of discrete data. The goal is to find short descriptions of the members of a collection that enable efficient processing of large collections while preserving the essential statistical relationships that are useful for basic tasks such as classification, novelty detection, summarization, and similarity and relevance judgments.\n\nSignificant progress has been made on this problem by researchers in the field of information retrieval (IR) (Baeza-Yates and Ribeiro-Neto, 1999).  The basic methodology proposed by IR researchers for text corpora\u2014a methodology successfully deployed in modern Internet search engines\u2014reduces each document in the corpus to a vector of real numbers, each of which represents ratios of counts.  In the popular _tf-idf_ scheme (Salton and McGill, 1983), a basic vocabulary of \u201cwords\u201d or \u201cterms\u201d is chosen, and, for each document in the corpus, a count is formed of the number of occurrences of each word.  After suitable normalization, this term frequency count is compared to an inverse document frequency count, which measures the number of occurrences of a\n\nword in the entire corpus (generally on a log scale, and again suitably normalized). The end result\nis a term-by-document matrix $X$ whose columns contain the *tf-idf* values for each of the documents\nin the corpus. Thus the *tf-idf* scheme reduces documents of arbitrary length to fixed-length lists of\nnumbers.\n\nWhile the tf-idf reduction has some appealing features\u2014notably in its basic identification of sets\nof words that are discriminative for documents in the collection\u2014the approach also provides a rela-\ntively small amount of reduction in description length and reveals little in the way of inter- or intra-\ndocument statistical structure. To address these shortcomings, IR researchers have proposed several\nother dimensionality reduction techniques, most notably *latent semantic indexing (LSI)* (Deerwester\net al., 1990). LSI uses a singular value decomposition of the $X$ matrix to identify a linear subspace\nin the space of *tf-idf* features that captures most of the variance in the collection. This approach can\nachieve significant compression in large collections. Furthermore, Deerwester et al. argue that the\nderived features of LSI, which are linear combinations of the original *tf-idf* features, can capture\nsome aspects of basic linguistic notions such as synonymy and polysemy.\n\nTo substantiate the claims regarding LSI, and to study its relative strengths and weaknesses, it is\nuseful to develop a generative probabilistic model of text corpora and to study the ability of LSI to\nrecover aspects of the generative model from data (Papadimitriou et al., 1998). Given a generative\nmodel of text, however, it is not clear why one should adopt the LSI methodology\u2014one can attempt\nto proceed more directly, fitting the model to data using maximum likelihood or Bayesian methods.\n\nA significant step forward in this regard was made by Hofmann (1999), who presented the *probabilistic LSI (pLSI)* model, also known as the *aspect model*, as an alternative to LSI. The pLSI approach, which we describe in detail in Section 4.3, models each word in a document as a sample from a mixture model, where the mixture components are multinomial random variables that can be viewed as representations of \u201ctopics.\u201d Thus each word is generated from a single topic, and different words in a document may be generated from different topics. Each document is represented as a list of mixing proportions for these mixture components and thereby reduced to a probability distribution on a fixed set of topics. This distribution is the \u201creduced description\u201d associated with the document.\n\nWhile Hofmann\u2019s work is a useful step toward probabilistic modeling of text, it is incomplete\nin that it provides no probabilistic model at the level of documents.  In pLSI, each document is\nrepresented as a list of numbers (the mixing proportions for topics), and there is no generative\nprobabilistic model for these numbers.  This leads to several problems: (1) the number of parameters in the model grows linearly with the size of the corpus, which leads to serious problems with\noverfitting, and (2) it is not clear how to assign probability to a document outside of the training set.\n\nTo see how to proceed beyond pLSI, let us consider the fundamental probabilistic assumptions\nunderlying the class of dimensionality reduction methods that includes LSI and pLSI. All of these\nmethods are based on the \u201cbag-of-words\u201d assumption\u2014that the order of words in a document can\nbe neglected. In the language of probability theory, this is an assumption of *exchangeability* for the\nwords in a document (Aldous, 1985). Moreover, although less often stated formally, these methods\nalso assume that documents are exchangeable; the specific ordering of the documents in a corpus\ncan also be neglected.\n\nA classic representation theorem due to de Finetti (1990) establishes that any collection of exchangeable random variables has a representation as a mixture distribution\u2014in general an infinite mixture. Thus, if we wish to consider exchangeable representations for documents and words, we need to consider mixture models that capture the exchangeability of both words and documents.\n\nThis line of thinking leads to the *latent Dirichlet allocation (LDA)* model that we present in the current paper.\n\nIt is important to emphasize that an assumption of exchangeability is not equivalent to an as-\nsumption that the random variables are independent and identically distributed. Rather, exchange-\nability essentially can be interpreted as meaning \u201c*conditionally* independent and identically dis-\ntributed,\u201d where the conditioning is with respect to an underlying latent parameter of a probability\ndistribution. Conditionally, the joint distribution of the random variables is simple and factored\nwhile marginally over the latent parameter, the joint distribution can be quite complex. Thus, while\nan assumption of exchangeability is clearly a major simplifying assumption in the domain of text\nmodeling, and its principal justification is that it leads to methods that are computationally efficient,\nthe exchangeability assumptions do not necessarily lead to methods that are restricted to simple\nfrequency counts or linear operations. We aim to demonstrate in the current paper that, by taking\nthe de Finetti theorem seriously, we can capture significant intra-document statistical structure via\nthe mixing distribution.\n\nIt is also worth noting that there are a large number of generalizations of the basic notion of\nexchangeability, including various forms of partial exchangeability, and that representation theo-\nrems are available for these cases as well (Diaconis, 1988). Thus, while the work that we discuss in\nthe current paper focuses on simple \u201cbag-of-words\u201d models, which lead to mixture distributions for\nsingle words (unigrams), our methods are also applicable to richer models that involve mixtures for\nlarger structural units such as *n*-grams or paragraphs.\n\nThe paper is organized as follows. In Section 2 we introduce basic notation and terminology.\nThe LDA model is presented in Section 3 and is compared to related latent variable models in\nSection 4. We discuss inference and parameter estimation for LDA in Section 5. An illustrative\nexample of fitting LDA to data is provided in Section 6. Empirical results in text modeling, text\nclassification and collaborative filtering are presented in Section 7. Finally, Section 8 presents our\nconclusions."
        },
        {
            "text": "While the tf-idf reduction has some appealing features\u2014notably in its basic identification of sets\nof words that are discriminative for documents in the collection\u2014the approach also provides a rela-\ntively small amount of reduction in description length and reveals little in the way of inter- or intra-\ndocument statistical structure. To address these shortcomings, IR researchers have proposed several\nother dimensionality reduction techniques, most notably *latent semantic indexing (LSI)* (Deerwester\net al., 1990). LSI uses a singular value decomposition of the $X$ matrix to identify a linear subspace\nin the space of *tf-idf* features that captures most of the variance in the collection. This approach can\nachieve significant compression in large collections. Furthermore, Deerwester et al. argue that the\nderived features of LSI, which are linear combinations of the original *tf-idf* features, can capture\nsome aspects of basic linguistic notions such as synonymy and polysemy.",
            "page": 2,
            "x": 87,
            "y": 147,
            "width": 435,
            "height": 134,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "eee1df93-2cef-42f8-aa17-7a6b0165f69a",
            "group_text": "1. Introduction\n\nIn this paper we consider the problem of modeling text corpora and other collections of discrete data. The goal is to find short descriptions of the members of a collection that enable efficient processing of large collections while preserving the essential statistical relationships that are useful for basic tasks such as classification, novelty detection, summarization, and similarity and relevance judgments.\n\nSignificant progress has been made on this problem by researchers in the field of information retrieval (IR) (Baeza-Yates and Ribeiro-Neto, 1999).  The basic methodology proposed by IR researchers for text corpora\u2014a methodology successfully deployed in modern Internet search engines\u2014reduces each document in the corpus to a vector of real numbers, each of which represents ratios of counts.  In the popular _tf-idf_ scheme (Salton and McGill, 1983), a basic vocabulary of \u201cwords\u201d or \u201cterms\u201d is chosen, and, for each document in the corpus, a count is formed of the number of occurrences of each word.  After suitable normalization, this term frequency count is compared to an inverse document frequency count, which measures the number of occurrences of a\n\nword in the entire corpus (generally on a log scale, and again suitably normalized). The end result\nis a term-by-document matrix $X$ whose columns contain the *tf-idf* values for each of the documents\nin the corpus. Thus the *tf-idf* scheme reduces documents of arbitrary length to fixed-length lists of\nnumbers.\n\nWhile the tf-idf reduction has some appealing features\u2014notably in its basic identification of sets\nof words that are discriminative for documents in the collection\u2014the approach also provides a rela-\ntively small amount of reduction in description length and reveals little in the way of inter- or intra-\ndocument statistical structure. To address these shortcomings, IR researchers have proposed several\nother dimensionality reduction techniques, most notably *latent semantic indexing (LSI)* (Deerwester\net al., 1990). LSI uses a singular value decomposition of the $X$ matrix to identify a linear subspace\nin the space of *tf-idf* features that captures most of the variance in the collection. This approach can\nachieve significant compression in large collections. Furthermore, Deerwester et al. argue that the\nderived features of LSI, which are linear combinations of the original *tf-idf* features, can capture\nsome aspects of basic linguistic notions such as synonymy and polysemy.\n\nTo substantiate the claims regarding LSI, and to study its relative strengths and weaknesses, it is\nuseful to develop a generative probabilistic model of text corpora and to study the ability of LSI to\nrecover aspects of the generative model from data (Papadimitriou et al., 1998). Given a generative\nmodel of text, however, it is not clear why one should adopt the LSI methodology\u2014one can attempt\nto proceed more directly, fitting the model to data using maximum likelihood or Bayesian methods.\n\nA significant step forward in this regard was made by Hofmann (1999), who presented the *probabilistic LSI (pLSI)* model, also known as the *aspect model*, as an alternative to LSI. The pLSI approach, which we describe in detail in Section 4.3, models each word in a document as a sample from a mixture model, where the mixture components are multinomial random variables that can be viewed as representations of \u201ctopics.\u201d Thus each word is generated from a single topic, and different words in a document may be generated from different topics. Each document is represented as a list of mixing proportions for these mixture components and thereby reduced to a probability distribution on a fixed set of topics. This distribution is the \u201creduced description\u201d associated with the document.\n\nWhile Hofmann\u2019s work is a useful step toward probabilistic modeling of text, it is incomplete\nin that it provides no probabilistic model at the level of documents.  In pLSI, each document is\nrepresented as a list of numbers (the mixing proportions for topics), and there is no generative\nprobabilistic model for these numbers.  This leads to several problems: (1) the number of parameters in the model grows linearly with the size of the corpus, which leads to serious problems with\noverfitting, and (2) it is not clear how to assign probability to a document outside of the training set.\n\nTo see how to proceed beyond pLSI, let us consider the fundamental probabilistic assumptions\nunderlying the class of dimensionality reduction methods that includes LSI and pLSI. All of these\nmethods are based on the \u201cbag-of-words\u201d assumption\u2014that the order of words in a document can\nbe neglected. In the language of probability theory, this is an assumption of *exchangeability* for the\nwords in a document (Aldous, 1985). Moreover, although less often stated formally, these methods\nalso assume that documents are exchangeable; the specific ordering of the documents in a corpus\ncan also be neglected.\n\nA classic representation theorem due to de Finetti (1990) establishes that any collection of exchangeable random variables has a representation as a mixture distribution\u2014in general an infinite mixture. Thus, if we wish to consider exchangeable representations for documents and words, we need to consider mixture models that capture the exchangeability of both words and documents.\n\nThis line of thinking leads to the *latent Dirichlet allocation (LDA)* model that we present in the current paper.\n\nIt is important to emphasize that an assumption of exchangeability is not equivalent to an as-\nsumption that the random variables are independent and identically distributed. Rather, exchange-\nability essentially can be interpreted as meaning \u201c*conditionally* independent and identically dis-\ntributed,\u201d where the conditioning is with respect to an underlying latent parameter of a probability\ndistribution. Conditionally, the joint distribution of the random variables is simple and factored\nwhile marginally over the latent parameter, the joint distribution can be quite complex. Thus, while\nan assumption of exchangeability is clearly a major simplifying assumption in the domain of text\nmodeling, and its principal justification is that it leads to methods that are computationally efficient,\nthe exchangeability assumptions do not necessarily lead to methods that are restricted to simple\nfrequency counts or linear operations. We aim to demonstrate in the current paper that, by taking\nthe de Finetti theorem seriously, we can capture significant intra-document statistical structure via\nthe mixing distribution.\n\nIt is also worth noting that there are a large number of generalizations of the basic notion of\nexchangeability, including various forms of partial exchangeability, and that representation theo-\nrems are available for these cases as well (Diaconis, 1988). Thus, while the work that we discuss in\nthe current paper focuses on simple \u201cbag-of-words\u201d models, which lead to mixture distributions for\nsingle words (unigrams), our methods are also applicable to richer models that involve mixtures for\nlarger structural units such as *n*-grams or paragraphs.\n\nThe paper is organized as follows. In Section 2 we introduce basic notation and terminology.\nThe LDA model is presented in Section 3 and is compared to related latent variable models in\nSection 4. We discuss inference and parameter estimation for LDA in Section 5. An illustrative\nexample of fitting LDA to data is provided in Section 6. Empirical results in text modeling, text\nclassification and collaborative filtering are presented in Section 7. Finally, Section 8 presents our\nconclusions."
        },
        {
            "text": "To substantiate the claims regarding LSI, and to study its relative strengths and weaknesses, it is\nuseful to develop a generative probabilistic model of text corpora and to study the ability of LSI to\nrecover aspects of the generative model from data (Papadimitriou et al., 1998). Given a generative\nmodel of text, however, it is not clear why one should adopt the LSI methodology\u2014one can attempt\nto proceed more directly, fitting the model to data using maximum likelihood or Bayesian methods.",
            "page": 2,
            "x": 88,
            "y": 282,
            "width": 434,
            "height": 68,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "5de67b83-b73e-4dcb-be0b-44535af90252",
            "group_text": "1. Introduction\n\nIn this paper we consider the problem of modeling text corpora and other collections of discrete data. The goal is to find short descriptions of the members of a collection that enable efficient processing of large collections while preserving the essential statistical relationships that are useful for basic tasks such as classification, novelty detection, summarization, and similarity and relevance judgments.\n\nSignificant progress has been made on this problem by researchers in the field of information retrieval (IR) (Baeza-Yates and Ribeiro-Neto, 1999).  The basic methodology proposed by IR researchers for text corpora\u2014a methodology successfully deployed in modern Internet search engines\u2014reduces each document in the corpus to a vector of real numbers, each of which represents ratios of counts.  In the popular _tf-idf_ scheme (Salton and McGill, 1983), a basic vocabulary of \u201cwords\u201d or \u201cterms\u201d is chosen, and, for each document in the corpus, a count is formed of the number of occurrences of each word.  After suitable normalization, this term frequency count is compared to an inverse document frequency count, which measures the number of occurrences of a\n\nword in the entire corpus (generally on a log scale, and again suitably normalized). The end result\nis a term-by-document matrix $X$ whose columns contain the *tf-idf* values for each of the documents\nin the corpus. Thus the *tf-idf* scheme reduces documents of arbitrary length to fixed-length lists of\nnumbers.\n\nWhile the tf-idf reduction has some appealing features\u2014notably in its basic identification of sets\nof words that are discriminative for documents in the collection\u2014the approach also provides a rela-\ntively small amount of reduction in description length and reveals little in the way of inter- or intra-\ndocument statistical structure. To address these shortcomings, IR researchers have proposed several\nother dimensionality reduction techniques, most notably *latent semantic indexing (LSI)* (Deerwester\net al., 1990). LSI uses a singular value decomposition of the $X$ matrix to identify a linear subspace\nin the space of *tf-idf* features that captures most of the variance in the collection. This approach can\nachieve significant compression in large collections. Furthermore, Deerwester et al. argue that the\nderived features of LSI, which are linear combinations of the original *tf-idf* features, can capture\nsome aspects of basic linguistic notions such as synonymy and polysemy.\n\nTo substantiate the claims regarding LSI, and to study its relative strengths and weaknesses, it is\nuseful to develop a generative probabilistic model of text corpora and to study the ability of LSI to\nrecover aspects of the generative model from data (Papadimitriou et al., 1998). Given a generative\nmodel of text, however, it is not clear why one should adopt the LSI methodology\u2014one can attempt\nto proceed more directly, fitting the model to data using maximum likelihood or Bayesian methods.\n\nA significant step forward in this regard was made by Hofmann (1999), who presented the *probabilistic LSI (pLSI)* model, also known as the *aspect model*, as an alternative to LSI. The pLSI approach, which we describe in detail in Section 4.3, models each word in a document as a sample from a mixture model, where the mixture components are multinomial random variables that can be viewed as representations of \u201ctopics.\u201d Thus each word is generated from a single topic, and different words in a document may be generated from different topics. Each document is represented as a list of mixing proportions for these mixture components and thereby reduced to a probability distribution on a fixed set of topics. This distribution is the \u201creduced description\u201d associated with the document.\n\nWhile Hofmann\u2019s work is a useful step toward probabilistic modeling of text, it is incomplete\nin that it provides no probabilistic model at the level of documents.  In pLSI, each document is\nrepresented as a list of numbers (the mixing proportions for topics), and there is no generative\nprobabilistic model for these numbers.  This leads to several problems: (1) the number of parameters in the model grows linearly with the size of the corpus, which leads to serious problems with\noverfitting, and (2) it is not clear how to assign probability to a document outside of the training set.\n\nTo see how to proceed beyond pLSI, let us consider the fundamental probabilistic assumptions\nunderlying the class of dimensionality reduction methods that includes LSI and pLSI. All of these\nmethods are based on the \u201cbag-of-words\u201d assumption\u2014that the order of words in a document can\nbe neglected. In the language of probability theory, this is an assumption of *exchangeability* for the\nwords in a document (Aldous, 1985). Moreover, although less often stated formally, these methods\nalso assume that documents are exchangeable; the specific ordering of the documents in a corpus\ncan also be neglected.\n\nA classic representation theorem due to de Finetti (1990) establishes that any collection of exchangeable random variables has a representation as a mixture distribution\u2014in general an infinite mixture. Thus, if we wish to consider exchangeable representations for documents and words, we need to consider mixture models that capture the exchangeability of both words and documents.\n\nThis line of thinking leads to the *latent Dirichlet allocation (LDA)* model that we present in the current paper.\n\nIt is important to emphasize that an assumption of exchangeability is not equivalent to an as-\nsumption that the random variables are independent and identically distributed. Rather, exchange-\nability essentially can be interpreted as meaning \u201c*conditionally* independent and identically dis-\ntributed,\u201d where the conditioning is with respect to an underlying latent parameter of a probability\ndistribution. Conditionally, the joint distribution of the random variables is simple and factored\nwhile marginally over the latent parameter, the joint distribution can be quite complex. Thus, while\nan assumption of exchangeability is clearly a major simplifying assumption in the domain of text\nmodeling, and its principal justification is that it leads to methods that are computationally efficient,\nthe exchangeability assumptions do not necessarily lead to methods that are restricted to simple\nfrequency counts or linear operations. We aim to demonstrate in the current paper that, by taking\nthe de Finetti theorem seriously, we can capture significant intra-document statistical structure via\nthe mixing distribution.\n\nIt is also worth noting that there are a large number of generalizations of the basic notion of\nexchangeability, including various forms of partial exchangeability, and that representation theo-\nrems are available for these cases as well (Diaconis, 1988). Thus, while the work that we discuss in\nthe current paper focuses on simple \u201cbag-of-words\u201d models, which lead to mixture distributions for\nsingle words (unigrams), our methods are also applicable to richer models that involve mixtures for\nlarger structural units such as *n*-grams or paragraphs.\n\nThe paper is organized as follows. In Section 2 we introduce basic notation and terminology.\nThe LDA model is presented in Section 3 and is compared to related latent variable models in\nSection 4. We discuss inference and parameter estimation for LDA in Section 5. An illustrative\nexample of fitting LDA to data is provided in Section 6. Empirical results in text modeling, text\nclassification and collaborative filtering are presented in Section 7. Finally, Section 8 presents our\nconclusions."
        },
        {
            "text": "A significant step forward in this regard was made by Hofmann (1999), who presented the *probabilistic LSI (pLSI)* model, also known as the *aspect model*, as an alternative to LSI. The pLSI approach, which we describe in detail in Section 4.3, models each word in a document as a sample from a mixture model, where the mixture components are multinomial random variables that can be viewed as representations of \u201ctopics.\u201d Thus each word is generated from a single topic, and different words in a document may be generated from different topics. Each document is represented as a list of mixing proportions for these mixture components and thereby reduced to a probability distribution on a fixed set of topics. This distribution is the \u201creduced description\u201d associated with the document.",
            "page": 2,
            "x": 88,
            "y": 352,
            "width": 434,
            "height": 120,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "5f26ab81-411e-4001-992b-3baf63db7b26",
            "group_text": "1. Introduction\n\nIn this paper we consider the problem of modeling text corpora and other collections of discrete data. The goal is to find short descriptions of the members of a collection that enable efficient processing of large collections while preserving the essential statistical relationships that are useful for basic tasks such as classification, novelty detection, summarization, and similarity and relevance judgments.\n\nSignificant progress has been made on this problem by researchers in the field of information retrieval (IR) (Baeza-Yates and Ribeiro-Neto, 1999).  The basic methodology proposed by IR researchers for text corpora\u2014a methodology successfully deployed in modern Internet search engines\u2014reduces each document in the corpus to a vector of real numbers, each of which represents ratios of counts.  In the popular _tf-idf_ scheme (Salton and McGill, 1983), a basic vocabulary of \u201cwords\u201d or \u201cterms\u201d is chosen, and, for each document in the corpus, a count is formed of the number of occurrences of each word.  After suitable normalization, this term frequency count is compared to an inverse document frequency count, which measures the number of occurrences of a\n\nword in the entire corpus (generally on a log scale, and again suitably normalized). The end result\nis a term-by-document matrix $X$ whose columns contain the *tf-idf* values for each of the documents\nin the corpus. Thus the *tf-idf* scheme reduces documents of arbitrary length to fixed-length lists of\nnumbers.\n\nWhile the tf-idf reduction has some appealing features\u2014notably in its basic identification of sets\nof words that are discriminative for documents in the collection\u2014the approach also provides a rela-\ntively small amount of reduction in description length and reveals little in the way of inter- or intra-\ndocument statistical structure. To address these shortcomings, IR researchers have proposed several\nother dimensionality reduction techniques, most notably *latent semantic indexing (LSI)* (Deerwester\net al., 1990). LSI uses a singular value decomposition of the $X$ matrix to identify a linear subspace\nin the space of *tf-idf* features that captures most of the variance in the collection. This approach can\nachieve significant compression in large collections. Furthermore, Deerwester et al. argue that the\nderived features of LSI, which are linear combinations of the original *tf-idf* features, can capture\nsome aspects of basic linguistic notions such as synonymy and polysemy.\n\nTo substantiate the claims regarding LSI, and to study its relative strengths and weaknesses, it is\nuseful to develop a generative probabilistic model of text corpora and to study the ability of LSI to\nrecover aspects of the generative model from data (Papadimitriou et al., 1998). Given a generative\nmodel of text, however, it is not clear why one should adopt the LSI methodology\u2014one can attempt\nto proceed more directly, fitting the model to data using maximum likelihood or Bayesian methods.\n\nA significant step forward in this regard was made by Hofmann (1999), who presented the *probabilistic LSI (pLSI)* model, also known as the *aspect model*, as an alternative to LSI. The pLSI approach, which we describe in detail in Section 4.3, models each word in a document as a sample from a mixture model, where the mixture components are multinomial random variables that can be viewed as representations of \u201ctopics.\u201d Thus each word is generated from a single topic, and different words in a document may be generated from different topics. Each document is represented as a list of mixing proportions for these mixture components and thereby reduced to a probability distribution on a fixed set of topics. This distribution is the \u201creduced description\u201d associated with the document.\n\nWhile Hofmann\u2019s work is a useful step toward probabilistic modeling of text, it is incomplete\nin that it provides no probabilistic model at the level of documents.  In pLSI, each document is\nrepresented as a list of numbers (the mixing proportions for topics), and there is no generative\nprobabilistic model for these numbers.  This leads to several problems: (1) the number of parameters in the model grows linearly with the size of the corpus, which leads to serious problems with\noverfitting, and (2) it is not clear how to assign probability to a document outside of the training set.\n\nTo see how to proceed beyond pLSI, let us consider the fundamental probabilistic assumptions\nunderlying the class of dimensionality reduction methods that includes LSI and pLSI. All of these\nmethods are based on the \u201cbag-of-words\u201d assumption\u2014that the order of words in a document can\nbe neglected. In the language of probability theory, this is an assumption of *exchangeability* for the\nwords in a document (Aldous, 1985). Moreover, although less often stated formally, these methods\nalso assume that documents are exchangeable; the specific ordering of the documents in a corpus\ncan also be neglected.\n\nA classic representation theorem due to de Finetti (1990) establishes that any collection of exchangeable random variables has a representation as a mixture distribution\u2014in general an infinite mixture. Thus, if we wish to consider exchangeable representations for documents and words, we need to consider mixture models that capture the exchangeability of both words and documents.\n\nThis line of thinking leads to the *latent Dirichlet allocation (LDA)* model that we present in the current paper.\n\nIt is important to emphasize that an assumption of exchangeability is not equivalent to an as-\nsumption that the random variables are independent and identically distributed. Rather, exchange-\nability essentially can be interpreted as meaning \u201c*conditionally* independent and identically dis-\ntributed,\u201d where the conditioning is with respect to an underlying latent parameter of a probability\ndistribution. Conditionally, the joint distribution of the random variables is simple and factored\nwhile marginally over the latent parameter, the joint distribution can be quite complex. Thus, while\nan assumption of exchangeability is clearly a major simplifying assumption in the domain of text\nmodeling, and its principal justification is that it leads to methods that are computationally efficient,\nthe exchangeability assumptions do not necessarily lead to methods that are restricted to simple\nfrequency counts or linear operations. We aim to demonstrate in the current paper that, by taking\nthe de Finetti theorem seriously, we can capture significant intra-document statistical structure via\nthe mixing distribution.\n\nIt is also worth noting that there are a large number of generalizations of the basic notion of\nexchangeability, including various forms of partial exchangeability, and that representation theo-\nrems are available for these cases as well (Diaconis, 1988). Thus, while the work that we discuss in\nthe current paper focuses on simple \u201cbag-of-words\u201d models, which lead to mixture distributions for\nsingle words (unigrams), our methods are also applicable to richer models that involve mixtures for\nlarger structural units such as *n*-grams or paragraphs.\n\nThe paper is organized as follows. In Section 2 we introduce basic notation and terminology.\nThe LDA model is presented in Section 3 and is compared to related latent variable models in\nSection 4. We discuss inference and parameter estimation for LDA in Section 5. An illustrative\nexample of fitting LDA to data is provided in Section 6. Empirical results in text modeling, text\nclassification and collaborative filtering are presented in Section 7. Finally, Section 8 presents our\nconclusions."
        },
        {
            "text": "While Hofmann\u2019s work is a useful step toward probabilistic modeling of text, it is incomplete\nin that it provides no probabilistic model at the level of documents.  In pLSI, each document is\nrepresented as a list of numbers (the mixing proportions for topics), and there is no generative\nprobabilistic model for these numbers.  This leads to several problems: (1) the number of parameters in the model grows linearly with the size of the corpus, which leads to serious problems with\noverfitting, and (2) it is not clear how to assign probability to a document outside of the training set.",
            "page": 2,
            "x": 88,
            "y": 474,
            "width": 433,
            "height": 80,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "8ad61a99-7486-4cd1-bd90-78438b45ad13",
            "group_text": "1. Introduction\n\nIn this paper we consider the problem of modeling text corpora and other collections of discrete data. The goal is to find short descriptions of the members of a collection that enable efficient processing of large collections while preserving the essential statistical relationships that are useful for basic tasks such as classification, novelty detection, summarization, and similarity and relevance judgments.\n\nSignificant progress has been made on this problem by researchers in the field of information retrieval (IR) (Baeza-Yates and Ribeiro-Neto, 1999).  The basic methodology proposed by IR researchers for text corpora\u2014a methodology successfully deployed in modern Internet search engines\u2014reduces each document in the corpus to a vector of real numbers, each of which represents ratios of counts.  In the popular _tf-idf_ scheme (Salton and McGill, 1983), a basic vocabulary of \u201cwords\u201d or \u201cterms\u201d is chosen, and, for each document in the corpus, a count is formed of the number of occurrences of each word.  After suitable normalization, this term frequency count is compared to an inverse document frequency count, which measures the number of occurrences of a\n\nword in the entire corpus (generally on a log scale, and again suitably normalized). The end result\nis a term-by-document matrix $X$ whose columns contain the *tf-idf* values for each of the documents\nin the corpus. Thus the *tf-idf* scheme reduces documents of arbitrary length to fixed-length lists of\nnumbers.\n\nWhile the tf-idf reduction has some appealing features\u2014notably in its basic identification of sets\nof words that are discriminative for documents in the collection\u2014the approach also provides a rela-\ntively small amount of reduction in description length and reveals little in the way of inter- or intra-\ndocument statistical structure. To address these shortcomings, IR researchers have proposed several\nother dimensionality reduction techniques, most notably *latent semantic indexing (LSI)* (Deerwester\net al., 1990). LSI uses a singular value decomposition of the $X$ matrix to identify a linear subspace\nin the space of *tf-idf* features that captures most of the variance in the collection. This approach can\nachieve significant compression in large collections. Furthermore, Deerwester et al. argue that the\nderived features of LSI, which are linear combinations of the original *tf-idf* features, can capture\nsome aspects of basic linguistic notions such as synonymy and polysemy.\n\nTo substantiate the claims regarding LSI, and to study its relative strengths and weaknesses, it is\nuseful to develop a generative probabilistic model of text corpora and to study the ability of LSI to\nrecover aspects of the generative model from data (Papadimitriou et al., 1998). Given a generative\nmodel of text, however, it is not clear why one should adopt the LSI methodology\u2014one can attempt\nto proceed more directly, fitting the model to data using maximum likelihood or Bayesian methods.\n\nA significant step forward in this regard was made by Hofmann (1999), who presented the *probabilistic LSI (pLSI)* model, also known as the *aspect model*, as an alternative to LSI. The pLSI approach, which we describe in detail in Section 4.3, models each word in a document as a sample from a mixture model, where the mixture components are multinomial random variables that can be viewed as representations of \u201ctopics.\u201d Thus each word is generated from a single topic, and different words in a document may be generated from different topics. Each document is represented as a list of mixing proportions for these mixture components and thereby reduced to a probability distribution on a fixed set of topics. This distribution is the \u201creduced description\u201d associated with the document.\n\nWhile Hofmann\u2019s work is a useful step toward probabilistic modeling of text, it is incomplete\nin that it provides no probabilistic model at the level of documents.  In pLSI, each document is\nrepresented as a list of numbers (the mixing proportions for topics), and there is no generative\nprobabilistic model for these numbers.  This leads to several problems: (1) the number of parameters in the model grows linearly with the size of the corpus, which leads to serious problems with\noverfitting, and (2) it is not clear how to assign probability to a document outside of the training set.\n\nTo see how to proceed beyond pLSI, let us consider the fundamental probabilistic assumptions\nunderlying the class of dimensionality reduction methods that includes LSI and pLSI. All of these\nmethods are based on the \u201cbag-of-words\u201d assumption\u2014that the order of words in a document can\nbe neglected. In the language of probability theory, this is an assumption of *exchangeability* for the\nwords in a document (Aldous, 1985). Moreover, although less often stated formally, these methods\nalso assume that documents are exchangeable; the specific ordering of the documents in a corpus\ncan also be neglected.\n\nA classic representation theorem due to de Finetti (1990) establishes that any collection of exchangeable random variables has a representation as a mixture distribution\u2014in general an infinite mixture. Thus, if we wish to consider exchangeable representations for documents and words, we need to consider mixture models that capture the exchangeability of both words and documents.\n\nThis line of thinking leads to the *latent Dirichlet allocation (LDA)* model that we present in the current paper.\n\nIt is important to emphasize that an assumption of exchangeability is not equivalent to an as-\nsumption that the random variables are independent and identically distributed. Rather, exchange-\nability essentially can be interpreted as meaning \u201c*conditionally* independent and identically dis-\ntributed,\u201d where the conditioning is with respect to an underlying latent parameter of a probability\ndistribution. Conditionally, the joint distribution of the random variables is simple and factored\nwhile marginally over the latent parameter, the joint distribution can be quite complex. Thus, while\nan assumption of exchangeability is clearly a major simplifying assumption in the domain of text\nmodeling, and its principal justification is that it leads to methods that are computationally efficient,\nthe exchangeability assumptions do not necessarily lead to methods that are restricted to simple\nfrequency counts or linear operations. We aim to demonstrate in the current paper that, by taking\nthe de Finetti theorem seriously, we can capture significant intra-document statistical structure via\nthe mixing distribution.\n\nIt is also worth noting that there are a large number of generalizations of the basic notion of\nexchangeability, including various forms of partial exchangeability, and that representation theo-\nrems are available for these cases as well (Diaconis, 1988). Thus, while the work that we discuss in\nthe current paper focuses on simple \u201cbag-of-words\u201d models, which lead to mixture distributions for\nsingle words (unigrams), our methods are also applicable to richer models that involve mixtures for\nlarger structural units such as *n*-grams or paragraphs.\n\nThe paper is organized as follows. In Section 2 we introduce basic notation and terminology.\nThe LDA model is presented in Section 3 and is compared to related latent variable models in\nSection 4. We discuss inference and parameter estimation for LDA in Section 5. An illustrative\nexample of fitting LDA to data is provided in Section 6. Empirical results in text modeling, text\nclassification and collaborative filtering are presented in Section 7. Finally, Section 8 presents our\nconclusions."
        },
        {
            "text": "To see how to proceed beyond pLSI, let us consider the fundamental probabilistic assumptions\nunderlying the class of dimensionality reduction methods that includes LSI and pLSI. All of these\nmethods are based on the \u201cbag-of-words\u201d assumption\u2014that the order of words in a document can\nbe neglected. In the language of probability theory, this is an assumption of *exchangeability* for the\nwords in a document (Aldous, 1985). Moreover, although less often stated formally, these methods\nalso assume that documents are exchangeable; the specific ordering of the documents in a corpus\ncan also be neglected.",
            "page": 2,
            "x": 88,
            "y": 557,
            "width": 432,
            "height": 93,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "fcf576e6-8002-4faf-b894-65f91af2c2a6",
            "group_text": "1. Introduction\n\nIn this paper we consider the problem of modeling text corpora and other collections of discrete data. The goal is to find short descriptions of the members of a collection that enable efficient processing of large collections while preserving the essential statistical relationships that are useful for basic tasks such as classification, novelty detection, summarization, and similarity and relevance judgments.\n\nSignificant progress has been made on this problem by researchers in the field of information retrieval (IR) (Baeza-Yates and Ribeiro-Neto, 1999).  The basic methodology proposed by IR researchers for text corpora\u2014a methodology successfully deployed in modern Internet search engines\u2014reduces each document in the corpus to a vector of real numbers, each of which represents ratios of counts.  In the popular _tf-idf_ scheme (Salton and McGill, 1983), a basic vocabulary of \u201cwords\u201d or \u201cterms\u201d is chosen, and, for each document in the corpus, a count is formed of the number of occurrences of each word.  After suitable normalization, this term frequency count is compared to an inverse document frequency count, which measures the number of occurrences of a\n\nword in the entire corpus (generally on a log scale, and again suitably normalized). The end result\nis a term-by-document matrix $X$ whose columns contain the *tf-idf* values for each of the documents\nin the corpus. Thus the *tf-idf* scheme reduces documents of arbitrary length to fixed-length lists of\nnumbers.\n\nWhile the tf-idf reduction has some appealing features\u2014notably in its basic identification of sets\nof words that are discriminative for documents in the collection\u2014the approach also provides a rela-\ntively small amount of reduction in description length and reveals little in the way of inter- or intra-\ndocument statistical structure. To address these shortcomings, IR researchers have proposed several\nother dimensionality reduction techniques, most notably *latent semantic indexing (LSI)* (Deerwester\net al., 1990). LSI uses a singular value decomposition of the $X$ matrix to identify a linear subspace\nin the space of *tf-idf* features that captures most of the variance in the collection. This approach can\nachieve significant compression in large collections. Furthermore, Deerwester et al. argue that the\nderived features of LSI, which are linear combinations of the original *tf-idf* features, can capture\nsome aspects of basic linguistic notions such as synonymy and polysemy.\n\nTo substantiate the claims regarding LSI, and to study its relative strengths and weaknesses, it is\nuseful to develop a generative probabilistic model of text corpora and to study the ability of LSI to\nrecover aspects of the generative model from data (Papadimitriou et al., 1998). Given a generative\nmodel of text, however, it is not clear why one should adopt the LSI methodology\u2014one can attempt\nto proceed more directly, fitting the model to data using maximum likelihood or Bayesian methods.\n\nA significant step forward in this regard was made by Hofmann (1999), who presented the *probabilistic LSI (pLSI)* model, also known as the *aspect model*, as an alternative to LSI. The pLSI approach, which we describe in detail in Section 4.3, models each word in a document as a sample from a mixture model, where the mixture components are multinomial random variables that can be viewed as representations of \u201ctopics.\u201d Thus each word is generated from a single topic, and different words in a document may be generated from different topics. Each document is represented as a list of mixing proportions for these mixture components and thereby reduced to a probability distribution on a fixed set of topics. This distribution is the \u201creduced description\u201d associated with the document.\n\nWhile Hofmann\u2019s work is a useful step toward probabilistic modeling of text, it is incomplete\nin that it provides no probabilistic model at the level of documents.  In pLSI, each document is\nrepresented as a list of numbers (the mixing proportions for topics), and there is no generative\nprobabilistic model for these numbers.  This leads to several problems: (1) the number of parameters in the model grows linearly with the size of the corpus, which leads to serious problems with\noverfitting, and (2) it is not clear how to assign probability to a document outside of the training set.\n\nTo see how to proceed beyond pLSI, let us consider the fundamental probabilistic assumptions\nunderlying the class of dimensionality reduction methods that includes LSI and pLSI. All of these\nmethods are based on the \u201cbag-of-words\u201d assumption\u2014that the order of words in a document can\nbe neglected. In the language of probability theory, this is an assumption of *exchangeability* for the\nwords in a document (Aldous, 1985). Moreover, although less often stated formally, these methods\nalso assume that documents are exchangeable; the specific ordering of the documents in a corpus\ncan also be neglected.\n\nA classic representation theorem due to de Finetti (1990) establishes that any collection of exchangeable random variables has a representation as a mixture distribution\u2014in general an infinite mixture. Thus, if we wish to consider exchangeable representations for documents and words, we need to consider mixture models that capture the exchangeability of both words and documents.\n\nThis line of thinking leads to the *latent Dirichlet allocation (LDA)* model that we present in the current paper.\n\nIt is important to emphasize that an assumption of exchangeability is not equivalent to an as-\nsumption that the random variables are independent and identically distributed. Rather, exchange-\nability essentially can be interpreted as meaning \u201c*conditionally* independent and identically dis-\ntributed,\u201d where the conditioning is with respect to an underlying latent parameter of a probability\ndistribution. Conditionally, the joint distribution of the random variables is simple and factored\nwhile marginally over the latent parameter, the joint distribution can be quite complex. Thus, while\nan assumption of exchangeability is clearly a major simplifying assumption in the domain of text\nmodeling, and its principal justification is that it leads to methods that are computationally efficient,\nthe exchangeability assumptions do not necessarily lead to methods that are restricted to simple\nfrequency counts or linear operations. We aim to demonstrate in the current paper that, by taking\nthe de Finetti theorem seriously, we can capture significant intra-document statistical structure via\nthe mixing distribution.\n\nIt is also worth noting that there are a large number of generalizations of the basic notion of\nexchangeability, including various forms of partial exchangeability, and that representation theo-\nrems are available for these cases as well (Diaconis, 1988). Thus, while the work that we discuss in\nthe current paper focuses on simple \u201cbag-of-words\u201d models, which lead to mixture distributions for\nsingle words (unigrams), our methods are also applicable to richer models that involve mixtures for\nlarger structural units such as *n*-grams or paragraphs.\n\nThe paper is organized as follows. In Section 2 we introduce basic notation and terminology.\nThe LDA model is presented in Section 3 and is compared to related latent variable models in\nSection 4. We discuss inference and parameter estimation for LDA in Section 5. An illustrative\nexample of fitting LDA to data is provided in Section 6. Empirical results in text modeling, text\nclassification and collaborative filtering are presented in Section 7. Finally, Section 8 presents our\nconclusions."
        },
        {
            "text": "A classic representation theorem due to de Finetti (1990) establishes that any collection of exchangeable random variables has a representation as a mixture distribution\u2014in general an infinite mixture. Thus, if we wish to consider exchangeable representations for documents and words, we need to consider mixture models that capture the exchangeability of both words and documents.",
            "page": 2,
            "x": 89,
            "y": 653,
            "width": 432,
            "height": 52,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "1c9a157c-ccd0-4a59-ac6c-bf1dc03793b9",
            "group_text": "1. Introduction\n\nIn this paper we consider the problem of modeling text corpora and other collections of discrete data. The goal is to find short descriptions of the members of a collection that enable efficient processing of large collections while preserving the essential statistical relationships that are useful for basic tasks such as classification, novelty detection, summarization, and similarity and relevance judgments.\n\nSignificant progress has been made on this problem by researchers in the field of information retrieval (IR) (Baeza-Yates and Ribeiro-Neto, 1999).  The basic methodology proposed by IR researchers for text corpora\u2014a methodology successfully deployed in modern Internet search engines\u2014reduces each document in the corpus to a vector of real numbers, each of which represents ratios of counts.  In the popular _tf-idf_ scheme (Salton and McGill, 1983), a basic vocabulary of \u201cwords\u201d or \u201cterms\u201d is chosen, and, for each document in the corpus, a count is formed of the number of occurrences of each word.  After suitable normalization, this term frequency count is compared to an inverse document frequency count, which measures the number of occurrences of a\n\nword in the entire corpus (generally on a log scale, and again suitably normalized). The end result\nis a term-by-document matrix $X$ whose columns contain the *tf-idf* values for each of the documents\nin the corpus. Thus the *tf-idf* scheme reduces documents of arbitrary length to fixed-length lists of\nnumbers.\n\nWhile the tf-idf reduction has some appealing features\u2014notably in its basic identification of sets\nof words that are discriminative for documents in the collection\u2014the approach also provides a rela-\ntively small amount of reduction in description length and reveals little in the way of inter- or intra-\ndocument statistical structure. To address these shortcomings, IR researchers have proposed several\nother dimensionality reduction techniques, most notably *latent semantic indexing (LSI)* (Deerwester\net al., 1990). LSI uses a singular value decomposition of the $X$ matrix to identify a linear subspace\nin the space of *tf-idf* features that captures most of the variance in the collection. This approach can\nachieve significant compression in large collections. Furthermore, Deerwester et al. argue that the\nderived features of LSI, which are linear combinations of the original *tf-idf* features, can capture\nsome aspects of basic linguistic notions such as synonymy and polysemy.\n\nTo substantiate the claims regarding LSI, and to study its relative strengths and weaknesses, it is\nuseful to develop a generative probabilistic model of text corpora and to study the ability of LSI to\nrecover aspects of the generative model from data (Papadimitriou et al., 1998). Given a generative\nmodel of text, however, it is not clear why one should adopt the LSI methodology\u2014one can attempt\nto proceed more directly, fitting the model to data using maximum likelihood or Bayesian methods.\n\nA significant step forward in this regard was made by Hofmann (1999), who presented the *probabilistic LSI (pLSI)* model, also known as the *aspect model*, as an alternative to LSI. The pLSI approach, which we describe in detail in Section 4.3, models each word in a document as a sample from a mixture model, where the mixture components are multinomial random variables that can be viewed as representations of \u201ctopics.\u201d Thus each word is generated from a single topic, and different words in a document may be generated from different topics. Each document is represented as a list of mixing proportions for these mixture components and thereby reduced to a probability distribution on a fixed set of topics. This distribution is the \u201creduced description\u201d associated with the document.\n\nWhile Hofmann\u2019s work is a useful step toward probabilistic modeling of text, it is incomplete\nin that it provides no probabilistic model at the level of documents.  In pLSI, each document is\nrepresented as a list of numbers (the mixing proportions for topics), and there is no generative\nprobabilistic model for these numbers.  This leads to several problems: (1) the number of parameters in the model grows linearly with the size of the corpus, which leads to serious problems with\noverfitting, and (2) it is not clear how to assign probability to a document outside of the training set.\n\nTo see how to proceed beyond pLSI, let us consider the fundamental probabilistic assumptions\nunderlying the class of dimensionality reduction methods that includes LSI and pLSI. All of these\nmethods are based on the \u201cbag-of-words\u201d assumption\u2014that the order of words in a document can\nbe neglected. In the language of probability theory, this is an assumption of *exchangeability* for the\nwords in a document (Aldous, 1985). Moreover, although less often stated formally, these methods\nalso assume that documents are exchangeable; the specific ordering of the documents in a corpus\ncan also be neglected.\n\nA classic representation theorem due to de Finetti (1990) establishes that any collection of exchangeable random variables has a representation as a mixture distribution\u2014in general an infinite mixture. Thus, if we wish to consider exchangeable representations for documents and words, we need to consider mixture models that capture the exchangeability of both words and documents.\n\nThis line of thinking leads to the *latent Dirichlet allocation (LDA)* model that we present in the current paper.\n\nIt is important to emphasize that an assumption of exchangeability is not equivalent to an as-\nsumption that the random variables are independent and identically distributed. Rather, exchange-\nability essentially can be interpreted as meaning \u201c*conditionally* independent and identically dis-\ntributed,\u201d where the conditioning is with respect to an underlying latent parameter of a probability\ndistribution. Conditionally, the joint distribution of the random variables is simple and factored\nwhile marginally over the latent parameter, the joint distribution can be quite complex. Thus, while\nan assumption of exchangeability is clearly a major simplifying assumption in the domain of text\nmodeling, and its principal justification is that it leads to methods that are computationally efficient,\nthe exchangeability assumptions do not necessarily lead to methods that are restricted to simple\nfrequency counts or linear operations. We aim to demonstrate in the current paper that, by taking\nthe de Finetti theorem seriously, we can capture significant intra-document statistical structure via\nthe mixing distribution.\n\nIt is also worth noting that there are a large number of generalizations of the basic notion of\nexchangeability, including various forms of partial exchangeability, and that representation theo-\nrems are available for these cases as well (Diaconis, 1988). Thus, while the work that we discuss in\nthe current paper focuses on simple \u201cbag-of-words\u201d models, which lead to mixture distributions for\nsingle words (unigrams), our methods are also applicable to richer models that involve mixtures for\nlarger structural units such as *n*-grams or paragraphs.\n\nThe paper is organized as follows. In Section 2 we introduce basic notation and terminology.\nThe LDA model is presented in Section 3 and is compared to related latent variable models in\nSection 4. We discuss inference and parameter estimation for LDA in Section 5. An illustrative\nexample of fitting LDA to data is provided in Section 6. Empirical results in text modeling, text\nclassification and collaborative filtering are presented in Section 7. Finally, Section 8 presents our\nconclusions."
        },
        {
            "text": "This line of thinking leads to the *latent Dirichlet allocation (LDA)* model that we present in the current paper.",
            "page": 3,
            "x": 86,
            "y": 89,
            "width": 437,
            "height": 28,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "2b6a6f81-edf3-435f-b314-669b88887086",
            "group_text": "1. Introduction\n\nIn this paper we consider the problem of modeling text corpora and other collections of discrete data. The goal is to find short descriptions of the members of a collection that enable efficient processing of large collections while preserving the essential statistical relationships that are useful for basic tasks such as classification, novelty detection, summarization, and similarity and relevance judgments.\n\nSignificant progress has been made on this problem by researchers in the field of information retrieval (IR) (Baeza-Yates and Ribeiro-Neto, 1999).  The basic methodology proposed by IR researchers for text corpora\u2014a methodology successfully deployed in modern Internet search engines\u2014reduces each document in the corpus to a vector of real numbers, each of which represents ratios of counts.  In the popular _tf-idf_ scheme (Salton and McGill, 1983), a basic vocabulary of \u201cwords\u201d or \u201cterms\u201d is chosen, and, for each document in the corpus, a count is formed of the number of occurrences of each word.  After suitable normalization, this term frequency count is compared to an inverse document frequency count, which measures the number of occurrences of a\n\nword in the entire corpus (generally on a log scale, and again suitably normalized). The end result\nis a term-by-document matrix $X$ whose columns contain the *tf-idf* values for each of the documents\nin the corpus. Thus the *tf-idf* scheme reduces documents of arbitrary length to fixed-length lists of\nnumbers.\n\nWhile the tf-idf reduction has some appealing features\u2014notably in its basic identification of sets\nof words that are discriminative for documents in the collection\u2014the approach also provides a rela-\ntively small amount of reduction in description length and reveals little in the way of inter- or intra-\ndocument statistical structure. To address these shortcomings, IR researchers have proposed several\nother dimensionality reduction techniques, most notably *latent semantic indexing (LSI)* (Deerwester\net al., 1990). LSI uses a singular value decomposition of the $X$ matrix to identify a linear subspace\nin the space of *tf-idf* features that captures most of the variance in the collection. This approach can\nachieve significant compression in large collections. Furthermore, Deerwester et al. argue that the\nderived features of LSI, which are linear combinations of the original *tf-idf* features, can capture\nsome aspects of basic linguistic notions such as synonymy and polysemy.\n\nTo substantiate the claims regarding LSI, and to study its relative strengths and weaknesses, it is\nuseful to develop a generative probabilistic model of text corpora and to study the ability of LSI to\nrecover aspects of the generative model from data (Papadimitriou et al., 1998). Given a generative\nmodel of text, however, it is not clear why one should adopt the LSI methodology\u2014one can attempt\nto proceed more directly, fitting the model to data using maximum likelihood or Bayesian methods.\n\nA significant step forward in this regard was made by Hofmann (1999), who presented the *probabilistic LSI (pLSI)* model, also known as the *aspect model*, as an alternative to LSI. The pLSI approach, which we describe in detail in Section 4.3, models each word in a document as a sample from a mixture model, where the mixture components are multinomial random variables that can be viewed as representations of \u201ctopics.\u201d Thus each word is generated from a single topic, and different words in a document may be generated from different topics. Each document is represented as a list of mixing proportions for these mixture components and thereby reduced to a probability distribution on a fixed set of topics. This distribution is the \u201creduced description\u201d associated with the document.\n\nWhile Hofmann\u2019s work is a useful step toward probabilistic modeling of text, it is incomplete\nin that it provides no probabilistic model at the level of documents.  In pLSI, each document is\nrepresented as a list of numbers (the mixing proportions for topics), and there is no generative\nprobabilistic model for these numbers.  This leads to several problems: (1) the number of parameters in the model grows linearly with the size of the corpus, which leads to serious problems with\noverfitting, and (2) it is not clear how to assign probability to a document outside of the training set.\n\nTo see how to proceed beyond pLSI, let us consider the fundamental probabilistic assumptions\nunderlying the class of dimensionality reduction methods that includes LSI and pLSI. All of these\nmethods are based on the \u201cbag-of-words\u201d assumption\u2014that the order of words in a document can\nbe neglected. In the language of probability theory, this is an assumption of *exchangeability* for the\nwords in a document (Aldous, 1985). Moreover, although less often stated formally, these methods\nalso assume that documents are exchangeable; the specific ordering of the documents in a corpus\ncan also be neglected.\n\nA classic representation theorem due to de Finetti (1990) establishes that any collection of exchangeable random variables has a representation as a mixture distribution\u2014in general an infinite mixture. Thus, if we wish to consider exchangeable representations for documents and words, we need to consider mixture models that capture the exchangeability of both words and documents.\n\nThis line of thinking leads to the *latent Dirichlet allocation (LDA)* model that we present in the current paper.\n\nIt is important to emphasize that an assumption of exchangeability is not equivalent to an as-\nsumption that the random variables are independent and identically distributed. Rather, exchange-\nability essentially can be interpreted as meaning \u201c*conditionally* independent and identically dis-\ntributed,\u201d where the conditioning is with respect to an underlying latent parameter of a probability\ndistribution. Conditionally, the joint distribution of the random variables is simple and factored\nwhile marginally over the latent parameter, the joint distribution can be quite complex. Thus, while\nan assumption of exchangeability is clearly a major simplifying assumption in the domain of text\nmodeling, and its principal justification is that it leads to methods that are computationally efficient,\nthe exchangeability assumptions do not necessarily lead to methods that are restricted to simple\nfrequency counts or linear operations. We aim to demonstrate in the current paper that, by taking\nthe de Finetti theorem seriously, we can capture significant intra-document statistical structure via\nthe mixing distribution.\n\nIt is also worth noting that there are a large number of generalizations of the basic notion of\nexchangeability, including various forms of partial exchangeability, and that representation theo-\nrems are available for these cases as well (Diaconis, 1988). Thus, while the work that we discuss in\nthe current paper focuses on simple \u201cbag-of-words\u201d models, which lead to mixture distributions for\nsingle words (unigrams), our methods are also applicable to richer models that involve mixtures for\nlarger structural units such as *n*-grams or paragraphs.\n\nThe paper is organized as follows. In Section 2 we introduce basic notation and terminology.\nThe LDA model is presented in Section 3 and is compared to related latent variable models in\nSection 4. We discuss inference and parameter estimation for LDA in Section 5. An illustrative\nexample of fitting LDA to data is provided in Section 6. Empirical results in text modeling, text\nclassification and collaborative filtering are presented in Section 7. Finally, Section 8 presents our\nconclusions."
        },
        {
            "text": "It is important to emphasize that an assumption of exchangeability is not equivalent to an as-\nsumption that the random variables are independent and identically distributed. Rather, exchange-\nability essentially can be interpreted as meaning \u201c*conditionally* independent and identically dis-\ntributed,\u201d where the conditioning is with respect to an underlying latent parameter of a probability\ndistribution. Conditionally, the joint distribution of the random variables is simple and factored\nwhile marginally over the latent parameter, the joint distribution can be quite complex. Thus, while\nan assumption of exchangeability is clearly a major simplifying assumption in the domain of text\nmodeling, and its principal justification is that it leads to methods that are computationally efficient,\nthe exchangeability assumptions do not necessarily lead to methods that are restricted to simple\nfrequency counts or linear operations. We aim to demonstrate in the current paper that, by taking\nthe de Finetti theorem seriously, we can capture significant intra-document statistical structure via\nthe mixing distribution.",
            "page": 3,
            "x": 87,
            "y": 119,
            "width": 436,
            "height": 161,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "aa4cefb9-2b2f-4b2f-980d-f326d80a051a",
            "group_text": "1. Introduction\n\nIn this paper we consider the problem of modeling text corpora and other collections of discrete data. The goal is to find short descriptions of the members of a collection that enable efficient processing of large collections while preserving the essential statistical relationships that are useful for basic tasks such as classification, novelty detection, summarization, and similarity and relevance judgments.\n\nSignificant progress has been made on this problem by researchers in the field of information retrieval (IR) (Baeza-Yates and Ribeiro-Neto, 1999).  The basic methodology proposed by IR researchers for text corpora\u2014a methodology successfully deployed in modern Internet search engines\u2014reduces each document in the corpus to a vector of real numbers, each of which represents ratios of counts.  In the popular _tf-idf_ scheme (Salton and McGill, 1983), a basic vocabulary of \u201cwords\u201d or \u201cterms\u201d is chosen, and, for each document in the corpus, a count is formed of the number of occurrences of each word.  After suitable normalization, this term frequency count is compared to an inverse document frequency count, which measures the number of occurrences of a\n\nword in the entire corpus (generally on a log scale, and again suitably normalized). The end result\nis a term-by-document matrix $X$ whose columns contain the *tf-idf* values for each of the documents\nin the corpus. Thus the *tf-idf* scheme reduces documents of arbitrary length to fixed-length lists of\nnumbers.\n\nWhile the tf-idf reduction has some appealing features\u2014notably in its basic identification of sets\nof words that are discriminative for documents in the collection\u2014the approach also provides a rela-\ntively small amount of reduction in description length and reveals little in the way of inter- or intra-\ndocument statistical structure. To address these shortcomings, IR researchers have proposed several\nother dimensionality reduction techniques, most notably *latent semantic indexing (LSI)* (Deerwester\net al., 1990). LSI uses a singular value decomposition of the $X$ matrix to identify a linear subspace\nin the space of *tf-idf* features that captures most of the variance in the collection. This approach can\nachieve significant compression in large collections. Furthermore, Deerwester et al. argue that the\nderived features of LSI, which are linear combinations of the original *tf-idf* features, can capture\nsome aspects of basic linguistic notions such as synonymy and polysemy.\n\nTo substantiate the claims regarding LSI, and to study its relative strengths and weaknesses, it is\nuseful to develop a generative probabilistic model of text corpora and to study the ability of LSI to\nrecover aspects of the generative model from data (Papadimitriou et al., 1998). Given a generative\nmodel of text, however, it is not clear why one should adopt the LSI methodology\u2014one can attempt\nto proceed more directly, fitting the model to data using maximum likelihood or Bayesian methods.\n\nA significant step forward in this regard was made by Hofmann (1999), who presented the *probabilistic LSI (pLSI)* model, also known as the *aspect model*, as an alternative to LSI. The pLSI approach, which we describe in detail in Section 4.3, models each word in a document as a sample from a mixture model, where the mixture components are multinomial random variables that can be viewed as representations of \u201ctopics.\u201d Thus each word is generated from a single topic, and different words in a document may be generated from different topics. Each document is represented as a list of mixing proportions for these mixture components and thereby reduced to a probability distribution on a fixed set of topics. This distribution is the \u201creduced description\u201d associated with the document.\n\nWhile Hofmann\u2019s work is a useful step toward probabilistic modeling of text, it is incomplete\nin that it provides no probabilistic model at the level of documents.  In pLSI, each document is\nrepresented as a list of numbers (the mixing proportions for topics), and there is no generative\nprobabilistic model for these numbers.  This leads to several problems: (1) the number of parameters in the model grows linearly with the size of the corpus, which leads to serious problems with\noverfitting, and (2) it is not clear how to assign probability to a document outside of the training set.\n\nTo see how to proceed beyond pLSI, let us consider the fundamental probabilistic assumptions\nunderlying the class of dimensionality reduction methods that includes LSI and pLSI. All of these\nmethods are based on the \u201cbag-of-words\u201d assumption\u2014that the order of words in a document can\nbe neglected. In the language of probability theory, this is an assumption of *exchangeability* for the\nwords in a document (Aldous, 1985). Moreover, although less often stated formally, these methods\nalso assume that documents are exchangeable; the specific ordering of the documents in a corpus\ncan also be neglected.\n\nA classic representation theorem due to de Finetti (1990) establishes that any collection of exchangeable random variables has a representation as a mixture distribution\u2014in general an infinite mixture. Thus, if we wish to consider exchangeable representations for documents and words, we need to consider mixture models that capture the exchangeability of both words and documents.\n\nThis line of thinking leads to the *latent Dirichlet allocation (LDA)* model that we present in the current paper.\n\nIt is important to emphasize that an assumption of exchangeability is not equivalent to an as-\nsumption that the random variables are independent and identically distributed. Rather, exchange-\nability essentially can be interpreted as meaning \u201c*conditionally* independent and identically dis-\ntributed,\u201d where the conditioning is with respect to an underlying latent parameter of a probability\ndistribution. Conditionally, the joint distribution of the random variables is simple and factored\nwhile marginally over the latent parameter, the joint distribution can be quite complex. Thus, while\nan assumption of exchangeability is clearly a major simplifying assumption in the domain of text\nmodeling, and its principal justification is that it leads to methods that are computationally efficient,\nthe exchangeability assumptions do not necessarily lead to methods that are restricted to simple\nfrequency counts or linear operations. We aim to demonstrate in the current paper that, by taking\nthe de Finetti theorem seriously, we can capture significant intra-document statistical structure via\nthe mixing distribution.\n\nIt is also worth noting that there are a large number of generalizations of the basic notion of\nexchangeability, including various forms of partial exchangeability, and that representation theo-\nrems are available for these cases as well (Diaconis, 1988). Thus, while the work that we discuss in\nthe current paper focuses on simple \u201cbag-of-words\u201d models, which lead to mixture distributions for\nsingle words (unigrams), our methods are also applicable to richer models that involve mixtures for\nlarger structural units such as *n*-grams or paragraphs.\n\nThe paper is organized as follows. In Section 2 we introduce basic notation and terminology.\nThe LDA model is presented in Section 3 and is compared to related latent variable models in\nSection 4. We discuss inference and parameter estimation for LDA in Section 5. An illustrative\nexample of fitting LDA to data is provided in Section 6. Empirical results in text modeling, text\nclassification and collaborative filtering are presented in Section 7. Finally, Section 8 presents our\nconclusions."
        },
        {
            "text": "It is also worth noting that there are a large number of generalizations of the basic notion of\nexchangeability, including various forms of partial exchangeability, and that representation theo-\nrems are available for these cases as well (Diaconis, 1988). Thus, while the work that we discuss in\nthe current paper focuses on simple \u201cbag-of-words\u201d models, which lead to mixture distributions for\nsingle words (unigrams), our methods are also applicable to richer models that involve mixtures for\nlarger structural units such as *n*-grams or paragraphs.",
            "page": 3,
            "x": 87,
            "y": 282,
            "width": 435,
            "height": 80,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "7bea092e-6155-4425-aa05-85fee707eecd",
            "group_text": "1. Introduction\n\nIn this paper we consider the problem of modeling text corpora and other collections of discrete data. The goal is to find short descriptions of the members of a collection that enable efficient processing of large collections while preserving the essential statistical relationships that are useful for basic tasks such as classification, novelty detection, summarization, and similarity and relevance judgments.\n\nSignificant progress has been made on this problem by researchers in the field of information retrieval (IR) (Baeza-Yates and Ribeiro-Neto, 1999).  The basic methodology proposed by IR researchers for text corpora\u2014a methodology successfully deployed in modern Internet search engines\u2014reduces each document in the corpus to a vector of real numbers, each of which represents ratios of counts.  In the popular _tf-idf_ scheme (Salton and McGill, 1983), a basic vocabulary of \u201cwords\u201d or \u201cterms\u201d is chosen, and, for each document in the corpus, a count is formed of the number of occurrences of each word.  After suitable normalization, this term frequency count is compared to an inverse document frequency count, which measures the number of occurrences of a\n\nword in the entire corpus (generally on a log scale, and again suitably normalized). The end result\nis a term-by-document matrix $X$ whose columns contain the *tf-idf* values for each of the documents\nin the corpus. Thus the *tf-idf* scheme reduces documents of arbitrary length to fixed-length lists of\nnumbers.\n\nWhile the tf-idf reduction has some appealing features\u2014notably in its basic identification of sets\nof words that are discriminative for documents in the collection\u2014the approach also provides a rela-\ntively small amount of reduction in description length and reveals little in the way of inter- or intra-\ndocument statistical structure. To address these shortcomings, IR researchers have proposed several\nother dimensionality reduction techniques, most notably *latent semantic indexing (LSI)* (Deerwester\net al., 1990). LSI uses a singular value decomposition of the $X$ matrix to identify a linear subspace\nin the space of *tf-idf* features that captures most of the variance in the collection. This approach can\nachieve significant compression in large collections. Furthermore, Deerwester et al. argue that the\nderived features of LSI, which are linear combinations of the original *tf-idf* features, can capture\nsome aspects of basic linguistic notions such as synonymy and polysemy.\n\nTo substantiate the claims regarding LSI, and to study its relative strengths and weaknesses, it is\nuseful to develop a generative probabilistic model of text corpora and to study the ability of LSI to\nrecover aspects of the generative model from data (Papadimitriou et al., 1998). Given a generative\nmodel of text, however, it is not clear why one should adopt the LSI methodology\u2014one can attempt\nto proceed more directly, fitting the model to data using maximum likelihood or Bayesian methods.\n\nA significant step forward in this regard was made by Hofmann (1999), who presented the *probabilistic LSI (pLSI)* model, also known as the *aspect model*, as an alternative to LSI. The pLSI approach, which we describe in detail in Section 4.3, models each word in a document as a sample from a mixture model, where the mixture components are multinomial random variables that can be viewed as representations of \u201ctopics.\u201d Thus each word is generated from a single topic, and different words in a document may be generated from different topics. Each document is represented as a list of mixing proportions for these mixture components and thereby reduced to a probability distribution on a fixed set of topics. This distribution is the \u201creduced description\u201d associated with the document.\n\nWhile Hofmann\u2019s work is a useful step toward probabilistic modeling of text, it is incomplete\nin that it provides no probabilistic model at the level of documents.  In pLSI, each document is\nrepresented as a list of numbers (the mixing proportions for topics), and there is no generative\nprobabilistic model for these numbers.  This leads to several problems: (1) the number of parameters in the model grows linearly with the size of the corpus, which leads to serious problems with\noverfitting, and (2) it is not clear how to assign probability to a document outside of the training set.\n\nTo see how to proceed beyond pLSI, let us consider the fundamental probabilistic assumptions\nunderlying the class of dimensionality reduction methods that includes LSI and pLSI. All of these\nmethods are based on the \u201cbag-of-words\u201d assumption\u2014that the order of words in a document can\nbe neglected. In the language of probability theory, this is an assumption of *exchangeability* for the\nwords in a document (Aldous, 1985). Moreover, although less often stated formally, these methods\nalso assume that documents are exchangeable; the specific ordering of the documents in a corpus\ncan also be neglected.\n\nA classic representation theorem due to de Finetti (1990) establishes that any collection of exchangeable random variables has a representation as a mixture distribution\u2014in general an infinite mixture. Thus, if we wish to consider exchangeable representations for documents and words, we need to consider mixture models that capture the exchangeability of both words and documents.\n\nThis line of thinking leads to the *latent Dirichlet allocation (LDA)* model that we present in the current paper.\n\nIt is important to emphasize that an assumption of exchangeability is not equivalent to an as-\nsumption that the random variables are independent and identically distributed. Rather, exchange-\nability essentially can be interpreted as meaning \u201c*conditionally* independent and identically dis-\ntributed,\u201d where the conditioning is with respect to an underlying latent parameter of a probability\ndistribution. Conditionally, the joint distribution of the random variables is simple and factored\nwhile marginally over the latent parameter, the joint distribution can be quite complex. Thus, while\nan assumption of exchangeability is clearly a major simplifying assumption in the domain of text\nmodeling, and its principal justification is that it leads to methods that are computationally efficient,\nthe exchangeability assumptions do not necessarily lead to methods that are restricted to simple\nfrequency counts or linear operations. We aim to demonstrate in the current paper that, by taking\nthe de Finetti theorem seriously, we can capture significant intra-document statistical structure via\nthe mixing distribution.\n\nIt is also worth noting that there are a large number of generalizations of the basic notion of\nexchangeability, including various forms of partial exchangeability, and that representation theo-\nrems are available for these cases as well (Diaconis, 1988). Thus, while the work that we discuss in\nthe current paper focuses on simple \u201cbag-of-words\u201d models, which lead to mixture distributions for\nsingle words (unigrams), our methods are also applicable to richer models that involve mixtures for\nlarger structural units such as *n*-grams or paragraphs.\n\nThe paper is organized as follows. In Section 2 we introduce basic notation and terminology.\nThe LDA model is presented in Section 3 and is compared to related latent variable models in\nSection 4. We discuss inference and parameter estimation for LDA in Section 5. An illustrative\nexample of fitting LDA to data is provided in Section 6. Empirical results in text modeling, text\nclassification and collaborative filtering are presented in Section 7. Finally, Section 8 presents our\nconclusions."
        },
        {
            "text": "The paper is organized as follows. In Section 2 we introduce basic notation and terminology.\nThe LDA model is presented in Section 3 and is compared to related latent variable models in\nSection 4. We discuss inference and parameter estimation for LDA in Section 5. An illustrative\nexample of fitting LDA to data is provided in Section 6. Empirical results in text modeling, text\nclassification and collaborative filtering are presented in Section 7. Finally, Section 8 presents our\nconclusions.",
            "page": 3,
            "x": 88,
            "y": 363,
            "width": 435,
            "height": 80,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "6ff3bb7e-07a7-4d4e-9cee-ca1a55bc94d9",
            "group_text": "1. Introduction\n\nIn this paper we consider the problem of modeling text corpora and other collections of discrete data. The goal is to find short descriptions of the members of a collection that enable efficient processing of large collections while preserving the essential statistical relationships that are useful for basic tasks such as classification, novelty detection, summarization, and similarity and relevance judgments.\n\nSignificant progress has been made on this problem by researchers in the field of information retrieval (IR) (Baeza-Yates and Ribeiro-Neto, 1999).  The basic methodology proposed by IR researchers for text corpora\u2014a methodology successfully deployed in modern Internet search engines\u2014reduces each document in the corpus to a vector of real numbers, each of which represents ratios of counts.  In the popular _tf-idf_ scheme (Salton and McGill, 1983), a basic vocabulary of \u201cwords\u201d or \u201cterms\u201d is chosen, and, for each document in the corpus, a count is formed of the number of occurrences of each word.  After suitable normalization, this term frequency count is compared to an inverse document frequency count, which measures the number of occurrences of a\n\nword in the entire corpus (generally on a log scale, and again suitably normalized). The end result\nis a term-by-document matrix $X$ whose columns contain the *tf-idf* values for each of the documents\nin the corpus. Thus the *tf-idf* scheme reduces documents of arbitrary length to fixed-length lists of\nnumbers.\n\nWhile the tf-idf reduction has some appealing features\u2014notably in its basic identification of sets\nof words that are discriminative for documents in the collection\u2014the approach also provides a rela-\ntively small amount of reduction in description length and reveals little in the way of inter- or intra-\ndocument statistical structure. To address these shortcomings, IR researchers have proposed several\nother dimensionality reduction techniques, most notably *latent semantic indexing (LSI)* (Deerwester\net al., 1990). LSI uses a singular value decomposition of the $X$ matrix to identify a linear subspace\nin the space of *tf-idf* features that captures most of the variance in the collection. This approach can\nachieve significant compression in large collections. Furthermore, Deerwester et al. argue that the\nderived features of LSI, which are linear combinations of the original *tf-idf* features, can capture\nsome aspects of basic linguistic notions such as synonymy and polysemy.\n\nTo substantiate the claims regarding LSI, and to study its relative strengths and weaknesses, it is\nuseful to develop a generative probabilistic model of text corpora and to study the ability of LSI to\nrecover aspects of the generative model from data (Papadimitriou et al., 1998). Given a generative\nmodel of text, however, it is not clear why one should adopt the LSI methodology\u2014one can attempt\nto proceed more directly, fitting the model to data using maximum likelihood or Bayesian methods.\n\nA significant step forward in this regard was made by Hofmann (1999), who presented the *probabilistic LSI (pLSI)* model, also known as the *aspect model*, as an alternative to LSI. The pLSI approach, which we describe in detail in Section 4.3, models each word in a document as a sample from a mixture model, where the mixture components are multinomial random variables that can be viewed as representations of \u201ctopics.\u201d Thus each word is generated from a single topic, and different words in a document may be generated from different topics. Each document is represented as a list of mixing proportions for these mixture components and thereby reduced to a probability distribution on a fixed set of topics. This distribution is the \u201creduced description\u201d associated with the document.\n\nWhile Hofmann\u2019s work is a useful step toward probabilistic modeling of text, it is incomplete\nin that it provides no probabilistic model at the level of documents.  In pLSI, each document is\nrepresented as a list of numbers (the mixing proportions for topics), and there is no generative\nprobabilistic model for these numbers.  This leads to several problems: (1) the number of parameters in the model grows linearly with the size of the corpus, which leads to serious problems with\noverfitting, and (2) it is not clear how to assign probability to a document outside of the training set.\n\nTo see how to proceed beyond pLSI, let us consider the fundamental probabilistic assumptions\nunderlying the class of dimensionality reduction methods that includes LSI and pLSI. All of these\nmethods are based on the \u201cbag-of-words\u201d assumption\u2014that the order of words in a document can\nbe neglected. In the language of probability theory, this is an assumption of *exchangeability* for the\nwords in a document (Aldous, 1985). Moreover, although less often stated formally, these methods\nalso assume that documents are exchangeable; the specific ordering of the documents in a corpus\ncan also be neglected.\n\nA classic representation theorem due to de Finetti (1990) establishes that any collection of exchangeable random variables has a representation as a mixture distribution\u2014in general an infinite mixture. Thus, if we wish to consider exchangeable representations for documents and words, we need to consider mixture models that capture the exchangeability of both words and documents.\n\nThis line of thinking leads to the *latent Dirichlet allocation (LDA)* model that we present in the current paper.\n\nIt is important to emphasize that an assumption of exchangeability is not equivalent to an as-\nsumption that the random variables are independent and identically distributed. Rather, exchange-\nability essentially can be interpreted as meaning \u201c*conditionally* independent and identically dis-\ntributed,\u201d where the conditioning is with respect to an underlying latent parameter of a probability\ndistribution. Conditionally, the joint distribution of the random variables is simple and factored\nwhile marginally over the latent parameter, the joint distribution can be quite complex. Thus, while\nan assumption of exchangeability is clearly a major simplifying assumption in the domain of text\nmodeling, and its principal justification is that it leads to methods that are computationally efficient,\nthe exchangeability assumptions do not necessarily lead to methods that are restricted to simple\nfrequency counts or linear operations. We aim to demonstrate in the current paper that, by taking\nthe de Finetti theorem seriously, we can capture significant intra-document statistical structure via\nthe mixing distribution.\n\nIt is also worth noting that there are a large number of generalizations of the basic notion of\nexchangeability, including various forms of partial exchangeability, and that representation theo-\nrems are available for these cases as well (Diaconis, 1988). Thus, while the work that we discuss in\nthe current paper focuses on simple \u201cbag-of-words\u201d models, which lead to mixture distributions for\nsingle words (unigrams), our methods are also applicable to richer models that involve mixtures for\nlarger structural units such as *n*-grams or paragraphs.\n\nThe paper is organized as follows. In Section 2 we introduce basic notation and terminology.\nThe LDA model is presented in Section 3 and is compared to related latent variable models in\nSection 4. We discuss inference and parameter estimation for LDA in Section 5. An illustrative\nexample of fitting LDA to data is provided in Section 6. Empirical results in text modeling, text\nclassification and collaborative filtering are presented in Section 7. Finally, Section 8 presents our\nconclusions."
        },
        {
            "text": "## 2. Notation and terminology\n\nWe use the language of text collections throughout the paper, referring to entities such as \u201cwords,\u201d \u201cdocuments,\u201d and \u201ccorpora.\u201d This is useful in that it helps to guide intuition, particularly when we introduce latent variables which aim to capture abstract notions such as topics. It is important to note, however, that the LDA model is not necessarily tied to text, and has applications to other problems involving collections of data, including data from domains such as collaborative filtering, content-based image retrieval and bioinformatics. Indeed, in Section 7.3, we present experimental results in the collaborative filtering domain.",
            "page": 3,
            "x": 87,
            "y": 460,
            "width": 434,
            "height": 115,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "3-notation",
            "chunk_id": "206839be-a907-4285-9431-d485d0219fe9",
            "group_text": "## 2. Notation and terminology\n\nWe use the language of text collections throughout the paper, referring to entities such as \u201cwords,\u201d \u201cdocuments,\u201d and \u201ccorpora.\u201d This is useful in that it helps to guide intuition, particularly when we introduce latent variables which aim to capture abstract notions such as topics. It is important to note, however, that the LDA model is not necessarily tied to text, and has applications to other problems involving collections of data, including data from domains such as collaborative filtering, content-based image retrieval and bioinformatics. Indeed, in Section 7.3, we present experimental results in the collaborative filtering domain.\n\nFormally, we define the following terms:\n\u2022 A *word* is the basic unit of discrete data, defined to be an item from a vocabulary indexed by $\\{1, \\ldots, V\\}$. We represent words using unit-basis vectors that have a single component equal to one and all other components equal to zero. Thus, using superscripts to denote components, the $v$th word in the vocabulary is represented by a $V$-vector $w$ such that $w^v = 1$ and $w^u = 0$ for $u \\neq v$.\n\n- \u2022 A *document* is a sequence of $N$ words denoted by $\\mathbf{w} = (w_1, w_2, \\ldots, w_N)$, where $w_n$ is the $n$th word in the sequence.\n- \u2022 A *corpus* is a collection of $M$ documents denoted by $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$.\n\nWe wish to find a probabilistic model of a corpus that not only assigns high probability to members of the corpus, but also assigns high probability to other \u201csimilar\u201d documents."
        },
        {
            "text": "Formally, we define the following terms:\n\u2022 A *word* is the basic unit of discrete data, defined to be an item from a vocabulary indexed by $\\{1, \\ldots, V\\}$. We represent words using unit-basis vectors that have a single component equal to one and all other components equal to zero. Thus, using superscripts to denote components, the $v$th word in the vocabulary is represented by a $V$-vector $w$ such that $w^v = 1$ and $w^u = 0$ for $u \\neq v$.",
            "page": 3,
            "x": 104,
            "y": 577,
            "width": 419,
            "height": 80,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "3-notation",
            "chunk_id": "b6bd7993-32a8-47f6-b308-487a80f828fe",
            "group_text": "## 2. Notation and terminology\n\nWe use the language of text collections throughout the paper, referring to entities such as \u201cwords,\u201d \u201cdocuments,\u201d and \u201ccorpora.\u201d This is useful in that it helps to guide intuition, particularly when we introduce latent variables which aim to capture abstract notions such as topics. It is important to note, however, that the LDA model is not necessarily tied to text, and has applications to other problems involving collections of data, including data from domains such as collaborative filtering, content-based image retrieval and bioinformatics. Indeed, in Section 7.3, we present experimental results in the collaborative filtering domain.\n\nFormally, we define the following terms:\n\u2022 A *word* is the basic unit of discrete data, defined to be an item from a vocabulary indexed by $\\{1, \\ldots, V\\}$. We represent words using unit-basis vectors that have a single component equal to one and all other components equal to zero. Thus, using superscripts to denote components, the $v$th word in the vocabulary is represented by a $V$-vector $w$ such that $w^v = 1$ and $w^u = 0$ for $u \\neq v$.\n\n- \u2022 A *document* is a sequence of $N$ words denoted by $\\mathbf{w} = (w_1, w_2, \\ldots, w_N)$, where $w_n$ is the $n$th word in the sequence.\n- \u2022 A *corpus* is a collection of $M$ documents denoted by $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$.\n\nWe wish to find a probabilistic model of a corpus that not only assigns high probability to members of the corpus, but also assigns high probability to other \u201csimilar\u201d documents."
        },
        {
            "text": "- \u2022 A *document* is a sequence of $N$ words denoted by $\\mathbf{w} = (w_1, w_2, \\ldots, w_N)$, where $w_n$ is the $n$th word in the sequence.\n- \u2022 A *corpus* is a collection of $M$ documents denoted by $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$.",
            "page": 3,
            "x": 104,
            "y": 662,
            "width": 418,
            "height": 43,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "3-notation",
            "chunk_id": "86932bff-25ae-465a-bb4a-d40dc6a71e30",
            "group_text": "## 2. Notation and terminology\n\nWe use the language of text collections throughout the paper, referring to entities such as \u201cwords,\u201d \u201cdocuments,\u201d and \u201ccorpora.\u201d This is useful in that it helps to guide intuition, particularly when we introduce latent variables which aim to capture abstract notions such as topics. It is important to note, however, that the LDA model is not necessarily tied to text, and has applications to other problems involving collections of data, including data from domains such as collaborative filtering, content-based image retrieval and bioinformatics. Indeed, in Section 7.3, we present experimental results in the collaborative filtering domain.\n\nFormally, we define the following terms:\n\u2022 A *word* is the basic unit of discrete data, defined to be an item from a vocabulary indexed by $\\{1, \\ldots, V\\}$. We represent words using unit-basis vectors that have a single component equal to one and all other components equal to zero. Thus, using superscripts to denote components, the $v$th word in the vocabulary is represented by a $V$-vector $w$ such that $w^v = 1$ and $w^u = 0$ for $u \\neq v$.\n\n- \u2022 A *document* is a sequence of $N$ words denoted by $\\mathbf{w} = (w_1, w_2, \\ldots, w_N)$, where $w_n$ is the $n$th word in the sequence.\n- \u2022 A *corpus* is a collection of $M$ documents denoted by $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$.\n\nWe wish to find a probabilistic model of a corpus that not only assigns high probability to members of the corpus, but also assigns high probability to other \u201csimilar\u201d documents."
        },
        {
            "text": "We wish to find a probabilistic model of a corpus that not only assigns high probability to members of the corpus, but also assigns high probability to other \u201csimilar\u201d documents.",
            "page": 4,
            "x": 86,
            "y": 88,
            "width": 438,
            "height": 32,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "3-notation",
            "chunk_id": "5282e0bc-9e3f-43ff-a5a9-8fc570792304",
            "group_text": "## 2. Notation and terminology\n\nWe use the language of text collections throughout the paper, referring to entities such as \u201cwords,\u201d \u201cdocuments,\u201d and \u201ccorpora.\u201d This is useful in that it helps to guide intuition, particularly when we introduce latent variables which aim to capture abstract notions such as topics. It is important to note, however, that the LDA model is not necessarily tied to text, and has applications to other problems involving collections of data, including data from domains such as collaborative filtering, content-based image retrieval and bioinformatics. Indeed, in Section 7.3, we present experimental results in the collaborative filtering domain.\n\nFormally, we define the following terms:\n\u2022 A *word* is the basic unit of discrete data, defined to be an item from a vocabulary indexed by $\\{1, \\ldots, V\\}$. We represent words using unit-basis vectors that have a single component equal to one and all other components equal to zero. Thus, using superscripts to denote components, the $v$th word in the vocabulary is represented by a $V$-vector $w$ such that $w^v = 1$ and $w^u = 0$ for $u \\neq v$.\n\n- \u2022 A *document* is a sequence of $N$ words denoted by $\\mathbf{w} = (w_1, w_2, \\ldots, w_N)$, where $w_n$ is the $n$th word in the sequence.\n- \u2022 A *corpus* is a collection of $M$ documents denoted by $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$.\n\nWe wish to find a probabilistic model of a corpus that not only assigns high probability to members of the corpus, but also assigns high probability to other \u201csimilar\u201d documents."
        },
        {
            "text": "3. Latent Dirichlet allocation\n\nLatent Dirichlet allocation (LDA) is a generative probabilistic model of a corpus. The basic idea is that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words.\u00b9",
            "page": 4,
            "x": 86,
            "y": 134,
            "width": 437,
            "height": 64,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "4-latent",
            "chunk_id": "0149bf5e-bc69-4f5a-9d8d-bac606f933d7",
            "group_text": "3. Latent Dirichlet allocation\n\nLatent Dirichlet allocation (LDA) is a generative probabilistic model of a corpus. The basic idea is that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words.\u00b9\n\nLDA assumes the following generative process for each document **w** in a corpus $\\mathcal{D}$:\n\n1. Choose $N \\sim \\text{Poisson}(\\xi)$.\n2. Choose $\\theta \\sim \\text{Dir}(\\alpha)$.\n3. For each of the $N$ words $w_n$:\n   (a) Choose a topic $z_n \\sim \\text{Multinomial}(\\theta)$.\n   (b) Choose a word $w_n$ from $p(w_n \\mid z_n, \\beta)$, a multinomial probability conditioned on the topic $z_n$.\n\nSeveral simplifying assumptions are made in this basic model, some of which we remove in subse-\nquent sections. First, the dimensionality $k$ of the Dirichlet distribution (and thus the dimensionality\nof the topic variable $z$) is assumed known and fixed. Second, the word probabilities are parameter-\nized by a $k \\times V$ matrix $\\beta$ where $\\beta_{ij} = p(w^j = 1 | z^i = 1)$, which for now we treat as a fixed quantity\nthat is to be estimated. Finally, the Poisson assumption is not critical to anything that follows and\nmore realistic document length distributions can be used as needed. Furthermore, note that $N$ is\nindependent of all the other data generating variables ($\\theta$ and $\\mathbf{z}$). It is thus an ancillary variable and\nwe will generally ignore its randomness in the subsequent development.\n\nA $k$-dimensional Dirichlet random variable $\\theta$ can take values in the $(k-1)$-simplex (a $k$-vector\n\n\u03b8 lies in the $(k-1)$-simplex if $\\theta_i \\geq 0, \\sum_{i=1}^k \\theta_i = 1)$, and has the following probability density on this simplex:\n\n$p(\\theta|\\alpha) = \\frac{\\Gamma\\left(\\sum_{i=1}^k \\alpha_i\\right)}{\\prod_{i=1}^k \\Gamma(\\alpha_i)} \\theta_1^{\\alpha_1-1} \\ldots \\theta_k^{\\alpha_k-1}, \\hspace{2cm} (1)$\n\nwhere the parameter $\\alpha$ is a $k$-vector with components $\\alpha_i > 0$, and where $\\Gamma(x)$ is the Gamma function. The Dirichlet is a convenient distribution on the simplex \u2014 it is in the exponential family, has finite dimensional sufficient statistics, and is conjugate to the multinomial distribution. In Section 5, these properties will facilitate the development of inference and parameter estimation algorithms for LDA.\n\nGiven the parameters $\\alpha$ and $\\beta$, the joint distribution of a topic mixture $\\theta$, a set of $N$ topics $\\mathbf{z}$, and a set of $N$ words $\\mathbf{w}$ is given by:\n\n$p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta) = p(\\boldsymbol{\\theta} \\mid \\alpha) \\prod_{n=1}^{N} p(z_n \\mid \\boldsymbol{\\theta}) p(w_n \\mid z_n, \\beta), \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (2)$"
        },
        {
            "text": "LDA assumes the following generative process for each document **w** in a corpus $\\mathcal{D}$:\n\n1. Choose $N \\sim \\text{Poisson}(\\xi)$.\n2. Choose $\\theta \\sim \\text{Dir}(\\alpha)$.\n3. For each of the $N$ words $w_n$:\n   (a) Choose a topic $z_n \\sim \\text{Multinomial}(\\theta)$.\n   (b) Choose a word $w_n$ from $p(w_n \\mid z_n, \\beta)$, a multinomial probability conditioned on the topic $z_n$.",
            "page": 4,
            "x": 100,
            "y": 200,
            "width": 423,
            "height": 136,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "4-latent",
            "chunk_id": "df3c5559-d632-4bac-aa12-369e81348b07",
            "group_text": "3. Latent Dirichlet allocation\n\nLatent Dirichlet allocation (LDA) is a generative probabilistic model of a corpus. The basic idea is that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words.\u00b9\n\nLDA assumes the following generative process for each document **w** in a corpus $\\mathcal{D}$:\n\n1. Choose $N \\sim \\text{Poisson}(\\xi)$.\n2. Choose $\\theta \\sim \\text{Dir}(\\alpha)$.\n3. For each of the $N$ words $w_n$:\n   (a) Choose a topic $z_n \\sim \\text{Multinomial}(\\theta)$.\n   (b) Choose a word $w_n$ from $p(w_n \\mid z_n, \\beta)$, a multinomial probability conditioned on the topic $z_n$.\n\nSeveral simplifying assumptions are made in this basic model, some of which we remove in subse-\nquent sections. First, the dimensionality $k$ of the Dirichlet distribution (and thus the dimensionality\nof the topic variable $z$) is assumed known and fixed. Second, the word probabilities are parameter-\nized by a $k \\times V$ matrix $\\beta$ where $\\beta_{ij} = p(w^j = 1 | z^i = 1)$, which for now we treat as a fixed quantity\nthat is to be estimated. Finally, the Poisson assumption is not critical to anything that follows and\nmore realistic document length distributions can be used as needed. Furthermore, note that $N$ is\nindependent of all the other data generating variables ($\\theta$ and $\\mathbf{z}$). It is thus an ancillary variable and\nwe will generally ignore its randomness in the subsequent development.\n\nA $k$-dimensional Dirichlet random variable $\\theta$ can take values in the $(k-1)$-simplex (a $k$-vector\n\n\u03b8 lies in the $(k-1)$-simplex if $\\theta_i \\geq 0, \\sum_{i=1}^k \\theta_i = 1)$, and has the following probability density on this simplex:\n\n$p(\\theta|\\alpha) = \\frac{\\Gamma\\left(\\sum_{i=1}^k \\alpha_i\\right)}{\\prod_{i=1}^k \\Gamma(\\alpha_i)} \\theta_1^{\\alpha_1-1} \\ldots \\theta_k^{\\alpha_k-1}, \\hspace{2cm} (1)$\n\nwhere the parameter $\\alpha$ is a $k$-vector with components $\\alpha_i > 0$, and where $\\Gamma(x)$ is the Gamma function. The Dirichlet is a convenient distribution on the simplex \u2014 it is in the exponential family, has finite dimensional sufficient statistics, and is conjugate to the multinomial distribution. In Section 5, these properties will facilitate the development of inference and parameter estimation algorithms for LDA.\n\nGiven the parameters $\\alpha$ and $\\beta$, the joint distribution of a topic mixture $\\theta$, a set of $N$ topics $\\mathbf{z}$, and a set of $N$ words $\\mathbf{w}$ is given by:\n\n$p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta) = p(\\boldsymbol{\\theta} \\mid \\alpha) \\prod_{n=1}^{N} p(z_n \\mid \\boldsymbol{\\theta}) p(w_n \\mid z_n, \\beta), \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (2)$"
        },
        {
            "text": "Several simplifying assumptions are made in this basic model, some of which we remove in subse-\nquent sections. First, the dimensionality $k$ of the Dirichlet distribution (and thus the dimensionality\nof the topic variable $z$) is assumed known and fixed. Second, the word probabilities are parameter-\nized by a $k \\times V$ matrix $\\beta$ where $\\beta_{ij} = p(w^j = 1 | z^i = 1)$, which for now we treat as a fixed quantity\nthat is to be estimated. Finally, the Poisson assumption is not critical to anything that follows and\nmore realistic document length distributions can be used as needed. Furthermore, note that $N$ is\nindependent of all the other data generating variables ($\\theta$ and $\\mathbf{z}$). It is thus an ancillary variable and\nwe will generally ignore its randomness in the subsequent development.\n\nA $k$-dimensional Dirichlet random variable $\\theta$ can take values in the $(k-1)$-simplex (a $k$-vector",
            "page": 4,
            "x": 87,
            "y": 347,
            "width": 435,
            "height": 122,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "4-latent",
            "chunk_id": "3472ec87-bed9-4832-b8c4-2f2f0c3aa85f",
            "group_text": "3. Latent Dirichlet allocation\n\nLatent Dirichlet allocation (LDA) is a generative probabilistic model of a corpus. The basic idea is that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words.\u00b9\n\nLDA assumes the following generative process for each document **w** in a corpus $\\mathcal{D}$:\n\n1. Choose $N \\sim \\text{Poisson}(\\xi)$.\n2. Choose $\\theta \\sim \\text{Dir}(\\alpha)$.\n3. For each of the $N$ words $w_n$:\n   (a) Choose a topic $z_n \\sim \\text{Multinomial}(\\theta)$.\n   (b) Choose a word $w_n$ from $p(w_n \\mid z_n, \\beta)$, a multinomial probability conditioned on the topic $z_n$.\n\nSeveral simplifying assumptions are made in this basic model, some of which we remove in subse-\nquent sections. First, the dimensionality $k$ of the Dirichlet distribution (and thus the dimensionality\nof the topic variable $z$) is assumed known and fixed. Second, the word probabilities are parameter-\nized by a $k \\times V$ matrix $\\beta$ where $\\beta_{ij} = p(w^j = 1 | z^i = 1)$, which for now we treat as a fixed quantity\nthat is to be estimated. Finally, the Poisson assumption is not critical to anything that follows and\nmore realistic document length distributions can be used as needed. Furthermore, note that $N$ is\nindependent of all the other data generating variables ($\\theta$ and $\\mathbf{z}$). It is thus an ancillary variable and\nwe will generally ignore its randomness in the subsequent development.\n\nA $k$-dimensional Dirichlet random variable $\\theta$ can take values in the $(k-1)$-simplex (a $k$-vector\n\n\u03b8 lies in the $(k-1)$-simplex if $\\theta_i \\geq 0, \\sum_{i=1}^k \\theta_i = 1)$, and has the following probability density on this simplex:\n\n$p(\\theta|\\alpha) = \\frac{\\Gamma\\left(\\sum_{i=1}^k \\alpha_i\\right)}{\\prod_{i=1}^k \\Gamma(\\alpha_i)} \\theta_1^{\\alpha_1-1} \\ldots \\theta_k^{\\alpha_k-1}, \\hspace{2cm} (1)$\n\nwhere the parameter $\\alpha$ is a $k$-vector with components $\\alpha_i > 0$, and where $\\Gamma(x)$ is the Gamma function. The Dirichlet is a convenient distribution on the simplex \u2014 it is in the exponential family, has finite dimensional sufficient statistics, and is conjugate to the multinomial distribution. In Section 5, these properties will facilitate the development of inference and parameter estimation algorithms for LDA.\n\nGiven the parameters $\\alpha$ and $\\beta$, the joint distribution of a topic mixture $\\theta$, a set of $N$ topics $\\mathbf{z}$, and a set of $N$ words $\\mathbf{w}$ is given by:\n\n$p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta) = p(\\boldsymbol{\\theta} \\mid \\alpha) \\prod_{n=1}^{N} p(z_n \\mid \\boldsymbol{\\theta}) p(w_n \\mid z_n, \\beta), \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (2)$"
        },
        {
            "text": "\u03b8 lies in the $(k-1)$-simplex if $\\theta_i \\geq 0, \\sum_{i=1}^k \\theta_i = 1)$, and has the following probability density on this simplex:",
            "page": 4,
            "x": 87,
            "y": 470,
            "width": 435,
            "height": 29,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "4-latent",
            "chunk_id": "f34848ce-cacc-4d6a-9c0a-c53a7652f45e",
            "group_text": "3. Latent Dirichlet allocation\n\nLatent Dirichlet allocation (LDA) is a generative probabilistic model of a corpus. The basic idea is that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words.\u00b9\n\nLDA assumes the following generative process for each document **w** in a corpus $\\mathcal{D}$:\n\n1. Choose $N \\sim \\text{Poisson}(\\xi)$.\n2. Choose $\\theta \\sim \\text{Dir}(\\alpha)$.\n3. For each of the $N$ words $w_n$:\n   (a) Choose a topic $z_n \\sim \\text{Multinomial}(\\theta)$.\n   (b) Choose a word $w_n$ from $p(w_n \\mid z_n, \\beta)$, a multinomial probability conditioned on the topic $z_n$.\n\nSeveral simplifying assumptions are made in this basic model, some of which we remove in subse-\nquent sections. First, the dimensionality $k$ of the Dirichlet distribution (and thus the dimensionality\nof the topic variable $z$) is assumed known and fixed. Second, the word probabilities are parameter-\nized by a $k \\times V$ matrix $\\beta$ where $\\beta_{ij} = p(w^j = 1 | z^i = 1)$, which for now we treat as a fixed quantity\nthat is to be estimated. Finally, the Poisson assumption is not critical to anything that follows and\nmore realistic document length distributions can be used as needed. Furthermore, note that $N$ is\nindependent of all the other data generating variables ($\\theta$ and $\\mathbf{z}$). It is thus an ancillary variable and\nwe will generally ignore its randomness in the subsequent development.\n\nA $k$-dimensional Dirichlet random variable $\\theta$ can take values in the $(k-1)$-simplex (a $k$-vector\n\n\u03b8 lies in the $(k-1)$-simplex if $\\theta_i \\geq 0, \\sum_{i=1}^k \\theta_i = 1)$, and has the following probability density on this simplex:\n\n$p(\\theta|\\alpha) = \\frac{\\Gamma\\left(\\sum_{i=1}^k \\alpha_i\\right)}{\\prod_{i=1}^k \\Gamma(\\alpha_i)} \\theta_1^{\\alpha_1-1} \\ldots \\theta_k^{\\alpha_k-1}, \\hspace{2cm} (1)$\n\nwhere the parameter $\\alpha$ is a $k$-vector with components $\\alpha_i > 0$, and where $\\Gamma(x)$ is the Gamma function. The Dirichlet is a convenient distribution on the simplex \u2014 it is in the exponential family, has finite dimensional sufficient statistics, and is conjugate to the multinomial distribution. In Section 5, these properties will facilitate the development of inference and parameter estimation algorithms for LDA.\n\nGiven the parameters $\\alpha$ and $\\beta$, the joint distribution of a topic mixture $\\theta$, a set of $N$ topics $\\mathbf{z}$, and a set of $N$ words $\\mathbf{w}$ is given by:\n\n$p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta) = p(\\boldsymbol{\\theta} \\mid \\alpha) \\prod_{n=1}^{N} p(z_n \\mid \\boldsymbol{\\theta}) p(w_n \\mid z_n, \\beta), \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (2)$"
        },
        {
            "text": "$p(\\theta|\\alpha) = \\frac{\\Gamma\\left(\\sum_{i=1}^k \\alpha_i\\right)}{\\prod_{i=1}^k \\Gamma(\\alpha_i)} \\theta_1^{\\alpha_1-1} \\ldots \\theta_k^{\\alpha_k-1}, \\hspace{2cm} (1)$",
            "page": 4,
            "x": 220,
            "y": 497,
            "width": 303,
            "height": 37,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "4-latent",
            "chunk_id": "f0b09232-be4e-4ccc-ac41-4444be1e5872",
            "group_text": "3. Latent Dirichlet allocation\n\nLatent Dirichlet allocation (LDA) is a generative probabilistic model of a corpus. The basic idea is that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words.\u00b9\n\nLDA assumes the following generative process for each document **w** in a corpus $\\mathcal{D}$:\n\n1. Choose $N \\sim \\text{Poisson}(\\xi)$.\n2. Choose $\\theta \\sim \\text{Dir}(\\alpha)$.\n3. For each of the $N$ words $w_n$:\n   (a) Choose a topic $z_n \\sim \\text{Multinomial}(\\theta)$.\n   (b) Choose a word $w_n$ from $p(w_n \\mid z_n, \\beta)$, a multinomial probability conditioned on the topic $z_n$.\n\nSeveral simplifying assumptions are made in this basic model, some of which we remove in subse-\nquent sections. First, the dimensionality $k$ of the Dirichlet distribution (and thus the dimensionality\nof the topic variable $z$) is assumed known and fixed. Second, the word probabilities are parameter-\nized by a $k \\times V$ matrix $\\beta$ where $\\beta_{ij} = p(w^j = 1 | z^i = 1)$, which for now we treat as a fixed quantity\nthat is to be estimated. Finally, the Poisson assumption is not critical to anything that follows and\nmore realistic document length distributions can be used as needed. Furthermore, note that $N$ is\nindependent of all the other data generating variables ($\\theta$ and $\\mathbf{z}$). It is thus an ancillary variable and\nwe will generally ignore its randomness in the subsequent development.\n\nA $k$-dimensional Dirichlet random variable $\\theta$ can take values in the $(k-1)$-simplex (a $k$-vector\n\n\u03b8 lies in the $(k-1)$-simplex if $\\theta_i \\geq 0, \\sum_{i=1}^k \\theta_i = 1)$, and has the following probability density on this simplex:\n\n$p(\\theta|\\alpha) = \\frac{\\Gamma\\left(\\sum_{i=1}^k \\alpha_i\\right)}{\\prod_{i=1}^k \\Gamma(\\alpha_i)} \\theta_1^{\\alpha_1-1} \\ldots \\theta_k^{\\alpha_k-1}, \\hspace{2cm} (1)$\n\nwhere the parameter $\\alpha$ is a $k$-vector with components $\\alpha_i > 0$, and where $\\Gamma(x)$ is the Gamma function. The Dirichlet is a convenient distribution on the simplex \u2014 it is in the exponential family, has finite dimensional sufficient statistics, and is conjugate to the multinomial distribution. In Section 5, these properties will facilitate the development of inference and parameter estimation algorithms for LDA.\n\nGiven the parameters $\\alpha$ and $\\beta$, the joint distribution of a topic mixture $\\theta$, a set of $N$ topics $\\mathbf{z}$, and a set of $N$ words $\\mathbf{w}$ is given by:\n\n$p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta) = p(\\boldsymbol{\\theta} \\mid \\alpha) \\prod_{n=1}^{N} p(z_n \\mid \\boldsymbol{\\theta}) p(w_n \\mid z_n, \\beta), \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (2)$"
        },
        {
            "text": "where the parameter $\\alpha$ is a $k$-vector with components $\\alpha_i > 0$, and where $\\Gamma(x)$ is the Gamma function. The Dirichlet is a convenient distribution on the simplex \u2014 it is in the exponential family, has finite dimensional sufficient statistics, and is conjugate to the multinomial distribution. In Section 5, these properties will facilitate the development of inference and parameter estimation algorithms for LDA.\n\nGiven the parameters $\\alpha$ and $\\beta$, the joint distribution of a topic mixture $\\theta$, a set of $N$ topics $\\mathbf{z}$, and a set of $N$ words $\\mathbf{w}$ is given by:",
            "page": 4,
            "x": 86,
            "y": 538,
            "width": 437,
            "height": 82,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "4-latent",
            "chunk_id": "8c016163-8dc9-4b33-a438-3e692ab66f7c",
            "group_text": "3. Latent Dirichlet allocation\n\nLatent Dirichlet allocation (LDA) is a generative probabilistic model of a corpus. The basic idea is that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words.\u00b9\n\nLDA assumes the following generative process for each document **w** in a corpus $\\mathcal{D}$:\n\n1. Choose $N \\sim \\text{Poisson}(\\xi)$.\n2. Choose $\\theta \\sim \\text{Dir}(\\alpha)$.\n3. For each of the $N$ words $w_n$:\n   (a) Choose a topic $z_n \\sim \\text{Multinomial}(\\theta)$.\n   (b) Choose a word $w_n$ from $p(w_n \\mid z_n, \\beta)$, a multinomial probability conditioned on the topic $z_n$.\n\nSeveral simplifying assumptions are made in this basic model, some of which we remove in subse-\nquent sections. First, the dimensionality $k$ of the Dirichlet distribution (and thus the dimensionality\nof the topic variable $z$) is assumed known and fixed. Second, the word probabilities are parameter-\nized by a $k \\times V$ matrix $\\beta$ where $\\beta_{ij} = p(w^j = 1 | z^i = 1)$, which for now we treat as a fixed quantity\nthat is to be estimated. Finally, the Poisson assumption is not critical to anything that follows and\nmore realistic document length distributions can be used as needed. Furthermore, note that $N$ is\nindependent of all the other data generating variables ($\\theta$ and $\\mathbf{z}$). It is thus an ancillary variable and\nwe will generally ignore its randomness in the subsequent development.\n\nA $k$-dimensional Dirichlet random variable $\\theta$ can take values in the $(k-1)$-simplex (a $k$-vector\n\n\u03b8 lies in the $(k-1)$-simplex if $\\theta_i \\geq 0, \\sum_{i=1}^k \\theta_i = 1)$, and has the following probability density on this simplex:\n\n$p(\\theta|\\alpha) = \\frac{\\Gamma\\left(\\sum_{i=1}^k \\alpha_i\\right)}{\\prod_{i=1}^k \\Gamma(\\alpha_i)} \\theta_1^{\\alpha_1-1} \\ldots \\theta_k^{\\alpha_k-1}, \\hspace{2cm} (1)$\n\nwhere the parameter $\\alpha$ is a $k$-vector with components $\\alpha_i > 0$, and where $\\Gamma(x)$ is the Gamma function. The Dirichlet is a convenient distribution on the simplex \u2014 it is in the exponential family, has finite dimensional sufficient statistics, and is conjugate to the multinomial distribution. In Section 5, these properties will facilitate the development of inference and parameter estimation algorithms for LDA.\n\nGiven the parameters $\\alpha$ and $\\beta$, the joint distribution of a topic mixture $\\theta$, a set of $N$ topics $\\mathbf{z}$, and a set of $N$ words $\\mathbf{w}$ is given by:\n\n$p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta) = p(\\boldsymbol{\\theta} \\mid \\alpha) \\prod_{n=1}^{N} p(z_n \\mid \\boldsymbol{\\theta}) p(w_n \\mid z_n, \\beta), \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (2)$"
        },
        {
            "text": "$p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta) = p(\\boldsymbol{\\theta} \\mid \\alpha) \\prod_{n=1}^{N} p(z_n \\mid \\boldsymbol{\\theta}) p(w_n \\mid z_n, \\beta), \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (2)$",
            "page": 4,
            "x": 195,
            "y": 633,
            "width": 328,
            "height": 33,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "4-latent",
            "chunk_id": "0e0ffdbb-f3fd-4268-8d7d-3a0929fe46d1",
            "group_text": "3. Latent Dirichlet allocation\n\nLatent Dirichlet allocation (LDA) is a generative probabilistic model of a corpus. The basic idea is that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words.\u00b9\n\nLDA assumes the following generative process for each document **w** in a corpus $\\mathcal{D}$:\n\n1. Choose $N \\sim \\text{Poisson}(\\xi)$.\n2. Choose $\\theta \\sim \\text{Dir}(\\alpha)$.\n3. For each of the $N$ words $w_n$:\n   (a) Choose a topic $z_n \\sim \\text{Multinomial}(\\theta)$.\n   (b) Choose a word $w_n$ from $p(w_n \\mid z_n, \\beta)$, a multinomial probability conditioned on the topic $z_n$.\n\nSeveral simplifying assumptions are made in this basic model, some of which we remove in subse-\nquent sections. First, the dimensionality $k$ of the Dirichlet distribution (and thus the dimensionality\nof the topic variable $z$) is assumed known and fixed. Second, the word probabilities are parameter-\nized by a $k \\times V$ matrix $\\beta$ where $\\beta_{ij} = p(w^j = 1 | z^i = 1)$, which for now we treat as a fixed quantity\nthat is to be estimated. Finally, the Poisson assumption is not critical to anything that follows and\nmore realistic document length distributions can be used as needed. Furthermore, note that $N$ is\nindependent of all the other data generating variables ($\\theta$ and $\\mathbf{z}$). It is thus an ancillary variable and\nwe will generally ignore its randomness in the subsequent development.\n\nA $k$-dimensional Dirichlet random variable $\\theta$ can take values in the $(k-1)$-simplex (a $k$-vector\n\n\u03b8 lies in the $(k-1)$-simplex if $\\theta_i \\geq 0, \\sum_{i=1}^k \\theta_i = 1)$, and has the following probability density on this simplex:\n\n$p(\\theta|\\alpha) = \\frac{\\Gamma\\left(\\sum_{i=1}^k \\alpha_i\\right)}{\\prod_{i=1}^k \\Gamma(\\alpha_i)} \\theta_1^{\\alpha_1-1} \\ldots \\theta_k^{\\alpha_k-1}, \\hspace{2cm} (1)$\n\nwhere the parameter $\\alpha$ is a $k$-vector with components $\\alpha_i > 0$, and where $\\Gamma(x)$ is the Gamma function. The Dirichlet is a convenient distribution on the simplex \u2014 it is in the exponential family, has finite dimensional sufficient statistics, and is conjugate to the multinomial distribution. In Section 5, these properties will facilitate the development of inference and parameter estimation algorithms for LDA.\n\nGiven the parameters $\\alpha$ and $\\beta$, the joint distribution of a topic mixture $\\theta$, a set of $N$ topics $\\mathbf{z}$, and a set of $N$ words $\\mathbf{w}$ is given by:\n\n$p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta) = p(\\boldsymbol{\\theta} \\mid \\alpha) \\prod_{n=1}^{N} p(z_n \\mid \\boldsymbol{\\theta}) p(w_n \\mid z_n, \\beta), \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (2)$"
        },
        {
            "text": "1. We refer to the latent multinomial variables in the LDA model as topics, so as to exploit text-oriented intuitions, but we make no epistemological claims regarding these latent variables beyond their utility in representing probability distributions on sets of words.",
            "page": 4,
            "x": 92,
            "y": 670,
            "width": 431,
            "height": 36,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-we",
            "chunk_id": "a7de60a9-4d0e-4d07-b2de-aad5a9af3bc9",
            "group_text": "1. We refer to the latent multinomial variables in the LDA model as topics, so as to exploit text-oriented intuitions, but we make no epistemological claims regarding these latent variables beyond their utility in representing probability distributions on sets of words.\n\nwhere $p(z_n|\\theta)$ is simply $\\theta_i$ for the unique $i$ such that $z_n^i = 1$. Integrating over $\\theta$ and summing over $z$, we obtain the marginal distribution of a document:\n\n$p(\\mathbf{w}|\\alpha, \\beta) = \\int p(\\theta|\\alpha) \\left( \\prod_{n=1}^{N} \\sum_{z_n} p(z_n|\\theta) p(w_n|z_n, \\beta) \\right) d\\theta.$\n\nFinally, taking the product of the marginal probabilities of single documents, we obtain the probability of a corpus:\n\n$p(\\mathcal{D}|\\alpha, \\beta) = \\prod_{d=1}^M \\int p(\\theta_d|\\alpha) \\left( \\prod_{n=1}^{N_d} \\sum_{z_{dn}} p(z_{dn}|\\theta_d) p(w_{dn}|z_{dn},\\beta) \\right) d\\theta_d.$\n\nThe LDA model is represented as a probabilistic graphical model in Figure 1. As the figure makes clear, there are three levels to the LDA representation. The parameters \u03b1 and \u03b2 are corpus-level parameters, assumed to be sampled once in the process of generating a corpus. The variables \u03b8_d are document-level variables, sampled once per document. Finally, the variables z_dn and w_dn are word-level variables and are sampled once for each word in each document.\n\nIt is important to distinguish LDA from a simple Dirichlet-multinomial clustering model.  A classical clustering model would involve a two-level model in which a Dirichlet is sampled once for a corpus, a multinomial clustering variable is selected once for each document in the corpus, and a set of words are selected for the document conditional on the cluster variable.  As with many clustering models, such a model restricts a document to being associated with a single topic.  LDA, on the other hand, involves three levels, and notably the topic node is sampled *repeatedly* within the document.  Under this model, documents can be associated with multiple topics.\n\nStructures similar to that shown in Figure 1 are often studied in Bayesian statistical modeling, where they are referred to as *hierarchical models* (Gelman et al., 1995), or more precisely as *conditionally independent hierarchical models* (Kass and Steffey, 1989). Such models are also often referred to as *parametric empirical Bayes models*, a term that refers not only to a particular model structure, but also to the methods used for estimating parameters in the model (Morris, 1983). Indeed, as we discuss in Section 5, we adopt the empirical Bayes approach to estimating parameters such as $\\alpha$ and $\\beta$ in simple implementations of LDA, but we also consider fuller Bayesian approaches as well."
        },
        {
            "text": "where $p(z_n|\\theta)$ is simply $\\theta_i$ for the unique $i$ such that $z_n^i = 1$. Integrating over $\\theta$ and summing over $z$, we obtain the marginal distribution of a document:",
            "page": 5,
            "x": 86,
            "y": 287,
            "width": 436,
            "height": 28,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-we",
            "chunk_id": "6bf018da-bb32-4739-a14d-1724d971e6b3",
            "group_text": "1. We refer to the latent multinomial variables in the LDA model as topics, so as to exploit text-oriented intuitions, but we make no epistemological claims regarding these latent variables beyond their utility in representing probability distributions on sets of words.\n\nwhere $p(z_n|\\theta)$ is simply $\\theta_i$ for the unique $i$ such that $z_n^i = 1$. Integrating over $\\theta$ and summing over $z$, we obtain the marginal distribution of a document:\n\n$p(\\mathbf{w}|\\alpha, \\beta) = \\int p(\\theta|\\alpha) \\left( \\prod_{n=1}^{N} \\sum_{z_n} p(z_n|\\theta) p(w_n|z_n, \\beta) \\right) d\\theta.$\n\nFinally, taking the product of the marginal probabilities of single documents, we obtain the probability of a corpus:\n\n$p(\\mathcal{D}|\\alpha, \\beta) = \\prod_{d=1}^M \\int p(\\theta_d|\\alpha) \\left( \\prod_{n=1}^{N_d} \\sum_{z_{dn}} p(z_{dn}|\\theta_d) p(w_{dn}|z_{dn},\\beta) \\right) d\\theta_d.$\n\nThe LDA model is represented as a probabilistic graphical model in Figure 1. As the figure makes clear, there are three levels to the LDA representation. The parameters \u03b1 and \u03b2 are corpus-level parameters, assumed to be sampled once in the process of generating a corpus. The variables \u03b8_d are document-level variables, sampled once per document. Finally, the variables z_dn and w_dn are word-level variables and are sampled once for each word in each document.\n\nIt is important to distinguish LDA from a simple Dirichlet-multinomial clustering model.  A classical clustering model would involve a two-level model in which a Dirichlet is sampled once for a corpus, a multinomial clustering variable is selected once for each document in the corpus, and a set of words are selected for the document conditional on the cluster variable.  As with many clustering models, such a model restricts a document to being associated with a single topic.  LDA, on the other hand, involves three levels, and notably the topic node is sampled *repeatedly* within the document.  Under this model, documents can be associated with multiple topics.\n\nStructures similar to that shown in Figure 1 are often studied in Bayesian statistical modeling, where they are referred to as *hierarchical models* (Gelman et al., 1995), or more precisely as *conditionally independent hierarchical models* (Kass and Steffey, 1989). Such models are also often referred to as *parametric empirical Bayes models*, a term that refers not only to a particular model structure, but also to the methods used for estimating parameters in the model (Morris, 1983). Indeed, as we discuss in Section 5, we adopt the empirical Bayes approach to estimating parameters such as $\\alpha$ and $\\beta$ in simple implementations of LDA, but we also consider fuller Bayesian approaches as well."
        },
        {
            "text": "$p(\\mathbf{w}|\\alpha, \\beta) = \\int p(\\theta|\\alpha) \\left( \\prod_{n=1}^{N} \\sum_{z_n} p(z_n|\\theta) p(w_n|z_n, \\beta) \\right) d\\theta.$",
            "page": 5,
            "x": 179,
            "y": 321,
            "width": 343,
            "height": 37,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-we",
            "chunk_id": "04ec81eb-4650-4f90-b585-eef584c38c36",
            "group_text": "1. We refer to the latent multinomial variables in the LDA model as topics, so as to exploit text-oriented intuitions, but we make no epistemological claims regarding these latent variables beyond their utility in representing probability distributions on sets of words.\n\nwhere $p(z_n|\\theta)$ is simply $\\theta_i$ for the unique $i$ such that $z_n^i = 1$. Integrating over $\\theta$ and summing over $z$, we obtain the marginal distribution of a document:\n\n$p(\\mathbf{w}|\\alpha, \\beta) = \\int p(\\theta|\\alpha) \\left( \\prod_{n=1}^{N} \\sum_{z_n} p(z_n|\\theta) p(w_n|z_n, \\beta) \\right) d\\theta.$\n\nFinally, taking the product of the marginal probabilities of single documents, we obtain the probability of a corpus:\n\n$p(\\mathcal{D}|\\alpha, \\beta) = \\prod_{d=1}^M \\int p(\\theta_d|\\alpha) \\left( \\prod_{n=1}^{N_d} \\sum_{z_{dn}} p(z_{dn}|\\theta_d) p(w_{dn}|z_{dn},\\beta) \\right) d\\theta_d.$\n\nThe LDA model is represented as a probabilistic graphical model in Figure 1. As the figure makes clear, there are three levels to the LDA representation. The parameters \u03b1 and \u03b2 are corpus-level parameters, assumed to be sampled once in the process of generating a corpus. The variables \u03b8_d are document-level variables, sampled once per document. Finally, the variables z_dn and w_dn are word-level variables and are sampled once for each word in each document.\n\nIt is important to distinguish LDA from a simple Dirichlet-multinomial clustering model.  A classical clustering model would involve a two-level model in which a Dirichlet is sampled once for a corpus, a multinomial clustering variable is selected once for each document in the corpus, and a set of words are selected for the document conditional on the cluster variable.  As with many clustering models, such a model restricts a document to being associated with a single topic.  LDA, on the other hand, involves three levels, and notably the topic node is sampled *repeatedly* within the document.  Under this model, documents can be associated with multiple topics.\n\nStructures similar to that shown in Figure 1 are often studied in Bayesian statistical modeling, where they are referred to as *hierarchical models* (Gelman et al., 1995), or more precisely as *conditionally independent hierarchical models* (Kass and Steffey, 1989). Such models are also often referred to as *parametric empirical Bayes models*, a term that refers not only to a particular model structure, but also to the methods used for estimating parameters in the model (Morris, 1983). Indeed, as we discuss in Section 5, we adopt the empirical Bayes approach to estimating parameters such as $\\alpha$ and $\\beta$ in simple implementations of LDA, but we also consider fuller Bayesian approaches as well."
        },
        {
            "text": "Finally, taking the product of the marginal probabilities of single documents, we obtain the probability of a corpus:",
            "page": 5,
            "x": 87,
            "y": 361,
            "width": 433,
            "height": 27,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-we",
            "chunk_id": "fff24e9a-86b7-4b0c-adcc-cd7b7f72983b",
            "group_text": "1. We refer to the latent multinomial variables in the LDA model as topics, so as to exploit text-oriented intuitions, but we make no epistemological claims regarding these latent variables beyond their utility in representing probability distributions on sets of words.\n\nwhere $p(z_n|\\theta)$ is simply $\\theta_i$ for the unique $i$ such that $z_n^i = 1$. Integrating over $\\theta$ and summing over $z$, we obtain the marginal distribution of a document:\n\n$p(\\mathbf{w}|\\alpha, \\beta) = \\int p(\\theta|\\alpha) \\left( \\prod_{n=1}^{N} \\sum_{z_n} p(z_n|\\theta) p(w_n|z_n, \\beta) \\right) d\\theta.$\n\nFinally, taking the product of the marginal probabilities of single documents, we obtain the probability of a corpus:\n\n$p(\\mathcal{D}|\\alpha, \\beta) = \\prod_{d=1}^M \\int p(\\theta_d|\\alpha) \\left( \\prod_{n=1}^{N_d} \\sum_{z_{dn}} p(z_{dn}|\\theta_d) p(w_{dn}|z_{dn},\\beta) \\right) d\\theta_d.$\n\nThe LDA model is represented as a probabilistic graphical model in Figure 1. As the figure makes clear, there are three levels to the LDA representation. The parameters \u03b1 and \u03b2 are corpus-level parameters, assumed to be sampled once in the process of generating a corpus. The variables \u03b8_d are document-level variables, sampled once per document. Finally, the variables z_dn and w_dn are word-level variables and are sampled once for each word in each document.\n\nIt is important to distinguish LDA from a simple Dirichlet-multinomial clustering model.  A classical clustering model would involve a two-level model in which a Dirichlet is sampled once for a corpus, a multinomial clustering variable is selected once for each document in the corpus, and a set of words are selected for the document conditional on the cluster variable.  As with many clustering models, such a model restricts a document to being associated with a single topic.  LDA, on the other hand, involves three levels, and notably the topic node is sampled *repeatedly* within the document.  Under this model, documents can be associated with multiple topics.\n\nStructures similar to that shown in Figure 1 are often studied in Bayesian statistical modeling, where they are referred to as *hierarchical models* (Gelman et al., 1995), or more precisely as *conditionally independent hierarchical models* (Kass and Steffey, 1989). Such models are also often referred to as *parametric empirical Bayes models*, a term that refers not only to a particular model structure, but also to the methods used for estimating parameters in the model (Morris, 1983). Indeed, as we discuss in Section 5, we adopt the empirical Bayes approach to estimating parameters such as $\\alpha$ and $\\beta$ in simple implementations of LDA, but we also consider fuller Bayesian approaches as well."
        },
        {
            "text": "$p(\\mathcal{D}|\\alpha, \\beta) = \\prod_{d=1}^M \\int p(\\theta_d|\\alpha) \\left( \\prod_{n=1}^{N_d} \\sum_{z_{dn}} p(z_{dn}|\\theta_d) p(w_{dn}|z_{dn},\\beta) \\right) d\\theta_d.$",
            "page": 5,
            "x": 156,
            "y": 393,
            "width": 297,
            "height": 37,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-we",
            "chunk_id": "2e8dc868-aa77-4a78-b57d-13c1d21fdfd8",
            "group_text": "1. We refer to the latent multinomial variables in the LDA model as topics, so as to exploit text-oriented intuitions, but we make no epistemological claims regarding these latent variables beyond their utility in representing probability distributions on sets of words.\n\nwhere $p(z_n|\\theta)$ is simply $\\theta_i$ for the unique $i$ such that $z_n^i = 1$. Integrating over $\\theta$ and summing over $z$, we obtain the marginal distribution of a document:\n\n$p(\\mathbf{w}|\\alpha, \\beta) = \\int p(\\theta|\\alpha) \\left( \\prod_{n=1}^{N} \\sum_{z_n} p(z_n|\\theta) p(w_n|z_n, \\beta) \\right) d\\theta.$\n\nFinally, taking the product of the marginal probabilities of single documents, we obtain the probability of a corpus:\n\n$p(\\mathcal{D}|\\alpha, \\beta) = \\prod_{d=1}^M \\int p(\\theta_d|\\alpha) \\left( \\prod_{n=1}^{N_d} \\sum_{z_{dn}} p(z_{dn}|\\theta_d) p(w_{dn}|z_{dn},\\beta) \\right) d\\theta_d.$\n\nThe LDA model is represented as a probabilistic graphical model in Figure 1. As the figure makes clear, there are three levels to the LDA representation. The parameters \u03b1 and \u03b2 are corpus-level parameters, assumed to be sampled once in the process of generating a corpus. The variables \u03b8_d are document-level variables, sampled once per document. Finally, the variables z_dn and w_dn are word-level variables and are sampled once for each word in each document.\n\nIt is important to distinguish LDA from a simple Dirichlet-multinomial clustering model.  A classical clustering model would involve a two-level model in which a Dirichlet is sampled once for a corpus, a multinomial clustering variable is selected once for each document in the corpus, and a set of words are selected for the document conditional on the cluster variable.  As with many clustering models, such a model restricts a document to being associated with a single topic.  LDA, on the other hand, involves three levels, and notably the topic node is sampled *repeatedly* within the document.  Under this model, documents can be associated with multiple topics.\n\nStructures similar to that shown in Figure 1 are often studied in Bayesian statistical modeling, where they are referred to as *hierarchical models* (Gelman et al., 1995), or more precisely as *conditionally independent hierarchical models* (Kass and Steffey, 1989). Such models are also often referred to as *parametric empirical Bayes models*, a term that refers not only to a particular model structure, but also to the methods used for estimating parameters in the model (Morris, 1983). Indeed, as we discuss in Section 5, we adopt the empirical Bayes approach to estimating parameters such as $\\alpha$ and $\\beta$ in simple implementations of LDA, but we also consider fuller Bayesian approaches as well."
        },
        {
            "text": "The LDA model is represented as a probabilistic graphical model in Figure 1. As the figure makes clear, there are three levels to the LDA representation. The parameters \u03b1 and \u03b2 are corpus-level parameters, assumed to be sampled once in the process of generating a corpus. The variables \u03b8_d are document-level variables, sampled once per document. Finally, the variables z_dn and w_dn are word-level variables and are sampled once for each word in each document.",
            "page": 5,
            "x": 88,
            "y": 435,
            "width": 434,
            "height": 66,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-we",
            "chunk_id": "af7de5d2-26af-4ab6-8cba-a81d513ecf99",
            "group_text": "1. We refer to the latent multinomial variables in the LDA model as topics, so as to exploit text-oriented intuitions, but we make no epistemological claims regarding these latent variables beyond their utility in representing probability distributions on sets of words.\n\nwhere $p(z_n|\\theta)$ is simply $\\theta_i$ for the unique $i$ such that $z_n^i = 1$. Integrating over $\\theta$ and summing over $z$, we obtain the marginal distribution of a document:\n\n$p(\\mathbf{w}|\\alpha, \\beta) = \\int p(\\theta|\\alpha) \\left( \\prod_{n=1}^{N} \\sum_{z_n} p(z_n|\\theta) p(w_n|z_n, \\beta) \\right) d\\theta.$\n\nFinally, taking the product of the marginal probabilities of single documents, we obtain the probability of a corpus:\n\n$p(\\mathcal{D}|\\alpha, \\beta) = \\prod_{d=1}^M \\int p(\\theta_d|\\alpha) \\left( \\prod_{n=1}^{N_d} \\sum_{z_{dn}} p(z_{dn}|\\theta_d) p(w_{dn}|z_{dn},\\beta) \\right) d\\theta_d.$\n\nThe LDA model is represented as a probabilistic graphical model in Figure 1. As the figure makes clear, there are three levels to the LDA representation. The parameters \u03b1 and \u03b2 are corpus-level parameters, assumed to be sampled once in the process of generating a corpus. The variables \u03b8_d are document-level variables, sampled once per document. Finally, the variables z_dn and w_dn are word-level variables and are sampled once for each word in each document.\n\nIt is important to distinguish LDA from a simple Dirichlet-multinomial clustering model.  A classical clustering model would involve a two-level model in which a Dirichlet is sampled once for a corpus, a multinomial clustering variable is selected once for each document in the corpus, and a set of words are selected for the document conditional on the cluster variable.  As with many clustering models, such a model restricts a document to being associated with a single topic.  LDA, on the other hand, involves three levels, and notably the topic node is sampled *repeatedly* within the document.  Under this model, documents can be associated with multiple topics.\n\nStructures similar to that shown in Figure 1 are often studied in Bayesian statistical modeling, where they are referred to as *hierarchical models* (Gelman et al., 1995), or more precisely as *conditionally independent hierarchical models* (Kass and Steffey, 1989). Such models are also often referred to as *parametric empirical Bayes models*, a term that refers not only to a particular model structure, but also to the methods used for estimating parameters in the model (Morris, 1983). Indeed, as we discuss in Section 5, we adopt the empirical Bayes approach to estimating parameters such as $\\alpha$ and $\\beta$ in simple implementations of LDA, but we also consider fuller Bayesian approaches as well."
        },
        {
            "text": "It is important to distinguish LDA from a simple Dirichlet-multinomial clustering model.  A classical clustering model would involve a two-level model in which a Dirichlet is sampled once for a corpus, a multinomial clustering variable is selected once for each document in the corpus, and a set of words are selected for the document conditional on the cluster variable.  As with many clustering models, such a model restricts a document to being associated with a single topic.  LDA, on the other hand, involves three levels, and notably the topic node is sampled *repeatedly* within the document.  Under this model, documents can be associated with multiple topics.",
            "page": 5,
            "x": 89,
            "y": 503,
            "width": 433,
            "height": 94,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-we",
            "chunk_id": "b52a3073-d8ea-48be-9705-37e5abc77f33",
            "group_text": "1. We refer to the latent multinomial variables in the LDA model as topics, so as to exploit text-oriented intuitions, but we make no epistemological claims regarding these latent variables beyond their utility in representing probability distributions on sets of words.\n\nwhere $p(z_n|\\theta)$ is simply $\\theta_i$ for the unique $i$ such that $z_n^i = 1$. Integrating over $\\theta$ and summing over $z$, we obtain the marginal distribution of a document:\n\n$p(\\mathbf{w}|\\alpha, \\beta) = \\int p(\\theta|\\alpha) \\left( \\prod_{n=1}^{N} \\sum_{z_n} p(z_n|\\theta) p(w_n|z_n, \\beta) \\right) d\\theta.$\n\nFinally, taking the product of the marginal probabilities of single documents, we obtain the probability of a corpus:\n\n$p(\\mathcal{D}|\\alpha, \\beta) = \\prod_{d=1}^M \\int p(\\theta_d|\\alpha) \\left( \\prod_{n=1}^{N_d} \\sum_{z_{dn}} p(z_{dn}|\\theta_d) p(w_{dn}|z_{dn},\\beta) \\right) d\\theta_d.$\n\nThe LDA model is represented as a probabilistic graphical model in Figure 1. As the figure makes clear, there are three levels to the LDA representation. The parameters \u03b1 and \u03b2 are corpus-level parameters, assumed to be sampled once in the process of generating a corpus. The variables \u03b8_d are document-level variables, sampled once per document. Finally, the variables z_dn and w_dn are word-level variables and are sampled once for each word in each document.\n\nIt is important to distinguish LDA from a simple Dirichlet-multinomial clustering model.  A classical clustering model would involve a two-level model in which a Dirichlet is sampled once for a corpus, a multinomial clustering variable is selected once for each document in the corpus, and a set of words are selected for the document conditional on the cluster variable.  As with many clustering models, such a model restricts a document to being associated with a single topic.  LDA, on the other hand, involves three levels, and notably the topic node is sampled *repeatedly* within the document.  Under this model, documents can be associated with multiple topics.\n\nStructures similar to that shown in Figure 1 are often studied in Bayesian statistical modeling, where they are referred to as *hierarchical models* (Gelman et al., 1995), or more precisely as *conditionally independent hierarchical models* (Kass and Steffey, 1989). Such models are also often referred to as *parametric empirical Bayes models*, a term that refers not only to a particular model structure, but also to the methods used for estimating parameters in the model (Morris, 1983). Indeed, as we discuss in Section 5, we adopt the empirical Bayes approach to estimating parameters such as $\\alpha$ and $\\beta$ in simple implementations of LDA, but we also consider fuller Bayesian approaches as well."
        },
        {
            "text": "Structures similar to that shown in Figure 1 are often studied in Bayesian statistical modeling, where they are referred to as *hierarchical models* (Gelman et al., 1995), or more precisely as *conditionally independent hierarchical models* (Kass and Steffey, 1989). Such models are also often referred to as *parametric empirical Bayes models*, a term that refers not only to a particular model structure, but also to the methods used for estimating parameters in the model (Morris, 1983). Indeed, as we discuss in Section 5, we adopt the empirical Bayes approach to estimating parameters such as $\\alpha$ and $\\beta$ in simple implementations of LDA, but we also consider fuller Bayesian approaches as well.",
            "page": 5,
            "x": 89,
            "y": 598,
            "width": 432,
            "height": 106,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-we",
            "chunk_id": "2a7aa5d4-f020-47d2-8f43-370603610b47",
            "group_text": "1. We refer to the latent multinomial variables in the LDA model as topics, so as to exploit text-oriented intuitions, but we make no epistemological claims regarding these latent variables beyond their utility in representing probability distributions on sets of words.\n\nwhere $p(z_n|\\theta)$ is simply $\\theta_i$ for the unique $i$ such that $z_n^i = 1$. Integrating over $\\theta$ and summing over $z$, we obtain the marginal distribution of a document:\n\n$p(\\mathbf{w}|\\alpha, \\beta) = \\int p(\\theta|\\alpha) \\left( \\prod_{n=1}^{N} \\sum_{z_n} p(z_n|\\theta) p(w_n|z_n, \\beta) \\right) d\\theta.$\n\nFinally, taking the product of the marginal probabilities of single documents, we obtain the probability of a corpus:\n\n$p(\\mathcal{D}|\\alpha, \\beta) = \\prod_{d=1}^M \\int p(\\theta_d|\\alpha) \\left( \\prod_{n=1}^{N_d} \\sum_{z_{dn}} p(z_{dn}|\\theta_d) p(w_{dn}|z_{dn},\\beta) \\right) d\\theta_d.$\n\nThe LDA model is represented as a probabilistic graphical model in Figure 1. As the figure makes clear, there are three levels to the LDA representation. The parameters \u03b1 and \u03b2 are corpus-level parameters, assumed to be sampled once in the process of generating a corpus. The variables \u03b8_d are document-level variables, sampled once per document. Finally, the variables z_dn and w_dn are word-level variables and are sampled once for each word in each document.\n\nIt is important to distinguish LDA from a simple Dirichlet-multinomial clustering model.  A classical clustering model would involve a two-level model in which a Dirichlet is sampled once for a corpus, a multinomial clustering variable is selected once for each document in the corpus, and a set of words are selected for the document conditional on the cluster variable.  As with many clustering models, such a model restricts a document to being associated with a single topic.  LDA, on the other hand, involves three levels, and notably the topic node is sampled *repeatedly* within the document.  Under this model, documents can be associated with multiple topics.\n\nStructures similar to that shown in Figure 1 are often studied in Bayesian statistical modeling, where they are referred to as *hierarchical models* (Gelman et al., 1995), or more precisely as *conditionally independent hierarchical models* (Kass and Steffey, 1989). Such models are also often referred to as *parametric empirical Bayes models*, a term that refers not only to a particular model structure, but also to the methods used for estimating parameters in the model (Morris, 1983). Indeed, as we discuss in Section 5, we adopt the empirical Bayes approach to estimating parameters such as $\\alpha$ and $\\beta$ in simple implementations of LDA, but we also consider fuller Bayesian approaches as well."
        },
        {
            "text": "3.1 LDA and exchangeability\n\nA finite set of random variables $\\{z_1, \\ldots, z_N\\}$ is said to be *exchangeable* if the joint distribution is invariant to permutation. If $\\pi$ is a permutation of the integers from 1 to $N$:",
            "page": 6,
            "x": 84,
            "y": 88,
            "width": 439,
            "height": 51,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "6-lda",
            "chunk_id": "11a492c4-59ea-4795-a85c-db496bb672d3",
            "group_text": "3.1 LDA and exchangeability\n\nA finite set of random variables $\\{z_1, \\ldots, z_N\\}$ is said to be *exchangeable* if the joint distribution is invariant to permutation. If $\\pi$ is a permutation of the integers from 1 to $N$:\n\n$p(z_1, \\ldots, z_N) = p(z_{\\pi(1)}, \\ldots, z_{\\pi(N)}).$\n\nAn infinite sequence of random variables is *infinitely exchangeable* if every finite subsequence is exchangeable.\n\nDe Finetti\u2019s representation theorem states that the joint distribution of an infinitely exchangeable sequence of random variables is as if a random parameter were drawn from some distribution and then the random variables in question were independent and identically distributed, conditioned on that parameter.\n\nIn LDA, we assume that words are generated by topics (by fixed conditional distributions) and\nthat those topics are infinitely exchangeable within a document. By de Finetti\u2019s theorem, the prob-\nability of a sequence of words and topics must therefore have the form:\n\n$p(\\mathbf{w}, \\mathbf{z}) = \\int p(\\theta) \\left( \\prod_{n=1}^{N} p(z_n \\mid \\theta) p(w_n \\mid z_n) \\right) d\\theta,$\n\nwhere \u03b8 is the random parameter of a multinomial over topics. We obtain the LDA distribution\non documents in Eq. (3) by marginalizing out the topic variables and endowing \u03b8 with a Dirichlet\ndistribution."
        },
        {
            "text": "$p(z_1, \\ldots, z_N) = p(z_{\\pi(1)}, \\ldots, z_{\\pi(N)}).$",
            "page": 6,
            "x": 226,
            "y": 146,
            "width": 157,
            "height": 20,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "6-lda",
            "chunk_id": "58fc19f1-14e3-4a14-a916-e20cd60f8f48",
            "group_text": "3.1 LDA and exchangeability\n\nA finite set of random variables $\\{z_1, \\ldots, z_N\\}$ is said to be *exchangeable* if the joint distribution is invariant to permutation. If $\\pi$ is a permutation of the integers from 1 to $N$:\n\n$p(z_1, \\ldots, z_N) = p(z_{\\pi(1)}, \\ldots, z_{\\pi(N)}).$\n\nAn infinite sequence of random variables is *infinitely exchangeable* if every finite subsequence is exchangeable.\n\nDe Finetti\u2019s representation theorem states that the joint distribution of an infinitely exchangeable sequence of random variables is as if a random parameter were drawn from some distribution and then the random variables in question were independent and identically distributed, conditioned on that parameter.\n\nIn LDA, we assume that words are generated by topics (by fixed conditional distributions) and\nthat those topics are infinitely exchangeable within a document. By de Finetti\u2019s theorem, the prob-\nability of a sequence of words and topics must therefore have the form:\n\n$p(\\mathbf{w}, \\mathbf{z}) = \\int p(\\theta) \\left( \\prod_{n=1}^{N} p(z_n \\mid \\theta) p(w_n \\mid z_n) \\right) d\\theta,$\n\nwhere \u03b8 is the random parameter of a multinomial over topics. We obtain the LDA distribution\non documents in Eq. (3) by marginalizing out the topic variables and endowing \u03b8 with a Dirichlet\ndistribution."
        },
        {
            "text": "An infinite sequence of random variables is *infinitely exchangeable* if every finite subsequence is exchangeable.",
            "page": 6,
            "x": 87,
            "y": 172,
            "width": 436,
            "height": 27,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "6-lda",
            "chunk_id": "d2489ed9-ee69-4d3a-bf97-3f5a074e8cf2",
            "group_text": "3.1 LDA and exchangeability\n\nA finite set of random variables $\\{z_1, \\ldots, z_N\\}$ is said to be *exchangeable* if the joint distribution is invariant to permutation. If $\\pi$ is a permutation of the integers from 1 to $N$:\n\n$p(z_1, \\ldots, z_N) = p(z_{\\pi(1)}, \\ldots, z_{\\pi(N)}).$\n\nAn infinite sequence of random variables is *infinitely exchangeable* if every finite subsequence is exchangeable.\n\nDe Finetti\u2019s representation theorem states that the joint distribution of an infinitely exchangeable sequence of random variables is as if a random parameter were drawn from some distribution and then the random variables in question were independent and identically distributed, conditioned on that parameter.\n\nIn LDA, we assume that words are generated by topics (by fixed conditional distributions) and\nthat those topics are infinitely exchangeable within a document. By de Finetti\u2019s theorem, the prob-\nability of a sequence of words and topics must therefore have the form:\n\n$p(\\mathbf{w}, \\mathbf{z}) = \\int p(\\theta) \\left( \\prod_{n=1}^{N} p(z_n \\mid \\theta) p(w_n \\mid z_n) \\right) d\\theta,$\n\nwhere \u03b8 is the random parameter of a multinomial over topics. We obtain the LDA distribution\non documents in Eq. (3) by marginalizing out the topic variables and endowing \u03b8 with a Dirichlet\ndistribution."
        },
        {
            "text": "De Finetti\u2019s representation theorem states that the joint distribution of an infinitely exchangeable sequence of random variables is as if a random parameter were drawn from some distribution and then the random variables in question were independent and identically distributed, conditioned on that parameter.",
            "page": 6,
            "x": 87,
            "y": 200,
            "width": 435,
            "height": 53,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "6-lda",
            "chunk_id": "31beff94-3cc0-4352-b5aa-525f71efcfdb",
            "group_text": "3.1 LDA and exchangeability\n\nA finite set of random variables $\\{z_1, \\ldots, z_N\\}$ is said to be *exchangeable* if the joint distribution is invariant to permutation. If $\\pi$ is a permutation of the integers from 1 to $N$:\n\n$p(z_1, \\ldots, z_N) = p(z_{\\pi(1)}, \\ldots, z_{\\pi(N)}).$\n\nAn infinite sequence of random variables is *infinitely exchangeable* if every finite subsequence is exchangeable.\n\nDe Finetti\u2019s representation theorem states that the joint distribution of an infinitely exchangeable sequence of random variables is as if a random parameter were drawn from some distribution and then the random variables in question were independent and identically distributed, conditioned on that parameter.\n\nIn LDA, we assume that words are generated by topics (by fixed conditional distributions) and\nthat those topics are infinitely exchangeable within a document. By de Finetti\u2019s theorem, the prob-\nability of a sequence of words and topics must therefore have the form:\n\n$p(\\mathbf{w}, \\mathbf{z}) = \\int p(\\theta) \\left( \\prod_{n=1}^{N} p(z_n \\mid \\theta) p(w_n \\mid z_n) \\right) d\\theta,$\n\nwhere \u03b8 is the random parameter of a multinomial over topics. We obtain the LDA distribution\non documents in Eq. (3) by marginalizing out the topic variables and endowing \u03b8 with a Dirichlet\ndistribution."
        },
        {
            "text": "In LDA, we assume that words are generated by topics (by fixed conditional distributions) and\nthat those topics are infinitely exchangeable within a document. By de Finetti\u2019s theorem, the prob-\nability of a sequence of words and topics must therefore have the form:",
            "page": 6,
            "x": 88,
            "y": 254,
            "width": 434,
            "height": 42,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "6-lda",
            "chunk_id": "9a94bdc7-8efb-4f7f-b2c1-272aa09c8542",
            "group_text": "3.1 LDA and exchangeability\n\nA finite set of random variables $\\{z_1, \\ldots, z_N\\}$ is said to be *exchangeable* if the joint distribution is invariant to permutation. If $\\pi$ is a permutation of the integers from 1 to $N$:\n\n$p(z_1, \\ldots, z_N) = p(z_{\\pi(1)}, \\ldots, z_{\\pi(N)}).$\n\nAn infinite sequence of random variables is *infinitely exchangeable* if every finite subsequence is exchangeable.\n\nDe Finetti\u2019s representation theorem states that the joint distribution of an infinitely exchangeable sequence of random variables is as if a random parameter were drawn from some distribution and then the random variables in question were independent and identically distributed, conditioned on that parameter.\n\nIn LDA, we assume that words are generated by topics (by fixed conditional distributions) and\nthat those topics are infinitely exchangeable within a document. By de Finetti\u2019s theorem, the prob-\nability of a sequence of words and topics must therefore have the form:\n\n$p(\\mathbf{w}, \\mathbf{z}) = \\int p(\\theta) \\left( \\prod_{n=1}^{N} p(z_n \\mid \\theta) p(w_n \\mid z_n) \\right) d\\theta,$\n\nwhere \u03b8 is the random parameter of a multinomial over topics. We obtain the LDA distribution\non documents in Eq. (3) by marginalizing out the topic variables and endowing \u03b8 with a Dirichlet\ndistribution."
        },
        {
            "text": "$p(\\mathbf{w}, \\mathbf{z}) = \\int p(\\theta) \\left( \\prod_{n=1}^{N} p(z_n \\mid \\theta) p(w_n \\mid z_n) \\right) d\\theta,$",
            "page": 6,
            "x": 201,
            "y": 303,
            "width": 207,
            "height": 39,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "6-lda",
            "chunk_id": "27dee379-411e-4685-8abb-f1e1c42b2f17",
            "group_text": "3.1 LDA and exchangeability\n\nA finite set of random variables $\\{z_1, \\ldots, z_N\\}$ is said to be *exchangeable* if the joint distribution is invariant to permutation. If $\\pi$ is a permutation of the integers from 1 to $N$:\n\n$p(z_1, \\ldots, z_N) = p(z_{\\pi(1)}, \\ldots, z_{\\pi(N)}).$\n\nAn infinite sequence of random variables is *infinitely exchangeable* if every finite subsequence is exchangeable.\n\nDe Finetti\u2019s representation theorem states that the joint distribution of an infinitely exchangeable sequence of random variables is as if a random parameter were drawn from some distribution and then the random variables in question were independent and identically distributed, conditioned on that parameter.\n\nIn LDA, we assume that words are generated by topics (by fixed conditional distributions) and\nthat those topics are infinitely exchangeable within a document. By de Finetti\u2019s theorem, the prob-\nability of a sequence of words and topics must therefore have the form:\n\n$p(\\mathbf{w}, \\mathbf{z}) = \\int p(\\theta) \\left( \\prod_{n=1}^{N} p(z_n \\mid \\theta) p(w_n \\mid z_n) \\right) d\\theta,$\n\nwhere \u03b8 is the random parameter of a multinomial over topics. We obtain the LDA distribution\non documents in Eq. (3) by marginalizing out the topic variables and endowing \u03b8 with a Dirichlet\ndistribution."
        },
        {
            "text": "where \u03b8 is the random parameter of a multinomial over topics. We obtain the LDA distribution\non documents in Eq. (3) by marginalizing out the topic variables and endowing \u03b8 with a Dirichlet\ndistribution.",
            "page": 6,
            "x": 87,
            "y": 347,
            "width": 435,
            "height": 41,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "6-lda",
            "chunk_id": "9b9592b0-53e5-4ac9-8486-ac97e2875dbc",
            "group_text": "3.1 LDA and exchangeability\n\nA finite set of random variables $\\{z_1, \\ldots, z_N\\}$ is said to be *exchangeable* if the joint distribution is invariant to permutation. If $\\pi$ is a permutation of the integers from 1 to $N$:\n\n$p(z_1, \\ldots, z_N) = p(z_{\\pi(1)}, \\ldots, z_{\\pi(N)}).$\n\nAn infinite sequence of random variables is *infinitely exchangeable* if every finite subsequence is exchangeable.\n\nDe Finetti\u2019s representation theorem states that the joint distribution of an infinitely exchangeable sequence of random variables is as if a random parameter were drawn from some distribution and then the random variables in question were independent and identically distributed, conditioned on that parameter.\n\nIn LDA, we assume that words are generated by topics (by fixed conditional distributions) and\nthat those topics are infinitely exchangeable within a document. By de Finetti\u2019s theorem, the prob-\nability of a sequence of words and topics must therefore have the form:\n\n$p(\\mathbf{w}, \\mathbf{z}) = \\int p(\\theta) \\left( \\prod_{n=1}^{N} p(z_n \\mid \\theta) p(w_n \\mid z_n) \\right) d\\theta,$\n\nwhere \u03b8 is the random parameter of a multinomial over topics. We obtain the LDA distribution\non documents in Eq. (3) by marginalizing out the topic variables and endowing \u03b8 with a Dirichlet\ndistribution."
        },
        {
            "text": "3.2 A continuous mixture of unigrams\n\nThe LDA model shown in Figure 1 is somewhat more elaborate than the two-level models often studied in the classical hierarchical Bayesian literature. By marginalizing over the hidden topic variable z, however, we can understand LDA as a two-level model.",
            "page": 6,
            "x": 86,
            "y": 402,
            "width": 435,
            "height": 61,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-a",
            "chunk_id": "7abca7fe-3f3e-4951-b4df-a2a5140c65c8",
            "group_text": "3.2 A continuous mixture of unigrams\n\nThe LDA model shown in Figure 1 is somewhat more elaborate than the two-level models often studied in the classical hierarchical Bayesian literature. By marginalizing over the hidden topic variable z, however, we can understand LDA as a two-level model.\n\nIn particular, let us form the word distribution $p(w|\\theta, \\beta)$:\n\n$p(w|\\theta, \\beta) = \\sum_z p(w|z, \\beta) p(z|\\theta).$\n\nNote that this is a random quantity since it depends on $\\theta$.\n\nFigure 2:  An example density on unigram distributions $p(w|\\theta, \\beta)$ under LDA for three words and four topics. The triangle embedded in the x-y plane is the 2-D simplex representing all possible multinomial distributions over three words. Each of the vertices of the triangle corresponds to a deterministic distribution that assigns probability one to one of the words; the midpoint of an edge gives probability 0.5 to two of the words; and the centroid of the triangle is the uniform distribution over all three words. The four points marked with an $x$ are the locations of the multinomial distributions $p(w|z)$ for each of the four topics, and the surface shown on top of the simplex is an example of a density over the $(V-1)$-simplex (multinomial distributions of words) given by LDA.\n\nWe now define the following generative process for a document **w**:\n\n1.  Choose $\\theta \\sim \\mathrm{Dir}(\\alpha)$.\n\n2.  For each of the $N$ words $w_n$:\n\n    (a)  Choose a word $w_n$ from $p(w_n \\mid \\theta, \\beta)$.\n\nThis process defines the marginal distribution of a document as a continuous mixture distribution:\n\n$p(\\mathbf{w} \\mid \\alpha, \\beta) = \\int p(\\theta \\mid \\alpha) \\left( \\prod_{n=1}^{N} p(w_n \\mid \\theta, \\beta) \\right) d\\theta,$\n\nwhere $p(w_n|\\theta, \\beta)$ are the mixture components and $p(\\theta|\\alpha)$ are the mixture weights.\n\nFigure 2 illustrates this interpretation of LDA. It depicts the distribution on $p(w|\\theta, \\beta)$ which is induced from a particular instance of an LDA model. Note that this distribution on the $(V-1)$-simplex is attained with only $k+kV$ parameters yet exhibits a very interesting multimodal structure.\n\nFigure 3: Graphical model representation of different models of discrete data."
        },
        {
            "text": "In particular, let us form the word distribution $p(w|\\theta, \\beta)$:",
            "page": 6,
            "x": 105,
            "y": 463,
            "width": 253,
            "height": 16,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-a",
            "chunk_id": "075ca3dd-dd7b-4339-ac97-8f9cc4f59350",
            "group_text": "3.2 A continuous mixture of unigrams\n\nThe LDA model shown in Figure 1 is somewhat more elaborate than the two-level models often studied in the classical hierarchical Bayesian literature. By marginalizing over the hidden topic variable z, however, we can understand LDA as a two-level model.\n\nIn particular, let us form the word distribution $p(w|\\theta, \\beta)$:\n\n$p(w|\\theta, \\beta) = \\sum_z p(w|z, \\beta) p(z|\\theta).$\n\nNote that this is a random quantity since it depends on $\\theta$.\n\nFigure 2:  An example density on unigram distributions $p(w|\\theta, \\beta)$ under LDA for three words and four topics. The triangle embedded in the x-y plane is the 2-D simplex representing all possible multinomial distributions over three words. Each of the vertices of the triangle corresponds to a deterministic distribution that assigns probability one to one of the words; the midpoint of an edge gives probability 0.5 to two of the words; and the centroid of the triangle is the uniform distribution over all three words. The four points marked with an $x$ are the locations of the multinomial distributions $p(w|z)$ for each of the four topics, and the surface shown on top of the simplex is an example of a density over the $(V-1)$-simplex (multinomial distributions of words) given by LDA.\n\nWe now define the following generative process for a document **w**:\n\n1.  Choose $\\theta \\sim \\mathrm{Dir}(\\alpha)$.\n\n2.  For each of the $N$ words $w_n$:\n\n    (a)  Choose a word $w_n$ from $p(w_n \\mid \\theta, \\beta)$.\n\nThis process defines the marginal distribution of a document as a continuous mixture distribution:\n\n$p(\\mathbf{w} \\mid \\alpha, \\beta) = \\int p(\\theta \\mid \\alpha) \\left( \\prod_{n=1}^{N} p(w_n \\mid \\theta, \\beta) \\right) d\\theta,$\n\nwhere $p(w_n|\\theta, \\beta)$ are the mixture components and $p(\\theta|\\alpha)$ are the mixture weights.\n\nFigure 2 illustrates this interpretation of LDA. It depicts the distribution on $p(w|\\theta, \\beta)$ which is induced from a particular instance of an LDA model. Note that this distribution on the $(V-1)$-simplex is attained with only $k+kV$ parameters yet exhibits a very interesting multimodal structure.\n\nFigure 3: Graphical model representation of different models of discrete data."
        },
        {
            "text": "$p(w|\\theta, \\beta) = \\sum_z p(w|z, \\beta) p(z|\\theta).$",
            "page": 6,
            "x": 231,
            "y": 482,
            "width": 147,
            "height": 27,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-a",
            "chunk_id": "962d4e44-fd1f-4928-b59f-a39212ba80d7",
            "group_text": "3.2 A continuous mixture of unigrams\n\nThe LDA model shown in Figure 1 is somewhat more elaborate than the two-level models often studied in the classical hierarchical Bayesian literature. By marginalizing over the hidden topic variable z, however, we can understand LDA as a two-level model.\n\nIn particular, let us form the word distribution $p(w|\\theta, \\beta)$:\n\n$p(w|\\theta, \\beta) = \\sum_z p(w|z, \\beta) p(z|\\theta).$\n\nNote that this is a random quantity since it depends on $\\theta$.\n\nFigure 2:  An example density on unigram distributions $p(w|\\theta, \\beta)$ under LDA for three words and four topics. The triangle embedded in the x-y plane is the 2-D simplex representing all possible multinomial distributions over three words. Each of the vertices of the triangle corresponds to a deterministic distribution that assigns probability one to one of the words; the midpoint of an edge gives probability 0.5 to two of the words; and the centroid of the triangle is the uniform distribution over all three words. The four points marked with an $x$ are the locations of the multinomial distributions $p(w|z)$ for each of the four topics, and the surface shown on top of the simplex is an example of a density over the $(V-1)$-simplex (multinomial distributions of words) given by LDA.\n\nWe now define the following generative process for a document **w**:\n\n1.  Choose $\\theta \\sim \\mathrm{Dir}(\\alpha)$.\n\n2.  For each of the $N$ words $w_n$:\n\n    (a)  Choose a word $w_n$ from $p(w_n \\mid \\theta, \\beta)$.\n\nThis process defines the marginal distribution of a document as a continuous mixture distribution:\n\n$p(\\mathbf{w} \\mid \\alpha, \\beta) = \\int p(\\theta \\mid \\alpha) \\left( \\prod_{n=1}^{N} p(w_n \\mid \\theta, \\beta) \\right) d\\theta,$\n\nwhere $p(w_n|\\theta, \\beta)$ are the mixture components and $p(\\theta|\\alpha)$ are the mixture weights.\n\nFigure 2 illustrates this interpretation of LDA. It depicts the distribution on $p(w|\\theta, \\beta)$ which is induced from a particular instance of an LDA model. Note that this distribution on the $(V-1)$-simplex is attained with only $k+kV$ parameters yet exhibits a very interesting multimodal structure.\n\nFigure 3: Graphical model representation of different models of discrete data."
        },
        {
            "text": "Note that this is a random quantity since it depends on $\\theta$.",
            "page": 6,
            "x": 88,
            "y": 519,
            "width": 250,
            "height": 15,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-a",
            "chunk_id": "e98f712f-376a-4e41-9b74-4a5ae34873b4",
            "group_text": "3.2 A continuous mixture of unigrams\n\nThe LDA model shown in Figure 1 is somewhat more elaborate than the two-level models often studied in the classical hierarchical Bayesian literature. By marginalizing over the hidden topic variable z, however, we can understand LDA as a two-level model.\n\nIn particular, let us form the word distribution $p(w|\\theta, \\beta)$:\n\n$p(w|\\theta, \\beta) = \\sum_z p(w|z, \\beta) p(z|\\theta).$\n\nNote that this is a random quantity since it depends on $\\theta$.\n\nFigure 2:  An example density on unigram distributions $p(w|\\theta, \\beta)$ under LDA for three words and four topics. The triangle embedded in the x-y plane is the 2-D simplex representing all possible multinomial distributions over three words. Each of the vertices of the triangle corresponds to a deterministic distribution that assigns probability one to one of the words; the midpoint of an edge gives probability 0.5 to two of the words; and the centroid of the triangle is the uniform distribution over all three words. The four points marked with an $x$ are the locations of the multinomial distributions $p(w|z)$ for each of the four topics, and the surface shown on top of the simplex is an example of a density over the $(V-1)$-simplex (multinomial distributions of words) given by LDA.\n\nWe now define the following generative process for a document **w**:\n\n1.  Choose $\\theta \\sim \\mathrm{Dir}(\\alpha)$.\n\n2.  For each of the $N$ words $w_n$:\n\n    (a)  Choose a word $w_n$ from $p(w_n \\mid \\theta, \\beta)$.\n\nThis process defines the marginal distribution of a document as a continuous mixture distribution:\n\n$p(\\mathbf{w} \\mid \\alpha, \\beta) = \\int p(\\theta \\mid \\alpha) \\left( \\prod_{n=1}^{N} p(w_n \\mid \\theta, \\beta) \\right) d\\theta,$\n\nwhere $p(w_n|\\theta, \\beta)$ are the mixture components and $p(\\theta|\\alpha)$ are the mixture weights.\n\nFigure 2 illustrates this interpretation of LDA. It depicts the distribution on $p(w|\\theta, \\beta)$ which is induced from a particular instance of an LDA model. Note that this distribution on the $(V-1)$-simplex is attained with only $k+kV$ parameters yet exhibits a very interesting multimodal structure.\n\nFigure 3: Graphical model representation of different models of discrete data."
        },
        {
            "text": "Figure 2:  An example density on unigram distributions $p(w|\\theta, \\beta)$ under LDA for three words and four topics. The triangle embedded in the x-y plane is the 2-D simplex representing all possible multinomial distributions over three words. Each of the vertices of the triangle corresponds to a deterministic distribution that assigns probability one to one of the words; the midpoint of an edge gives probability 0.5 to two of the words; and the centroid of the triangle is the uniform distribution over all three words. The four points marked with an $x$ are the locations of the multinomial distributions $p(w|z)$ for each of the four topics, and the surface shown on top of the simplex is an example of a density over the $(V-1)$-simplex (multinomial distributions of words) given by LDA.",
            "page": 7,
            "x": 86,
            "y": 322,
            "width": 438,
            "height": 126,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-a",
            "chunk_id": "4917e31f-ba7c-43b5-9e91-3a18fb9a30af",
            "group_text": "3.2 A continuous mixture of unigrams\n\nThe LDA model shown in Figure 1 is somewhat more elaborate than the two-level models often studied in the classical hierarchical Bayesian literature. By marginalizing over the hidden topic variable z, however, we can understand LDA as a two-level model.\n\nIn particular, let us form the word distribution $p(w|\\theta, \\beta)$:\n\n$p(w|\\theta, \\beta) = \\sum_z p(w|z, \\beta) p(z|\\theta).$\n\nNote that this is a random quantity since it depends on $\\theta$.\n\nFigure 2:  An example density on unigram distributions $p(w|\\theta, \\beta)$ under LDA for three words and four topics. The triangle embedded in the x-y plane is the 2-D simplex representing all possible multinomial distributions over three words. Each of the vertices of the triangle corresponds to a deterministic distribution that assigns probability one to one of the words; the midpoint of an edge gives probability 0.5 to two of the words; and the centroid of the triangle is the uniform distribution over all three words. The four points marked with an $x$ are the locations of the multinomial distributions $p(w|z)$ for each of the four topics, and the surface shown on top of the simplex is an example of a density over the $(V-1)$-simplex (multinomial distributions of words) given by LDA.\n\nWe now define the following generative process for a document **w**:\n\n1.  Choose $\\theta \\sim \\mathrm{Dir}(\\alpha)$.\n\n2.  For each of the $N$ words $w_n$:\n\n    (a)  Choose a word $w_n$ from $p(w_n \\mid \\theta, \\beta)$.\n\nThis process defines the marginal distribution of a document as a continuous mixture distribution:\n\n$p(\\mathbf{w} \\mid \\alpha, \\beta) = \\int p(\\theta \\mid \\alpha) \\left( \\prod_{n=1}^{N} p(w_n \\mid \\theta, \\beta) \\right) d\\theta,$\n\nwhere $p(w_n|\\theta, \\beta)$ are the mixture components and $p(\\theta|\\alpha)$ are the mixture weights.\n\nFigure 2 illustrates this interpretation of LDA. It depicts the distribution on $p(w|\\theta, \\beta)$ which is induced from a particular instance of an LDA model. Note that this distribution on the $(V-1)$-simplex is attained with only $k+kV$ parameters yet exhibits a very interesting multimodal structure.\n\nFigure 3: Graphical model representation of different models of discrete data."
        },
        {
            "text": "We now define the following generative process for a document **w**:\n\n1.  Choose $\\theta \\sim \\mathrm{Dir}(\\alpha)$.\n\n2.  For each of the $N$ words $w_n$:\n\n    (a)  Choose a word $w_n$ from $p(w_n \\mid \\theta, \\beta)$.",
            "page": 7,
            "x": 98,
            "y": 478,
            "width": 303,
            "height": 92,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-a",
            "chunk_id": "5f7d0454-365c-4e9a-b5bb-e62ba1b0c37d",
            "group_text": "3.2 A continuous mixture of unigrams\n\nThe LDA model shown in Figure 1 is somewhat more elaborate than the two-level models often studied in the classical hierarchical Bayesian literature. By marginalizing over the hidden topic variable z, however, we can understand LDA as a two-level model.\n\nIn particular, let us form the word distribution $p(w|\\theta, \\beta)$:\n\n$p(w|\\theta, \\beta) = \\sum_z p(w|z, \\beta) p(z|\\theta).$\n\nNote that this is a random quantity since it depends on $\\theta$.\n\nFigure 2:  An example density on unigram distributions $p(w|\\theta, \\beta)$ under LDA for three words and four topics. The triangle embedded in the x-y plane is the 2-D simplex representing all possible multinomial distributions over three words. Each of the vertices of the triangle corresponds to a deterministic distribution that assigns probability one to one of the words; the midpoint of an edge gives probability 0.5 to two of the words; and the centroid of the triangle is the uniform distribution over all three words. The four points marked with an $x$ are the locations of the multinomial distributions $p(w|z)$ for each of the four topics, and the surface shown on top of the simplex is an example of a density over the $(V-1)$-simplex (multinomial distributions of words) given by LDA.\n\nWe now define the following generative process for a document **w**:\n\n1.  Choose $\\theta \\sim \\mathrm{Dir}(\\alpha)$.\n\n2.  For each of the $N$ words $w_n$:\n\n    (a)  Choose a word $w_n$ from $p(w_n \\mid \\theta, \\beta)$.\n\nThis process defines the marginal distribution of a document as a continuous mixture distribution:\n\n$p(\\mathbf{w} \\mid \\alpha, \\beta) = \\int p(\\theta \\mid \\alpha) \\left( \\prod_{n=1}^{N} p(w_n \\mid \\theta, \\beta) \\right) d\\theta,$\n\nwhere $p(w_n|\\theta, \\beta)$ are the mixture components and $p(\\theta|\\alpha)$ are the mixture weights.\n\nFigure 2 illustrates this interpretation of LDA. It depicts the distribution on $p(w|\\theta, \\beta)$ which is induced from a particular instance of an LDA model. Note that this distribution on the $(V-1)$-simplex is attained with only $k+kV$ parameters yet exhibits a very interesting multimodal structure.\n\nFigure 3: Graphical model representation of different models of discrete data."
        },
        {
            "text": "This process defines the marginal distribution of a document as a continuous mixture distribution:",
            "page": 7,
            "x": 86,
            "y": 576,
            "width": 429,
            "height": 18,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-a",
            "chunk_id": "1ecb939c-90b7-471c-85cd-66df9f6dd38c",
            "group_text": "3.2 A continuous mixture of unigrams\n\nThe LDA model shown in Figure 1 is somewhat more elaborate than the two-level models often studied in the classical hierarchical Bayesian literature. By marginalizing over the hidden topic variable z, however, we can understand LDA as a two-level model.\n\nIn particular, let us form the word distribution $p(w|\\theta, \\beta)$:\n\n$p(w|\\theta, \\beta) = \\sum_z p(w|z, \\beta) p(z|\\theta).$\n\nNote that this is a random quantity since it depends on $\\theta$.\n\nFigure 2:  An example density on unigram distributions $p(w|\\theta, \\beta)$ under LDA for three words and four topics. The triangle embedded in the x-y plane is the 2-D simplex representing all possible multinomial distributions over three words. Each of the vertices of the triangle corresponds to a deterministic distribution that assigns probability one to one of the words; the midpoint of an edge gives probability 0.5 to two of the words; and the centroid of the triangle is the uniform distribution over all three words. The four points marked with an $x$ are the locations of the multinomial distributions $p(w|z)$ for each of the four topics, and the surface shown on top of the simplex is an example of a density over the $(V-1)$-simplex (multinomial distributions of words) given by LDA.\n\nWe now define the following generative process for a document **w**:\n\n1.  Choose $\\theta \\sim \\mathrm{Dir}(\\alpha)$.\n\n2.  For each of the $N$ words $w_n$:\n\n    (a)  Choose a word $w_n$ from $p(w_n \\mid \\theta, \\beta)$.\n\nThis process defines the marginal distribution of a document as a continuous mixture distribution:\n\n$p(\\mathbf{w} \\mid \\alpha, \\beta) = \\int p(\\theta \\mid \\alpha) \\left( \\prod_{n=1}^{N} p(w_n \\mid \\theta, \\beta) \\right) d\\theta,$\n\nwhere $p(w_n|\\theta, \\beta)$ are the mixture components and $p(\\theta|\\alpha)$ are the mixture weights.\n\nFigure 2 illustrates this interpretation of LDA. It depicts the distribution on $p(w|\\theta, \\beta)$ which is induced from a particular instance of an LDA model. Note that this distribution on the $(V-1)$-simplex is attained with only $k+kV$ parameters yet exhibits a very interesting multimodal structure.\n\nFigure 3: Graphical model representation of different models of discrete data."
        },
        {
            "text": "$p(\\mathbf{w} \\mid \\alpha, \\beta) = \\int p(\\theta \\mid \\alpha) \\left( \\prod_{n=1}^{N} p(w_n \\mid \\theta, \\beta) \\right) d\\theta,$",
            "page": 7,
            "x": 203,
            "y": 601,
            "width": 205,
            "height": 42,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-a",
            "chunk_id": "b618d2da-3cbc-4c90-aafd-236190a1627e",
            "group_text": "3.2 A continuous mixture of unigrams\n\nThe LDA model shown in Figure 1 is somewhat more elaborate than the two-level models often studied in the classical hierarchical Bayesian literature. By marginalizing over the hidden topic variable z, however, we can understand LDA as a two-level model.\n\nIn particular, let us form the word distribution $p(w|\\theta, \\beta)$:\n\n$p(w|\\theta, \\beta) = \\sum_z p(w|z, \\beta) p(z|\\theta).$\n\nNote that this is a random quantity since it depends on $\\theta$.\n\nFigure 2:  An example density on unigram distributions $p(w|\\theta, \\beta)$ under LDA for three words and four topics. The triangle embedded in the x-y plane is the 2-D simplex representing all possible multinomial distributions over three words. Each of the vertices of the triangle corresponds to a deterministic distribution that assigns probability one to one of the words; the midpoint of an edge gives probability 0.5 to two of the words; and the centroid of the triangle is the uniform distribution over all three words. The four points marked with an $x$ are the locations of the multinomial distributions $p(w|z)$ for each of the four topics, and the surface shown on top of the simplex is an example of a density over the $(V-1)$-simplex (multinomial distributions of words) given by LDA.\n\nWe now define the following generative process for a document **w**:\n\n1.  Choose $\\theta \\sim \\mathrm{Dir}(\\alpha)$.\n\n2.  For each of the $N$ words $w_n$:\n\n    (a)  Choose a word $w_n$ from $p(w_n \\mid \\theta, \\beta)$.\n\nThis process defines the marginal distribution of a document as a continuous mixture distribution:\n\n$p(\\mathbf{w} \\mid \\alpha, \\beta) = \\int p(\\theta \\mid \\alpha) \\left( \\prod_{n=1}^{N} p(w_n \\mid \\theta, \\beta) \\right) d\\theta,$\n\nwhere $p(w_n|\\theta, \\beta)$ are the mixture components and $p(\\theta|\\alpha)$ are the mixture weights.\n\nFigure 2 illustrates this interpretation of LDA. It depicts the distribution on $p(w|\\theta, \\beta)$ which is induced from a particular instance of an LDA model. Note that this distribution on the $(V-1)$-simplex is attained with only $k+kV$ parameters yet exhibits a very interesting multimodal structure.\n\nFigure 3: Graphical model representation of different models of discrete data."
        },
        {
            "text": "where $p(w_n|\\theta, \\beta)$ are the mixture components and $p(\\theta|\\alpha)$ are the mixture weights.\n\nFigure 2 illustrates this interpretation of LDA. It depicts the distribution on $p(w|\\theta, \\beta)$ which is induced from a particular instance of an LDA model. Note that this distribution on the $(V-1)$-simplex is attained with only $k+kV$ parameters yet exhibits a very interesting multimodal structure.",
            "page": 7,
            "x": 86,
            "y": 650,
            "width": 436,
            "height": 58,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-a",
            "chunk_id": "1a78a7da-69a2-4433-a7fb-06ca80e269b2",
            "group_text": "3.2 A continuous mixture of unigrams\n\nThe LDA model shown in Figure 1 is somewhat more elaborate than the two-level models often studied in the classical hierarchical Bayesian literature. By marginalizing over the hidden topic variable z, however, we can understand LDA as a two-level model.\n\nIn particular, let us form the word distribution $p(w|\\theta, \\beta)$:\n\n$p(w|\\theta, \\beta) = \\sum_z p(w|z, \\beta) p(z|\\theta).$\n\nNote that this is a random quantity since it depends on $\\theta$.\n\nFigure 2:  An example density on unigram distributions $p(w|\\theta, \\beta)$ under LDA for three words and four topics. The triangle embedded in the x-y plane is the 2-D simplex representing all possible multinomial distributions over three words. Each of the vertices of the triangle corresponds to a deterministic distribution that assigns probability one to one of the words; the midpoint of an edge gives probability 0.5 to two of the words; and the centroid of the triangle is the uniform distribution over all three words. The four points marked with an $x$ are the locations of the multinomial distributions $p(w|z)$ for each of the four topics, and the surface shown on top of the simplex is an example of a density over the $(V-1)$-simplex (multinomial distributions of words) given by LDA.\n\nWe now define the following generative process for a document **w**:\n\n1.  Choose $\\theta \\sim \\mathrm{Dir}(\\alpha)$.\n\n2.  For each of the $N$ words $w_n$:\n\n    (a)  Choose a word $w_n$ from $p(w_n \\mid \\theta, \\beta)$.\n\nThis process defines the marginal distribution of a document as a continuous mixture distribution:\n\n$p(\\mathbf{w} \\mid \\alpha, \\beta) = \\int p(\\theta \\mid \\alpha) \\left( \\prod_{n=1}^{N} p(w_n \\mid \\theta, \\beta) \\right) d\\theta,$\n\nwhere $p(w_n|\\theta, \\beta)$ are the mixture components and $p(\\theta|\\alpha)$ are the mixture weights.\n\nFigure 2 illustrates this interpretation of LDA. It depicts the distribution on $p(w|\\theta, \\beta)$ which is induced from a particular instance of an LDA model. Note that this distribution on the $(V-1)$-simplex is attained with only $k+kV$ parameters yet exhibits a very interesting multimodal structure.\n\nFigure 3: Graphical model representation of different models of discrete data."
        },
        {
            "text": "Figure 3: Graphical model representation of different models of discrete data.",
            "page": 8,
            "x": 134,
            "y": 366,
            "width": 341,
            "height": 16,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-a",
            "chunk_id": "87352a44-58fb-4212-a2ea-67863f9f2043",
            "group_text": "3.2 A continuous mixture of unigrams\n\nThe LDA model shown in Figure 1 is somewhat more elaborate than the two-level models often studied in the classical hierarchical Bayesian literature. By marginalizing over the hidden topic variable z, however, we can understand LDA as a two-level model.\n\nIn particular, let us form the word distribution $p(w|\\theta, \\beta)$:\n\n$p(w|\\theta, \\beta) = \\sum_z p(w|z, \\beta) p(z|\\theta).$\n\nNote that this is a random quantity since it depends on $\\theta$.\n\nFigure 2:  An example density on unigram distributions $p(w|\\theta, \\beta)$ under LDA for three words and four topics. The triangle embedded in the x-y plane is the 2-D simplex representing all possible multinomial distributions over three words. Each of the vertices of the triangle corresponds to a deterministic distribution that assigns probability one to one of the words; the midpoint of an edge gives probability 0.5 to two of the words; and the centroid of the triangle is the uniform distribution over all three words. The four points marked with an $x$ are the locations of the multinomial distributions $p(w|z)$ for each of the four topics, and the surface shown on top of the simplex is an example of a density over the $(V-1)$-simplex (multinomial distributions of words) given by LDA.\n\nWe now define the following generative process for a document **w**:\n\n1.  Choose $\\theta \\sim \\mathrm{Dir}(\\alpha)$.\n\n2.  For each of the $N$ words $w_n$:\n\n    (a)  Choose a word $w_n$ from $p(w_n \\mid \\theta, \\beta)$.\n\nThis process defines the marginal distribution of a document as a continuous mixture distribution:\n\n$p(\\mathbf{w} \\mid \\alpha, \\beta) = \\int p(\\theta \\mid \\alpha) \\left( \\prod_{n=1}^{N} p(w_n \\mid \\theta, \\beta) \\right) d\\theta,$\n\nwhere $p(w_n|\\theta, \\beta)$ are the mixture components and $p(\\theta|\\alpha)$ are the mixture weights.\n\nFigure 2 illustrates this interpretation of LDA. It depicts the distribution on $p(w|\\theta, \\beta)$ which is induced from a particular instance of an LDA model. Note that this distribution on the $(V-1)$-simplex is attained with only $k+kV$ parameters yet exhibits a very interesting multimodal structure.\n\nFigure 3: Graphical model representation of different models of discrete data."
        },
        {
            "text": "4. Relationship with other latent variable models\n\nIn this section we compare LDA to simpler latent variable models for text\u2014the unigram model, a mixture of unigrams, and the pLSI model. Furthermore, we present a unified geometric interpretation of these models which highlights their key differences and similarities.",
            "page": 8,
            "x": 86,
            "y": 409,
            "width": 436,
            "height": 65,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "8-relationship",
            "chunk_id": "e4225937-cbe7-41af-b844-0b2d15f35b5a",
            "group_text": "4. Relationship with other latent variable models\n\nIn this section we compare LDA to simpler latent variable models for text\u2014the unigram model, a mixture of unigrams, and the pLSI model. Furthermore, we present a unified geometric interpretation of these models which highlights their key differences and similarities."
        },
        {
            "text": "4.1 Unigram model\n\nUnder the unigram model, the words of every document are drawn independently from a single multinomial distribution:",
            "page": 8,
            "x": 87,
            "y": 486,
            "width": 434,
            "height": 48,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-unigram",
            "chunk_id": "e942ef83-cf67-413b-8a3a-6acff14a369c",
            "group_text": "4.1 Unigram model\n\nUnder the unigram model, the words of every document are drawn independently from a single multinomial distribution:\n\n$p(\\mathbf{w}) = \\prod_{n=1}^{N} p(w_n).$\n\nThis is illustrated in the graphical model in Figure 3a."
        },
        {
            "text": "$p(\\mathbf{w}) = \\prod_{n=1}^{N} p(w_n).$",
            "page": 8,
            "x": 263,
            "y": 530,
            "width": 84,
            "height": 33,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-unigram",
            "chunk_id": "0b5a53f9-6d8d-421f-b086-663d3aa0d01d",
            "group_text": "4.1 Unigram model\n\nUnder the unigram model, the words of every document are drawn independently from a single multinomial distribution:\n\n$p(\\mathbf{w}) = \\prod_{n=1}^{N} p(w_n).$\n\nThis is illustrated in the graphical model in Figure 3a."
        },
        {
            "text": "This is illustrated in the graphical model in Figure 3a.",
            "page": 8,
            "x": 88,
            "y": 566,
            "width": 237,
            "height": 15,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-unigram",
            "chunk_id": "85b17569-b2b9-4c9d-b3b6-3cf2d3a2df25",
            "group_text": "4.1 Unigram model\n\nUnder the unigram model, the words of every document are drawn independently from a single multinomial distribution:\n\n$p(\\mathbf{w}) = \\prod_{n=1}^{N} p(w_n).$\n\nThis is illustrated in the graphical model in Figure 3a."
        },
        {
            "text": "4.2 Mixture of unigrams\n\nIf we augment the unigram model with a discrete random topic variable $z$ (Figure 3b), we obtain a *mixture of unigrams* model (Nigam et al., 2000). Under this mixture model, each document is generated by first choosing a topic $z$ and then generating $N$ words independently from the conditional multinomial $p(w|z)$. The probability of a document is:",
            "page": 8,
            "x": 87,
            "y": 593,
            "width": 435,
            "height": 75,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "10-mixture",
            "chunk_id": "8fc8638a-03b1-41e1-9140-56023edbce3d",
            "group_text": "4.2 Mixture of unigrams\n\nIf we augment the unigram model with a discrete random topic variable $z$ (Figure 3b), we obtain a *mixture of unigrams* model (Nigam et al., 2000). Under this mixture model, each document is generated by first choosing a topic $z$ and then generating $N$ words independently from the conditional multinomial $p(w|z)$. The probability of a document is:\n\n$p(\\mathbf{w}) = \\sum_z p(z) \\prod_{n=1}^N p(w_n \\mid z).$\n\nWhen estimated from a corpus, the word distributions can be viewed as representations of topics\nunder the assumption that each document exhibits exactly one topic. As the empirical results in\nSection 7 illustrate, this assumption is often too limiting to effectively model a large collection of\ndocuments.\n\nIn contrast, the LDA model allows documents to exhibit multiple topics to different degrees. This is achieved at a cost of just one additional parameter: there are $k-1$ parameters associated with $p(z)$ in the mixture of unigrams, versus the $k$ parameters associated with $p(\\theta \\mid \\alpha)$ in LDA."
        },
        {
            "text": "$p(\\mathbf{w}) = \\sum_z p(z) \\prod_{n=1}^N p(w_n \\mid z).$",
            "page": 8,
            "x": 242,
            "y": 675,
            "width": 126,
            "height": 35,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "10-mixture",
            "chunk_id": "62df7f2e-872a-483f-80ba-cadd918f6788",
            "group_text": "4.2 Mixture of unigrams\n\nIf we augment the unigram model with a discrete random topic variable $z$ (Figure 3b), we obtain a *mixture of unigrams* model (Nigam et al., 2000). Under this mixture model, each document is generated by first choosing a topic $z$ and then generating $N$ words independently from the conditional multinomial $p(w|z)$. The probability of a document is:\n\n$p(\\mathbf{w}) = \\sum_z p(z) \\prod_{n=1}^N p(w_n \\mid z).$\n\nWhen estimated from a corpus, the word distributions can be viewed as representations of topics\nunder the assumption that each document exhibits exactly one topic. As the empirical results in\nSection 7 illustrate, this assumption is often too limiting to effectively model a large collection of\ndocuments.\n\nIn contrast, the LDA model allows documents to exhibit multiple topics to different degrees. This is achieved at a cost of just one additional parameter: there are $k-1$ parameters associated with $p(z)$ in the mixture of unigrams, versus the $k$ parameters associated with $p(\\theta \\mid \\alpha)$ in LDA."
        },
        {
            "text": "When estimated from a corpus, the word distributions can be viewed as representations of topics\nunder the assumption that each document exhibits exactly one topic. As the empirical results in\nSection 7 illustrate, this assumption is often too limiting to effectively model a large collection of\ndocuments.",
            "page": 9,
            "x": 86,
            "y": 89,
            "width": 438,
            "height": 56,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "10-mixture",
            "chunk_id": "9f65a33c-7a96-4071-bff0-96ec6bbcb034",
            "group_text": "4.2 Mixture of unigrams\n\nIf we augment the unigram model with a discrete random topic variable $z$ (Figure 3b), we obtain a *mixture of unigrams* model (Nigam et al., 2000). Under this mixture model, each document is generated by first choosing a topic $z$ and then generating $N$ words independently from the conditional multinomial $p(w|z)$. The probability of a document is:\n\n$p(\\mathbf{w}) = \\sum_z p(z) \\prod_{n=1}^N p(w_n \\mid z).$\n\nWhen estimated from a corpus, the word distributions can be viewed as representations of topics\nunder the assumption that each document exhibits exactly one topic. As the empirical results in\nSection 7 illustrate, this assumption is often too limiting to effectively model a large collection of\ndocuments.\n\nIn contrast, the LDA model allows documents to exhibit multiple topics to different degrees. This is achieved at a cost of just one additional parameter: there are $k-1$ parameters associated with $p(z)$ in the mixture of unigrams, versus the $k$ parameters associated with $p(\\theta \\mid \\alpha)$ in LDA."
        },
        {
            "text": "In contrast, the LDA model allows documents to exhibit multiple topics to different degrees. This is achieved at a cost of just one additional parameter: there are $k-1$ parameters associated with $p(z)$ in the mixture of unigrams, versus the $k$ parameters associated with $p(\\theta \\mid \\alpha)$ in LDA.",
            "page": 9,
            "x": 87,
            "y": 146,
            "width": 436,
            "height": 43,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "10-mixture",
            "chunk_id": "35a8c1ed-c6b8-44eb-8393-a8c74212e8bf",
            "group_text": "4.2 Mixture of unigrams\n\nIf we augment the unigram model with a discrete random topic variable $z$ (Figure 3b), we obtain a *mixture of unigrams* model (Nigam et al., 2000). Under this mixture model, each document is generated by first choosing a topic $z$ and then generating $N$ words independently from the conditional multinomial $p(w|z)$. The probability of a document is:\n\n$p(\\mathbf{w}) = \\sum_z p(z) \\prod_{n=1}^N p(w_n \\mid z).$\n\nWhen estimated from a corpus, the word distributions can be viewed as representations of topics\nunder the assumption that each document exhibits exactly one topic. As the empirical results in\nSection 7 illustrate, this assumption is often too limiting to effectively model a large collection of\ndocuments.\n\nIn contrast, the LDA model allows documents to exhibit multiple topics to different degrees. This is achieved at a cost of just one additional parameter: there are $k-1$ parameters associated with $p(z)$ in the mixture of unigrams, versus the $k$ parameters associated with $p(\\theta \\mid \\alpha)$ in LDA."
        },
        {
            "text": "4.3 Probabilistic latent semantic indexing\n\nProbabilistic latent semantic indexing (pLSI) is another widely used document model (Hofmann, 1999). The pLSI model, illustrated in Figure 3c, posits that a document label $d$ and a word $w_n$ are conditionally independent given an unobserved topic $z$:",
            "page": 9,
            "x": 87,
            "y": 201,
            "width": 436,
            "height": 63,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-probabilistic",
            "chunk_id": "a43ed139-ba72-4bc8-a2d5-7b53e0ff5ece",
            "group_text": "4.3 Probabilistic latent semantic indexing\n\nProbabilistic latent semantic indexing (pLSI) is another widely used document model (Hofmann, 1999). The pLSI model, illustrated in Figure 3c, posits that a document label $d$ and a word $w_n$ are conditionally independent given an unobserved topic $z$:\n\n$p(d, w_n) = p(d) \\sum_z p(w_n \\mid z) p(z \\mid d).$\n\nThe pLSI model attempts to relax the simplifying assumption made in the mixture of unigrams model that each document is generated from only one topic. In a sense, it does capture the possibility that a document may contain multiple topics since $p(z|d)$ serves as the mixture weights of the topics for a particular document $d$. However, it is important to note that $d$ is a dummy index into the list of documents in the *training set*. Thus, $d$ is a multinomial random variable with as many possible values as there are training documents and the model learns the topic mixtures $p(z|d)$ only for those documents on which it is trained. For this reason, pLSI is not a well-defined generative model of documents; there is no natural way to use it to assign probability to a previously unseen document.\n\nA further difficulty with pLSI, which also stems from the use of a distribution indexed by training documents, is that the number of parameters which must be estimated grows linearly with the number of training documents. The parameters for a $k$-topic pLSI model are $k$ multinomial distributions of size $V$ and $M$ mixtures over the $k$ hidden topics. This gives $kV + kM$ parameters and therefore linear growth in $M$. The linear growth in parameters suggests that the model is prone to overfitting and, empirically, overfitting is indeed a serious problem (see Section 7.1). In practice, a tempering heuristic is used to smooth the parameters of the model for acceptable predictive performance. It has been shown, however, that overfitting can occur even when tempering is used (Popescul et al., 2001).\n\nLDA overcomes both of these problems by treating the topic mixture weights as a $k$-parameter hidden *random variable* rather than a large set of individual parameters which are explicitly linked to the training set. As described in Section 3, LDA is a well-defined generative model and generalizes easily to new documents. Furthermore, the $k + kV$ parameters in a $k$-topic LDA model do not grow with the size of the training corpus. We will see in Section 7.1 that LDA does not suffer from the same overfitting issues as pLSI."
        },
        {
            "text": "$p(d, w_n) = p(d) \\sum_z p(w_n \\mid z) p(z \\mid d).$",
            "page": 9,
            "x": 226,
            "y": 274,
            "width": 158,
            "height": 27,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-probabilistic",
            "chunk_id": "4aa54708-6b47-45b5-9a6d-08549ba65070",
            "group_text": "4.3 Probabilistic latent semantic indexing\n\nProbabilistic latent semantic indexing (pLSI) is another widely used document model (Hofmann, 1999). The pLSI model, illustrated in Figure 3c, posits that a document label $d$ and a word $w_n$ are conditionally independent given an unobserved topic $z$:\n\n$p(d, w_n) = p(d) \\sum_z p(w_n \\mid z) p(z \\mid d).$\n\nThe pLSI model attempts to relax the simplifying assumption made in the mixture of unigrams model that each document is generated from only one topic. In a sense, it does capture the possibility that a document may contain multiple topics since $p(z|d)$ serves as the mixture weights of the topics for a particular document $d$. However, it is important to note that $d$ is a dummy index into the list of documents in the *training set*. Thus, $d$ is a multinomial random variable with as many possible values as there are training documents and the model learns the topic mixtures $p(z|d)$ only for those documents on which it is trained. For this reason, pLSI is not a well-defined generative model of documents; there is no natural way to use it to assign probability to a previously unseen document.\n\nA further difficulty with pLSI, which also stems from the use of a distribution indexed by training documents, is that the number of parameters which must be estimated grows linearly with the number of training documents. The parameters for a $k$-topic pLSI model are $k$ multinomial distributions of size $V$ and $M$ mixtures over the $k$ hidden topics. This gives $kV + kM$ parameters and therefore linear growth in $M$. The linear growth in parameters suggests that the model is prone to overfitting and, empirically, overfitting is indeed a serious problem (see Section 7.1). In practice, a tempering heuristic is used to smooth the parameters of the model for acceptable predictive performance. It has been shown, however, that overfitting can occur even when tempering is used (Popescul et al., 2001).\n\nLDA overcomes both of these problems by treating the topic mixture weights as a $k$-parameter hidden *random variable* rather than a large set of individual parameters which are explicitly linked to the training set. As described in Section 3, LDA is a well-defined generative model and generalizes easily to new documents. Furthermore, the $k + kV$ parameters in a $k$-topic LDA model do not grow with the size of the training corpus. We will see in Section 7.1 that LDA does not suffer from the same overfitting issues as pLSI."
        },
        {
            "text": "The pLSI model attempts to relax the simplifying assumption made in the mixture of unigrams model that each document is generated from only one topic. In a sense, it does capture the possibility that a document may contain multiple topics since $p(z|d)$ serves as the mixture weights of the topics for a particular document $d$. However, it is important to note that $d$ is a dummy index into the list of documents in the *training set*. Thus, $d$ is a multinomial random variable with as many possible values as there are training documents and the model learns the topic mixtures $p(z|d)$ only for those documents on which it is trained. For this reason, pLSI is not a well-defined generative model of documents; there is no natural way to use it to assign probability to a previously unseen document.",
            "page": 9,
            "x": 87,
            "y": 315,
            "width": 436,
            "height": 109,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-probabilistic",
            "chunk_id": "f727650f-8791-4fb4-a5fc-0d0b115d50a4",
            "group_text": "4.3 Probabilistic latent semantic indexing\n\nProbabilistic latent semantic indexing (pLSI) is another widely used document model (Hofmann, 1999). The pLSI model, illustrated in Figure 3c, posits that a document label $d$ and a word $w_n$ are conditionally independent given an unobserved topic $z$:\n\n$p(d, w_n) = p(d) \\sum_z p(w_n \\mid z) p(z \\mid d).$\n\nThe pLSI model attempts to relax the simplifying assumption made in the mixture of unigrams model that each document is generated from only one topic. In a sense, it does capture the possibility that a document may contain multiple topics since $p(z|d)$ serves as the mixture weights of the topics for a particular document $d$. However, it is important to note that $d$ is a dummy index into the list of documents in the *training set*. Thus, $d$ is a multinomial random variable with as many possible values as there are training documents and the model learns the topic mixtures $p(z|d)$ only for those documents on which it is trained. For this reason, pLSI is not a well-defined generative model of documents; there is no natural way to use it to assign probability to a previously unseen document.\n\nA further difficulty with pLSI, which also stems from the use of a distribution indexed by training documents, is that the number of parameters which must be estimated grows linearly with the number of training documents. The parameters for a $k$-topic pLSI model are $k$ multinomial distributions of size $V$ and $M$ mixtures over the $k$ hidden topics. This gives $kV + kM$ parameters and therefore linear growth in $M$. The linear growth in parameters suggests that the model is prone to overfitting and, empirically, overfitting is indeed a serious problem (see Section 7.1). In practice, a tempering heuristic is used to smooth the parameters of the model for acceptable predictive performance. It has been shown, however, that overfitting can occur even when tempering is used (Popescul et al., 2001).\n\nLDA overcomes both of these problems by treating the topic mixture weights as a $k$-parameter hidden *random variable* rather than a large set of individual parameters which are explicitly linked to the training set. As described in Section 3, LDA is a well-defined generative model and generalizes easily to new documents. Furthermore, the $k + kV$ parameters in a $k$-topic LDA model do not grow with the size of the training corpus. We will see in Section 7.1 that LDA does not suffer from the same overfitting issues as pLSI."
        },
        {
            "text": "A further difficulty with pLSI, which also stems from the use of a distribution indexed by training documents, is that the number of parameters which must be estimated grows linearly with the number of training documents. The parameters for a $k$-topic pLSI model are $k$ multinomial distributions of size $V$ and $M$ mixtures over the $k$ hidden topics. This gives $kV + kM$ parameters and therefore linear growth in $M$. The linear growth in parameters suggests that the model is prone to overfitting and, empirically, overfitting is indeed a serious problem (see Section 7.1). In practice, a tempering heuristic is used to smooth the parameters of the model for acceptable predictive performance. It has been shown, however, that overfitting can occur even when tempering is used (Popescul et al., 2001).",
            "page": 9,
            "x": 87,
            "y": 425,
            "width": 435,
            "height": 121,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-probabilistic",
            "chunk_id": "3bf41104-8425-4ea2-920a-92523896c3e3",
            "group_text": "4.3 Probabilistic latent semantic indexing\n\nProbabilistic latent semantic indexing (pLSI) is another widely used document model (Hofmann, 1999). The pLSI model, illustrated in Figure 3c, posits that a document label $d$ and a word $w_n$ are conditionally independent given an unobserved topic $z$:\n\n$p(d, w_n) = p(d) \\sum_z p(w_n \\mid z) p(z \\mid d).$\n\nThe pLSI model attempts to relax the simplifying assumption made in the mixture of unigrams model that each document is generated from only one topic. In a sense, it does capture the possibility that a document may contain multiple topics since $p(z|d)$ serves as the mixture weights of the topics for a particular document $d$. However, it is important to note that $d$ is a dummy index into the list of documents in the *training set*. Thus, $d$ is a multinomial random variable with as many possible values as there are training documents and the model learns the topic mixtures $p(z|d)$ only for those documents on which it is trained. For this reason, pLSI is not a well-defined generative model of documents; there is no natural way to use it to assign probability to a previously unseen document.\n\nA further difficulty with pLSI, which also stems from the use of a distribution indexed by training documents, is that the number of parameters which must be estimated grows linearly with the number of training documents. The parameters for a $k$-topic pLSI model are $k$ multinomial distributions of size $V$ and $M$ mixtures over the $k$ hidden topics. This gives $kV + kM$ parameters and therefore linear growth in $M$. The linear growth in parameters suggests that the model is prone to overfitting and, empirically, overfitting is indeed a serious problem (see Section 7.1). In practice, a tempering heuristic is used to smooth the parameters of the model for acceptable predictive performance. It has been shown, however, that overfitting can occur even when tempering is used (Popescul et al., 2001).\n\nLDA overcomes both of these problems by treating the topic mixture weights as a $k$-parameter hidden *random variable* rather than a large set of individual parameters which are explicitly linked to the training set. As described in Section 3, LDA is a well-defined generative model and generalizes easily to new documents. Furthermore, the $k + kV$ parameters in a $k$-topic LDA model do not grow with the size of the training corpus. We will see in Section 7.1 that LDA does not suffer from the same overfitting issues as pLSI."
        },
        {
            "text": "LDA overcomes both of these problems by treating the topic mixture weights as a $k$-parameter hidden *random variable* rather than a large set of individual parameters which are explicitly linked to the training set. As described in Section 3, LDA is a well-defined generative model and generalizes easily to new documents. Furthermore, the $k + kV$ parameters in a $k$-topic LDA model do not grow with the size of the training corpus. We will see in Section 7.1 that LDA does not suffer from the same overfitting issues as pLSI.",
            "page": 9,
            "x": 88,
            "y": 548,
            "width": 434,
            "height": 80,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-probabilistic",
            "chunk_id": "be0bb532-39c0-4a9a-b98f-9a3ee672a5b6",
            "group_text": "4.3 Probabilistic latent semantic indexing\n\nProbabilistic latent semantic indexing (pLSI) is another widely used document model (Hofmann, 1999). The pLSI model, illustrated in Figure 3c, posits that a document label $d$ and a word $w_n$ are conditionally independent given an unobserved topic $z$:\n\n$p(d, w_n) = p(d) \\sum_z p(w_n \\mid z) p(z \\mid d).$\n\nThe pLSI model attempts to relax the simplifying assumption made in the mixture of unigrams model that each document is generated from only one topic. In a sense, it does capture the possibility that a document may contain multiple topics since $p(z|d)$ serves as the mixture weights of the topics for a particular document $d$. However, it is important to note that $d$ is a dummy index into the list of documents in the *training set*. Thus, $d$ is a multinomial random variable with as many possible values as there are training documents and the model learns the topic mixtures $p(z|d)$ only for those documents on which it is trained. For this reason, pLSI is not a well-defined generative model of documents; there is no natural way to use it to assign probability to a previously unseen document.\n\nA further difficulty with pLSI, which also stems from the use of a distribution indexed by training documents, is that the number of parameters which must be estimated grows linearly with the number of training documents. The parameters for a $k$-topic pLSI model are $k$ multinomial distributions of size $V$ and $M$ mixtures over the $k$ hidden topics. This gives $kV + kM$ parameters and therefore linear growth in $M$. The linear growth in parameters suggests that the model is prone to overfitting and, empirically, overfitting is indeed a serious problem (see Section 7.1). In practice, a tempering heuristic is used to smooth the parameters of the model for acceptable predictive performance. It has been shown, however, that overfitting can occur even when tempering is used (Popescul et al., 2001).\n\nLDA overcomes both of these problems by treating the topic mixture weights as a $k$-parameter hidden *random variable* rather than a large set of individual parameters which are explicitly linked to the training set. As described in Section 3, LDA is a well-defined generative model and generalizes easily to new documents. Furthermore, the $k + kV$ parameters in a $k$-topic LDA model do not grow with the size of the training corpus. We will see in Section 7.1 that LDA does not suffer from the same overfitting issues as pLSI."
        },
        {
            "text": "4.4 A geometric interpretation\n\nA good way of illustrating the differences between LDA and the other latent topic models is by considering the geometry of the latent space, and seeing how a document is represented in that geometry under each model.",
            "page": 9,
            "x": 87,
            "y": 644,
            "width": 434,
            "height": 62,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-a",
            "chunk_id": "1bc71bb3-b1a6-4bc9-a01b-9948ad32ba22",
            "group_text": "4.4 A geometric interpretation\n\nA good way of illustrating the differences between LDA and the other latent topic models is by considering the geometry of the latent space, and seeing how a document is represented in that geometry under each model.\n\nFigure 4: The topic simplex for three topics embedded in the word simplex for three words. The corners of the word simplex correspond to the three distributions where each word (respectively) has probability one. The three points of the topic simplex correspond to three different distributions over words. The mixture of unigrams places each document at one of the corners of the topic simplex. The pLSI model induces an empirical distribution on the topic simplex denoted by x. LDA places a smooth distribution on the topic simplex denoted by the contour lines.\n\nAll four of the models described above\u2014unigram, mixture of unigrams, pLSI, and LDA\u2014operate in the space of distributions over words. Each such distribution can be viewed as a point on the $(V-1)$-simplex, which we call the word simplex.\n\nThe unigram model finds a single point on the word simplex and posits that all words in the corpus come from the corresponding distribution. The latent variable models consider *k* points on the word simplex and form a sub-simplex based on those points, which we call the topic simplex. Note that any point on the topic simplex is also a point on the word simplex. The different latent variable models use the topic simplex in different ways to generate a document.\n\n- The mixture of unigrams model posits that for each document, one of the $k$ points on the word simplex (that is, one of the corners of the topic simplex) is chosen randomly and all the words of the document are drawn from the distribution corresponding to that point.\n\n- The pLSI model posits that each word of a *training* document comes from a randomly chosen topic. The topics are themselves drawn from a document-specific distribution over topics, i.e., a point on the topic simplex. There is one such distribution for each document; the set of training documents thus defines an empirical distribution on the topic simplex.\n\n- \u2022 LDA posits that each word of both the observed and unseen documents is generated by a randomly chosen topic which is drawn from a distribution with a randomly chosen parameter. This parameter is sampled once per document from a smooth distribution on the topic simplex.\n\nThese differences are highlighted in Figure 4.\n\n**5. Inference and Parameter Estimation**\n\nWe have described the motivation behind LDA and illustrated its conceptual advantages over other latent topic models. In this section, we turn our attention to procedures for inference and parameter estimation under LDA."
        },
        {
            "text": "Figure 4: The topic simplex for three topics embedded in the word simplex for three words. The corners of the word simplex correspond to the three distributions where each word (respectively) has probability one. The three points of the topic simplex correspond to three different distributions over words. The mixture of unigrams places each document at one of the corners of the topic simplex. The pLSI model induces an empirical distribution on the topic simplex denoted by x. LDA places a smooth distribution on the topic simplex denoted by the contour lines.",
            "page": 10,
            "x": 85,
            "y": 472,
            "width": 438,
            "height": 97,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-a",
            "chunk_id": "765346bc-0d61-4d06-84d2-d281871a5719",
            "group_text": "4.4 A geometric interpretation\n\nA good way of illustrating the differences between LDA and the other latent topic models is by considering the geometry of the latent space, and seeing how a document is represented in that geometry under each model.\n\nFigure 4: The topic simplex for three topics embedded in the word simplex for three words. The corners of the word simplex correspond to the three distributions where each word (respectively) has probability one. The three points of the topic simplex correspond to three different distributions over words. The mixture of unigrams places each document at one of the corners of the topic simplex. The pLSI model induces an empirical distribution on the topic simplex denoted by x. LDA places a smooth distribution on the topic simplex denoted by the contour lines.\n\nAll four of the models described above\u2014unigram, mixture of unigrams, pLSI, and LDA\u2014operate in the space of distributions over words. Each such distribution can be viewed as a point on the $(V-1)$-simplex, which we call the word simplex.\n\nThe unigram model finds a single point on the word simplex and posits that all words in the corpus come from the corresponding distribution. The latent variable models consider *k* points on the word simplex and form a sub-simplex based on those points, which we call the topic simplex. Note that any point on the topic simplex is also a point on the word simplex. The different latent variable models use the topic simplex in different ways to generate a document.\n\n- The mixture of unigrams model posits that for each document, one of the $k$ points on the word simplex (that is, one of the corners of the topic simplex) is chosen randomly and all the words of the document are drawn from the distribution corresponding to that point.\n\n- The pLSI model posits that each word of a *training* document comes from a randomly chosen topic. The topics are themselves drawn from a document-specific distribution over topics, i.e., a point on the topic simplex. There is one such distribution for each document; the set of training documents thus defines an empirical distribution on the topic simplex.\n\n- \u2022 LDA posits that each word of both the observed and unseen documents is generated by a randomly chosen topic which is drawn from a distribution with a randomly chosen parameter. This parameter is sampled once per document from a smooth distribution on the topic simplex.\n\nThese differences are highlighted in Figure 4.\n\n**5. Inference and Parameter Estimation**\n\nWe have described the motivation behind LDA and illustrated its conceptual advantages over other latent topic models. In this section, we turn our attention to procedures for inference and parameter estimation under LDA."
        },
        {
            "text": "All four of the models described above\u2014unigram, mixture of unigrams, pLSI, and LDA\u2014operate in the space of distributions over words. Each such distribution can be viewed as a point on the $(V-1)$-simplex, which we call the word simplex.",
            "page": 11,
            "x": 87,
            "y": 276,
            "width": 437,
            "height": 42,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-a",
            "chunk_id": "62afe89f-0d28-4284-85de-c8e95c81bd03",
            "group_text": "4.4 A geometric interpretation\n\nA good way of illustrating the differences between LDA and the other latent topic models is by considering the geometry of the latent space, and seeing how a document is represented in that geometry under each model.\n\nFigure 4: The topic simplex for three topics embedded in the word simplex for three words. The corners of the word simplex correspond to the three distributions where each word (respectively) has probability one. The three points of the topic simplex correspond to three different distributions over words. The mixture of unigrams places each document at one of the corners of the topic simplex. The pLSI model induces an empirical distribution on the topic simplex denoted by x. LDA places a smooth distribution on the topic simplex denoted by the contour lines.\n\nAll four of the models described above\u2014unigram, mixture of unigrams, pLSI, and LDA\u2014operate in the space of distributions over words. Each such distribution can be viewed as a point on the $(V-1)$-simplex, which we call the word simplex.\n\nThe unigram model finds a single point on the word simplex and posits that all words in the corpus come from the corresponding distribution. The latent variable models consider *k* points on the word simplex and form a sub-simplex based on those points, which we call the topic simplex. Note that any point on the topic simplex is also a point on the word simplex. The different latent variable models use the topic simplex in different ways to generate a document.\n\n- The mixture of unigrams model posits that for each document, one of the $k$ points on the word simplex (that is, one of the corners of the topic simplex) is chosen randomly and all the words of the document are drawn from the distribution corresponding to that point.\n\n- The pLSI model posits that each word of a *training* document comes from a randomly chosen topic. The topics are themselves drawn from a document-specific distribution over topics, i.e., a point on the topic simplex. There is one such distribution for each document; the set of training documents thus defines an empirical distribution on the topic simplex.\n\n- \u2022 LDA posits that each word of both the observed and unseen documents is generated by a randomly chosen topic which is drawn from a distribution with a randomly chosen parameter. This parameter is sampled once per document from a smooth distribution on the topic simplex.\n\nThese differences are highlighted in Figure 4.\n\n**5. Inference and Parameter Estimation**\n\nWe have described the motivation behind LDA and illustrated its conceptual advantages over other latent topic models. In this section, we turn our attention to procedures for inference and parameter estimation under LDA."
        },
        {
            "text": "The unigram model finds a single point on the word simplex and posits that all words in the corpus come from the corresponding distribution. The latent variable models consider *k* points on the word simplex and form a sub-simplex based on those points, which we call the topic simplex. Note that any point on the topic simplex is also a point on the word simplex. The different latent variable models use the topic simplex in different ways to generate a document.",
            "page": 11,
            "x": 87,
            "y": 321,
            "width": 435,
            "height": 68,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-a",
            "chunk_id": "04c96623-547d-4706-b333-340e96f37f56",
            "group_text": "4.4 A geometric interpretation\n\nA good way of illustrating the differences between LDA and the other latent topic models is by considering the geometry of the latent space, and seeing how a document is represented in that geometry under each model.\n\nFigure 4: The topic simplex for three topics embedded in the word simplex for three words. The corners of the word simplex correspond to the three distributions where each word (respectively) has probability one. The three points of the topic simplex correspond to three different distributions over words. The mixture of unigrams places each document at one of the corners of the topic simplex. The pLSI model induces an empirical distribution on the topic simplex denoted by x. LDA places a smooth distribution on the topic simplex denoted by the contour lines.\n\nAll four of the models described above\u2014unigram, mixture of unigrams, pLSI, and LDA\u2014operate in the space of distributions over words. Each such distribution can be viewed as a point on the $(V-1)$-simplex, which we call the word simplex.\n\nThe unigram model finds a single point on the word simplex and posits that all words in the corpus come from the corresponding distribution. The latent variable models consider *k* points on the word simplex and form a sub-simplex based on those points, which we call the topic simplex. Note that any point on the topic simplex is also a point on the word simplex. The different latent variable models use the topic simplex in different ways to generate a document.\n\n- The mixture of unigrams model posits that for each document, one of the $k$ points on the word simplex (that is, one of the corners of the topic simplex) is chosen randomly and all the words of the document are drawn from the distribution corresponding to that point.\n\n- The pLSI model posits that each word of a *training* document comes from a randomly chosen topic. The topics are themselves drawn from a document-specific distribution over topics, i.e., a point on the topic simplex. There is one such distribution for each document; the set of training documents thus defines an empirical distribution on the topic simplex.\n\n- \u2022 LDA posits that each word of both the observed and unseen documents is generated by a randomly chosen topic which is drawn from a distribution with a randomly chosen parameter. This parameter is sampled once per document from a smooth distribution on the topic simplex.\n\nThese differences are highlighted in Figure 4.\n\n**5. Inference and Parameter Estimation**\n\nWe have described the motivation behind LDA and illustrated its conceptual advantages over other latent topic models. In this section, we turn our attention to procedures for inference and parameter estimation under LDA."
        },
        {
            "text": "- The mixture of unigrams model posits that for each document, one of the $k$ points on the word simplex (that is, one of the corners of the topic simplex) is chosen randomly and all the words of the document are drawn from the distribution corresponding to that point.",
            "page": 11,
            "x": 104,
            "y": 408,
            "width": 418,
            "height": 41,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-a",
            "chunk_id": "422bc730-e748-4c12-a849-55cd5a92a72a",
            "group_text": "4.4 A geometric interpretation\n\nA good way of illustrating the differences between LDA and the other latent topic models is by considering the geometry of the latent space, and seeing how a document is represented in that geometry under each model.\n\nFigure 4: The topic simplex for three topics embedded in the word simplex for three words. The corners of the word simplex correspond to the three distributions where each word (respectively) has probability one. The three points of the topic simplex correspond to three different distributions over words. The mixture of unigrams places each document at one of the corners of the topic simplex. The pLSI model induces an empirical distribution on the topic simplex denoted by x. LDA places a smooth distribution on the topic simplex denoted by the contour lines.\n\nAll four of the models described above\u2014unigram, mixture of unigrams, pLSI, and LDA\u2014operate in the space of distributions over words. Each such distribution can be viewed as a point on the $(V-1)$-simplex, which we call the word simplex.\n\nThe unigram model finds a single point on the word simplex and posits that all words in the corpus come from the corresponding distribution. The latent variable models consider *k* points on the word simplex and form a sub-simplex based on those points, which we call the topic simplex. Note that any point on the topic simplex is also a point on the word simplex. The different latent variable models use the topic simplex in different ways to generate a document.\n\n- The mixture of unigrams model posits that for each document, one of the $k$ points on the word simplex (that is, one of the corners of the topic simplex) is chosen randomly and all the words of the document are drawn from the distribution corresponding to that point.\n\n- The pLSI model posits that each word of a *training* document comes from a randomly chosen topic. The topics are themselves drawn from a document-specific distribution over topics, i.e., a point on the topic simplex. There is one such distribution for each document; the set of training documents thus defines an empirical distribution on the topic simplex.\n\n- \u2022 LDA posits that each word of both the observed and unseen documents is generated by a randomly chosen topic which is drawn from a distribution with a randomly chosen parameter. This parameter is sampled once per document from a smooth distribution on the topic simplex.\n\nThese differences are highlighted in Figure 4.\n\n**5. Inference and Parameter Estimation**\n\nWe have described the motivation behind LDA and illustrated its conceptual advantages over other latent topic models. In this section, we turn our attention to procedures for inference and parameter estimation under LDA."
        },
        {
            "text": "- The pLSI model posits that each word of a *training* document comes from a randomly chosen topic. The topics are themselves drawn from a document-specific distribution over topics, i.e., a point on the topic simplex. There is one such distribution for each document; the set of training documents thus defines an empirical distribution on the topic simplex.",
            "page": 11,
            "x": 104,
            "y": 470,
            "width": 418,
            "height": 54,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-a",
            "chunk_id": "4a87b820-22f6-4733-b113-797b8587ca48",
            "group_text": "4.4 A geometric interpretation\n\nA good way of illustrating the differences between LDA and the other latent topic models is by considering the geometry of the latent space, and seeing how a document is represented in that geometry under each model.\n\nFigure 4: The topic simplex for three topics embedded in the word simplex for three words. The corners of the word simplex correspond to the three distributions where each word (respectively) has probability one. The three points of the topic simplex correspond to three different distributions over words. The mixture of unigrams places each document at one of the corners of the topic simplex. The pLSI model induces an empirical distribution on the topic simplex denoted by x. LDA places a smooth distribution on the topic simplex denoted by the contour lines.\n\nAll four of the models described above\u2014unigram, mixture of unigrams, pLSI, and LDA\u2014operate in the space of distributions over words. Each such distribution can be viewed as a point on the $(V-1)$-simplex, which we call the word simplex.\n\nThe unigram model finds a single point on the word simplex and posits that all words in the corpus come from the corresponding distribution. The latent variable models consider *k* points on the word simplex and form a sub-simplex based on those points, which we call the topic simplex. Note that any point on the topic simplex is also a point on the word simplex. The different latent variable models use the topic simplex in different ways to generate a document.\n\n- The mixture of unigrams model posits that for each document, one of the $k$ points on the word simplex (that is, one of the corners of the topic simplex) is chosen randomly and all the words of the document are drawn from the distribution corresponding to that point.\n\n- The pLSI model posits that each word of a *training* document comes from a randomly chosen topic. The topics are themselves drawn from a document-specific distribution over topics, i.e., a point on the topic simplex. There is one such distribution for each document; the set of training documents thus defines an empirical distribution on the topic simplex.\n\n- \u2022 LDA posits that each word of both the observed and unseen documents is generated by a randomly chosen topic which is drawn from a distribution with a randomly chosen parameter. This parameter is sampled once per document from a smooth distribution on the topic simplex.\n\nThese differences are highlighted in Figure 4.\n\n**5. Inference and Parameter Estimation**\n\nWe have described the motivation behind LDA and illustrated its conceptual advantages over other latent topic models. In this section, we turn our attention to procedures for inference and parameter estimation under LDA."
        },
        {
            "text": "- \u2022 LDA posits that each word of both the observed and unseen documents is generated by a randomly chosen topic which is drawn from a distribution with a randomly chosen parameter. This parameter is sampled once per document from a smooth distribution on the topic simplex.",
            "page": 11,
            "x": 105,
            "y": 546,
            "width": 417,
            "height": 41,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-a",
            "chunk_id": "64c3b8a1-825e-4e1a-b828-b756958c31d5",
            "group_text": "4.4 A geometric interpretation\n\nA good way of illustrating the differences between LDA and the other latent topic models is by considering the geometry of the latent space, and seeing how a document is represented in that geometry under each model.\n\nFigure 4: The topic simplex for three topics embedded in the word simplex for three words. The corners of the word simplex correspond to the three distributions where each word (respectively) has probability one. The three points of the topic simplex correspond to three different distributions over words. The mixture of unigrams places each document at one of the corners of the topic simplex. The pLSI model induces an empirical distribution on the topic simplex denoted by x. LDA places a smooth distribution on the topic simplex denoted by the contour lines.\n\nAll four of the models described above\u2014unigram, mixture of unigrams, pLSI, and LDA\u2014operate in the space of distributions over words. Each such distribution can be viewed as a point on the $(V-1)$-simplex, which we call the word simplex.\n\nThe unigram model finds a single point on the word simplex and posits that all words in the corpus come from the corresponding distribution. The latent variable models consider *k* points on the word simplex and form a sub-simplex based on those points, which we call the topic simplex. Note that any point on the topic simplex is also a point on the word simplex. The different latent variable models use the topic simplex in different ways to generate a document.\n\n- The mixture of unigrams model posits that for each document, one of the $k$ points on the word simplex (that is, one of the corners of the topic simplex) is chosen randomly and all the words of the document are drawn from the distribution corresponding to that point.\n\n- The pLSI model posits that each word of a *training* document comes from a randomly chosen topic. The topics are themselves drawn from a document-specific distribution over topics, i.e., a point on the topic simplex. There is one such distribution for each document; the set of training documents thus defines an empirical distribution on the topic simplex.\n\n- \u2022 LDA posits that each word of both the observed and unseen documents is generated by a randomly chosen topic which is drawn from a distribution with a randomly chosen parameter. This parameter is sampled once per document from a smooth distribution on the topic simplex.\n\nThese differences are highlighted in Figure 4.\n\n**5. Inference and Parameter Estimation**\n\nWe have described the motivation behind LDA and illustrated its conceptual advantages over other latent topic models. In this section, we turn our attention to procedures for inference and parameter estimation under LDA."
        },
        {
            "text": "These differences are highlighted in Figure 4.",
            "page": 11,
            "x": 88,
            "y": 606,
            "width": 201,
            "height": 14,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-a",
            "chunk_id": "38255087-f2a8-4c78-8aad-b7b1c388500e",
            "group_text": "4.4 A geometric interpretation\n\nA good way of illustrating the differences between LDA and the other latent topic models is by considering the geometry of the latent space, and seeing how a document is represented in that geometry under each model.\n\nFigure 4: The topic simplex for three topics embedded in the word simplex for three words. The corners of the word simplex correspond to the three distributions where each word (respectively) has probability one. The three points of the topic simplex correspond to three different distributions over words. The mixture of unigrams places each document at one of the corners of the topic simplex. The pLSI model induces an empirical distribution on the topic simplex denoted by x. LDA places a smooth distribution on the topic simplex denoted by the contour lines.\n\nAll four of the models described above\u2014unigram, mixture of unigrams, pLSI, and LDA\u2014operate in the space of distributions over words. Each such distribution can be viewed as a point on the $(V-1)$-simplex, which we call the word simplex.\n\nThe unigram model finds a single point on the word simplex and posits that all words in the corpus come from the corresponding distribution. The latent variable models consider *k* points on the word simplex and form a sub-simplex based on those points, which we call the topic simplex. Note that any point on the topic simplex is also a point on the word simplex. The different latent variable models use the topic simplex in different ways to generate a document.\n\n- The mixture of unigrams model posits that for each document, one of the $k$ points on the word simplex (that is, one of the corners of the topic simplex) is chosen randomly and all the words of the document are drawn from the distribution corresponding to that point.\n\n- The pLSI model posits that each word of a *training* document comes from a randomly chosen topic. The topics are themselves drawn from a document-specific distribution over topics, i.e., a point on the topic simplex. There is one such distribution for each document; the set of training documents thus defines an empirical distribution on the topic simplex.\n\n- \u2022 LDA posits that each word of both the observed and unseen documents is generated by a randomly chosen topic which is drawn from a distribution with a randomly chosen parameter. This parameter is sampled once per document from a smooth distribution on the topic simplex.\n\nThese differences are highlighted in Figure 4.\n\n**5. Inference and Parameter Estimation**\n\nWe have described the motivation behind LDA and illustrated its conceptual advantages over other latent topic models. In this section, we turn our attention to procedures for inference and parameter estimation under LDA."
        },
        {
            "text": "**5. Inference and Parameter Estimation**\n\nWe have described the motivation behind LDA and illustrated its conceptual advantages over other latent topic models. In this section, we turn our attention to procedures for inference and parameter estimation under LDA.",
            "page": 11,
            "x": 88,
            "y": 640,
            "width": 433,
            "height": 65,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-a",
            "chunk_id": "8ded6ca9-214e-486d-a659-3e160b6ed055",
            "group_text": "4.4 A geometric interpretation\n\nA good way of illustrating the differences between LDA and the other latent topic models is by considering the geometry of the latent space, and seeing how a document is represented in that geometry under each model.\n\nFigure 4: The topic simplex for three topics embedded in the word simplex for three words. The corners of the word simplex correspond to the three distributions where each word (respectively) has probability one. The three points of the topic simplex correspond to three different distributions over words. The mixture of unigrams places each document at one of the corners of the topic simplex. The pLSI model induces an empirical distribution on the topic simplex denoted by x. LDA places a smooth distribution on the topic simplex denoted by the contour lines.\n\nAll four of the models described above\u2014unigram, mixture of unigrams, pLSI, and LDA\u2014operate in the space of distributions over words. Each such distribution can be viewed as a point on the $(V-1)$-simplex, which we call the word simplex.\n\nThe unigram model finds a single point on the word simplex and posits that all words in the corpus come from the corresponding distribution. The latent variable models consider *k* points on the word simplex and form a sub-simplex based on those points, which we call the topic simplex. Note that any point on the topic simplex is also a point on the word simplex. The different latent variable models use the topic simplex in different ways to generate a document.\n\n- The mixture of unigrams model posits that for each document, one of the $k$ points on the word simplex (that is, one of the corners of the topic simplex) is chosen randomly and all the words of the document are drawn from the distribution corresponding to that point.\n\n- The pLSI model posits that each word of a *training* document comes from a randomly chosen topic. The topics are themselves drawn from a document-specific distribution over topics, i.e., a point on the topic simplex. There is one such distribution for each document; the set of training documents thus defines an empirical distribution on the topic simplex.\n\n- \u2022 LDA posits that each word of both the observed and unseen documents is generated by a randomly chosen topic which is drawn from a distribution with a randomly chosen parameter. This parameter is sampled once per document from a smooth distribution on the topic simplex.\n\nThese differences are highlighted in Figure 4.\n\n**5. Inference and Parameter Estimation**\n\nWe have described the motivation behind LDA and illustrated its conceptual advantages over other latent topic models. In this section, we turn our attention to procedures for inference and parameter estimation under LDA."
        },
        {
            "text": "5.1 Inference\n\nThe key inferential problem that we need to solve in order to use LDA is that of computing the posterior distribution of the hidden variables given a document:",
            "page": 12,
            "x": 85,
            "y": 89,
            "width": 438,
            "height": 50,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "13-inference",
            "chunk_id": "014c1d94-cc5d-471c-86bb-da0b866dd9a9",
            "group_text": "5.1 Inference\n\nThe key inferential problem that we need to solve in order to use LDA is that of computing the posterior distribution of the hidden variables given a document:\n\n$p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta) = \\frac{p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)}{p(\\mathbf{w} \\mid \\alpha, \\beta)}.$\n\nUnfortunately, this distribution is intractable to compute in general. Indeed, to normalize the distribution we marginalize over the hidden variables and write Eq. (3) in terms of the model parameters:\n\n$p(\\mathbf{w} \\mid \\alpha, \\beta) = \\frac{\\Gamma(\\sum_i \\alpha_i)}{\\prod_i \\Gamma(\\alpha_i)} \\int \\left( \\prod_{i=1}^k \\theta_i^{\\alpha_i-1} \\right) \\left( \\prod_{n=1}^N \\sum_{i=1}^k \\prod_{j=1}^V (\\theta_i \\beta_{ij})^{w_{jn}^i} \\right) d\\theta,$\n\na function which is intractable due to the coupling between \u03b8 and \u03b2 in the summation over latent topics (Dickey, 1983). Dickey shows that this function is an expectation under a particular extension to the Dirichlet distribution which can be represented with special hypergeometric functions. It has been used in a Bayesian context for censored discrete data to represent the posterior on \u03b8 which, in that setting, is a random parameter (Dickey et al., 1987).\n\nAlthough the posterior distribution is intractable for exact inference, a wide variety of approxi-\nmate inference algorithms can be considered for LDA, including Laplace approximation, variational\napproximation, and Markov chain Monte Carlo (Jordan, 1999). In this section we describe a simple\nconvexity-based variational algorithm for inference in LDA, and discuss some of the alternatives in\nSection 8."
        },
        {
            "text": "$p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta) = \\frac{p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)}{p(\\mathbf{w} \\mid \\alpha, \\beta)}.$",
            "page": 12,
            "x": 229,
            "y": 142,
            "width": 152,
            "height": 31,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "13-inference",
            "chunk_id": "bbe12031-0bc9-4dff-8b21-ac99a7b915f2",
            "group_text": "5.1 Inference\n\nThe key inferential problem that we need to solve in order to use LDA is that of computing the posterior distribution of the hidden variables given a document:\n\n$p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta) = \\frac{p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)}{p(\\mathbf{w} \\mid \\alpha, \\beta)}.$\n\nUnfortunately, this distribution is intractable to compute in general. Indeed, to normalize the distribution we marginalize over the hidden variables and write Eq. (3) in terms of the model parameters:\n\n$p(\\mathbf{w} \\mid \\alpha, \\beta) = \\frac{\\Gamma(\\sum_i \\alpha_i)}{\\prod_i \\Gamma(\\alpha_i)} \\int \\left( \\prod_{i=1}^k \\theta_i^{\\alpha_i-1} \\right) \\left( \\prod_{n=1}^N \\sum_{i=1}^k \\prod_{j=1}^V (\\theta_i \\beta_{ij})^{w_{jn}^i} \\right) d\\theta,$\n\na function which is intractable due to the coupling between \u03b8 and \u03b2 in the summation over latent topics (Dickey, 1983). Dickey shows that this function is an expectation under a particular extension to the Dirichlet distribution which can be represented with special hypergeometric functions. It has been used in a Bayesian context for censored discrete data to represent the posterior on \u03b8 which, in that setting, is a random parameter (Dickey et al., 1987).\n\nAlthough the posterior distribution is intractable for exact inference, a wide variety of approxi-\nmate inference algorithms can be considered for LDA, including Laplace approximation, variational\napproximation, and Markov chain Monte Carlo (Jordan, 1999). In this section we describe a simple\nconvexity-based variational algorithm for inference in LDA, and discuss some of the alternatives in\nSection 8."
        },
        {
            "text": "Unfortunately, this distribution is intractable to compute in general. Indeed, to normalize the distribution we marginalize over the hidden variables and write Eq. (3) in terms of the model parameters:",
            "page": 12,
            "x": 87,
            "y": 174,
            "width": 435,
            "height": 29,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "13-inference",
            "chunk_id": "6479e505-9496-4446-aeb5-3a51130208e2",
            "group_text": "5.1 Inference\n\nThe key inferential problem that we need to solve in order to use LDA is that of computing the posterior distribution of the hidden variables given a document:\n\n$p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta) = \\frac{p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)}{p(\\mathbf{w} \\mid \\alpha, \\beta)}.$\n\nUnfortunately, this distribution is intractable to compute in general. Indeed, to normalize the distribution we marginalize over the hidden variables and write Eq. (3) in terms of the model parameters:\n\n$p(\\mathbf{w} \\mid \\alpha, \\beta) = \\frac{\\Gamma(\\sum_i \\alpha_i)}{\\prod_i \\Gamma(\\alpha_i)} \\int \\left( \\prod_{i=1}^k \\theta_i^{\\alpha_i-1} \\right) \\left( \\prod_{n=1}^N \\sum_{i=1}^k \\prod_{j=1}^V (\\theta_i \\beta_{ij})^{w_{jn}^i} \\right) d\\theta,$\n\na function which is intractable due to the coupling between \u03b8 and \u03b2 in the summation over latent topics (Dickey, 1983). Dickey shows that this function is an expectation under a particular extension to the Dirichlet distribution which can be represented with special hypergeometric functions. It has been used in a Bayesian context for censored discrete data to represent the posterior on \u03b8 which, in that setting, is a random parameter (Dickey et al., 1987).\n\nAlthough the posterior distribution is intractable for exact inference, a wide variety of approxi-\nmate inference algorithms can be considered for LDA, including Laplace approximation, variational\napproximation, and Markov chain Monte Carlo (Jordan, 1999). In this section we describe a simple\nconvexity-based variational algorithm for inference in LDA, and discuss some of the alternatives in\nSection 8."
        },
        {
            "text": "$p(\\mathbf{w} \\mid \\alpha, \\beta) = \\frac{\\Gamma(\\sum_i \\alpha_i)}{\\prod_i \\Gamma(\\alpha_i)} \\int \\left( \\prod_{i=1}^k \\theta_i^{\\alpha_i-1} \\right) \\left( \\prod_{n=1}^N \\sum_{i=1}^k \\prod_{j=1}^V (\\theta_i \\beta_{ij})^{w_{jn}^i} \\right) d\\theta,$",
            "page": 12,
            "x": 163,
            "y": 206,
            "width": 285,
            "height": 40,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "13-inference",
            "chunk_id": "2afd0cf2-086e-4e74-a0dd-17d44db27c92",
            "group_text": "5.1 Inference\n\nThe key inferential problem that we need to solve in order to use LDA is that of computing the posterior distribution of the hidden variables given a document:\n\n$p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta) = \\frac{p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)}{p(\\mathbf{w} \\mid \\alpha, \\beta)}.$\n\nUnfortunately, this distribution is intractable to compute in general. Indeed, to normalize the distribution we marginalize over the hidden variables and write Eq. (3) in terms of the model parameters:\n\n$p(\\mathbf{w} \\mid \\alpha, \\beta) = \\frac{\\Gamma(\\sum_i \\alpha_i)}{\\prod_i \\Gamma(\\alpha_i)} \\int \\left( \\prod_{i=1}^k \\theta_i^{\\alpha_i-1} \\right) \\left( \\prod_{n=1}^N \\sum_{i=1}^k \\prod_{j=1}^V (\\theta_i \\beta_{ij})^{w_{jn}^i} \\right) d\\theta,$\n\na function which is intractable due to the coupling between \u03b8 and \u03b2 in the summation over latent topics (Dickey, 1983). Dickey shows that this function is an expectation under a particular extension to the Dirichlet distribution which can be represented with special hypergeometric functions. It has been used in a Bayesian context for censored discrete data to represent the posterior on \u03b8 which, in that setting, is a random parameter (Dickey et al., 1987).\n\nAlthough the posterior distribution is intractable for exact inference, a wide variety of approxi-\nmate inference algorithms can be considered for LDA, including Laplace approximation, variational\napproximation, and Markov chain Monte Carlo (Jordan, 1999). In this section we describe a simple\nconvexity-based variational algorithm for inference in LDA, and discuss some of the alternatives in\nSection 8."
        },
        {
            "text": "a function which is intractable due to the coupling between \u03b8 and \u03b2 in the summation over latent topics (Dickey, 1983). Dickey shows that this function is an expectation under a particular extension to the Dirichlet distribution which can be represented with special hypergeometric functions. It has been used in a Bayesian context for censored discrete data to represent the posterior on \u03b8 which, in that setting, is a random parameter (Dickey et al., 1987).",
            "page": 12,
            "x": 87,
            "y": 249,
            "width": 435,
            "height": 67,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "13-inference",
            "chunk_id": "4fa8081c-9e20-4890-922e-e365a4aaac1a",
            "group_text": "5.1 Inference\n\nThe key inferential problem that we need to solve in order to use LDA is that of computing the posterior distribution of the hidden variables given a document:\n\n$p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta) = \\frac{p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)}{p(\\mathbf{w} \\mid \\alpha, \\beta)}.$\n\nUnfortunately, this distribution is intractable to compute in general. Indeed, to normalize the distribution we marginalize over the hidden variables and write Eq. (3) in terms of the model parameters:\n\n$p(\\mathbf{w} \\mid \\alpha, \\beta) = \\frac{\\Gamma(\\sum_i \\alpha_i)}{\\prod_i \\Gamma(\\alpha_i)} \\int \\left( \\prod_{i=1}^k \\theta_i^{\\alpha_i-1} \\right) \\left( \\prod_{n=1}^N \\sum_{i=1}^k \\prod_{j=1}^V (\\theta_i \\beta_{ij})^{w_{jn}^i} \\right) d\\theta,$\n\na function which is intractable due to the coupling between \u03b8 and \u03b2 in the summation over latent topics (Dickey, 1983). Dickey shows that this function is an expectation under a particular extension to the Dirichlet distribution which can be represented with special hypergeometric functions. It has been used in a Bayesian context for censored discrete data to represent the posterior on \u03b8 which, in that setting, is a random parameter (Dickey et al., 1987).\n\nAlthough the posterior distribution is intractable for exact inference, a wide variety of approxi-\nmate inference algorithms can be considered for LDA, including Laplace approximation, variational\napproximation, and Markov chain Monte Carlo (Jordan, 1999). In this section we describe a simple\nconvexity-based variational algorithm for inference in LDA, and discuss some of the alternatives in\nSection 8."
        },
        {
            "text": "Although the posterior distribution is intractable for exact inference, a wide variety of approxi-\nmate inference algorithms can be considered for LDA, including Laplace approximation, variational\napproximation, and Markov chain Monte Carlo (Jordan, 1999). In this section we describe a simple\nconvexity-based variational algorithm for inference in LDA, and discuss some of the alternatives in\nSection 8.",
            "page": 12,
            "x": 87,
            "y": 317,
            "width": 434,
            "height": 67,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "13-inference",
            "chunk_id": "7df56fcb-b603-431e-aa0e-ed8d1dc6b723",
            "group_text": "5.1 Inference\n\nThe key inferential problem that we need to solve in order to use LDA is that of computing the posterior distribution of the hidden variables given a document:\n\n$p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta) = \\frac{p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)}{p(\\mathbf{w} \\mid \\alpha, \\beta)}.$\n\nUnfortunately, this distribution is intractable to compute in general. Indeed, to normalize the distribution we marginalize over the hidden variables and write Eq. (3) in terms of the model parameters:\n\n$p(\\mathbf{w} \\mid \\alpha, \\beta) = \\frac{\\Gamma(\\sum_i \\alpha_i)}{\\prod_i \\Gamma(\\alpha_i)} \\int \\left( \\prod_{i=1}^k \\theta_i^{\\alpha_i-1} \\right) \\left( \\prod_{n=1}^N \\sum_{i=1}^k \\prod_{j=1}^V (\\theta_i \\beta_{ij})^{w_{jn}^i} \\right) d\\theta,$\n\na function which is intractable due to the coupling between \u03b8 and \u03b2 in the summation over latent topics (Dickey, 1983). Dickey shows that this function is an expectation under a particular extension to the Dirichlet distribution which can be represented with special hypergeometric functions. It has been used in a Bayesian context for censored discrete data to represent the posterior on \u03b8 which, in that setting, is a random parameter (Dickey et al., 1987).\n\nAlthough the posterior distribution is intractable for exact inference, a wide variety of approxi-\nmate inference algorithms can be considered for LDA, including Laplace approximation, variational\napproximation, and Markov chain Monte Carlo (Jordan, 1999). In this section we describe a simple\nconvexity-based variational algorithm for inference in LDA, and discuss some of the alternatives in\nSection 8."
        },
        {
            "text": "5.2 Variational inference  \nThe basic idea of convexity-based variational inference is to make use of Jensen\u2019s inequality to obtain an adjustable lower bound on the log likelihood (Jordan et al., 1999). Essentially, one considers a family of lower bounds, indexed by a set of *variational parameters*. The variational parameters are chosen by an optimization procedure that attempts to find the tightest possible lower bound.",
            "page": 12,
            "x": 87,
            "y": 398,
            "width": 434,
            "height": 73,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "14-variational",
            "chunk_id": "1228a7e7-f622-44ec-999f-03549a3abe64",
            "group_text": "5.2 Variational inference  \nThe basic idea of convexity-based variational inference is to make use of Jensen\u2019s inequality to obtain an adjustable lower bound on the log likelihood (Jordan et al., 1999). Essentially, one considers a family of lower bounds, indexed by a set of *variational parameters*. The variational parameters are chosen by an optimization procedure that attempts to find the tightest possible lower bound.\n\nA simple way to obtain a tractable family of lower bounds is to consider simple modifications of the original graphical model in which some of the edges and nodes are removed.  Consider in particular the LDA model shown in Figure 5 (left).  The problematic coupling between \u03b8 and \u03b2 arises due to the edges between **\u03b8, z, and w**. By dropping these edges and the **w** nodes, and endowing the resulting simplified graphical model with free variational parameters, we obtain a family of distributions on the latent variables.  This family is characterized by the following variational distribution:\n\n$ q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi}) = q(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\boldsymbol{\\phi}_n), \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (4) $\n\nwhere the Dirichlet parameter $\\gamma$ and the multinomial parameters $(\\phi_1, \\ldots, \\phi_N)$ are the free variational parameters.\n\nHaving specified a simplified family of probability distributions, the next step is to set up an\noptimization problem that determines the values of the variational parameters $\\gamma$ and $\\phi$. As we show\nin Appendix A, the desideratum of finding a tight lower bound on the log likelihood translates\ndirectly into the following optimization problem:\n\n$$(\\gamma^*, \\phi^*) = \\arg\\min_{(\\gamma, \\phi)} \\mathrm{D}(q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\gamma, \\phi) \\;\\|\\; p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta)). \\tag{5}$$"
        },
        {
            "text": "A simple way to obtain a tractable family of lower bounds is to consider simple modifications of the original graphical model in which some of the edges and nodes are removed.  Consider in particular the LDA model shown in Figure 5 (left).  The problematic coupling between \u03b8 and \u03b2 arises due to the edges between **\u03b8, z, and w**. By dropping these edges and the **w** nodes, and endowing the resulting simplified graphical model with free variational parameters, we obtain a family of distributions on the latent variables.  This family is characterized by the following variational distribution:",
            "page": 12,
            "x": 87,
            "y": 473,
            "width": 435,
            "height": 93,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "14-variational",
            "chunk_id": "72c4c726-9641-4b0e-a0c8-65e58d5698a9",
            "group_text": "5.2 Variational inference  \nThe basic idea of convexity-based variational inference is to make use of Jensen\u2019s inequality to obtain an adjustable lower bound on the log likelihood (Jordan et al., 1999). Essentially, one considers a family of lower bounds, indexed by a set of *variational parameters*. The variational parameters are chosen by an optimization procedure that attempts to find the tightest possible lower bound.\n\nA simple way to obtain a tractable family of lower bounds is to consider simple modifications of the original graphical model in which some of the edges and nodes are removed.  Consider in particular the LDA model shown in Figure 5 (left).  The problematic coupling between \u03b8 and \u03b2 arises due to the edges between **\u03b8, z, and w**. By dropping these edges and the **w** nodes, and endowing the resulting simplified graphical model with free variational parameters, we obtain a family of distributions on the latent variables.  This family is characterized by the following variational distribution:\n\n$ q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi}) = q(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\boldsymbol{\\phi}_n), \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (4) $\n\nwhere the Dirichlet parameter $\\gamma$ and the multinomial parameters $(\\phi_1, \\ldots, \\phi_N)$ are the free variational parameters.\n\nHaving specified a simplified family of probability distributions, the next step is to set up an\noptimization problem that determines the values of the variational parameters $\\gamma$ and $\\phi$. As we show\nin Appendix A, the desideratum of finding a tight lower bound on the log likelihood translates\ndirectly into the following optimization problem:\n\n$$(\\gamma^*, \\phi^*) = \\arg\\min_{(\\gamma, \\phi)} \\mathrm{D}(q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\gamma, \\phi) \\;\\|\\; p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta)). \\tag{5}$$"
        },
        {
            "text": "$ q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi}) = q(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\boldsymbol{\\phi}_n), \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (4) $",
            "page": 12,
            "x": 229,
            "y": 568,
            "width": 293,
            "height": 29,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "14-variational",
            "chunk_id": "1ca9a259-8c74-4ea4-8756-944b31b1b1bd",
            "group_text": "5.2 Variational inference  \nThe basic idea of convexity-based variational inference is to make use of Jensen\u2019s inequality to obtain an adjustable lower bound on the log likelihood (Jordan et al., 1999). Essentially, one considers a family of lower bounds, indexed by a set of *variational parameters*. The variational parameters are chosen by an optimization procedure that attempts to find the tightest possible lower bound.\n\nA simple way to obtain a tractable family of lower bounds is to consider simple modifications of the original graphical model in which some of the edges and nodes are removed.  Consider in particular the LDA model shown in Figure 5 (left).  The problematic coupling between \u03b8 and \u03b2 arises due to the edges between **\u03b8, z, and w**. By dropping these edges and the **w** nodes, and endowing the resulting simplified graphical model with free variational parameters, we obtain a family of distributions on the latent variables.  This family is characterized by the following variational distribution:\n\n$ q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi}) = q(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\boldsymbol{\\phi}_n), \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (4) $\n\nwhere the Dirichlet parameter $\\gamma$ and the multinomial parameters $(\\phi_1, \\ldots, \\phi_N)$ are the free variational parameters.\n\nHaving specified a simplified family of probability distributions, the next step is to set up an\noptimization problem that determines the values of the variational parameters $\\gamma$ and $\\phi$. As we show\nin Appendix A, the desideratum of finding a tight lower bound on the log likelihood translates\ndirectly into the following optimization problem:\n\n$$(\\gamma^*, \\phi^*) = \\arg\\min_{(\\gamma, \\phi)} \\mathrm{D}(q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\gamma, \\phi) \\;\\|\\; p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta)). \\tag{5}$$"
        },
        {
            "text": "where the Dirichlet parameter $\\gamma$ and the multinomial parameters $(\\phi_1, \\ldots, \\phi_N)$ are the free variational parameters.",
            "page": 12,
            "x": 88,
            "y": 599,
            "width": 433,
            "height": 25,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "14-variational",
            "chunk_id": "2f2d84e4-9c2b-4b0a-b0ea-feb44a7e821c",
            "group_text": "5.2 Variational inference  \nThe basic idea of convexity-based variational inference is to make use of Jensen\u2019s inequality to obtain an adjustable lower bound on the log likelihood (Jordan et al., 1999). Essentially, one considers a family of lower bounds, indexed by a set of *variational parameters*. The variational parameters are chosen by an optimization procedure that attempts to find the tightest possible lower bound.\n\nA simple way to obtain a tractable family of lower bounds is to consider simple modifications of the original graphical model in which some of the edges and nodes are removed.  Consider in particular the LDA model shown in Figure 5 (left).  The problematic coupling between \u03b8 and \u03b2 arises due to the edges between **\u03b8, z, and w**. By dropping these edges and the **w** nodes, and endowing the resulting simplified graphical model with free variational parameters, we obtain a family of distributions on the latent variables.  This family is characterized by the following variational distribution:\n\n$ q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi}) = q(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\boldsymbol{\\phi}_n), \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (4) $\n\nwhere the Dirichlet parameter $\\gamma$ and the multinomial parameters $(\\phi_1, \\ldots, \\phi_N)$ are the free variational parameters.\n\nHaving specified a simplified family of probability distributions, the next step is to set up an\noptimization problem that determines the values of the variational parameters $\\gamma$ and $\\phi$. As we show\nin Appendix A, the desideratum of finding a tight lower bound on the log likelihood translates\ndirectly into the following optimization problem:\n\n$$(\\gamma^*, \\phi^*) = \\arg\\min_{(\\gamma, \\phi)} \\mathrm{D}(q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\gamma, \\phi) \\;\\|\\; p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta)). \\tag{5}$$"
        },
        {
            "text": "Having specified a simplified family of probability distributions, the next step is to set up an\noptimization problem that determines the values of the variational parameters $\\gamma$ and $\\phi$. As we show\nin Appendix A, the desideratum of finding a tight lower bound on the log likelihood translates\ndirectly into the following optimization problem:",
            "page": 12,
            "x": 88,
            "y": 626,
            "width": 433,
            "height": 52,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "14-variational",
            "chunk_id": "ce245b32-1f32-4ea9-bb8d-f972dd49b902",
            "group_text": "5.2 Variational inference  \nThe basic idea of convexity-based variational inference is to make use of Jensen\u2019s inequality to obtain an adjustable lower bound on the log likelihood (Jordan et al., 1999). Essentially, one considers a family of lower bounds, indexed by a set of *variational parameters*. The variational parameters are chosen by an optimization procedure that attempts to find the tightest possible lower bound.\n\nA simple way to obtain a tractable family of lower bounds is to consider simple modifications of the original graphical model in which some of the edges and nodes are removed.  Consider in particular the LDA model shown in Figure 5 (left).  The problematic coupling between \u03b8 and \u03b2 arises due to the edges between **\u03b8, z, and w**. By dropping these edges and the **w** nodes, and endowing the resulting simplified graphical model with free variational parameters, we obtain a family of distributions on the latent variables.  This family is characterized by the following variational distribution:\n\n$ q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi}) = q(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\boldsymbol{\\phi}_n), \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (4) $\n\nwhere the Dirichlet parameter $\\gamma$ and the multinomial parameters $(\\phi_1, \\ldots, \\phi_N)$ are the free variational parameters.\n\nHaving specified a simplified family of probability distributions, the next step is to set up an\noptimization problem that determines the values of the variational parameters $\\gamma$ and $\\phi$. As we show\nin Appendix A, the desideratum of finding a tight lower bound on the log likelihood translates\ndirectly into the following optimization problem:\n\n$$(\\gamma^*, \\phi^*) = \\arg\\min_{(\\gamma, \\phi)} \\mathrm{D}(q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\gamma, \\phi) \\;\\|\\; p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta)). \\tag{5}$$"
        },
        {
            "text": "$$(\\gamma^*, \\phi^*) = \\arg\\min_{(\\gamma, \\phi)} \\mathrm{D}(q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\gamma, \\phi) \\;\\|\\; p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta)). \\tag{5}$$",
            "page": 12,
            "x": 194,
            "y": 685,
            "width": 328,
            "height": 24,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "14-variational",
            "chunk_id": "14cb62d1-4b7b-43f1-925f-4965461c1cc7",
            "group_text": "5.2 Variational inference  \nThe basic idea of convexity-based variational inference is to make use of Jensen\u2019s inequality to obtain an adjustable lower bound on the log likelihood (Jordan et al., 1999). Essentially, one considers a family of lower bounds, indexed by a set of *variational parameters*. The variational parameters are chosen by an optimization procedure that attempts to find the tightest possible lower bound.\n\nA simple way to obtain a tractable family of lower bounds is to consider simple modifications of the original graphical model in which some of the edges and nodes are removed.  Consider in particular the LDA model shown in Figure 5 (left).  The problematic coupling between \u03b8 and \u03b2 arises due to the edges between **\u03b8, z, and w**. By dropping these edges and the **w** nodes, and endowing the resulting simplified graphical model with free variational parameters, we obtain a family of distributions on the latent variables.  This family is characterized by the following variational distribution:\n\n$ q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi}) = q(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\boldsymbol{\\phi}_n), \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (4) $\n\nwhere the Dirichlet parameter $\\gamma$ and the multinomial parameters $(\\phi_1, \\ldots, \\phi_N)$ are the free variational parameters.\n\nHaving specified a simplified family of probability distributions, the next step is to set up an\noptimization problem that determines the values of the variational parameters $\\gamma$ and $\\phi$. As we show\nin Appendix A, the desideratum of finding a tight lower bound on the log likelihood translates\ndirectly into the following optimization problem:\n\n$$(\\gamma^*, \\phi^*) = \\arg\\min_{(\\gamma, \\phi)} \\mathrm{D}(q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\gamma, \\phi) \\;\\|\\; p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta)). \\tag{5}$$"
        },
        {
            "text": "1. initialize $\\phi_{ni}^0 := 1/k$ for all $i$ and $n$\n2. initialize $\\gamma_i := \\alpha_i + N/k$ for all $i$\n3. repeat\n4. &nbsp;&nbsp;&nbsp;&nbsp;for $n = 1$ to $N$\n5. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $i = 1$ to $k$\n6. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\phi_{ni}^{t+1} := \\beta_{iw_n} \\exp(\\Psi(\\gamma_i^t))$\n7. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;normalize $\\phi_n^{t+1}$ to sum to 1.\n8. &nbsp;&nbsp;&nbsp;&nbsp;$\\gamma^{t+1} := \\alpha + \\sum_{n=1}^N \\phi_n^{t+1}$\n9. until convergence",
            "page": 13,
            "x": 121,
            "y": 99,
            "width": 191,
            "height": 129,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "15-initialize",
            "chunk_id": "1fca545d-2478-4059-acb8-eab9ef29f48f",
            "group_text": "1. initialize $\\phi_{ni}^0 := 1/k$ for all $i$ and $n$\n2. initialize $\\gamma_i := \\alpha_i + N/k$ for all $i$\n3. repeat\n4. &nbsp;&nbsp;&nbsp;&nbsp;for $n = 1$ to $N$\n5. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $i = 1$ to $k$\n6. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\phi_{ni}^{t+1} := \\beta_{iw_n} \\exp(\\Psi(\\gamma_i^t))$\n7. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;normalize $\\phi_n^{t+1}$ to sum to 1.\n8. &nbsp;&nbsp;&nbsp;&nbsp;$\\gamma^{t+1} := \\alpha + \\sum_{n=1}^N \\phi_n^{t+1}$\n9. until convergence\n\nFigure 6: A variational inference algorithm for LDA.\n\nThus the optimizing values of the variational parameters are found by minimizing the Kullback-Leibler (KL) divergence between the variational distribution and the true posterior $p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\boldsymbol{\\alpha}, \\boldsymbol{\\beta})$. This minimization can be achieved via an iterative fixed-point method. In particular, we show in Appendix A.3 that by computing the derivatives of the KL divergence and setting them equal to zero, we obtain the following pair of update equations:\n\n$ \\phi_{ni} \\propto \\beta_{iw_n} \\exp\\{ E_q[\\log(\\theta_i) \\mid \\gamma] \\} \\tag{6} $\n\n$ \\gamma_i = \\alpha_i + \\sum_{n=1}^N \\phi_{ni}. \\tag{7} $\n\nAs we show in Appendix A.1, the expectation in the multinomial update can be computed as follows:\n\n$ \\mathrm{E}_q[\\log(\\theta_i) \\mid \\gamma] = \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right), $\n\nwhere $\\Psi$ is the first derivative of the log $\\Gamma$ function which is computable via Taylor approxima-\ntions (Abramowitz and Stegun, 1970).\n\nEqs. (6) and (7) have an appealing intuitive interpretation. The Dirichlet update is a posterior Dirichlet given expected observations taken under the variational distribution, E[$z_n$ | $\\phi_n$]. The multinomial update is akin to using Bayes\u2019 theorem, $p(z_n|w_n) \\propto p(w_n|z_n)p(z_n)$, where $p(z_n)$ is approximated by the exponential of the expected value of its logarithm under the variational distribution.\n\nIt is important to note that the variational distribution is actually a conditional distribution,\nvarying as a function of **w**. This occurs because the optimization problem in Eq. (5) is conducted\nfor fixed **w**, and thus yields optimizing parameters ($\\gamma^*$, $\\phi^*$) that are a function of **w**. We can write\nthe resulting variational distribution as $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\gamma^*(\\mathbf{w}), \\phi^*(\\mathbf{w}))$, where we have made the dependence\non **w** explicit. Thus the variational distribution can be viewed as an approximation to the posterior\ndistribution $p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta)$.\n\nIn the language of text, the optimizing parameters $(\\gamma^*(\\mathbf{w}), \\phi^*(\\mathbf{w}))$ are document-specific. In particular, we view the Dirichlet parameters $\\gamma^*(\\mathbf{w})$ as providing a representation of a document in the topic simplex.\n\nWe summarize the variational inference procedure in Figure 6, with appropriate starting points for $\\gamma$ and $\\phi_n$. From the pseudocode it is clear that each iteration of variational inference for LDA requires O($(N+1)k$) operations. Empirically, we find that the number of iterations required for a\n\nsingle document is on the order of the number of words in the document. This yields a total number\nof operations roughly on the order of $N^2 k$."
        },
        {
            "text": "Figure 6: A variational inference algorithm for LDA.",
            "page": 13,
            "x": 186,
            "y": 246,
            "width": 236,
            "height": 17,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "15-initialize",
            "chunk_id": "883c4487-9267-475e-bf67-b729ac34d52c",
            "group_text": "1. initialize $\\phi_{ni}^0 := 1/k$ for all $i$ and $n$\n2. initialize $\\gamma_i := \\alpha_i + N/k$ for all $i$\n3. repeat\n4. &nbsp;&nbsp;&nbsp;&nbsp;for $n = 1$ to $N$\n5. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $i = 1$ to $k$\n6. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\phi_{ni}^{t+1} := \\beta_{iw_n} \\exp(\\Psi(\\gamma_i^t))$\n7. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;normalize $\\phi_n^{t+1}$ to sum to 1.\n8. &nbsp;&nbsp;&nbsp;&nbsp;$\\gamma^{t+1} := \\alpha + \\sum_{n=1}^N \\phi_n^{t+1}$\n9. until convergence\n\nFigure 6: A variational inference algorithm for LDA.\n\nThus the optimizing values of the variational parameters are found by minimizing the Kullback-Leibler (KL) divergence between the variational distribution and the true posterior $p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\boldsymbol{\\alpha}, \\boldsymbol{\\beta})$. This minimization can be achieved via an iterative fixed-point method. In particular, we show in Appendix A.3 that by computing the derivatives of the KL divergence and setting them equal to zero, we obtain the following pair of update equations:\n\n$ \\phi_{ni} \\propto \\beta_{iw_n} \\exp\\{ E_q[\\log(\\theta_i) \\mid \\gamma] \\} \\tag{6} $\n\n$ \\gamma_i = \\alpha_i + \\sum_{n=1}^N \\phi_{ni}. \\tag{7} $\n\nAs we show in Appendix A.1, the expectation in the multinomial update can be computed as follows:\n\n$ \\mathrm{E}_q[\\log(\\theta_i) \\mid \\gamma] = \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right), $\n\nwhere $\\Psi$ is the first derivative of the log $\\Gamma$ function which is computable via Taylor approxima-\ntions (Abramowitz and Stegun, 1970).\n\nEqs. (6) and (7) have an appealing intuitive interpretation. The Dirichlet update is a posterior Dirichlet given expected observations taken under the variational distribution, E[$z_n$ | $\\phi_n$]. The multinomial update is akin to using Bayes\u2019 theorem, $p(z_n|w_n) \\propto p(w_n|z_n)p(z_n)$, where $p(z_n)$ is approximated by the exponential of the expected value of its logarithm under the variational distribution.\n\nIt is important to note that the variational distribution is actually a conditional distribution,\nvarying as a function of **w**. This occurs because the optimization problem in Eq. (5) is conducted\nfor fixed **w**, and thus yields optimizing parameters ($\\gamma^*$, $\\phi^*$) that are a function of **w**. We can write\nthe resulting variational distribution as $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\gamma^*(\\mathbf{w}), \\phi^*(\\mathbf{w}))$, where we have made the dependence\non **w** explicit. Thus the variational distribution can be viewed as an approximation to the posterior\ndistribution $p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta)$.\n\nIn the language of text, the optimizing parameters $(\\gamma^*(\\mathbf{w}), \\phi^*(\\mathbf{w}))$ are document-specific. In particular, we view the Dirichlet parameters $\\gamma^*(\\mathbf{w})$ as providing a representation of a document in the topic simplex.\n\nWe summarize the variational inference procedure in Figure 6, with appropriate starting points for $\\gamma$ and $\\phi_n$. From the pseudocode it is clear that each iteration of variational inference for LDA requires O($(N+1)k$) operations. Empirically, we find that the number of iterations required for a\n\nsingle document is on the order of the number of words in the document. This yields a total number\nof operations roughly on the order of $N^2 k$."
        },
        {
            "text": "Thus the optimizing values of the variational parameters are found by minimizing the Kullback-Leibler (KL) divergence between the variational distribution and the true posterior $p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\boldsymbol{\\alpha}, \\boldsymbol{\\beta})$. This minimization can be achieved via an iterative fixed-point method. In particular, we show in Appendix A.3 that by computing the derivatives of the KL divergence and setting them equal to zero, we obtain the following pair of update equations:",
            "page": 13,
            "x": 85,
            "y": 279,
            "width": 438,
            "height": 72,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "15-initialize",
            "chunk_id": "189390c7-8e08-4fa1-b36b-739fc63f840a",
            "group_text": "1. initialize $\\phi_{ni}^0 := 1/k$ for all $i$ and $n$\n2. initialize $\\gamma_i := \\alpha_i + N/k$ for all $i$\n3. repeat\n4. &nbsp;&nbsp;&nbsp;&nbsp;for $n = 1$ to $N$\n5. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $i = 1$ to $k$\n6. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\phi_{ni}^{t+1} := \\beta_{iw_n} \\exp(\\Psi(\\gamma_i^t))$\n7. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;normalize $\\phi_n^{t+1}$ to sum to 1.\n8. &nbsp;&nbsp;&nbsp;&nbsp;$\\gamma^{t+1} := \\alpha + \\sum_{n=1}^N \\phi_n^{t+1}$\n9. until convergence\n\nFigure 6: A variational inference algorithm for LDA.\n\nThus the optimizing values of the variational parameters are found by minimizing the Kullback-Leibler (KL) divergence between the variational distribution and the true posterior $p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\boldsymbol{\\alpha}, \\boldsymbol{\\beta})$. This minimization can be achieved via an iterative fixed-point method. In particular, we show in Appendix A.3 that by computing the derivatives of the KL divergence and setting them equal to zero, we obtain the following pair of update equations:\n\n$ \\phi_{ni} \\propto \\beta_{iw_n} \\exp\\{ E_q[\\log(\\theta_i) \\mid \\gamma] \\} \\tag{6} $\n\n$ \\gamma_i = \\alpha_i + \\sum_{n=1}^N \\phi_{ni}. \\tag{7} $\n\nAs we show in Appendix A.1, the expectation in the multinomial update can be computed as follows:\n\n$ \\mathrm{E}_q[\\log(\\theta_i) \\mid \\gamma] = \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right), $\n\nwhere $\\Psi$ is the first derivative of the log $\\Gamma$ function which is computable via Taylor approxima-\ntions (Abramowitz and Stegun, 1970).\n\nEqs. (6) and (7) have an appealing intuitive interpretation. The Dirichlet update is a posterior Dirichlet given expected observations taken under the variational distribution, E[$z_n$ | $\\phi_n$]. The multinomial update is akin to using Bayes\u2019 theorem, $p(z_n|w_n) \\propto p(w_n|z_n)p(z_n)$, where $p(z_n)$ is approximated by the exponential of the expected value of its logarithm under the variational distribution.\n\nIt is important to note that the variational distribution is actually a conditional distribution,\nvarying as a function of **w**. This occurs because the optimization problem in Eq. (5) is conducted\nfor fixed **w**, and thus yields optimizing parameters ($\\gamma^*$, $\\phi^*$) that are a function of **w**. We can write\nthe resulting variational distribution as $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\gamma^*(\\mathbf{w}), \\phi^*(\\mathbf{w}))$, where we have made the dependence\non **w** explicit. Thus the variational distribution can be viewed as an approximation to the posterior\ndistribution $p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta)$.\n\nIn the language of text, the optimizing parameters $(\\gamma^*(\\mathbf{w}), \\phi^*(\\mathbf{w}))$ are document-specific. In particular, we view the Dirichlet parameters $\\gamma^*(\\mathbf{w})$ as providing a representation of a document in the topic simplex.\n\nWe summarize the variational inference procedure in Figure 6, with appropriate starting points for $\\gamma$ and $\\phi_n$. From the pseudocode it is clear that each iteration of variational inference for LDA requires O($(N+1)k$) operations. Empirically, we find that the number of iterations required for a\n\nsingle document is on the order of the number of words in the document. This yields a total number\nof operations roughly on the order of $N^2 k$."
        },
        {
            "text": "$ \\phi_{ni} \\propto \\beta_{iw_n} \\exp\\{ E_q[\\log(\\theta_i) \\mid \\gamma] \\} \\tag{6} $\n\n$ \\gamma_i = \\alpha_i + \\sum_{n=1}^N \\phi_{ni}. \\tag{7} $",
            "page": 13,
            "x": 230,
            "y": 357,
            "width": 294,
            "height": 36,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "15-initialize",
            "chunk_id": "1db864f3-0b53-4900-93c2-4b017ec5aef0",
            "group_text": "1. initialize $\\phi_{ni}^0 := 1/k$ for all $i$ and $n$\n2. initialize $\\gamma_i := \\alpha_i + N/k$ for all $i$\n3. repeat\n4. &nbsp;&nbsp;&nbsp;&nbsp;for $n = 1$ to $N$\n5. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $i = 1$ to $k$\n6. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\phi_{ni}^{t+1} := \\beta_{iw_n} \\exp(\\Psi(\\gamma_i^t))$\n7. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;normalize $\\phi_n^{t+1}$ to sum to 1.\n8. &nbsp;&nbsp;&nbsp;&nbsp;$\\gamma^{t+1} := \\alpha + \\sum_{n=1}^N \\phi_n^{t+1}$\n9. until convergence\n\nFigure 6: A variational inference algorithm for LDA.\n\nThus the optimizing values of the variational parameters are found by minimizing the Kullback-Leibler (KL) divergence between the variational distribution and the true posterior $p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\boldsymbol{\\alpha}, \\boldsymbol{\\beta})$. This minimization can be achieved via an iterative fixed-point method. In particular, we show in Appendix A.3 that by computing the derivatives of the KL divergence and setting them equal to zero, we obtain the following pair of update equations:\n\n$ \\phi_{ni} \\propto \\beta_{iw_n} \\exp\\{ E_q[\\log(\\theta_i) \\mid \\gamma] \\} \\tag{6} $\n\n$ \\gamma_i = \\alpha_i + \\sum_{n=1}^N \\phi_{ni}. \\tag{7} $\n\nAs we show in Appendix A.1, the expectation in the multinomial update can be computed as follows:\n\n$ \\mathrm{E}_q[\\log(\\theta_i) \\mid \\gamma] = \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right), $\n\nwhere $\\Psi$ is the first derivative of the log $\\Gamma$ function which is computable via Taylor approxima-\ntions (Abramowitz and Stegun, 1970).\n\nEqs. (6) and (7) have an appealing intuitive interpretation. The Dirichlet update is a posterior Dirichlet given expected observations taken under the variational distribution, E[$z_n$ | $\\phi_n$]. The multinomial update is akin to using Bayes\u2019 theorem, $p(z_n|w_n) \\propto p(w_n|z_n)p(z_n)$, where $p(z_n)$ is approximated by the exponential of the expected value of its logarithm under the variational distribution.\n\nIt is important to note that the variational distribution is actually a conditional distribution,\nvarying as a function of **w**. This occurs because the optimization problem in Eq. (5) is conducted\nfor fixed **w**, and thus yields optimizing parameters ($\\gamma^*$, $\\phi^*$) that are a function of **w**. We can write\nthe resulting variational distribution as $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\gamma^*(\\mathbf{w}), \\phi^*(\\mathbf{w}))$, where we have made the dependence\non **w** explicit. Thus the variational distribution can be viewed as an approximation to the posterior\ndistribution $p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta)$.\n\nIn the language of text, the optimizing parameters $(\\gamma^*(\\mathbf{w}), \\phi^*(\\mathbf{w}))$ are document-specific. In particular, we view the Dirichlet parameters $\\gamma^*(\\mathbf{w})$ as providing a representation of a document in the topic simplex.\n\nWe summarize the variational inference procedure in Figure 6, with appropriate starting points for $\\gamma$ and $\\phi_n$. From the pseudocode it is clear that each iteration of variational inference for LDA requires O($(N+1)k$) operations. Empirically, we find that the number of iterations required for a\n\nsingle document is on the order of the number of words in the document. This yields a total number\nof operations roughly on the order of $N^2 k$."
        },
        {
            "text": "As we show in Appendix A.1, the expectation in the multinomial update can be computed as follows:",
            "page": 13,
            "x": 87,
            "y": 399,
            "width": 435,
            "height": 15,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "15-initialize",
            "chunk_id": "9af2eb6d-5237-4da2-87b2-3e644ed0bf65",
            "group_text": "1. initialize $\\phi_{ni}^0 := 1/k$ for all $i$ and $n$\n2. initialize $\\gamma_i := \\alpha_i + N/k$ for all $i$\n3. repeat\n4. &nbsp;&nbsp;&nbsp;&nbsp;for $n = 1$ to $N$\n5. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $i = 1$ to $k$\n6. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\phi_{ni}^{t+1} := \\beta_{iw_n} \\exp(\\Psi(\\gamma_i^t))$\n7. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;normalize $\\phi_n^{t+1}$ to sum to 1.\n8. &nbsp;&nbsp;&nbsp;&nbsp;$\\gamma^{t+1} := \\alpha + \\sum_{n=1}^N \\phi_n^{t+1}$\n9. until convergence\n\nFigure 6: A variational inference algorithm for LDA.\n\nThus the optimizing values of the variational parameters are found by minimizing the Kullback-Leibler (KL) divergence between the variational distribution and the true posterior $p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\boldsymbol{\\alpha}, \\boldsymbol{\\beta})$. This minimization can be achieved via an iterative fixed-point method. In particular, we show in Appendix A.3 that by computing the derivatives of the KL divergence and setting them equal to zero, we obtain the following pair of update equations:\n\n$ \\phi_{ni} \\propto \\beta_{iw_n} \\exp\\{ E_q[\\log(\\theta_i) \\mid \\gamma] \\} \\tag{6} $\n\n$ \\gamma_i = \\alpha_i + \\sum_{n=1}^N \\phi_{ni}. \\tag{7} $\n\nAs we show in Appendix A.1, the expectation in the multinomial update can be computed as follows:\n\n$ \\mathrm{E}_q[\\log(\\theta_i) \\mid \\gamma] = \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right), $\n\nwhere $\\Psi$ is the first derivative of the log $\\Gamma$ function which is computable via Taylor approxima-\ntions (Abramowitz and Stegun, 1970).\n\nEqs. (6) and (7) have an appealing intuitive interpretation. The Dirichlet update is a posterior Dirichlet given expected observations taken under the variational distribution, E[$z_n$ | $\\phi_n$]. The multinomial update is akin to using Bayes\u2019 theorem, $p(z_n|w_n) \\propto p(w_n|z_n)p(z_n)$, where $p(z_n)$ is approximated by the exponential of the expected value of its logarithm under the variational distribution.\n\nIt is important to note that the variational distribution is actually a conditional distribution,\nvarying as a function of **w**. This occurs because the optimization problem in Eq. (5) is conducted\nfor fixed **w**, and thus yields optimizing parameters ($\\gamma^*$, $\\phi^*$) that are a function of **w**. We can write\nthe resulting variational distribution as $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\gamma^*(\\mathbf{w}), \\phi^*(\\mathbf{w}))$, where we have made the dependence\non **w** explicit. Thus the variational distribution can be viewed as an approximation to the posterior\ndistribution $p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta)$.\n\nIn the language of text, the optimizing parameters $(\\gamma^*(\\mathbf{w}), \\phi^*(\\mathbf{w}))$ are document-specific. In particular, we view the Dirichlet parameters $\\gamma^*(\\mathbf{w})$ as providing a representation of a document in the topic simplex.\n\nWe summarize the variational inference procedure in Figure 6, with appropriate starting points for $\\gamma$ and $\\phi_n$. From the pseudocode it is clear that each iteration of variational inference for LDA requires O($(N+1)k$) operations. Empirically, we find that the number of iterations required for a\n\nsingle document is on the order of the number of words in the document. This yields a total number\nof operations roughly on the order of $N^2 k$."
        },
        {
            "text": "$ \\mathrm{E}_q[\\log(\\theta_i) \\mid \\gamma] = \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right), $",
            "page": 13,
            "x": 222,
            "y": 420,
            "width": 301,
            "height": 21,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "15-initialize",
            "chunk_id": "eb15eccf-a5d4-4727-a5c1-fd7e2043c87b",
            "group_text": "1. initialize $\\phi_{ni}^0 := 1/k$ for all $i$ and $n$\n2. initialize $\\gamma_i := \\alpha_i + N/k$ for all $i$\n3. repeat\n4. &nbsp;&nbsp;&nbsp;&nbsp;for $n = 1$ to $N$\n5. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $i = 1$ to $k$\n6. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\phi_{ni}^{t+1} := \\beta_{iw_n} \\exp(\\Psi(\\gamma_i^t))$\n7. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;normalize $\\phi_n^{t+1}$ to sum to 1.\n8. &nbsp;&nbsp;&nbsp;&nbsp;$\\gamma^{t+1} := \\alpha + \\sum_{n=1}^N \\phi_n^{t+1}$\n9. until convergence\n\nFigure 6: A variational inference algorithm for LDA.\n\nThus the optimizing values of the variational parameters are found by minimizing the Kullback-Leibler (KL) divergence between the variational distribution and the true posterior $p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\boldsymbol{\\alpha}, \\boldsymbol{\\beta})$. This minimization can be achieved via an iterative fixed-point method. In particular, we show in Appendix A.3 that by computing the derivatives of the KL divergence and setting them equal to zero, we obtain the following pair of update equations:\n\n$ \\phi_{ni} \\propto \\beta_{iw_n} \\exp\\{ E_q[\\log(\\theta_i) \\mid \\gamma] \\} \\tag{6} $\n\n$ \\gamma_i = \\alpha_i + \\sum_{n=1}^N \\phi_{ni}. \\tag{7} $\n\nAs we show in Appendix A.1, the expectation in the multinomial update can be computed as follows:\n\n$ \\mathrm{E}_q[\\log(\\theta_i) \\mid \\gamma] = \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right), $\n\nwhere $\\Psi$ is the first derivative of the log $\\Gamma$ function which is computable via Taylor approxima-\ntions (Abramowitz and Stegun, 1970).\n\nEqs. (6) and (7) have an appealing intuitive interpretation. The Dirichlet update is a posterior Dirichlet given expected observations taken under the variational distribution, E[$z_n$ | $\\phi_n$]. The multinomial update is akin to using Bayes\u2019 theorem, $p(z_n|w_n) \\propto p(w_n|z_n)p(z_n)$, where $p(z_n)$ is approximated by the exponential of the expected value of its logarithm under the variational distribution.\n\nIt is important to note that the variational distribution is actually a conditional distribution,\nvarying as a function of **w**. This occurs because the optimization problem in Eq. (5) is conducted\nfor fixed **w**, and thus yields optimizing parameters ($\\gamma^*$, $\\phi^*$) that are a function of **w**. We can write\nthe resulting variational distribution as $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\gamma^*(\\mathbf{w}), \\phi^*(\\mathbf{w}))$, where we have made the dependence\non **w** explicit. Thus the variational distribution can be viewed as an approximation to the posterior\ndistribution $p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta)$.\n\nIn the language of text, the optimizing parameters $(\\gamma^*(\\mathbf{w}), \\phi^*(\\mathbf{w}))$ are document-specific. In particular, we view the Dirichlet parameters $\\gamma^*(\\mathbf{w})$ as providing a representation of a document in the topic simplex.\n\nWe summarize the variational inference procedure in Figure 6, with appropriate starting points for $\\gamma$ and $\\phi_n$. From the pseudocode it is clear that each iteration of variational inference for LDA requires O($(N+1)k$) operations. Empirically, we find that the number of iterations required for a\n\nsingle document is on the order of the number of words in the document. This yields a total number\nof operations roughly on the order of $N^2 k$."
        },
        {
            "text": "where $\\Psi$ is the first derivative of the log $\\Gamma$ function which is computable via Taylor approxima-\ntions (Abramowitz and Stegun, 1970).",
            "page": 13,
            "x": 88,
            "y": 448,
            "width": 434,
            "height": 26,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "15-initialize",
            "chunk_id": "cd2b56b0-25f0-48f4-b6e5-6a09d0c843a8",
            "group_text": "1. initialize $\\phi_{ni}^0 := 1/k$ for all $i$ and $n$\n2. initialize $\\gamma_i := \\alpha_i + N/k$ for all $i$\n3. repeat\n4. &nbsp;&nbsp;&nbsp;&nbsp;for $n = 1$ to $N$\n5. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $i = 1$ to $k$\n6. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\phi_{ni}^{t+1} := \\beta_{iw_n} \\exp(\\Psi(\\gamma_i^t))$\n7. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;normalize $\\phi_n^{t+1}$ to sum to 1.\n8. &nbsp;&nbsp;&nbsp;&nbsp;$\\gamma^{t+1} := \\alpha + \\sum_{n=1}^N \\phi_n^{t+1}$\n9. until convergence\n\nFigure 6: A variational inference algorithm for LDA.\n\nThus the optimizing values of the variational parameters are found by minimizing the Kullback-Leibler (KL) divergence between the variational distribution and the true posterior $p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\boldsymbol{\\alpha}, \\boldsymbol{\\beta})$. This minimization can be achieved via an iterative fixed-point method. In particular, we show in Appendix A.3 that by computing the derivatives of the KL divergence and setting them equal to zero, we obtain the following pair of update equations:\n\n$ \\phi_{ni} \\propto \\beta_{iw_n} \\exp\\{ E_q[\\log(\\theta_i) \\mid \\gamma] \\} \\tag{6} $\n\n$ \\gamma_i = \\alpha_i + \\sum_{n=1}^N \\phi_{ni}. \\tag{7} $\n\nAs we show in Appendix A.1, the expectation in the multinomial update can be computed as follows:\n\n$ \\mathrm{E}_q[\\log(\\theta_i) \\mid \\gamma] = \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right), $\n\nwhere $\\Psi$ is the first derivative of the log $\\Gamma$ function which is computable via Taylor approxima-\ntions (Abramowitz and Stegun, 1970).\n\nEqs. (6) and (7) have an appealing intuitive interpretation. The Dirichlet update is a posterior Dirichlet given expected observations taken under the variational distribution, E[$z_n$ | $\\phi_n$]. The multinomial update is akin to using Bayes\u2019 theorem, $p(z_n|w_n) \\propto p(w_n|z_n)p(z_n)$, where $p(z_n)$ is approximated by the exponential of the expected value of its logarithm under the variational distribution.\n\nIt is important to note that the variational distribution is actually a conditional distribution,\nvarying as a function of **w**. This occurs because the optimization problem in Eq. (5) is conducted\nfor fixed **w**, and thus yields optimizing parameters ($\\gamma^*$, $\\phi^*$) that are a function of **w**. We can write\nthe resulting variational distribution as $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\gamma^*(\\mathbf{w}), \\phi^*(\\mathbf{w}))$, where we have made the dependence\non **w** explicit. Thus the variational distribution can be viewed as an approximation to the posterior\ndistribution $p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta)$.\n\nIn the language of text, the optimizing parameters $(\\gamma^*(\\mathbf{w}), \\phi^*(\\mathbf{w}))$ are document-specific. In particular, we view the Dirichlet parameters $\\gamma^*(\\mathbf{w})$ as providing a representation of a document in the topic simplex.\n\nWe summarize the variational inference procedure in Figure 6, with appropriate starting points for $\\gamma$ and $\\phi_n$. From the pseudocode it is clear that each iteration of variational inference for LDA requires O($(N+1)k$) operations. Empirically, we find that the number of iterations required for a\n\nsingle document is on the order of the number of words in the document. This yields a total number\nof operations roughly on the order of $N^2 k$."
        },
        {
            "text": "Eqs. (6) and (7) have an appealing intuitive interpretation. The Dirichlet update is a posterior Dirichlet given expected observations taken under the variational distribution, E[$z_n$ | $\\phi_n$]. The multinomial update is akin to using Bayes\u2019 theorem, $p(z_n|w_n) \\propto p(w_n|z_n)p(z_n)$, where $p(z_n)$ is approximated by the exponential of the expected value of its logarithm under the variational distribution.",
            "page": 13,
            "x": 88,
            "y": 476,
            "width": 435,
            "height": 66,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "15-initialize",
            "chunk_id": "dd397918-844e-424c-bcd2-afe76cfb8dd9",
            "group_text": "1. initialize $\\phi_{ni}^0 := 1/k$ for all $i$ and $n$\n2. initialize $\\gamma_i := \\alpha_i + N/k$ for all $i$\n3. repeat\n4. &nbsp;&nbsp;&nbsp;&nbsp;for $n = 1$ to $N$\n5. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $i = 1$ to $k$\n6. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\phi_{ni}^{t+1} := \\beta_{iw_n} \\exp(\\Psi(\\gamma_i^t))$\n7. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;normalize $\\phi_n^{t+1}$ to sum to 1.\n8. &nbsp;&nbsp;&nbsp;&nbsp;$\\gamma^{t+1} := \\alpha + \\sum_{n=1}^N \\phi_n^{t+1}$\n9. until convergence\n\nFigure 6: A variational inference algorithm for LDA.\n\nThus the optimizing values of the variational parameters are found by minimizing the Kullback-Leibler (KL) divergence between the variational distribution and the true posterior $p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\boldsymbol{\\alpha}, \\boldsymbol{\\beta})$. This minimization can be achieved via an iterative fixed-point method. In particular, we show in Appendix A.3 that by computing the derivatives of the KL divergence and setting them equal to zero, we obtain the following pair of update equations:\n\n$ \\phi_{ni} \\propto \\beta_{iw_n} \\exp\\{ E_q[\\log(\\theta_i) \\mid \\gamma] \\} \\tag{6} $\n\n$ \\gamma_i = \\alpha_i + \\sum_{n=1}^N \\phi_{ni}. \\tag{7} $\n\nAs we show in Appendix A.1, the expectation in the multinomial update can be computed as follows:\n\n$ \\mathrm{E}_q[\\log(\\theta_i) \\mid \\gamma] = \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right), $\n\nwhere $\\Psi$ is the first derivative of the log $\\Gamma$ function which is computable via Taylor approxima-\ntions (Abramowitz and Stegun, 1970).\n\nEqs. (6) and (7) have an appealing intuitive interpretation. The Dirichlet update is a posterior Dirichlet given expected observations taken under the variational distribution, E[$z_n$ | $\\phi_n$]. The multinomial update is akin to using Bayes\u2019 theorem, $p(z_n|w_n) \\propto p(w_n|z_n)p(z_n)$, where $p(z_n)$ is approximated by the exponential of the expected value of its logarithm under the variational distribution.\n\nIt is important to note that the variational distribution is actually a conditional distribution,\nvarying as a function of **w**. This occurs because the optimization problem in Eq. (5) is conducted\nfor fixed **w**, and thus yields optimizing parameters ($\\gamma^*$, $\\phi^*$) that are a function of **w**. We can write\nthe resulting variational distribution as $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\gamma^*(\\mathbf{w}), \\phi^*(\\mathbf{w}))$, where we have made the dependence\non **w** explicit. Thus the variational distribution can be viewed as an approximation to the posterior\ndistribution $p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta)$.\n\nIn the language of text, the optimizing parameters $(\\gamma^*(\\mathbf{w}), \\phi^*(\\mathbf{w}))$ are document-specific. In particular, we view the Dirichlet parameters $\\gamma^*(\\mathbf{w})$ as providing a representation of a document in the topic simplex.\n\nWe summarize the variational inference procedure in Figure 6, with appropriate starting points for $\\gamma$ and $\\phi_n$. From the pseudocode it is clear that each iteration of variational inference for LDA requires O($(N+1)k$) operations. Empirically, we find that the number of iterations required for a\n\nsingle document is on the order of the number of words in the document. This yields a total number\nof operations roughly on the order of $N^2 k$."
        },
        {
            "text": "It is important to note that the variational distribution is actually a conditional distribution,\nvarying as a function of **w**. This occurs because the optimization problem in Eq. (5) is conducted\nfor fixed **w**, and thus yields optimizing parameters ($\\gamma^*$, $\\phi^*$) that are a function of **w**. We can write\nthe resulting variational distribution as $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\gamma^*(\\mathbf{w}), \\phi^*(\\mathbf{w}))$, where we have made the dependence\non **w** explicit. Thus the variational distribution can be viewed as an approximation to the posterior\ndistribution $p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta)$.",
            "page": 13,
            "x": 88,
            "y": 544,
            "width": 434,
            "height": 79,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "15-initialize",
            "chunk_id": "3c0a7e40-90ae-4943-a104-5d2bd922719d",
            "group_text": "1. initialize $\\phi_{ni}^0 := 1/k$ for all $i$ and $n$\n2. initialize $\\gamma_i := \\alpha_i + N/k$ for all $i$\n3. repeat\n4. &nbsp;&nbsp;&nbsp;&nbsp;for $n = 1$ to $N$\n5. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $i = 1$ to $k$\n6. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\phi_{ni}^{t+1} := \\beta_{iw_n} \\exp(\\Psi(\\gamma_i^t))$\n7. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;normalize $\\phi_n^{t+1}$ to sum to 1.\n8. &nbsp;&nbsp;&nbsp;&nbsp;$\\gamma^{t+1} := \\alpha + \\sum_{n=1}^N \\phi_n^{t+1}$\n9. until convergence\n\nFigure 6: A variational inference algorithm for LDA.\n\nThus the optimizing values of the variational parameters are found by minimizing the Kullback-Leibler (KL) divergence between the variational distribution and the true posterior $p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\boldsymbol{\\alpha}, \\boldsymbol{\\beta})$. This minimization can be achieved via an iterative fixed-point method. In particular, we show in Appendix A.3 that by computing the derivatives of the KL divergence and setting them equal to zero, we obtain the following pair of update equations:\n\n$ \\phi_{ni} \\propto \\beta_{iw_n} \\exp\\{ E_q[\\log(\\theta_i) \\mid \\gamma] \\} \\tag{6} $\n\n$ \\gamma_i = \\alpha_i + \\sum_{n=1}^N \\phi_{ni}. \\tag{7} $\n\nAs we show in Appendix A.1, the expectation in the multinomial update can be computed as follows:\n\n$ \\mathrm{E}_q[\\log(\\theta_i) \\mid \\gamma] = \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right), $\n\nwhere $\\Psi$ is the first derivative of the log $\\Gamma$ function which is computable via Taylor approxima-\ntions (Abramowitz and Stegun, 1970).\n\nEqs. (6) and (7) have an appealing intuitive interpretation. The Dirichlet update is a posterior Dirichlet given expected observations taken under the variational distribution, E[$z_n$ | $\\phi_n$]. The multinomial update is akin to using Bayes\u2019 theorem, $p(z_n|w_n) \\propto p(w_n|z_n)p(z_n)$, where $p(z_n)$ is approximated by the exponential of the expected value of its logarithm under the variational distribution.\n\nIt is important to note that the variational distribution is actually a conditional distribution,\nvarying as a function of **w**. This occurs because the optimization problem in Eq. (5) is conducted\nfor fixed **w**, and thus yields optimizing parameters ($\\gamma^*$, $\\phi^*$) that are a function of **w**. We can write\nthe resulting variational distribution as $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\gamma^*(\\mathbf{w}), \\phi^*(\\mathbf{w}))$, where we have made the dependence\non **w** explicit. Thus the variational distribution can be viewed as an approximation to the posterior\ndistribution $p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta)$.\n\nIn the language of text, the optimizing parameters $(\\gamma^*(\\mathbf{w}), \\phi^*(\\mathbf{w}))$ are document-specific. In particular, we view the Dirichlet parameters $\\gamma^*(\\mathbf{w})$ as providing a representation of a document in the topic simplex.\n\nWe summarize the variational inference procedure in Figure 6, with appropriate starting points for $\\gamma$ and $\\phi_n$. From the pseudocode it is clear that each iteration of variational inference for LDA requires O($(N+1)k$) operations. Empirically, we find that the number of iterations required for a\n\nsingle document is on the order of the number of words in the document. This yields a total number\nof operations roughly on the order of $N^2 k$."
        },
        {
            "text": "In the language of text, the optimizing parameters $(\\gamma^*(\\mathbf{w}), \\phi^*(\\mathbf{w}))$ are document-specific. In particular, we view the Dirichlet parameters $\\gamma^*(\\mathbf{w})$ as providing a representation of a document in the topic simplex.",
            "page": 13,
            "x": 88,
            "y": 624,
            "width": 434,
            "height": 40,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "15-initialize",
            "chunk_id": "63e95abd-d471-4d33-a821-9c148a627137",
            "group_text": "1. initialize $\\phi_{ni}^0 := 1/k$ for all $i$ and $n$\n2. initialize $\\gamma_i := \\alpha_i + N/k$ for all $i$\n3. repeat\n4. &nbsp;&nbsp;&nbsp;&nbsp;for $n = 1$ to $N$\n5. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $i = 1$ to $k$\n6. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\phi_{ni}^{t+1} := \\beta_{iw_n} \\exp(\\Psi(\\gamma_i^t))$\n7. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;normalize $\\phi_n^{t+1}$ to sum to 1.\n8. &nbsp;&nbsp;&nbsp;&nbsp;$\\gamma^{t+1} := \\alpha + \\sum_{n=1}^N \\phi_n^{t+1}$\n9. until convergence\n\nFigure 6: A variational inference algorithm for LDA.\n\nThus the optimizing values of the variational parameters are found by minimizing the Kullback-Leibler (KL) divergence between the variational distribution and the true posterior $p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\boldsymbol{\\alpha}, \\boldsymbol{\\beta})$. This minimization can be achieved via an iterative fixed-point method. In particular, we show in Appendix A.3 that by computing the derivatives of the KL divergence and setting them equal to zero, we obtain the following pair of update equations:\n\n$ \\phi_{ni} \\propto \\beta_{iw_n} \\exp\\{ E_q[\\log(\\theta_i) \\mid \\gamma] \\} \\tag{6} $\n\n$ \\gamma_i = \\alpha_i + \\sum_{n=1}^N \\phi_{ni}. \\tag{7} $\n\nAs we show in Appendix A.1, the expectation in the multinomial update can be computed as follows:\n\n$ \\mathrm{E}_q[\\log(\\theta_i) \\mid \\gamma] = \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right), $\n\nwhere $\\Psi$ is the first derivative of the log $\\Gamma$ function which is computable via Taylor approxima-\ntions (Abramowitz and Stegun, 1970).\n\nEqs. (6) and (7) have an appealing intuitive interpretation. The Dirichlet update is a posterior Dirichlet given expected observations taken under the variational distribution, E[$z_n$ | $\\phi_n$]. The multinomial update is akin to using Bayes\u2019 theorem, $p(z_n|w_n) \\propto p(w_n|z_n)p(z_n)$, where $p(z_n)$ is approximated by the exponential of the expected value of its logarithm under the variational distribution.\n\nIt is important to note that the variational distribution is actually a conditional distribution,\nvarying as a function of **w**. This occurs because the optimization problem in Eq. (5) is conducted\nfor fixed **w**, and thus yields optimizing parameters ($\\gamma^*$, $\\phi^*$) that are a function of **w**. We can write\nthe resulting variational distribution as $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\gamma^*(\\mathbf{w}), \\phi^*(\\mathbf{w}))$, where we have made the dependence\non **w** explicit. Thus the variational distribution can be viewed as an approximation to the posterior\ndistribution $p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta)$.\n\nIn the language of text, the optimizing parameters $(\\gamma^*(\\mathbf{w}), \\phi^*(\\mathbf{w}))$ are document-specific. In particular, we view the Dirichlet parameters $\\gamma^*(\\mathbf{w})$ as providing a representation of a document in the topic simplex.\n\nWe summarize the variational inference procedure in Figure 6, with appropriate starting points for $\\gamma$ and $\\phi_n$. From the pseudocode it is clear that each iteration of variational inference for LDA requires O($(N+1)k$) operations. Empirically, we find that the number of iterations required for a\n\nsingle document is on the order of the number of words in the document. This yields a total number\nof operations roughly on the order of $N^2 k$."
        },
        {
            "text": "We summarize the variational inference procedure in Figure 6, with appropriate starting points for $\\gamma$ and $\\phi_n$. From the pseudocode it is clear that each iteration of variational inference for LDA requires O($(N+1)k$) operations. Empirically, we find that the number of iterations required for a",
            "page": 13,
            "x": 88,
            "y": 666,
            "width": 434,
            "height": 41,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "15-initialize",
            "chunk_id": "46b71de6-add6-4b8a-ac7c-681a106b0bf2",
            "group_text": "1. initialize $\\phi_{ni}^0 := 1/k$ for all $i$ and $n$\n2. initialize $\\gamma_i := \\alpha_i + N/k$ for all $i$\n3. repeat\n4. &nbsp;&nbsp;&nbsp;&nbsp;for $n = 1$ to $N$\n5. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $i = 1$ to $k$\n6. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\phi_{ni}^{t+1} := \\beta_{iw_n} \\exp(\\Psi(\\gamma_i^t))$\n7. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;normalize $\\phi_n^{t+1}$ to sum to 1.\n8. &nbsp;&nbsp;&nbsp;&nbsp;$\\gamma^{t+1} := \\alpha + \\sum_{n=1}^N \\phi_n^{t+1}$\n9. until convergence\n\nFigure 6: A variational inference algorithm for LDA.\n\nThus the optimizing values of the variational parameters are found by minimizing the Kullback-Leibler (KL) divergence between the variational distribution and the true posterior $p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\boldsymbol{\\alpha}, \\boldsymbol{\\beta})$. This minimization can be achieved via an iterative fixed-point method. In particular, we show in Appendix A.3 that by computing the derivatives of the KL divergence and setting them equal to zero, we obtain the following pair of update equations:\n\n$ \\phi_{ni} \\propto \\beta_{iw_n} \\exp\\{ E_q[\\log(\\theta_i) \\mid \\gamma] \\} \\tag{6} $\n\n$ \\gamma_i = \\alpha_i + \\sum_{n=1}^N \\phi_{ni}. \\tag{7} $\n\nAs we show in Appendix A.1, the expectation in the multinomial update can be computed as follows:\n\n$ \\mathrm{E}_q[\\log(\\theta_i) \\mid \\gamma] = \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right), $\n\nwhere $\\Psi$ is the first derivative of the log $\\Gamma$ function which is computable via Taylor approxima-\ntions (Abramowitz and Stegun, 1970).\n\nEqs. (6) and (7) have an appealing intuitive interpretation. The Dirichlet update is a posterior Dirichlet given expected observations taken under the variational distribution, E[$z_n$ | $\\phi_n$]. The multinomial update is akin to using Bayes\u2019 theorem, $p(z_n|w_n) \\propto p(w_n|z_n)p(z_n)$, where $p(z_n)$ is approximated by the exponential of the expected value of its logarithm under the variational distribution.\n\nIt is important to note that the variational distribution is actually a conditional distribution,\nvarying as a function of **w**. This occurs because the optimization problem in Eq. (5) is conducted\nfor fixed **w**, and thus yields optimizing parameters ($\\gamma^*$, $\\phi^*$) that are a function of **w**. We can write\nthe resulting variational distribution as $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\gamma^*(\\mathbf{w}), \\phi^*(\\mathbf{w}))$, where we have made the dependence\non **w** explicit. Thus the variational distribution can be viewed as an approximation to the posterior\ndistribution $p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta)$.\n\nIn the language of text, the optimizing parameters $(\\gamma^*(\\mathbf{w}), \\phi^*(\\mathbf{w}))$ are document-specific. In particular, we view the Dirichlet parameters $\\gamma^*(\\mathbf{w})$ as providing a representation of a document in the topic simplex.\n\nWe summarize the variational inference procedure in Figure 6, with appropriate starting points for $\\gamma$ and $\\phi_n$. From the pseudocode it is clear that each iteration of variational inference for LDA requires O($(N+1)k$) operations. Empirically, we find that the number of iterations required for a\n\nsingle document is on the order of the number of words in the document. This yields a total number\nof operations roughly on the order of $N^2 k$."
        },
        {
            "text": "single document is on the order of the number of words in the document. This yields a total number\nof operations roughly on the order of $N^2 k$.",
            "page": 14,
            "x": 86,
            "y": 90,
            "width": 436,
            "height": 29,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "15-initialize",
            "chunk_id": "4f3052ed-dc0b-4a6c-980d-3a52d198cc89",
            "group_text": "1. initialize $\\phi_{ni}^0 := 1/k$ for all $i$ and $n$\n2. initialize $\\gamma_i := \\alpha_i + N/k$ for all $i$\n3. repeat\n4. &nbsp;&nbsp;&nbsp;&nbsp;for $n = 1$ to $N$\n5. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $i = 1$ to $k$\n6. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\phi_{ni}^{t+1} := \\beta_{iw_n} \\exp(\\Psi(\\gamma_i^t))$\n7. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;normalize $\\phi_n^{t+1}$ to sum to 1.\n8. &nbsp;&nbsp;&nbsp;&nbsp;$\\gamma^{t+1} := \\alpha + \\sum_{n=1}^N \\phi_n^{t+1}$\n9. until convergence\n\nFigure 6: A variational inference algorithm for LDA.\n\nThus the optimizing values of the variational parameters are found by minimizing the Kullback-Leibler (KL) divergence between the variational distribution and the true posterior $p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\boldsymbol{\\alpha}, \\boldsymbol{\\beta})$. This minimization can be achieved via an iterative fixed-point method. In particular, we show in Appendix A.3 that by computing the derivatives of the KL divergence and setting them equal to zero, we obtain the following pair of update equations:\n\n$ \\phi_{ni} \\propto \\beta_{iw_n} \\exp\\{ E_q[\\log(\\theta_i) \\mid \\gamma] \\} \\tag{6} $\n\n$ \\gamma_i = \\alpha_i + \\sum_{n=1}^N \\phi_{ni}. \\tag{7} $\n\nAs we show in Appendix A.1, the expectation in the multinomial update can be computed as follows:\n\n$ \\mathrm{E}_q[\\log(\\theta_i) \\mid \\gamma] = \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right), $\n\nwhere $\\Psi$ is the first derivative of the log $\\Gamma$ function which is computable via Taylor approxima-\ntions (Abramowitz and Stegun, 1970).\n\nEqs. (6) and (7) have an appealing intuitive interpretation. The Dirichlet update is a posterior Dirichlet given expected observations taken under the variational distribution, E[$z_n$ | $\\phi_n$]. The multinomial update is akin to using Bayes\u2019 theorem, $p(z_n|w_n) \\propto p(w_n|z_n)p(z_n)$, where $p(z_n)$ is approximated by the exponential of the expected value of its logarithm under the variational distribution.\n\nIt is important to note that the variational distribution is actually a conditional distribution,\nvarying as a function of **w**. This occurs because the optimization problem in Eq. (5) is conducted\nfor fixed **w**, and thus yields optimizing parameters ($\\gamma^*$, $\\phi^*$) that are a function of **w**. We can write\nthe resulting variational distribution as $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\gamma^*(\\mathbf{w}), \\phi^*(\\mathbf{w}))$, where we have made the dependence\non **w** explicit. Thus the variational distribution can be viewed as an approximation to the posterior\ndistribution $p(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta)$.\n\nIn the language of text, the optimizing parameters $(\\gamma^*(\\mathbf{w}), \\phi^*(\\mathbf{w}))$ are document-specific. In particular, we view the Dirichlet parameters $\\gamma^*(\\mathbf{w})$ as providing a representation of a document in the topic simplex.\n\nWe summarize the variational inference procedure in Figure 6, with appropriate starting points for $\\gamma$ and $\\phi_n$. From the pseudocode it is clear that each iteration of variational inference for LDA requires O($(N+1)k$) operations. Empirically, we find that the number of iterations required for a\n\nsingle document is on the order of the number of words in the document. This yields a total number\nof operations roughly on the order of $N^2 k$."
        },
        {
            "text": "5.3 Parameter estimation\n\nIn this section we present an empirical Bayes method for parameter estimation in the LDA model (see Section 5.4 for a fuller Bayesian approach). In particular, given a corpus of documents $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$, we wish to find parameters $\\alpha$ and $\\beta$ that maximize the (marginal) log likelihood of the data:",
            "page": 14,
            "x": 86,
            "y": 132,
            "width": 437,
            "height": 74,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "16-parameter",
            "chunk_id": "51af0518-1c29-47fd-a190-cee937d14198",
            "group_text": "5.3 Parameter estimation\n\nIn this section we present an empirical Bayes method for parameter estimation in the LDA model (see Section 5.4 for a fuller Bayesian approach). In particular, given a corpus of documents $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$, we wish to find parameters $\\alpha$ and $\\beta$ that maximize the (marginal) log likelihood of the data:\n\n$\\ell(\\alpha, \\beta) = \\sum_{d=1}^{M} \\log p(\\mathbf{w}_d \\mid \\alpha, \\beta).$\n\nAs we have described above, the quantity $p(\\mathbf{w}|\\alpha, \\beta)$ cannot be computed tractably. However, variational inference provides us with a tractable lower bound on the log likelihood, a bound which we can maximize with respect to $\\alpha$ and $\\beta$. We can thus find approximate empirical Bayes estimates for the LDA model via an alternating _variational EM_ procedure that maximizes a lower bound with respect to the variational parameters $\\gamma$ and $\\phi$, and then, for fixed values of the variational parameters, maximizes the lower bound with respect to the model parameters $\\alpha$ and $\\beta$.\n\n1. (E-step) For each document, find the optimizing values of the variational parameters $\\{\\gamma_d^*, \\phi_d^* : d \\in \\mathcal{D}\\}$. This is done as described in the previous section.\n\n2. (M-step) Maximize the resulting lower bound on the log likelihood with respect to the model parameters $\\alpha$ and $\\beta$. This corresponds to finding maximum likelihood estimates with expected sufficient statistics for each document under the approximate posterior which is computed in the E-step.\n\nThese two steps are repeated until the lower bound on the log likelihood converges.\n    In Appendix A.4, we show that the M-step update for the conditional multinomial parameter \u03b2 can be written out analytically:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\Phi_{dni}^{*} w_{dn}^{j}.$\n\n(9)\n\nWe further show that the M-step update for Dirichlet parameter \u03b1 can be implemented using an\nefficient Newton-Raphson method in which the Hessian is inverted in linear time."
        },
        {
            "text": "$\\ell(\\alpha, \\beta) = \\sum_{d=1}^{M} \\log p(\\mathbf{w}_d \\mid \\alpha, \\beta).$",
            "page": 14,
            "x": 239,
            "y": 202,
            "width": 131,
            "height": 36,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "16-parameter",
            "chunk_id": "3f8b80a0-c7d6-4861-bc61-03fedf3876ee",
            "group_text": "5.3 Parameter estimation\n\nIn this section we present an empirical Bayes method for parameter estimation in the LDA model (see Section 5.4 for a fuller Bayesian approach). In particular, given a corpus of documents $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$, we wish to find parameters $\\alpha$ and $\\beta$ that maximize the (marginal) log likelihood of the data:\n\n$\\ell(\\alpha, \\beta) = \\sum_{d=1}^{M} \\log p(\\mathbf{w}_d \\mid \\alpha, \\beta).$\n\nAs we have described above, the quantity $p(\\mathbf{w}|\\alpha, \\beta)$ cannot be computed tractably. However, variational inference provides us with a tractable lower bound on the log likelihood, a bound which we can maximize with respect to $\\alpha$ and $\\beta$. We can thus find approximate empirical Bayes estimates for the LDA model via an alternating _variational EM_ procedure that maximizes a lower bound with respect to the variational parameters $\\gamma$ and $\\phi$, and then, for fixed values of the variational parameters, maximizes the lower bound with respect to the model parameters $\\alpha$ and $\\beta$.\n\n1. (E-step) For each document, find the optimizing values of the variational parameters $\\{\\gamma_d^*, \\phi_d^* : d \\in \\mathcal{D}\\}$. This is done as described in the previous section.\n\n2. (M-step) Maximize the resulting lower bound on the log likelihood with respect to the model parameters $\\alpha$ and $\\beta$. This corresponds to finding maximum likelihood estimates with expected sufficient statistics for each document under the approximate posterior which is computed in the E-step.\n\nThese two steps are repeated until the lower bound on the log likelihood converges.\n    In Appendix A.4, we show that the M-step update for the conditional multinomial parameter \u03b2 can be written out analytically:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\Phi_{dni}^{*} w_{dn}^{j}.$\n\n(9)\n\nWe further show that the M-step update for Dirichlet parameter \u03b1 can be implemented using an\nefficient Newton-Raphson method in which the Hessian is inverted in linear time."
        },
        {
            "text": "As we have described above, the quantity $p(\\mathbf{w}|\\alpha, \\beta)$ cannot be computed tractably. However, variational inference provides us with a tractable lower bound on the log likelihood, a bound which we can maximize with respect to $\\alpha$ and $\\beta$. We can thus find approximate empirical Bayes estimates for the LDA model via an alternating _variational EM_ procedure that maximizes a lower bound with respect to the variational parameters $\\gamma$ and $\\phi$, and then, for fixed values of the variational parameters, maximizes the lower bound with respect to the model parameters $\\alpha$ and $\\beta$.",
            "page": 14,
            "x": 87,
            "y": 240,
            "width": 436,
            "height": 81,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "16-parameter",
            "chunk_id": "cdbacec6-2c56-4aa5-a6eb-af0169017158",
            "group_text": "5.3 Parameter estimation\n\nIn this section we present an empirical Bayes method for parameter estimation in the LDA model (see Section 5.4 for a fuller Bayesian approach). In particular, given a corpus of documents $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$, we wish to find parameters $\\alpha$ and $\\beta$ that maximize the (marginal) log likelihood of the data:\n\n$\\ell(\\alpha, \\beta) = \\sum_{d=1}^{M} \\log p(\\mathbf{w}_d \\mid \\alpha, \\beta).$\n\nAs we have described above, the quantity $p(\\mathbf{w}|\\alpha, \\beta)$ cannot be computed tractably. However, variational inference provides us with a tractable lower bound on the log likelihood, a bound which we can maximize with respect to $\\alpha$ and $\\beta$. We can thus find approximate empirical Bayes estimates for the LDA model via an alternating _variational EM_ procedure that maximizes a lower bound with respect to the variational parameters $\\gamma$ and $\\phi$, and then, for fixed values of the variational parameters, maximizes the lower bound with respect to the model parameters $\\alpha$ and $\\beta$.\n\n1. (E-step) For each document, find the optimizing values of the variational parameters $\\{\\gamma_d^*, \\phi_d^* : d \\in \\mathcal{D}\\}$. This is done as described in the previous section.\n\n2. (M-step) Maximize the resulting lower bound on the log likelihood with respect to the model parameters $\\alpha$ and $\\beta$. This corresponds to finding maximum likelihood estimates with expected sufficient statistics for each document under the approximate posterior which is computed in the E-step.\n\nThese two steps are repeated until the lower bound on the log likelihood converges.\n    In Appendix A.4, we show that the M-step update for the conditional multinomial parameter \u03b2 can be written out analytically:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\Phi_{dni}^{*} w_{dn}^{j}.$\n\n(9)\n\nWe further show that the M-step update for Dirichlet parameter \u03b1 can be implemented using an\nefficient Newton-Raphson method in which the Hessian is inverted in linear time."
        },
        {
            "text": "1. (E-step) For each document, find the optimizing values of the variational parameters $\\{\\gamma_d^*, \\phi_d^* : d \\in \\mathcal{D}\\}$. This is done as described in the previous section.\n\n2. (M-step) Maximize the resulting lower bound on the log likelihood with respect to the model parameters $\\alpha$ and $\\beta$. This corresponds to finding maximum likelihood estimates with expected sufficient statistics for each document under the approximate posterior which is computed in the E-step.",
            "page": 14,
            "x": 88,
            "y": 323,
            "width": 435,
            "height": 127,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "16-parameter",
            "chunk_id": "59dce6db-df46-42e5-8b98-4c4327023cc2",
            "group_text": "5.3 Parameter estimation\n\nIn this section we present an empirical Bayes method for parameter estimation in the LDA model (see Section 5.4 for a fuller Bayesian approach). In particular, given a corpus of documents $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$, we wish to find parameters $\\alpha$ and $\\beta$ that maximize the (marginal) log likelihood of the data:\n\n$\\ell(\\alpha, \\beta) = \\sum_{d=1}^{M} \\log p(\\mathbf{w}_d \\mid \\alpha, \\beta).$\n\nAs we have described above, the quantity $p(\\mathbf{w}|\\alpha, \\beta)$ cannot be computed tractably. However, variational inference provides us with a tractable lower bound on the log likelihood, a bound which we can maximize with respect to $\\alpha$ and $\\beta$. We can thus find approximate empirical Bayes estimates for the LDA model via an alternating _variational EM_ procedure that maximizes a lower bound with respect to the variational parameters $\\gamma$ and $\\phi$, and then, for fixed values of the variational parameters, maximizes the lower bound with respect to the model parameters $\\alpha$ and $\\beta$.\n\n1. (E-step) For each document, find the optimizing values of the variational parameters $\\{\\gamma_d^*, \\phi_d^* : d \\in \\mathcal{D}\\}$. This is done as described in the previous section.\n\n2. (M-step) Maximize the resulting lower bound on the log likelihood with respect to the model parameters $\\alpha$ and $\\beta$. This corresponds to finding maximum likelihood estimates with expected sufficient statistics for each document under the approximate posterior which is computed in the E-step.\n\nThese two steps are repeated until the lower bound on the log likelihood converges.\n    In Appendix A.4, we show that the M-step update for the conditional multinomial parameter \u03b2 can be written out analytically:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\Phi_{dni}^{*} w_{dn}^{j}.$\n\n(9)\n\nWe further show that the M-step update for Dirichlet parameter \u03b1 can be implemented using an\nefficient Newton-Raphson method in which the Hessian is inverted in linear time."
        },
        {
            "text": "These two steps are repeated until the lower bound on the log likelihood converges.\n    In Appendix A.4, we show that the M-step update for the conditional multinomial parameter \u03b2 can be written out analytically:",
            "page": 14,
            "x": 88,
            "y": 458,
            "width": 435,
            "height": 41,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "16-parameter",
            "chunk_id": "70b6d9b6-2510-4328-bd32-f99ce4cd4462",
            "group_text": "5.3 Parameter estimation\n\nIn this section we present an empirical Bayes method for parameter estimation in the LDA model (see Section 5.4 for a fuller Bayesian approach). In particular, given a corpus of documents $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$, we wish to find parameters $\\alpha$ and $\\beta$ that maximize the (marginal) log likelihood of the data:\n\n$\\ell(\\alpha, \\beta) = \\sum_{d=1}^{M} \\log p(\\mathbf{w}_d \\mid \\alpha, \\beta).$\n\nAs we have described above, the quantity $p(\\mathbf{w}|\\alpha, \\beta)$ cannot be computed tractably. However, variational inference provides us with a tractable lower bound on the log likelihood, a bound which we can maximize with respect to $\\alpha$ and $\\beta$. We can thus find approximate empirical Bayes estimates for the LDA model via an alternating _variational EM_ procedure that maximizes a lower bound with respect to the variational parameters $\\gamma$ and $\\phi$, and then, for fixed values of the variational parameters, maximizes the lower bound with respect to the model parameters $\\alpha$ and $\\beta$.\n\n1. (E-step) For each document, find the optimizing values of the variational parameters $\\{\\gamma_d^*, \\phi_d^* : d \\in \\mathcal{D}\\}$. This is done as described in the previous section.\n\n2. (M-step) Maximize the resulting lower bound on the log likelihood with respect to the model parameters $\\alpha$ and $\\beta$. This corresponds to finding maximum likelihood estimates with expected sufficient statistics for each document under the approximate posterior which is computed in the E-step.\n\nThese two steps are repeated until the lower bound on the log likelihood converges.\n    In Appendix A.4, we show that the M-step update for the conditional multinomial parameter \u03b2 can be written out analytically:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\Phi_{dni}^{*} w_{dn}^{j}.$\n\n(9)\n\nWe further show that the M-step update for Dirichlet parameter \u03b1 can be implemented using an\nefficient Newton-Raphson method in which the Hessian is inverted in linear time."
        },
        {
            "text": "$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\Phi_{dni}^{*} w_{dn}^{j}.$\n\n(9)",
            "page": 14,
            "x": 258,
            "y": 499,
            "width": 265,
            "height": 35,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "16-parameter",
            "chunk_id": "858f5153-bd06-4f8f-a297-2e89a7a128d3",
            "group_text": "5.3 Parameter estimation\n\nIn this section we present an empirical Bayes method for parameter estimation in the LDA model (see Section 5.4 for a fuller Bayesian approach). In particular, given a corpus of documents $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$, we wish to find parameters $\\alpha$ and $\\beta$ that maximize the (marginal) log likelihood of the data:\n\n$\\ell(\\alpha, \\beta) = \\sum_{d=1}^{M} \\log p(\\mathbf{w}_d \\mid \\alpha, \\beta).$\n\nAs we have described above, the quantity $p(\\mathbf{w}|\\alpha, \\beta)$ cannot be computed tractably. However, variational inference provides us with a tractable lower bound on the log likelihood, a bound which we can maximize with respect to $\\alpha$ and $\\beta$. We can thus find approximate empirical Bayes estimates for the LDA model via an alternating _variational EM_ procedure that maximizes a lower bound with respect to the variational parameters $\\gamma$ and $\\phi$, and then, for fixed values of the variational parameters, maximizes the lower bound with respect to the model parameters $\\alpha$ and $\\beta$.\n\n1. (E-step) For each document, find the optimizing values of the variational parameters $\\{\\gamma_d^*, \\phi_d^* : d \\in \\mathcal{D}\\}$. This is done as described in the previous section.\n\n2. (M-step) Maximize the resulting lower bound on the log likelihood with respect to the model parameters $\\alpha$ and $\\beta$. This corresponds to finding maximum likelihood estimates with expected sufficient statistics for each document under the approximate posterior which is computed in the E-step.\n\nThese two steps are repeated until the lower bound on the log likelihood converges.\n    In Appendix A.4, we show that the M-step update for the conditional multinomial parameter \u03b2 can be written out analytically:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\Phi_{dni}^{*} w_{dn}^{j}.$\n\n(9)\n\nWe further show that the M-step update for Dirichlet parameter \u03b1 can be implemented using an\nefficient Newton-Raphson method in which the Hessian is inverted in linear time."
        },
        {
            "text": "We further show that the M-step update for Dirichlet parameter \u03b1 can be implemented using an\nefficient Newton-Raphson method in which the Hessian is inverted in linear time.",
            "page": 14,
            "x": 87,
            "y": 536,
            "width": 434,
            "height": 27,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "16-parameter",
            "chunk_id": "63ae0743-f595-41b3-8e7c-9fe4736ea91d",
            "group_text": "5.3 Parameter estimation\n\nIn this section we present an empirical Bayes method for parameter estimation in the LDA model (see Section 5.4 for a fuller Bayesian approach). In particular, given a corpus of documents $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$, we wish to find parameters $\\alpha$ and $\\beta$ that maximize the (marginal) log likelihood of the data:\n\n$\\ell(\\alpha, \\beta) = \\sum_{d=1}^{M} \\log p(\\mathbf{w}_d \\mid \\alpha, \\beta).$\n\nAs we have described above, the quantity $p(\\mathbf{w}|\\alpha, \\beta)$ cannot be computed tractably. However, variational inference provides us with a tractable lower bound on the log likelihood, a bound which we can maximize with respect to $\\alpha$ and $\\beta$. We can thus find approximate empirical Bayes estimates for the LDA model via an alternating _variational EM_ procedure that maximizes a lower bound with respect to the variational parameters $\\gamma$ and $\\phi$, and then, for fixed values of the variational parameters, maximizes the lower bound with respect to the model parameters $\\alpha$ and $\\beta$.\n\n1. (E-step) For each document, find the optimizing values of the variational parameters $\\{\\gamma_d^*, \\phi_d^* : d \\in \\mathcal{D}\\}$. This is done as described in the previous section.\n\n2. (M-step) Maximize the resulting lower bound on the log likelihood with respect to the model parameters $\\alpha$ and $\\beta$. This corresponds to finding maximum likelihood estimates with expected sufficient statistics for each document under the approximate posterior which is computed in the E-step.\n\nThese two steps are repeated until the lower bound on the log likelihood converges.\n    In Appendix A.4, we show that the M-step update for the conditional multinomial parameter \u03b2 can be written out analytically:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\Phi_{dni}^{*} w_{dn}^{j}.$\n\n(9)\n\nWe further show that the M-step update for Dirichlet parameter \u03b1 can be implemented using an\nefficient Newton-Raphson method in which the Hessian is inverted in linear time."
        },
        {
            "text": "5.4 Smoothing\n\nThe large vocabulary size that is characteristic of many document corpora creates serious problems of sparsity.  A new document is very likely to contain words that did not appear in any of the documents in a training corpus.  Maximum likelihood estimates of the multinomial parameters assign zero probability to such words, and thus zero probability to new documents.  The standard approach to coping with this problem is to \u201csmooth\u201d the multinomial parameters, assigning positive probability to all vocabulary items whether or not they are observed in the training set (Jelinek, 1997).  Laplace smoothing is commonly used; this essentially yields the mean of the posterior distribution under a uniform Dirichlet prior on the multinomial parameters.",
            "page": 14,
            "x": 87,
            "y": 578,
            "width": 435,
            "height": 127,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "17-smoothing",
            "chunk_id": "05995b6d-dbe6-4fb1-824e-21a1564d79d8",
            "group_text": "5.4 Smoothing\n\nThe large vocabulary size that is characteristic of many document corpora creates serious problems of sparsity.  A new document is very likely to contain words that did not appear in any of the documents in a training corpus.  Maximum likelihood estimates of the multinomial parameters assign zero probability to such words, and thus zero probability to new documents.  The standard approach to coping with this problem is to \u201csmooth\u201d the multinomial parameters, assigning positive probability to all vocabulary items whether or not they are observed in the training set (Jelinek, 1997).  Laplace smoothing is commonly used; this essentially yields the mean of the posterior distribution under a uniform Dirichlet prior on the multinomial parameters.\n\nUnfortunately, in the mixture model setting, simple Laplace smoothing is no longer justified as a maximum a posteriori method (although it is often implemented in practice; cf. Nigam et al., 1999). In fact, by placing a Dirichlet prior on the multinomial parameter we obtain an intractable posterior in the mixture model setting, for much the same reason that one obtains an intractable posterior in the basic LDA model. Our proposed solution to this problem is to simply apply variational inference methods to the extended model that includes Dirichlet smoothing on the multinomial parameter.\n\nIn the LDA setting, we obtain the extended graphical model shown in Figure 7. We treat \u03b2 as a $k \\times V$ random matrix (one row for each mixture component), where we assume that each row is independently drawn from an exchangeable Dirichlet distribution.$^2$ We now extend our inference procedures to treat the \u03b2$_i$ as random variables that are endowed with a posterior distribution, conditioned on the data. Thus we move beyond the empirical Bayes procedure of Section 5.3 and consider a fuller Bayesian approach to LDA.\n\nWe consider a variational approach to Bayesian inference that places a separable distribution on the random variables \u03b2, **\u03b8**, and **z** (Attias, 2000):\n\n$q(\\beta_{1:k}, \\mathbf{z}_{1:M}, \\boldsymbol{\\theta}_{1:M} \\mid \\boldsymbol{\\lambda}, \\boldsymbol{\\phi}, \\boldsymbol{\\gamma}) = \\prod_{i=1}^{k} \\mathrm{Dir}(\\beta_i \\mid \\lambda_i) \\prod_{d=1}^{M} q_d(\\theta_d, \\mathbf{z}_d \\mid \\phi_d, \\gamma_d),$\n\nwhere $q_{d}(\\theta, \\mathbf{z} \\mid \\boldsymbol{\\phi}, \\boldsymbol{\\gamma})$ is the variational distribution defined for LDA in Eq. (4). As is easily verified, the resulting variational inference procedure again yields Eqs. (6) and (7) as the update equations for the variational parameters $\\boldsymbol{\\phi}$ and $\\boldsymbol{\\gamma}$, respectively, as well as an additional update for the new variational parameter $\\boldsymbol{\\lambda}$:\n\n$\\lambda_{ij} = \\eta + \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni}^{*} w_{dn}^{j}.$\n\nIterating these equations to convergence yields an approximate posterior distribution on \u03b2, \u03b8, and **z**.\n\n    We are now left with the hyperparameter \u03b7 on the exchangeable Dirichlet, as well as the hyperparameter \u03b1 from before. Our approach to setting these hyperparameters is again (approximate) empirical Bayes\u2014we use variational EM to find maximum likelihood estimates of these parameters based on the marginal likelihood. These procedures are described in Appendix A.4."
        },
        {
            "text": "Unfortunately, in the mixture model setting, simple Laplace smoothing is no longer justified as a maximum a posteriori method (although it is often implemented in practice; cf. Nigam et al., 1999). In fact, by placing a Dirichlet prior on the multinomial parameter we obtain an intractable posterior in the mixture model setting, for much the same reason that one obtains an intractable posterior in the basic LDA model. Our proposed solution to this problem is to simply apply variational inference methods to the extended model that includes Dirichlet smoothing on the multinomial parameter.",
            "page": 15,
            "x": 87,
            "y": 268,
            "width": 435,
            "height": 81,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "17-smoothing",
            "chunk_id": "49558f36-4294-40c9-a18b-1985d401619d",
            "group_text": "5.4 Smoothing\n\nThe large vocabulary size that is characteristic of many document corpora creates serious problems of sparsity.  A new document is very likely to contain words that did not appear in any of the documents in a training corpus.  Maximum likelihood estimates of the multinomial parameters assign zero probability to such words, and thus zero probability to new documents.  The standard approach to coping with this problem is to \u201csmooth\u201d the multinomial parameters, assigning positive probability to all vocabulary items whether or not they are observed in the training set (Jelinek, 1997).  Laplace smoothing is commonly used; this essentially yields the mean of the posterior distribution under a uniform Dirichlet prior on the multinomial parameters.\n\nUnfortunately, in the mixture model setting, simple Laplace smoothing is no longer justified as a maximum a posteriori method (although it is often implemented in practice; cf. Nigam et al., 1999). In fact, by placing a Dirichlet prior on the multinomial parameter we obtain an intractable posterior in the mixture model setting, for much the same reason that one obtains an intractable posterior in the basic LDA model. Our proposed solution to this problem is to simply apply variational inference methods to the extended model that includes Dirichlet smoothing on the multinomial parameter.\n\nIn the LDA setting, we obtain the extended graphical model shown in Figure 7. We treat \u03b2 as a $k \\times V$ random matrix (one row for each mixture component), where we assume that each row is independently drawn from an exchangeable Dirichlet distribution.$^2$ We now extend our inference procedures to treat the \u03b2$_i$ as random variables that are endowed with a posterior distribution, conditioned on the data. Thus we move beyond the empirical Bayes procedure of Section 5.3 and consider a fuller Bayesian approach to LDA.\n\nWe consider a variational approach to Bayesian inference that places a separable distribution on the random variables \u03b2, **\u03b8**, and **z** (Attias, 2000):\n\n$q(\\beta_{1:k}, \\mathbf{z}_{1:M}, \\boldsymbol{\\theta}_{1:M} \\mid \\boldsymbol{\\lambda}, \\boldsymbol{\\phi}, \\boldsymbol{\\gamma}) = \\prod_{i=1}^{k} \\mathrm{Dir}(\\beta_i \\mid \\lambda_i) \\prod_{d=1}^{M} q_d(\\theta_d, \\mathbf{z}_d \\mid \\phi_d, \\gamma_d),$\n\nwhere $q_{d}(\\theta, \\mathbf{z} \\mid \\boldsymbol{\\phi}, \\boldsymbol{\\gamma})$ is the variational distribution defined for LDA in Eq. (4). As is easily verified, the resulting variational inference procedure again yields Eqs. (6) and (7) as the update equations for the variational parameters $\\boldsymbol{\\phi}$ and $\\boldsymbol{\\gamma}$, respectively, as well as an additional update for the new variational parameter $\\boldsymbol{\\lambda}$:\n\n$\\lambda_{ij} = \\eta + \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni}^{*} w_{dn}^{j}.$\n\nIterating these equations to convergence yields an approximate posterior distribution on \u03b2, \u03b8, and **z**.\n\n    We are now left with the hyperparameter \u03b7 on the exchangeable Dirichlet, as well as the hyperparameter \u03b1 from before. Our approach to setting these hyperparameters is again (approximate) empirical Bayes\u2014we use variational EM to find maximum likelihood estimates of these parameters based on the marginal likelihood. These procedures are described in Appendix A.4."
        },
        {
            "text": "In the LDA setting, we obtain the extended graphical model shown in Figure 7. We treat \u03b2 as a $k \\times V$ random matrix (one row for each mixture component), where we assume that each row is independently drawn from an exchangeable Dirichlet distribution.$^2$ We now extend our inference procedures to treat the \u03b2$_i$ as random variables that are endowed with a posterior distribution, conditioned on the data. Thus we move beyond the empirical Bayes procedure of Section 5.3 and consider a fuller Bayesian approach to LDA.",
            "page": 15,
            "x": 87,
            "y": 351,
            "width": 435,
            "height": 80,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "17-smoothing",
            "chunk_id": "3cd8c6cb-05d6-491b-a5f4-c2312b53736f",
            "group_text": "5.4 Smoothing\n\nThe large vocabulary size that is characteristic of many document corpora creates serious problems of sparsity.  A new document is very likely to contain words that did not appear in any of the documents in a training corpus.  Maximum likelihood estimates of the multinomial parameters assign zero probability to such words, and thus zero probability to new documents.  The standard approach to coping with this problem is to \u201csmooth\u201d the multinomial parameters, assigning positive probability to all vocabulary items whether or not they are observed in the training set (Jelinek, 1997).  Laplace smoothing is commonly used; this essentially yields the mean of the posterior distribution under a uniform Dirichlet prior on the multinomial parameters.\n\nUnfortunately, in the mixture model setting, simple Laplace smoothing is no longer justified as a maximum a posteriori method (although it is often implemented in practice; cf. Nigam et al., 1999). In fact, by placing a Dirichlet prior on the multinomial parameter we obtain an intractable posterior in the mixture model setting, for much the same reason that one obtains an intractable posterior in the basic LDA model. Our proposed solution to this problem is to simply apply variational inference methods to the extended model that includes Dirichlet smoothing on the multinomial parameter.\n\nIn the LDA setting, we obtain the extended graphical model shown in Figure 7. We treat \u03b2 as a $k \\times V$ random matrix (one row for each mixture component), where we assume that each row is independently drawn from an exchangeable Dirichlet distribution.$^2$ We now extend our inference procedures to treat the \u03b2$_i$ as random variables that are endowed with a posterior distribution, conditioned on the data. Thus we move beyond the empirical Bayes procedure of Section 5.3 and consider a fuller Bayesian approach to LDA.\n\nWe consider a variational approach to Bayesian inference that places a separable distribution on the random variables \u03b2, **\u03b8**, and **z** (Attias, 2000):\n\n$q(\\beta_{1:k}, \\mathbf{z}_{1:M}, \\boldsymbol{\\theta}_{1:M} \\mid \\boldsymbol{\\lambda}, \\boldsymbol{\\phi}, \\boldsymbol{\\gamma}) = \\prod_{i=1}^{k} \\mathrm{Dir}(\\beta_i \\mid \\lambda_i) \\prod_{d=1}^{M} q_d(\\theta_d, \\mathbf{z}_d \\mid \\phi_d, \\gamma_d),$\n\nwhere $q_{d}(\\theta, \\mathbf{z} \\mid \\boldsymbol{\\phi}, \\boldsymbol{\\gamma})$ is the variational distribution defined for LDA in Eq. (4). As is easily verified, the resulting variational inference procedure again yields Eqs. (6) and (7) as the update equations for the variational parameters $\\boldsymbol{\\phi}$ and $\\boldsymbol{\\gamma}$, respectively, as well as an additional update for the new variational parameter $\\boldsymbol{\\lambda}$:\n\n$\\lambda_{ij} = \\eta + \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni}^{*} w_{dn}^{j}.$\n\nIterating these equations to convergence yields an approximate posterior distribution on \u03b2, \u03b8, and **z**.\n\n    We are now left with the hyperparameter \u03b7 on the exchangeable Dirichlet, as well as the hyperparameter \u03b1 from before. Our approach to setting these hyperparameters is again (approximate) empirical Bayes\u2014we use variational EM to find maximum likelihood estimates of these parameters based on the marginal likelihood. These procedures are described in Appendix A.4."
        },
        {
            "text": "We consider a variational approach to Bayesian inference that places a separable distribution on the random variables \u03b2, **\u03b8**, and **z** (Attias, 2000):",
            "page": 15,
            "x": 87,
            "y": 432,
            "width": 434,
            "height": 26,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "17-smoothing",
            "chunk_id": "e7d18f23-644e-46d9-beb2-0369f7efde47",
            "group_text": "5.4 Smoothing\n\nThe large vocabulary size that is characteristic of many document corpora creates serious problems of sparsity.  A new document is very likely to contain words that did not appear in any of the documents in a training corpus.  Maximum likelihood estimates of the multinomial parameters assign zero probability to such words, and thus zero probability to new documents.  The standard approach to coping with this problem is to \u201csmooth\u201d the multinomial parameters, assigning positive probability to all vocabulary items whether or not they are observed in the training set (Jelinek, 1997).  Laplace smoothing is commonly used; this essentially yields the mean of the posterior distribution under a uniform Dirichlet prior on the multinomial parameters.\n\nUnfortunately, in the mixture model setting, simple Laplace smoothing is no longer justified as a maximum a posteriori method (although it is often implemented in practice; cf. Nigam et al., 1999). In fact, by placing a Dirichlet prior on the multinomial parameter we obtain an intractable posterior in the mixture model setting, for much the same reason that one obtains an intractable posterior in the basic LDA model. Our proposed solution to this problem is to simply apply variational inference methods to the extended model that includes Dirichlet smoothing on the multinomial parameter.\n\nIn the LDA setting, we obtain the extended graphical model shown in Figure 7. We treat \u03b2 as a $k \\times V$ random matrix (one row for each mixture component), where we assume that each row is independently drawn from an exchangeable Dirichlet distribution.$^2$ We now extend our inference procedures to treat the \u03b2$_i$ as random variables that are endowed with a posterior distribution, conditioned on the data. Thus we move beyond the empirical Bayes procedure of Section 5.3 and consider a fuller Bayesian approach to LDA.\n\nWe consider a variational approach to Bayesian inference that places a separable distribution on the random variables \u03b2, **\u03b8**, and **z** (Attias, 2000):\n\n$q(\\beta_{1:k}, \\mathbf{z}_{1:M}, \\boldsymbol{\\theta}_{1:M} \\mid \\boldsymbol{\\lambda}, \\boldsymbol{\\phi}, \\boldsymbol{\\gamma}) = \\prod_{i=1}^{k} \\mathrm{Dir}(\\beta_i \\mid \\lambda_i) \\prod_{d=1}^{M} q_d(\\theta_d, \\mathbf{z}_d \\mid \\phi_d, \\gamma_d),$\n\nwhere $q_{d}(\\theta, \\mathbf{z} \\mid \\boldsymbol{\\phi}, \\boldsymbol{\\gamma})$ is the variational distribution defined for LDA in Eq. (4). As is easily verified, the resulting variational inference procedure again yields Eqs. (6) and (7) as the update equations for the variational parameters $\\boldsymbol{\\phi}$ and $\\boldsymbol{\\gamma}$, respectively, as well as an additional update for the new variational parameter $\\boldsymbol{\\lambda}$:\n\n$\\lambda_{ij} = \\eta + \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni}^{*} w_{dn}^{j}.$\n\nIterating these equations to convergence yields an approximate posterior distribution on \u03b2, \u03b8, and **z**.\n\n    We are now left with the hyperparameter \u03b7 on the exchangeable Dirichlet, as well as the hyperparameter \u03b1 from before. Our approach to setting these hyperparameters is again (approximate) empirical Bayes\u2014we use variational EM to find maximum likelihood estimates of these parameters based on the marginal likelihood. These procedures are described in Appendix A.4."
        },
        {
            "text": "$q(\\beta_{1:k}, \\mathbf{z}_{1:M}, \\boldsymbol{\\theta}_{1:M} \\mid \\boldsymbol{\\lambda}, \\boldsymbol{\\phi}, \\boldsymbol{\\gamma}) = \\prod_{i=1}^{k} \\mathrm{Dir}(\\beta_i \\mid \\lambda_i) \\prod_{d=1}^{M} q_d(\\theta_d, \\mathbf{z}_d \\mid \\phi_d, \\gamma_d),$",
            "page": 15,
            "x": 168,
            "y": 469,
            "width": 274,
            "height": 36,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "17-smoothing",
            "chunk_id": "f05b3c8d-7743-4640-ba66-88e53d8b9b60",
            "group_text": "5.4 Smoothing\n\nThe large vocabulary size that is characteristic of many document corpora creates serious problems of sparsity.  A new document is very likely to contain words that did not appear in any of the documents in a training corpus.  Maximum likelihood estimates of the multinomial parameters assign zero probability to such words, and thus zero probability to new documents.  The standard approach to coping with this problem is to \u201csmooth\u201d the multinomial parameters, assigning positive probability to all vocabulary items whether or not they are observed in the training set (Jelinek, 1997).  Laplace smoothing is commonly used; this essentially yields the mean of the posterior distribution under a uniform Dirichlet prior on the multinomial parameters.\n\nUnfortunately, in the mixture model setting, simple Laplace smoothing is no longer justified as a maximum a posteriori method (although it is often implemented in practice; cf. Nigam et al., 1999). In fact, by placing a Dirichlet prior on the multinomial parameter we obtain an intractable posterior in the mixture model setting, for much the same reason that one obtains an intractable posterior in the basic LDA model. Our proposed solution to this problem is to simply apply variational inference methods to the extended model that includes Dirichlet smoothing on the multinomial parameter.\n\nIn the LDA setting, we obtain the extended graphical model shown in Figure 7. We treat \u03b2 as a $k \\times V$ random matrix (one row for each mixture component), where we assume that each row is independently drawn from an exchangeable Dirichlet distribution.$^2$ We now extend our inference procedures to treat the \u03b2$_i$ as random variables that are endowed with a posterior distribution, conditioned on the data. Thus we move beyond the empirical Bayes procedure of Section 5.3 and consider a fuller Bayesian approach to LDA.\n\nWe consider a variational approach to Bayesian inference that places a separable distribution on the random variables \u03b2, **\u03b8**, and **z** (Attias, 2000):\n\n$q(\\beta_{1:k}, \\mathbf{z}_{1:M}, \\boldsymbol{\\theta}_{1:M} \\mid \\boldsymbol{\\lambda}, \\boldsymbol{\\phi}, \\boldsymbol{\\gamma}) = \\prod_{i=1}^{k} \\mathrm{Dir}(\\beta_i \\mid \\lambda_i) \\prod_{d=1}^{M} q_d(\\theta_d, \\mathbf{z}_d \\mid \\phi_d, \\gamma_d),$\n\nwhere $q_{d}(\\theta, \\mathbf{z} \\mid \\boldsymbol{\\phi}, \\boldsymbol{\\gamma})$ is the variational distribution defined for LDA in Eq. (4). As is easily verified, the resulting variational inference procedure again yields Eqs. (6) and (7) as the update equations for the variational parameters $\\boldsymbol{\\phi}$ and $\\boldsymbol{\\gamma}$, respectively, as well as an additional update for the new variational parameter $\\boldsymbol{\\lambda}$:\n\n$\\lambda_{ij} = \\eta + \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni}^{*} w_{dn}^{j}.$\n\nIterating these equations to convergence yields an approximate posterior distribution on \u03b2, \u03b8, and **z**.\n\n    We are now left with the hyperparameter \u03b7 on the exchangeable Dirichlet, as well as the hyperparameter \u03b1 from before. Our approach to setting these hyperparameters is again (approximate) empirical Bayes\u2014we use variational EM to find maximum likelihood estimates of these parameters based on the marginal likelihood. These procedures are described in Appendix A.4."
        },
        {
            "text": "where $q_{d}(\\theta, \\mathbf{z} \\mid \\boldsymbol{\\phi}, \\boldsymbol{\\gamma})$ is the variational distribution defined for LDA in Eq. (4). As is easily verified, the resulting variational inference procedure again yields Eqs. (6) and (7) as the update equations for the variational parameters $\\boldsymbol{\\phi}$ and $\\boldsymbol{\\gamma}$, respectively, as well as an additional update for the new variational parameter $\\boldsymbol{\\lambda}$:",
            "page": 15,
            "x": 88,
            "y": 514,
            "width": 433,
            "height": 52,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "17-smoothing",
            "chunk_id": "dd6a5c62-842a-4939-817c-6c98b3d9bf87",
            "group_text": "5.4 Smoothing\n\nThe large vocabulary size that is characteristic of many document corpora creates serious problems of sparsity.  A new document is very likely to contain words that did not appear in any of the documents in a training corpus.  Maximum likelihood estimates of the multinomial parameters assign zero probability to such words, and thus zero probability to new documents.  The standard approach to coping with this problem is to \u201csmooth\u201d the multinomial parameters, assigning positive probability to all vocabulary items whether or not they are observed in the training set (Jelinek, 1997).  Laplace smoothing is commonly used; this essentially yields the mean of the posterior distribution under a uniform Dirichlet prior on the multinomial parameters.\n\nUnfortunately, in the mixture model setting, simple Laplace smoothing is no longer justified as a maximum a posteriori method (although it is often implemented in practice; cf. Nigam et al., 1999). In fact, by placing a Dirichlet prior on the multinomial parameter we obtain an intractable posterior in the mixture model setting, for much the same reason that one obtains an intractable posterior in the basic LDA model. Our proposed solution to this problem is to simply apply variational inference methods to the extended model that includes Dirichlet smoothing on the multinomial parameter.\n\nIn the LDA setting, we obtain the extended graphical model shown in Figure 7. We treat \u03b2 as a $k \\times V$ random matrix (one row for each mixture component), where we assume that each row is independently drawn from an exchangeable Dirichlet distribution.$^2$ We now extend our inference procedures to treat the \u03b2$_i$ as random variables that are endowed with a posterior distribution, conditioned on the data. Thus we move beyond the empirical Bayes procedure of Section 5.3 and consider a fuller Bayesian approach to LDA.\n\nWe consider a variational approach to Bayesian inference that places a separable distribution on the random variables \u03b2, **\u03b8**, and **z** (Attias, 2000):\n\n$q(\\beta_{1:k}, \\mathbf{z}_{1:M}, \\boldsymbol{\\theta}_{1:M} \\mid \\boldsymbol{\\lambda}, \\boldsymbol{\\phi}, \\boldsymbol{\\gamma}) = \\prod_{i=1}^{k} \\mathrm{Dir}(\\beta_i \\mid \\lambda_i) \\prod_{d=1}^{M} q_d(\\theta_d, \\mathbf{z}_d \\mid \\phi_d, \\gamma_d),$\n\nwhere $q_{d}(\\theta, \\mathbf{z} \\mid \\boldsymbol{\\phi}, \\boldsymbol{\\gamma})$ is the variational distribution defined for LDA in Eq. (4). As is easily verified, the resulting variational inference procedure again yields Eqs. (6) and (7) as the update equations for the variational parameters $\\boldsymbol{\\phi}$ and $\\boldsymbol{\\gamma}$, respectively, as well as an additional update for the new variational parameter $\\boldsymbol{\\lambda}$:\n\n$\\lambda_{ij} = \\eta + \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni}^{*} w_{dn}^{j}.$\n\nIterating these equations to convergence yields an approximate posterior distribution on \u03b2, \u03b8, and **z**.\n\n    We are now left with the hyperparameter \u03b7 on the exchangeable Dirichlet, as well as the hyperparameter \u03b1 from before. Our approach to setting these hyperparameters is again (approximate) empirical Bayes\u2014we use variational EM to find maximum likelihood estimates of these parameters based on the marginal likelihood. These procedures are described in Appendix A.4."
        },
        {
            "text": "$\\lambda_{ij} = \\eta + \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni}^{*} w_{dn}^{j}.$",
            "page": 15,
            "x": 248,
            "y": 567,
            "width": 113,
            "height": 34,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "17-smoothing",
            "chunk_id": "8983cb01-f32d-4b6e-9f7b-ee2d7c8bc59a",
            "group_text": "5.4 Smoothing\n\nThe large vocabulary size that is characteristic of many document corpora creates serious problems of sparsity.  A new document is very likely to contain words that did not appear in any of the documents in a training corpus.  Maximum likelihood estimates of the multinomial parameters assign zero probability to such words, and thus zero probability to new documents.  The standard approach to coping with this problem is to \u201csmooth\u201d the multinomial parameters, assigning positive probability to all vocabulary items whether or not they are observed in the training set (Jelinek, 1997).  Laplace smoothing is commonly used; this essentially yields the mean of the posterior distribution under a uniform Dirichlet prior on the multinomial parameters.\n\nUnfortunately, in the mixture model setting, simple Laplace smoothing is no longer justified as a maximum a posteriori method (although it is often implemented in practice; cf. Nigam et al., 1999). In fact, by placing a Dirichlet prior on the multinomial parameter we obtain an intractable posterior in the mixture model setting, for much the same reason that one obtains an intractable posterior in the basic LDA model. Our proposed solution to this problem is to simply apply variational inference methods to the extended model that includes Dirichlet smoothing on the multinomial parameter.\n\nIn the LDA setting, we obtain the extended graphical model shown in Figure 7. We treat \u03b2 as a $k \\times V$ random matrix (one row for each mixture component), where we assume that each row is independently drawn from an exchangeable Dirichlet distribution.$^2$ We now extend our inference procedures to treat the \u03b2$_i$ as random variables that are endowed with a posterior distribution, conditioned on the data. Thus we move beyond the empirical Bayes procedure of Section 5.3 and consider a fuller Bayesian approach to LDA.\n\nWe consider a variational approach to Bayesian inference that places a separable distribution on the random variables \u03b2, **\u03b8**, and **z** (Attias, 2000):\n\n$q(\\beta_{1:k}, \\mathbf{z}_{1:M}, \\boldsymbol{\\theta}_{1:M} \\mid \\boldsymbol{\\lambda}, \\boldsymbol{\\phi}, \\boldsymbol{\\gamma}) = \\prod_{i=1}^{k} \\mathrm{Dir}(\\beta_i \\mid \\lambda_i) \\prod_{d=1}^{M} q_d(\\theta_d, \\mathbf{z}_d \\mid \\phi_d, \\gamma_d),$\n\nwhere $q_{d}(\\theta, \\mathbf{z} \\mid \\boldsymbol{\\phi}, \\boldsymbol{\\gamma})$ is the variational distribution defined for LDA in Eq. (4). As is easily verified, the resulting variational inference procedure again yields Eqs. (6) and (7) as the update equations for the variational parameters $\\boldsymbol{\\phi}$ and $\\boldsymbol{\\gamma}$, respectively, as well as an additional update for the new variational parameter $\\boldsymbol{\\lambda}$:\n\n$\\lambda_{ij} = \\eta + \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni}^{*} w_{dn}^{j}.$\n\nIterating these equations to convergence yields an approximate posterior distribution on \u03b2, \u03b8, and **z**.\n\n    We are now left with the hyperparameter \u03b7 on the exchangeable Dirichlet, as well as the hyperparameter \u03b1 from before. Our approach to setting these hyperparameters is again (approximate) empirical Bayes\u2014we use variational EM to find maximum likelihood estimates of these parameters based on the marginal likelihood. These procedures are described in Appendix A.4."
        },
        {
            "text": "Iterating these equations to convergence yields an approximate posterior distribution on \u03b2, \u03b8, and **z**.\n\n    We are now left with the hyperparameter \u03b7 on the exchangeable Dirichlet, as well as the hyperparameter \u03b1 from before. Our approach to setting these hyperparameters is again (approximate) empirical Bayes\u2014we use variational EM to find maximum likelihood estimates of these parameters based on the marginal likelihood. These procedures are described in Appendix A.4.",
            "page": 15,
            "x": 88,
            "y": 608,
            "width": 434,
            "height": 67,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "17-smoothing",
            "chunk_id": "de90da31-574a-468e-9c32-a656bd04fb38",
            "group_text": "5.4 Smoothing\n\nThe large vocabulary size that is characteristic of many document corpora creates serious problems of sparsity.  A new document is very likely to contain words that did not appear in any of the documents in a training corpus.  Maximum likelihood estimates of the multinomial parameters assign zero probability to such words, and thus zero probability to new documents.  The standard approach to coping with this problem is to \u201csmooth\u201d the multinomial parameters, assigning positive probability to all vocabulary items whether or not they are observed in the training set (Jelinek, 1997).  Laplace smoothing is commonly used; this essentially yields the mean of the posterior distribution under a uniform Dirichlet prior on the multinomial parameters.\n\nUnfortunately, in the mixture model setting, simple Laplace smoothing is no longer justified as a maximum a posteriori method (although it is often implemented in practice; cf. Nigam et al., 1999). In fact, by placing a Dirichlet prior on the multinomial parameter we obtain an intractable posterior in the mixture model setting, for much the same reason that one obtains an intractable posterior in the basic LDA model. Our proposed solution to this problem is to simply apply variational inference methods to the extended model that includes Dirichlet smoothing on the multinomial parameter.\n\nIn the LDA setting, we obtain the extended graphical model shown in Figure 7. We treat \u03b2 as a $k \\times V$ random matrix (one row for each mixture component), where we assume that each row is independently drawn from an exchangeable Dirichlet distribution.$^2$ We now extend our inference procedures to treat the \u03b2$_i$ as random variables that are endowed with a posterior distribution, conditioned on the data. Thus we move beyond the empirical Bayes procedure of Section 5.3 and consider a fuller Bayesian approach to LDA.\n\nWe consider a variational approach to Bayesian inference that places a separable distribution on the random variables \u03b2, **\u03b8**, and **z** (Attias, 2000):\n\n$q(\\beta_{1:k}, \\mathbf{z}_{1:M}, \\boldsymbol{\\theta}_{1:M} \\mid \\boldsymbol{\\lambda}, \\boldsymbol{\\phi}, \\boldsymbol{\\gamma}) = \\prod_{i=1}^{k} \\mathrm{Dir}(\\beta_i \\mid \\lambda_i) \\prod_{d=1}^{M} q_d(\\theta_d, \\mathbf{z}_d \\mid \\phi_d, \\gamma_d),$\n\nwhere $q_{d}(\\theta, \\mathbf{z} \\mid \\boldsymbol{\\phi}, \\boldsymbol{\\gamma})$ is the variational distribution defined for LDA in Eq. (4). As is easily verified, the resulting variational inference procedure again yields Eqs. (6) and (7) as the update equations for the variational parameters $\\boldsymbol{\\phi}$ and $\\boldsymbol{\\gamma}$, respectively, as well as an additional update for the new variational parameter $\\boldsymbol{\\lambda}$:\n\n$\\lambda_{ij} = \\eta + \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni}^{*} w_{dn}^{j}.$\n\nIterating these equations to convergence yields an approximate posterior distribution on \u03b2, \u03b8, and **z**.\n\n    We are now left with the hyperparameter \u03b7 on the exchangeable Dirichlet, as well as the hyperparameter \u03b1 from before. Our approach to setting these hyperparameters is again (approximate) empirical Bayes\u2014we use variational EM to find maximum likelihood estimates of these parameters based on the marginal likelihood. These procedures are described in Appendix A.4."
        },
        {
            "text": "2. An exchangeable Dirichlet is simply a Dirichlet distribution with a single scalar parameter $\\eta$. The density is the same as a Dirichlet (Eq. 1) where $\\alpha_i = \\eta$ for each component.",
            "page": 15,
            "x": 92,
            "y": 683,
            "width": 429,
            "height": 23,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "18-an",
            "chunk_id": "a3d58fa1-233a-49fa-b890-98180329a962",
            "group_text": "2. An exchangeable Dirichlet is simply a Dirichlet distribution with a single scalar parameter $\\eta$. The density is the same as a Dirichlet (Eq. 1) where $\\alpha_i = \\eta$ for each component."
        },
        {
            "text": "6. Example\n\nIn this section, we provide an illustrative example of the use of an LDA model on real data. Our data are 16,000 documents from a subset of the TREC AP corpus (Harman, 1992). After removing a standard list of stop words, we used the EM algorithm described in Section 5.3 to find the Dirichlet and conditional multinomial parameters for a 100-topic LDA model. The top words from some of the resulting multinomial distributions $p(w|z)$ are illustrated in Figure 8 (top). As we have hoped, these distributions seem to capture some of the underlying topics in the corpus (and we have named them according to these topics).",
            "page": 16,
            "x": 85,
            "y": 88,
            "width": 439,
            "height": 120,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "19-example",
            "chunk_id": "d307b70d-3ce3-41dd-a3f1-aa373c8bbbee",
            "group_text": "6. Example\n\nIn this section, we provide an illustrative example of the use of an LDA model on real data. Our data are 16,000 documents from a subset of the TREC AP corpus (Harman, 1992). After removing a standard list of stop words, we used the EM algorithm described in Section 5.3 to find the Dirichlet and conditional multinomial parameters for a 100-topic LDA model. The top words from some of the resulting multinomial distributions $p(w|z)$ are illustrated in Figure 8 (top). As we have hoped, these distributions seem to capture some of the underlying topics in the corpus (and we have named them according to these topics).\n\nAs we emphasized in Section 4, one of the advantages of LDA over related latent variable models is that it provides well-defined inference procedures for previously unseen documents. Indeed, we can illustrate how LDA works by performing inference on a held-out document and examining the resulting variational posterior parameters.\n\nFigure 8 (bottom) is a document from the TREC AP corpus which was not used for parameter\nestimation. Using the algorithm in Section 5.1, we computed the variational posterior Dirichlet\nparameters $\\gamma$ for the article and variational posterior multinomial parameters $\\phi_n$ for each word in the\narticle.\n\nRecall that the *i*th posterior Dirichlet parameter $\\gamma_i$ is approximately the *i*th prior Dirichlet parameter $\\alpha_i$ plus the expected number of words which were generated by the *i*th topic (see Eq. 7). Therefore, the prior Dirichlet parameters subtracted from the posterior Dirichlet parameters indicate the expected number of words which were allocated to each topic for a particular document. For the example article in Figure 8 (bottom), most of the $\\gamma_i$ are close to $\\alpha_i$. Four topics, however, are significantly larger (by this, we mean $\\gamma_i - \\alpha_i \\geq 1$). Looking at the corresponding distributions over words identifies the topics which mixed to form this document (Figure 8, top).\n\nFurther insight comes from examining the $\\phi_n$ parameters. These distributions approximate $p(z_n \\mid \\mathbf{w})$ and tend to peak towards one of the $k$ possible topic values. In the article text in Figure 8, the words are color coded according to these values (i.e., the $i$th color is used if $q_n(z_n^i = 1) > 0.9$). With this illustration, one can identify how the different topics mixed in the document text.\n\nWhile demonstrating the power of LDA, the posterior analysis also highlights some of its limitations.  In particular, the bag-of-words assumption allows words that should be generated by the same topic (e.g., \u201cWilliam Randolph Hearst Foundation\u201d) to be allocated to several different topics.  Overcoming this limitation would require some form of extension of the basic LDA model; in particular, we might relax the bag-of-words assumption by assuming partial exchangeability or Markovianity of word sequences."
        },
        {
            "text": "As we emphasized in Section 4, one of the advantages of LDA over related latent variable models is that it provides well-defined inference procedures for previously unseen documents. Indeed, we can illustrate how LDA works by performing inference on a held-out document and examining the resulting variational posterior parameters.",
            "page": 16,
            "x": 86,
            "y": 210,
            "width": 437,
            "height": 54,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "19-example",
            "chunk_id": "e0f91aaa-4700-4105-a379-95039fbfd843",
            "group_text": "6. Example\n\nIn this section, we provide an illustrative example of the use of an LDA model on real data. Our data are 16,000 documents from a subset of the TREC AP corpus (Harman, 1992). After removing a standard list of stop words, we used the EM algorithm described in Section 5.3 to find the Dirichlet and conditional multinomial parameters for a 100-topic LDA model. The top words from some of the resulting multinomial distributions $p(w|z)$ are illustrated in Figure 8 (top). As we have hoped, these distributions seem to capture some of the underlying topics in the corpus (and we have named them according to these topics).\n\nAs we emphasized in Section 4, one of the advantages of LDA over related latent variable models is that it provides well-defined inference procedures for previously unseen documents. Indeed, we can illustrate how LDA works by performing inference on a held-out document and examining the resulting variational posterior parameters.\n\nFigure 8 (bottom) is a document from the TREC AP corpus which was not used for parameter\nestimation. Using the algorithm in Section 5.1, we computed the variational posterior Dirichlet\nparameters $\\gamma$ for the article and variational posterior multinomial parameters $\\phi_n$ for each word in the\narticle.\n\nRecall that the *i*th posterior Dirichlet parameter $\\gamma_i$ is approximately the *i*th prior Dirichlet parameter $\\alpha_i$ plus the expected number of words which were generated by the *i*th topic (see Eq. 7). Therefore, the prior Dirichlet parameters subtracted from the posterior Dirichlet parameters indicate the expected number of words which were allocated to each topic for a particular document. For the example article in Figure 8 (bottom), most of the $\\gamma_i$ are close to $\\alpha_i$. Four topics, however, are significantly larger (by this, we mean $\\gamma_i - \\alpha_i \\geq 1$). Looking at the corresponding distributions over words identifies the topics which mixed to form this document (Figure 8, top).\n\nFurther insight comes from examining the $\\phi_n$ parameters. These distributions approximate $p(z_n \\mid \\mathbf{w})$ and tend to peak towards one of the $k$ possible topic values. In the article text in Figure 8, the words are color coded according to these values (i.e., the $i$th color is used if $q_n(z_n^i = 1) > 0.9$). With this illustration, one can identify how the different topics mixed in the document text.\n\nWhile demonstrating the power of LDA, the posterior analysis also highlights some of its limitations.  In particular, the bag-of-words assumption allows words that should be generated by the same topic (e.g., \u201cWilliam Randolph Hearst Foundation\u201d) to be allocated to several different topics.  Overcoming this limitation would require some form of extension of the basic LDA model; in particular, we might relax the bag-of-words assumption by assuming partial exchangeability or Markovianity of word sequences."
        },
        {
            "text": "Figure 8 (bottom) is a document from the TREC AP corpus which was not used for parameter\nestimation. Using the algorithm in Section 5.1, we computed the variational posterior Dirichlet\nparameters $\\gamma$ for the article and variational posterior multinomial parameters $\\phi_n$ for each word in the\narticle.",
            "page": 16,
            "x": 87,
            "y": 265,
            "width": 436,
            "height": 53,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "19-example",
            "chunk_id": "bdf02db4-c655-4164-be64-39d392e5f8c9",
            "group_text": "6. Example\n\nIn this section, we provide an illustrative example of the use of an LDA model on real data. Our data are 16,000 documents from a subset of the TREC AP corpus (Harman, 1992). After removing a standard list of stop words, we used the EM algorithm described in Section 5.3 to find the Dirichlet and conditional multinomial parameters for a 100-topic LDA model. The top words from some of the resulting multinomial distributions $p(w|z)$ are illustrated in Figure 8 (top). As we have hoped, these distributions seem to capture some of the underlying topics in the corpus (and we have named them according to these topics).\n\nAs we emphasized in Section 4, one of the advantages of LDA over related latent variable models is that it provides well-defined inference procedures for previously unseen documents. Indeed, we can illustrate how LDA works by performing inference on a held-out document and examining the resulting variational posterior parameters.\n\nFigure 8 (bottom) is a document from the TREC AP corpus which was not used for parameter\nestimation. Using the algorithm in Section 5.1, we computed the variational posterior Dirichlet\nparameters $\\gamma$ for the article and variational posterior multinomial parameters $\\phi_n$ for each word in the\narticle.\n\nRecall that the *i*th posterior Dirichlet parameter $\\gamma_i$ is approximately the *i*th prior Dirichlet parameter $\\alpha_i$ plus the expected number of words which were generated by the *i*th topic (see Eq. 7). Therefore, the prior Dirichlet parameters subtracted from the posterior Dirichlet parameters indicate the expected number of words which were allocated to each topic for a particular document. For the example article in Figure 8 (bottom), most of the $\\gamma_i$ are close to $\\alpha_i$. Four topics, however, are significantly larger (by this, we mean $\\gamma_i - \\alpha_i \\geq 1$). Looking at the corresponding distributions over words identifies the topics which mixed to form this document (Figure 8, top).\n\nFurther insight comes from examining the $\\phi_n$ parameters. These distributions approximate $p(z_n \\mid \\mathbf{w})$ and tend to peak towards one of the $k$ possible topic values. In the article text in Figure 8, the words are color coded according to these values (i.e., the $i$th color is used if $q_n(z_n^i = 1) > 0.9$). With this illustration, one can identify how the different topics mixed in the document text.\n\nWhile demonstrating the power of LDA, the posterior analysis also highlights some of its limitations.  In particular, the bag-of-words assumption allows words that should be generated by the same topic (e.g., \u201cWilliam Randolph Hearst Foundation\u201d) to be allocated to several different topics.  Overcoming this limitation would require some form of extension of the basic LDA model; in particular, we might relax the bag-of-words assumption by assuming partial exchangeability or Markovianity of word sequences."
        },
        {
            "text": "Recall that the *i*th posterior Dirichlet parameter $\\gamma_i$ is approximately the *i*th prior Dirichlet parameter $\\alpha_i$ plus the expected number of words which were generated by the *i*th topic (see Eq. 7). Therefore, the prior Dirichlet parameters subtracted from the posterior Dirichlet parameters indicate the expected number of words which were allocated to each topic for a particular document. For the example article in Figure 8 (bottom), most of the $\\gamma_i$ are close to $\\alpha_i$. Four topics, however, are significantly larger (by this, we mean $\\gamma_i - \\alpha_i \\geq 1$). Looking at the corresponding distributions over words identifies the topics which mixed to form this document (Figure 8, top).",
            "page": 16,
            "x": 87,
            "y": 321,
            "width": 436,
            "height": 94,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "19-example",
            "chunk_id": "41fad3f8-0e6d-4d12-86ba-36e0551f43c7",
            "group_text": "6. Example\n\nIn this section, we provide an illustrative example of the use of an LDA model on real data. Our data are 16,000 documents from a subset of the TREC AP corpus (Harman, 1992). After removing a standard list of stop words, we used the EM algorithm described in Section 5.3 to find the Dirichlet and conditional multinomial parameters for a 100-topic LDA model. The top words from some of the resulting multinomial distributions $p(w|z)$ are illustrated in Figure 8 (top). As we have hoped, these distributions seem to capture some of the underlying topics in the corpus (and we have named them according to these topics).\n\nAs we emphasized in Section 4, one of the advantages of LDA over related latent variable models is that it provides well-defined inference procedures for previously unseen documents. Indeed, we can illustrate how LDA works by performing inference on a held-out document and examining the resulting variational posterior parameters.\n\nFigure 8 (bottom) is a document from the TREC AP corpus which was not used for parameter\nestimation. Using the algorithm in Section 5.1, we computed the variational posterior Dirichlet\nparameters $\\gamma$ for the article and variational posterior multinomial parameters $\\phi_n$ for each word in the\narticle.\n\nRecall that the *i*th posterior Dirichlet parameter $\\gamma_i$ is approximately the *i*th prior Dirichlet parameter $\\alpha_i$ plus the expected number of words which were generated by the *i*th topic (see Eq. 7). Therefore, the prior Dirichlet parameters subtracted from the posterior Dirichlet parameters indicate the expected number of words which were allocated to each topic for a particular document. For the example article in Figure 8 (bottom), most of the $\\gamma_i$ are close to $\\alpha_i$. Four topics, however, are significantly larger (by this, we mean $\\gamma_i - \\alpha_i \\geq 1$). Looking at the corresponding distributions over words identifies the topics which mixed to form this document (Figure 8, top).\n\nFurther insight comes from examining the $\\phi_n$ parameters. These distributions approximate $p(z_n \\mid \\mathbf{w})$ and tend to peak towards one of the $k$ possible topic values. In the article text in Figure 8, the words are color coded according to these values (i.e., the $i$th color is used if $q_n(z_n^i = 1) > 0.9$). With this illustration, one can identify how the different topics mixed in the document text.\n\nWhile demonstrating the power of LDA, the posterior analysis also highlights some of its limitations.  In particular, the bag-of-words assumption allows words that should be generated by the same topic (e.g., \u201cWilliam Randolph Hearst Foundation\u201d) to be allocated to several different topics.  Overcoming this limitation would require some form of extension of the basic LDA model; in particular, we might relax the bag-of-words assumption by assuming partial exchangeability or Markovianity of word sequences."
        },
        {
            "text": "Further insight comes from examining the $\\phi_n$ parameters. These distributions approximate $p(z_n \\mid \\mathbf{w})$ and tend to peak towards one of the $k$ possible topic values. In the article text in Figure 8, the words are color coded according to these values (i.e., the $i$th color is used if $q_n(z_n^i = 1) > 0.9$). With this illustration, one can identify how the different topics mixed in the document text.",
            "page": 16,
            "x": 88,
            "y": 417,
            "width": 435,
            "height": 55,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "19-example",
            "chunk_id": "e8b98694-6683-4364-af95-1e25b293a4ad",
            "group_text": "6. Example\n\nIn this section, we provide an illustrative example of the use of an LDA model on real data. Our data are 16,000 documents from a subset of the TREC AP corpus (Harman, 1992). After removing a standard list of stop words, we used the EM algorithm described in Section 5.3 to find the Dirichlet and conditional multinomial parameters for a 100-topic LDA model. The top words from some of the resulting multinomial distributions $p(w|z)$ are illustrated in Figure 8 (top). As we have hoped, these distributions seem to capture some of the underlying topics in the corpus (and we have named them according to these topics).\n\nAs we emphasized in Section 4, one of the advantages of LDA over related latent variable models is that it provides well-defined inference procedures for previously unseen documents. Indeed, we can illustrate how LDA works by performing inference on a held-out document and examining the resulting variational posterior parameters.\n\nFigure 8 (bottom) is a document from the TREC AP corpus which was not used for parameter\nestimation. Using the algorithm in Section 5.1, we computed the variational posterior Dirichlet\nparameters $\\gamma$ for the article and variational posterior multinomial parameters $\\phi_n$ for each word in the\narticle.\n\nRecall that the *i*th posterior Dirichlet parameter $\\gamma_i$ is approximately the *i*th prior Dirichlet parameter $\\alpha_i$ plus the expected number of words which were generated by the *i*th topic (see Eq. 7). Therefore, the prior Dirichlet parameters subtracted from the posterior Dirichlet parameters indicate the expected number of words which were allocated to each topic for a particular document. For the example article in Figure 8 (bottom), most of the $\\gamma_i$ are close to $\\alpha_i$. Four topics, however, are significantly larger (by this, we mean $\\gamma_i - \\alpha_i \\geq 1$). Looking at the corresponding distributions over words identifies the topics which mixed to form this document (Figure 8, top).\n\nFurther insight comes from examining the $\\phi_n$ parameters. These distributions approximate $p(z_n \\mid \\mathbf{w})$ and tend to peak towards one of the $k$ possible topic values. In the article text in Figure 8, the words are color coded according to these values (i.e., the $i$th color is used if $q_n(z_n^i = 1) > 0.9$). With this illustration, one can identify how the different topics mixed in the document text.\n\nWhile demonstrating the power of LDA, the posterior analysis also highlights some of its limitations.  In particular, the bag-of-words assumption allows words that should be generated by the same topic (e.g., \u201cWilliam Randolph Hearst Foundation\u201d) to be allocated to several different topics.  Overcoming this limitation would require some form of extension of the basic LDA model; in particular, we might relax the bag-of-words assumption by assuming partial exchangeability or Markovianity of word sequences."
        },
        {
            "text": "While demonstrating the power of LDA, the posterior analysis also highlights some of its limitations.  In particular, the bag-of-words assumption allows words that should be generated by the same topic (e.g., \u201cWilliam Randolph Hearst Foundation\u201d) to be allocated to several different topics.  Overcoming this limitation would require some form of extension of the basic LDA model; in particular, we might relax the bag-of-words assumption by assuming partial exchangeability or Markovianity of word sequences.",
            "page": 16,
            "x": 88,
            "y": 473,
            "width": 435,
            "height": 81,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "19-example",
            "chunk_id": "08a5771b-dd05-498f-9edb-e2de39d12e29",
            "group_text": "6. Example\n\nIn this section, we provide an illustrative example of the use of an LDA model on real data. Our data are 16,000 documents from a subset of the TREC AP corpus (Harman, 1992). After removing a standard list of stop words, we used the EM algorithm described in Section 5.3 to find the Dirichlet and conditional multinomial parameters for a 100-topic LDA model. The top words from some of the resulting multinomial distributions $p(w|z)$ are illustrated in Figure 8 (top). As we have hoped, these distributions seem to capture some of the underlying topics in the corpus (and we have named them according to these topics).\n\nAs we emphasized in Section 4, one of the advantages of LDA over related latent variable models is that it provides well-defined inference procedures for previously unseen documents. Indeed, we can illustrate how LDA works by performing inference on a held-out document and examining the resulting variational posterior parameters.\n\nFigure 8 (bottom) is a document from the TREC AP corpus which was not used for parameter\nestimation. Using the algorithm in Section 5.1, we computed the variational posterior Dirichlet\nparameters $\\gamma$ for the article and variational posterior multinomial parameters $\\phi_n$ for each word in the\narticle.\n\nRecall that the *i*th posterior Dirichlet parameter $\\gamma_i$ is approximately the *i*th prior Dirichlet parameter $\\alpha_i$ plus the expected number of words which were generated by the *i*th topic (see Eq. 7). Therefore, the prior Dirichlet parameters subtracted from the posterior Dirichlet parameters indicate the expected number of words which were allocated to each topic for a particular document. For the example article in Figure 8 (bottom), most of the $\\gamma_i$ are close to $\\alpha_i$. Four topics, however, are significantly larger (by this, we mean $\\gamma_i - \\alpha_i \\geq 1$). Looking at the corresponding distributions over words identifies the topics which mixed to form this document (Figure 8, top).\n\nFurther insight comes from examining the $\\phi_n$ parameters. These distributions approximate $p(z_n \\mid \\mathbf{w})$ and tend to peak towards one of the $k$ possible topic values. In the article text in Figure 8, the words are color coded according to these values (i.e., the $i$th color is used if $q_n(z_n^i = 1) > 0.9$). With this illustration, one can identify how the different topics mixed in the document text.\n\nWhile demonstrating the power of LDA, the posterior analysis also highlights some of its limitations.  In particular, the bag-of-words assumption allows words that should be generated by the same topic (e.g., \u201cWilliam Randolph Hearst Foundation\u201d) to be allocated to several different topics.  Overcoming this limitation would require some form of extension of the basic LDA model; in particular, we might relax the bag-of-words assumption by assuming partial exchangeability or Markovianity of word sequences."
        },
        {
            "text": "## 7. Applications and Empirical Results\n\nIn this section, we discuss our empirical evaluation of LDA in several problem domains\u2014document modeling, document classification, and collaborative filtering.",
            "page": 16,
            "x": 87,
            "y": 572,
            "width": 433,
            "height": 50,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "20-applications",
            "chunk_id": "8ccf0d6d-8015-4f74-a05c-6df58aa685a8",
            "group_text": "## 7. Applications and Empirical Results\n\nIn this section, we discuss our empirical evaluation of LDA in several problem domains\u2014document modeling, document classification, and collaborative filtering.\n\nIn all of the mixture models, the expected complete log likelihood of the data has local maxima at the points where all or some of the mixture components are equal to each other. To avoid these local maxima, it is important to initialize the EM algorithm appropriately. In our experiments, we initialize EM by seeding each conditional multinomial distribution with five documents, reducing their effective total length to two words, and smoothing across the whole vocabulary. This is essentially an approximation to the scheme described in Heckerman and Meila (2001).\n\nThe William Randolph Hearst Foundation will give $1.25 million to Lincoln Center, Metropolitan Opera Co., New York Philharmonic and Juilliard School. \u201cOur board felt that we had a real opportunity to make a mark on the future of the performing arts with these grants an act every bit as important as our traditional areas of support in health, medical research, education and the social services,\u201d Hearst Foundation President Randolph A. Hearst said Monday in announcing the grants. Lincoln Center\u2019s share will be $200,000 for its new building, which will house young artists and provide new public facilities. The Metropolitan Opera Co. and New York Philharmonic will receive $400,000 each. The Juilliard School, where music and the performing arts are taught, will get $250,000. The Hearst Foundation, a leading supporter of the Lincoln Center Consolidated Corporate Fund, will make its usual annual $100,000 donation, too.\n\nFigure 8: An example article from the AP corpus. Each color codes a different factor from which the word is putatively generated.\n\nTable 1: Overfitting in the mixture of unigrams and pLSI models for the AP corpus. Similar behavior is observed in the nematode corpus (not reported)."
        },
        {
            "text": "In all of the mixture models, the expected complete log likelihood of the data has local maxima at the points where all or some of the mixture components are equal to each other. To avoid these local maxima, it is important to initialize the EM algorithm appropriately. In our experiments, we initialize EM by seeding each conditional multinomial distribution with five documents, reducing their effective total length to two words, and smoothing across the whole vocabulary. This is essentially an approximation to the scheme described in Heckerman and Meila (2001).",
            "page": 16,
            "x": 88,
            "y": 625,
            "width": 434,
            "height": 80,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "20-applications",
            "chunk_id": "e840f856-3add-418c-b953-2599ba53fd1f",
            "group_text": "## 7. Applications and Empirical Results\n\nIn this section, we discuss our empirical evaluation of LDA in several problem domains\u2014document modeling, document classification, and collaborative filtering.\n\nIn all of the mixture models, the expected complete log likelihood of the data has local maxima at the points where all or some of the mixture components are equal to each other. To avoid these local maxima, it is important to initialize the EM algorithm appropriately. In our experiments, we initialize EM by seeding each conditional multinomial distribution with five documents, reducing their effective total length to two words, and smoothing across the whole vocabulary. This is essentially an approximation to the scheme described in Heckerman and Meila (2001).\n\nThe William Randolph Hearst Foundation will give $1.25 million to Lincoln Center, Metropolitan Opera Co., New York Philharmonic and Juilliard School. \u201cOur board felt that we had a real opportunity to make a mark on the future of the performing arts with these grants an act every bit as important as our traditional areas of support in health, medical research, education and the social services,\u201d Hearst Foundation President Randolph A. Hearst said Monday in announcing the grants. Lincoln Center\u2019s share will be $200,000 for its new building, which will house young artists and provide new public facilities. The Metropolitan Opera Co. and New York Philharmonic will receive $400,000 each. The Juilliard School, where music and the performing arts are taught, will get $250,000. The Hearst Foundation, a leading supporter of the Lincoln Center Consolidated Corporate Fund, will make its usual annual $100,000 donation, too.\n\nFigure 8: An example article from the AP corpus. Each color codes a different factor from which the word is putatively generated.\n\nTable 1: Overfitting in the mixture of unigrams and pLSI models for the AP corpus. Similar behavior is observed in the nematode corpus (not reported)."
        },
        {
            "text": "The William Randolph Hearst Foundation will give $1.25 million to Lincoln Center, Metropolitan Opera Co., New York Philharmonic and Juilliard School. \u201cOur board felt that we had a real opportunity to make a mark on the future of the performing arts with these grants an act every bit as important as our traditional areas of support in health, medical research, education and the social services,\u201d Hearst Foundation President Randolph A. Hearst said Monday in announcing the grants. Lincoln Center\u2019s share will be $200,000 for its new building, which will house young artists and provide new public facilities. The Metropolitan Opera Co. and New York Philharmonic will receive $400,000 each. The Juilliard School, where music and the performing arts are taught, will get $250,000. The Hearst Foundation, a leading supporter of the Lincoln Center Consolidated Corporate Fund, will make its usual annual $100,000 donation, too.",
            "page": 17,
            "x": 94,
            "y": 424,
            "width": 443,
            "height": 154,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "20-applications",
            "chunk_id": "e5e5d389-4171-4c04-9e95-7a4cdab4a880",
            "group_text": "## 7. Applications and Empirical Results\n\nIn this section, we discuss our empirical evaluation of LDA in several problem domains\u2014document modeling, document classification, and collaborative filtering.\n\nIn all of the mixture models, the expected complete log likelihood of the data has local maxima at the points where all or some of the mixture components are equal to each other. To avoid these local maxima, it is important to initialize the EM algorithm appropriately. In our experiments, we initialize EM by seeding each conditional multinomial distribution with five documents, reducing their effective total length to two words, and smoothing across the whole vocabulary. This is essentially an approximation to the scheme described in Heckerman and Meila (2001).\n\nThe William Randolph Hearst Foundation will give $1.25 million to Lincoln Center, Metropolitan Opera Co., New York Philharmonic and Juilliard School. \u201cOur board felt that we had a real opportunity to make a mark on the future of the performing arts with these grants an act every bit as important as our traditional areas of support in health, medical research, education and the social services,\u201d Hearst Foundation President Randolph A. Hearst said Monday in announcing the grants. Lincoln Center\u2019s share will be $200,000 for its new building, which will house young artists and provide new public facilities. The Metropolitan Opera Co. and New York Philharmonic will receive $400,000 each. The Juilliard School, where music and the performing arts are taught, will get $250,000. The Hearst Foundation, a leading supporter of the Lincoln Center Consolidated Corporate Fund, will make its usual annual $100,000 donation, too.\n\nFigure 8: An example article from the AP corpus. Each color codes a different factor from which the word is putatively generated.\n\nTable 1: Overfitting in the mixture of unigrams and pLSI models for the AP corpus. Similar behavior is observed in the nematode corpus (not reported)."
        },
        {
            "text": "Figure 8: An example article from the AP corpus. Each color codes a different factor from which the word is putatively generated.",
            "page": 17,
            "x": 86,
            "y": 586,
            "width": 436,
            "height": 29,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "20-applications",
            "chunk_id": "52eec556-3034-4bcf-8eb6-f3000970fc69",
            "group_text": "## 7. Applications and Empirical Results\n\nIn this section, we discuss our empirical evaluation of LDA in several problem domains\u2014document modeling, document classification, and collaborative filtering.\n\nIn all of the mixture models, the expected complete log likelihood of the data has local maxima at the points where all or some of the mixture components are equal to each other. To avoid these local maxima, it is important to initialize the EM algorithm appropriately. In our experiments, we initialize EM by seeding each conditional multinomial distribution with five documents, reducing their effective total length to two words, and smoothing across the whole vocabulary. This is essentially an approximation to the scheme described in Heckerman and Meila (2001).\n\nThe William Randolph Hearst Foundation will give $1.25 million to Lincoln Center, Metropolitan Opera Co., New York Philharmonic and Juilliard School. \u201cOur board felt that we had a real opportunity to make a mark on the future of the performing arts with these grants an act every bit as important as our traditional areas of support in health, medical research, education and the social services,\u201d Hearst Foundation President Randolph A. Hearst said Monday in announcing the grants. Lincoln Center\u2019s share will be $200,000 for its new building, which will house young artists and provide new public facilities. The Metropolitan Opera Co. and New York Philharmonic will receive $400,000 each. The Juilliard School, where music and the performing arts are taught, will get $250,000. The Hearst Foundation, a leading supporter of the Lincoln Center Consolidated Corporate Fund, will make its usual annual $100,000 donation, too.\n\nFigure 8: An example article from the AP corpus. Each color codes a different factor from which the word is putatively generated.\n\nTable 1: Overfitting in the mixture of unigrams and pLSI models for the AP corpus. Similar behavior is observed in the nematode corpus (not reported)."
        },
        {
            "text": "Table 1: Overfitting in the mixture of unigrams and pLSI models for the AP corpus. Similar behavior is observed in the nematode corpus (not reported).",
            "page": 18,
            "x": 86,
            "y": 223,
            "width": 436,
            "height": 29,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "20-applications",
            "chunk_id": "2621473b-5330-4993-a200-7ae768c488ce",
            "group_text": "## 7. Applications and Empirical Results\n\nIn this section, we discuss our empirical evaluation of LDA in several problem domains\u2014document modeling, document classification, and collaborative filtering.\n\nIn all of the mixture models, the expected complete log likelihood of the data has local maxima at the points where all or some of the mixture components are equal to each other. To avoid these local maxima, it is important to initialize the EM algorithm appropriately. In our experiments, we initialize EM by seeding each conditional multinomial distribution with five documents, reducing their effective total length to two words, and smoothing across the whole vocabulary. This is essentially an approximation to the scheme described in Heckerman and Meila (2001).\n\nThe William Randolph Hearst Foundation will give $1.25 million to Lincoln Center, Metropolitan Opera Co., New York Philharmonic and Juilliard School. \u201cOur board felt that we had a real opportunity to make a mark on the future of the performing arts with these grants an act every bit as important as our traditional areas of support in health, medical research, education and the social services,\u201d Hearst Foundation President Randolph A. Hearst said Monday in announcing the grants. Lincoln Center\u2019s share will be $200,000 for its new building, which will house young artists and provide new public facilities. The Metropolitan Opera Co. and New York Philharmonic will receive $400,000 each. The Juilliard School, where music and the performing arts are taught, will get $250,000. The Hearst Foundation, a leading supporter of the Lincoln Center Consolidated Corporate Fund, will make its usual annual $100,000 donation, too.\n\nFigure 8: An example article from the AP corpus. Each color codes a different factor from which the word is putatively generated.\n\nTable 1: Overfitting in the mixture of unigrams and pLSI models for the AP corpus. Similar behavior is observed in the nematode corpus (not reported)."
        },
        {
            "text": "7.1 Document modeling\n\nWe trained a number of latent variable models, including LDA, on two text corpora to compare the generalization performance of these models. The documents in the corpora are treated as unlabeled; thus, our goal is density estimation\u2014we wish to achieve high likelihood on a held-out test set. In particular, we computed the *perplexity* of a held-out test set to evaluate the models. The perplexity, used by convention in language modeling, is monotonically decreasing in the likelihood of the test data, and is algebraically equivalent to the inverse of the geometric mean per-word likelihood. A lower perplexity score indicates better generalization performance.\u00b3 More formally, for a test set of *M* documents, the perplexity is:",
            "page": 18,
            "x": 86,
            "y": 281,
            "width": 438,
            "height": 128,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "21-document",
            "chunk_id": "0e13949f-333c-4701-ad9b-9328ab786656",
            "group_text": "7.1 Document modeling\n\nWe trained a number of latent variable models, including LDA, on two text corpora to compare the generalization performance of these models. The documents in the corpora are treated as unlabeled; thus, our goal is density estimation\u2014we wish to achieve high likelihood on a held-out test set. In particular, we computed the *perplexity* of a held-out test set to evaluate the models. The perplexity, used by convention in language modeling, is monotonically decreasing in the likelihood of the test data, and is algebraically equivalent to the inverse of the geometric mean per-word likelihood. A lower perplexity score indicates better generalization performance.\u00b3 More formally, for a test set of *M* documents, the perplexity is:\n\n$perplexity(\\mathcal{D}_{\\text{test}}) = \\exp \\left\\{ -\\frac{\\sum_{d=1}^M \\log p(\\mathbf{w}_d)}{\\sum_{d=1}^M N_d} \\right\\}.$\n\nIn our experiments, we used a corpus of scientific abstracts from the C. Elegans community (Avery, 2002) containing 5,225 abstracts with 28,414 unique terms, and a subset of the TREC AP corpus containing 16,333 newswire articles with 23,075 unique terms. In both cases, we held out 10% of the data for test purposes and trained the models on the remaining 90%. In preprocessing the data, we removed a standard list of 50 stop words from each corpus. From the AP data, we further removed words that occurred only once.\n\nWe compared LDA with the unigram, mixture of unigrams, and pLSI models described in Section 4. We trained all the hidden variable models using EM with exactly the same stopping criteria, that the average change in expected log likelihood is less than 0.001%.\n\nBoth the pLSI model and the mixture of unigrams suffer from serious overfitting issues, though\nfor different reasons. This phenomenon is illustrated in Table 1. In the mixture of unigrams model,\noverfitting is a result of peaked posteriors in the training set; a phenomenon familiar in the super-\nvised setting, where this model is known as the naive Bayes model (Rennie, 2001). This leads to a"
        },
        {
            "text": "$perplexity(\\mathcal{D}_{\\text{test}}) = \\exp \\left\\{ -\\frac{\\sum_{d=1}^M \\log p(\\mathbf{w}_d)}{\\sum_{d=1}^M N_d} \\right\\}.$",
            "page": 18,
            "x": 202,
            "y": 418,
            "width": 206,
            "height": 34,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "21-document",
            "chunk_id": "31be9afc-d569-44fb-9e84-df665bfb17ec",
            "group_text": "7.1 Document modeling\n\nWe trained a number of latent variable models, including LDA, on two text corpora to compare the generalization performance of these models. The documents in the corpora are treated as unlabeled; thus, our goal is density estimation\u2014we wish to achieve high likelihood on a held-out test set. In particular, we computed the *perplexity* of a held-out test set to evaluate the models. The perplexity, used by convention in language modeling, is monotonically decreasing in the likelihood of the test data, and is algebraically equivalent to the inverse of the geometric mean per-word likelihood. A lower perplexity score indicates better generalization performance.\u00b3 More formally, for a test set of *M* documents, the perplexity is:\n\n$perplexity(\\mathcal{D}_{\\text{test}}) = \\exp \\left\\{ -\\frac{\\sum_{d=1}^M \\log p(\\mathbf{w}_d)}{\\sum_{d=1}^M N_d} \\right\\}.$\n\nIn our experiments, we used a corpus of scientific abstracts from the C. Elegans community (Avery, 2002) containing 5,225 abstracts with 28,414 unique terms, and a subset of the TREC AP corpus containing 16,333 newswire articles with 23,075 unique terms. In both cases, we held out 10% of the data for test purposes and trained the models on the remaining 90%. In preprocessing the data, we removed a standard list of 50 stop words from each corpus. From the AP data, we further removed words that occurred only once.\n\nWe compared LDA with the unigram, mixture of unigrams, and pLSI models described in Section 4. We trained all the hidden variable models using EM with exactly the same stopping criteria, that the average change in expected log likelihood is less than 0.001%.\n\nBoth the pLSI model and the mixture of unigrams suffer from serious overfitting issues, though\nfor different reasons. This phenomenon is illustrated in Table 1. In the mixture of unigrams model,\noverfitting is a result of peaked posteriors in the training set; a phenomenon familiar in the super-\nvised setting, where this model is known as the naive Bayes model (Rennie, 2001). This leads to a"
        },
        {
            "text": "In our experiments, we used a corpus of scientific abstracts from the C. Elegans community (Avery, 2002) containing 5,225 abstracts with 28,414 unique terms, and a subset of the TREC AP corpus containing 16,333 newswire articles with 23,075 unique terms. In both cases, we held out 10% of the data for test purposes and trained the models on the remaining 90%. In preprocessing the data, we removed a standard list of 50 stop words from each corpus. From the AP data, we further removed words that occurred only once.",
            "page": 18,
            "x": 87,
            "y": 457,
            "width": 436,
            "height": 80,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "21-document",
            "chunk_id": "21f98743-f2d0-4d36-883d-4e21da0efeee",
            "group_text": "7.1 Document modeling\n\nWe trained a number of latent variable models, including LDA, on two text corpora to compare the generalization performance of these models. The documents in the corpora are treated as unlabeled; thus, our goal is density estimation\u2014we wish to achieve high likelihood on a held-out test set. In particular, we computed the *perplexity* of a held-out test set to evaluate the models. The perplexity, used by convention in language modeling, is monotonically decreasing in the likelihood of the test data, and is algebraically equivalent to the inverse of the geometric mean per-word likelihood. A lower perplexity score indicates better generalization performance.\u00b3 More formally, for a test set of *M* documents, the perplexity is:\n\n$perplexity(\\mathcal{D}_{\\text{test}}) = \\exp \\left\\{ -\\frac{\\sum_{d=1}^M \\log p(\\mathbf{w}_d)}{\\sum_{d=1}^M N_d} \\right\\}.$\n\nIn our experiments, we used a corpus of scientific abstracts from the C. Elegans community (Avery, 2002) containing 5,225 abstracts with 28,414 unique terms, and a subset of the TREC AP corpus containing 16,333 newswire articles with 23,075 unique terms. In both cases, we held out 10% of the data for test purposes and trained the models on the remaining 90%. In preprocessing the data, we removed a standard list of 50 stop words from each corpus. From the AP data, we further removed words that occurred only once.\n\nWe compared LDA with the unigram, mixture of unigrams, and pLSI models described in Section 4. We trained all the hidden variable models using EM with exactly the same stopping criteria, that the average change in expected log likelihood is less than 0.001%.\n\nBoth the pLSI model and the mixture of unigrams suffer from serious overfitting issues, though\nfor different reasons. This phenomenon is illustrated in Table 1. In the mixture of unigrams model,\noverfitting is a result of peaked posteriors in the training set; a phenomenon familiar in the super-\nvised setting, where this model is known as the naive Bayes model (Rennie, 2001). This leads to a"
        },
        {
            "text": "We compared LDA with the unigram, mixture of unigrams, and pLSI models described in Section 4. We trained all the hidden variable models using EM with exactly the same stopping criteria, that the average change in expected log likelihood is less than 0.001%.",
            "page": 18,
            "x": 87,
            "y": 539,
            "width": 435,
            "height": 40,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "21-document",
            "chunk_id": "230eaa8b-5650-42e4-9462-a83e59cf9881",
            "group_text": "7.1 Document modeling\n\nWe trained a number of latent variable models, including LDA, on two text corpora to compare the generalization performance of these models. The documents in the corpora are treated as unlabeled; thus, our goal is density estimation\u2014we wish to achieve high likelihood on a held-out test set. In particular, we computed the *perplexity* of a held-out test set to evaluate the models. The perplexity, used by convention in language modeling, is monotonically decreasing in the likelihood of the test data, and is algebraically equivalent to the inverse of the geometric mean per-word likelihood. A lower perplexity score indicates better generalization performance.\u00b3 More formally, for a test set of *M* documents, the perplexity is:\n\n$perplexity(\\mathcal{D}_{\\text{test}}) = \\exp \\left\\{ -\\frac{\\sum_{d=1}^M \\log p(\\mathbf{w}_d)}{\\sum_{d=1}^M N_d} \\right\\}.$\n\nIn our experiments, we used a corpus of scientific abstracts from the C. Elegans community (Avery, 2002) containing 5,225 abstracts with 28,414 unique terms, and a subset of the TREC AP corpus containing 16,333 newswire articles with 23,075 unique terms. In both cases, we held out 10% of the data for test purposes and trained the models on the remaining 90%. In preprocessing the data, we removed a standard list of 50 stop words from each corpus. From the AP data, we further removed words that occurred only once.\n\nWe compared LDA with the unigram, mixture of unigrams, and pLSI models described in Section 4. We trained all the hidden variable models using EM with exactly the same stopping criteria, that the average change in expected log likelihood is less than 0.001%.\n\nBoth the pLSI model and the mixture of unigrams suffer from serious overfitting issues, though\nfor different reasons. This phenomenon is illustrated in Table 1. In the mixture of unigrams model,\noverfitting is a result of peaked posteriors in the training set; a phenomenon familiar in the super-\nvised setting, where this model is known as the naive Bayes model (Rennie, 2001). This leads to a"
        },
        {
            "text": "Both the pLSI model and the mixture of unigrams suffer from serious overfitting issues, though\nfor different reasons. This phenomenon is illustrated in Table 1. In the mixture of unigrams model,\noverfitting is a result of peaked posteriors in the training set; a phenomenon familiar in the super-\nvised setting, where this model is known as the naive Bayes model (Rennie, 2001). This leads to a",
            "page": 18,
            "x": 88,
            "y": 580,
            "width": 435,
            "height": 54,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "21-document",
            "chunk_id": "8c7175cc-21af-4beb-b0b1-37e549818b44",
            "group_text": "7.1 Document modeling\n\nWe trained a number of latent variable models, including LDA, on two text corpora to compare the generalization performance of these models. The documents in the corpora are treated as unlabeled; thus, our goal is density estimation\u2014we wish to achieve high likelihood on a held-out test set. In particular, we computed the *perplexity* of a held-out test set to evaluate the models. The perplexity, used by convention in language modeling, is monotonically decreasing in the likelihood of the test data, and is algebraically equivalent to the inverse of the geometric mean per-word likelihood. A lower perplexity score indicates better generalization performance.\u00b3 More formally, for a test set of *M* documents, the perplexity is:\n\n$perplexity(\\mathcal{D}_{\\text{test}}) = \\exp \\left\\{ -\\frac{\\sum_{d=1}^M \\log p(\\mathbf{w}_d)}{\\sum_{d=1}^M N_d} \\right\\}.$\n\nIn our experiments, we used a corpus of scientific abstracts from the C. Elegans community (Avery, 2002) containing 5,225 abstracts with 28,414 unique terms, and a subset of the TREC AP corpus containing 16,333 newswire articles with 23,075 unique terms. In both cases, we held out 10% of the data for test purposes and trained the models on the remaining 90%. In preprocessing the data, we removed a standard list of 50 stop words from each corpus. From the AP data, we further removed words that occurred only once.\n\nWe compared LDA with the unigram, mixture of unigrams, and pLSI models described in Section 4. We trained all the hidden variable models using EM with exactly the same stopping criteria, that the average change in expected log likelihood is less than 0.001%.\n\nBoth the pLSI model and the mixture of unigrams suffer from serious overfitting issues, though\nfor different reasons. This phenomenon is illustrated in Table 1. In the mixture of unigrams model,\noverfitting is a result of peaked posteriors in the training set; a phenomenon familiar in the super-\nvised setting, where this model is known as the naive Bayes model (Rennie, 2001). This leads to a"
        },
        {
            "text": "3. Note that we simply use perplexity as a figure of merit for comparing models. The models that we compare are all unigram (\u201cbag-of-words\u201d) models, which\u2014as we have discussed in the Introduction\u2014are of interest in the information retrieval context. We are *not* attempting to do language modeling in this paper\u2014an enterprise that would require us to examine trigram or other higher-order models. We note in passing, however, that extensions of LDA could be considered that involve Dirichlet-multinomial over trigrams instead of unigrams. We leave the exploration of such extensions to language modeling to future work.",
            "page": 18,
            "x": 91,
            "y": 639,
            "width": 431,
            "height": 67,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-note",
            "chunk_id": "b71eb4ad-801d-4cfe-beba-bb4530028fd6",
            "group_text": "3. Note that we simply use perplexity as a figure of merit for comparing models. The models that we compare are all unigram (\u201cbag-of-words\u201d) models, which\u2014as we have discussed in the Introduction\u2014are of interest in the information retrieval context. We are *not* attempting to do language modeling in this paper\u2014an enterprise that would require us to examine trigram or other higher-order models. We note in passing, however, that extensions of LDA could be considered that involve Dirichlet-multinomial over trigrams instead of unigrams. We leave the exploration of such extensions to language modeling to future work.\n\nFigure 9: Perplexity results on the nematode (Top) and AP (Bottom) corpora for LDA, the unigram\nmodel, mixture of unigrams, and pLSI.\n\nnearly deterministic clustering of the training documents (in the E-step) which is used to determine\nthe word probabilities in each mixture component (in the M-step). A previously unseen document\nmay best fit one of the resulting mixture components, but will probably contain at least one word\nwhich did not occur in the training documents that were assigned to that component. Such words\nwill have a very small probability, which causes the perplexity of the new document to explode.\nAs $k$ increases, the documents of the training corpus are partitioned into finer collections and thus\ninduce more words with small probabilities.\n\nIn the mixture of unigrams, we can alleviate overfitting through the variational Bayesian smoothing scheme presented in Section 5.4. This ensures that all words will have some probability under every mixture component.\n\nIn the pLSI case, the hard clustering problem is alleviated by the fact that each document is allowed to exhibit a different proportion of topics. However, pLSI only refers to the training documents and a different overfitting problem arises that is due to the dimensionality of the $p(z|d)$ parameter. One reasonable approach to assigning probability to a previously unseen document is by marginalizing over $d$:\n\n$p(\\mathbf{w}) = \\sum_d \\prod_{n=1}^N \\sum_z p(w_n \\mid z) p(z \\mid d) p(d).$\n\nEssentially, we are integrating over the empirical distribution on the topic simplex (see Figure 4).\n\nThis method of inference, though theoretically sound, causes the model to overfit. The document-\nspecific topic distribution has some components which are close to zero for those topics that do not\nappear in the document. Thus, certain words will have very small probability in the estimates of\neach mixture component. When determining the probability of a new document through marginal-\nization, only those training documents which exhibit a similar proportion of topics will contribute\nto the likelihood. For a given training document\u2019s topic proportions, any word which has small\nprobability in all the constituent topics will cause the perplexity to explode. As *k* gets larger, the\nchance that a training document will exhibit topics that cover all the words in the new document\ndecreases and thus the perplexity grows. Note that pLSI does not overfit as quickly (with respect to\n*k*) as the mixture of unigrams.\n\nThis overfitting problem essentially stems from the restriction that each future document exhibit\nthe same topic proportions as were seen in one or more of the training documents. Given this\nconstraint, we are not free to choose the most likely proportions of topics for the new document. An\nalternative approach is the \u201cfolding-in\u201d heuristic suggested by Hofmann (1999), where one ignores\nthe $p(z|d)$ parameters and refits $p(z|d_{new})$. Note that this gives the pLSI model an unfair advantage\nby allowing it to refit $k-1$ parameters to the test data.\n\nLDA suffers from neither of these problems. As in pLSI, each document can exhibit a different\nproportion of underlying topics. However, LDA can easily assign probability to a new document;\nno heuristics are needed for a new document to be endowed with a different set of topic proportions\nthan were associated with documents in the training corpus.\n\nFigure 9 presents the perplexity for each model on both corpora for different values of *k*. The pLSI model and mixture of unigrams are suitably corrected for overfitting. The latent variable models perform better than the simple unigram model. LDA consistently performs better than the other models."
        },
        {
            "text": "Figure 9: Perplexity results on the nematode (Top) and AP (Bottom) corpora for LDA, the unigram\nmodel, mixture of unigrams, and pLSI.",
            "page": 19,
            "x": 86,
            "y": 640,
            "width": 436,
            "height": 32,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-note",
            "chunk_id": "f453e066-4ef9-4ece-b723-0c21e07de051",
            "group_text": "3. Note that we simply use perplexity as a figure of merit for comparing models. The models that we compare are all unigram (\u201cbag-of-words\u201d) models, which\u2014as we have discussed in the Introduction\u2014are of interest in the information retrieval context. We are *not* attempting to do language modeling in this paper\u2014an enterprise that would require us to examine trigram or other higher-order models. We note in passing, however, that extensions of LDA could be considered that involve Dirichlet-multinomial over trigrams instead of unigrams. We leave the exploration of such extensions to language modeling to future work.\n\nFigure 9: Perplexity results on the nematode (Top) and AP (Bottom) corpora for LDA, the unigram\nmodel, mixture of unigrams, and pLSI.\n\nnearly deterministic clustering of the training documents (in the E-step) which is used to determine\nthe word probabilities in each mixture component (in the M-step). A previously unseen document\nmay best fit one of the resulting mixture components, but will probably contain at least one word\nwhich did not occur in the training documents that were assigned to that component. Such words\nwill have a very small probability, which causes the perplexity of the new document to explode.\nAs $k$ increases, the documents of the training corpus are partitioned into finer collections and thus\ninduce more words with small probabilities.\n\nIn the mixture of unigrams, we can alleviate overfitting through the variational Bayesian smoothing scheme presented in Section 5.4. This ensures that all words will have some probability under every mixture component.\n\nIn the pLSI case, the hard clustering problem is alleviated by the fact that each document is allowed to exhibit a different proportion of topics. However, pLSI only refers to the training documents and a different overfitting problem arises that is due to the dimensionality of the $p(z|d)$ parameter. One reasonable approach to assigning probability to a previously unseen document is by marginalizing over $d$:\n\n$p(\\mathbf{w}) = \\sum_d \\prod_{n=1}^N \\sum_z p(w_n \\mid z) p(z \\mid d) p(d).$\n\nEssentially, we are integrating over the empirical distribution on the topic simplex (see Figure 4).\n\nThis method of inference, though theoretically sound, causes the model to overfit. The document-\nspecific topic distribution has some components which are close to zero for those topics that do not\nappear in the document. Thus, certain words will have very small probability in the estimates of\neach mixture component. When determining the probability of a new document through marginal-\nization, only those training documents which exhibit a similar proportion of topics will contribute\nto the likelihood. For a given training document\u2019s topic proportions, any word which has small\nprobability in all the constituent topics will cause the perplexity to explode. As *k* gets larger, the\nchance that a training document will exhibit topics that cover all the words in the new document\ndecreases and thus the perplexity grows. Note that pLSI does not overfit as quickly (with respect to\n*k*) as the mixture of unigrams.\n\nThis overfitting problem essentially stems from the restriction that each future document exhibit\nthe same topic proportions as were seen in one or more of the training documents. Given this\nconstraint, we are not free to choose the most likely proportions of topics for the new document. An\nalternative approach is the \u201cfolding-in\u201d heuristic suggested by Hofmann (1999), where one ignores\nthe $p(z|d)$ parameters and refits $p(z|d_{new})$. Note that this gives the pLSI model an unfair advantage\nby allowing it to refit $k-1$ parameters to the test data.\n\nLDA suffers from neither of these problems. As in pLSI, each document can exhibit a different\nproportion of underlying topics. However, LDA can easily assign probability to a new document;\nno heuristics are needed for a new document to be endowed with a different set of topic proportions\nthan were associated with documents in the training corpus.\n\nFigure 9 presents the perplexity for each model on both corpora for different values of *k*. The pLSI model and mixture of unigrams are suitably corrected for overfitting. The latent variable models perform better than the simple unigram model. LDA consistently performs better than the other models."
        },
        {
            "text": "nearly deterministic clustering of the training documents (in the E-step) which is used to determine\nthe word probabilities in each mixture component (in the M-step). A previously unseen document\nmay best fit one of the resulting mixture components, but will probably contain at least one word\nwhich did not occur in the training documents that were assigned to that component. Such words\nwill have a very small probability, which causes the perplexity of the new document to explode.\nAs $k$ increases, the documents of the training corpus are partitioned into finer collections and thus\ninduce more words with small probabilities.",
            "page": 20,
            "x": 86,
            "y": 90,
            "width": 438,
            "height": 95,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-note",
            "chunk_id": "101bd690-2a74-434f-9a87-abae31747428",
            "group_text": "3. Note that we simply use perplexity as a figure of merit for comparing models. The models that we compare are all unigram (\u201cbag-of-words\u201d) models, which\u2014as we have discussed in the Introduction\u2014are of interest in the information retrieval context. We are *not* attempting to do language modeling in this paper\u2014an enterprise that would require us to examine trigram or other higher-order models. We note in passing, however, that extensions of LDA could be considered that involve Dirichlet-multinomial over trigrams instead of unigrams. We leave the exploration of such extensions to language modeling to future work.\n\nFigure 9: Perplexity results on the nematode (Top) and AP (Bottom) corpora for LDA, the unigram\nmodel, mixture of unigrams, and pLSI.\n\nnearly deterministic clustering of the training documents (in the E-step) which is used to determine\nthe word probabilities in each mixture component (in the M-step). A previously unseen document\nmay best fit one of the resulting mixture components, but will probably contain at least one word\nwhich did not occur in the training documents that were assigned to that component. Such words\nwill have a very small probability, which causes the perplexity of the new document to explode.\nAs $k$ increases, the documents of the training corpus are partitioned into finer collections and thus\ninduce more words with small probabilities.\n\nIn the mixture of unigrams, we can alleviate overfitting through the variational Bayesian smoothing scheme presented in Section 5.4. This ensures that all words will have some probability under every mixture component.\n\nIn the pLSI case, the hard clustering problem is alleviated by the fact that each document is allowed to exhibit a different proportion of topics. However, pLSI only refers to the training documents and a different overfitting problem arises that is due to the dimensionality of the $p(z|d)$ parameter. One reasonable approach to assigning probability to a previously unseen document is by marginalizing over $d$:\n\n$p(\\mathbf{w}) = \\sum_d \\prod_{n=1}^N \\sum_z p(w_n \\mid z) p(z \\mid d) p(d).$\n\nEssentially, we are integrating over the empirical distribution on the topic simplex (see Figure 4).\n\nThis method of inference, though theoretically sound, causes the model to overfit. The document-\nspecific topic distribution has some components which are close to zero for those topics that do not\nappear in the document. Thus, certain words will have very small probability in the estimates of\neach mixture component. When determining the probability of a new document through marginal-\nization, only those training documents which exhibit a similar proportion of topics will contribute\nto the likelihood. For a given training document\u2019s topic proportions, any word which has small\nprobability in all the constituent topics will cause the perplexity to explode. As *k* gets larger, the\nchance that a training document will exhibit topics that cover all the words in the new document\ndecreases and thus the perplexity grows. Note that pLSI does not overfit as quickly (with respect to\n*k*) as the mixture of unigrams.\n\nThis overfitting problem essentially stems from the restriction that each future document exhibit\nthe same topic proportions as were seen in one or more of the training documents. Given this\nconstraint, we are not free to choose the most likely proportions of topics for the new document. An\nalternative approach is the \u201cfolding-in\u201d heuristic suggested by Hofmann (1999), where one ignores\nthe $p(z|d)$ parameters and refits $p(z|d_{new})$. Note that this gives the pLSI model an unfair advantage\nby allowing it to refit $k-1$ parameters to the test data.\n\nLDA suffers from neither of these problems. As in pLSI, each document can exhibit a different\nproportion of underlying topics. However, LDA can easily assign probability to a new document;\nno heuristics are needed for a new document to be endowed with a different set of topic proportions\nthan were associated with documents in the training corpus.\n\nFigure 9 presents the perplexity for each model on both corpora for different values of *k*. The pLSI model and mixture of unigrams are suitably corrected for overfitting. The latent variable models perform better than the simple unigram model. LDA consistently performs better than the other models."
        },
        {
            "text": "In the mixture of unigrams, we can alleviate overfitting through the variational Bayesian smoothing scheme presented in Section 5.4. This ensures that all words will have some probability under every mixture component.",
            "page": 20,
            "x": 87,
            "y": 189,
            "width": 435,
            "height": 40,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-note",
            "chunk_id": "c2be4f61-b7b1-4f53-8b77-8f70cc7e40b8",
            "group_text": "3. Note that we simply use perplexity as a figure of merit for comparing models. The models that we compare are all unigram (\u201cbag-of-words\u201d) models, which\u2014as we have discussed in the Introduction\u2014are of interest in the information retrieval context. We are *not* attempting to do language modeling in this paper\u2014an enterprise that would require us to examine trigram or other higher-order models. We note in passing, however, that extensions of LDA could be considered that involve Dirichlet-multinomial over trigrams instead of unigrams. We leave the exploration of such extensions to language modeling to future work.\n\nFigure 9: Perplexity results on the nematode (Top) and AP (Bottom) corpora for LDA, the unigram\nmodel, mixture of unigrams, and pLSI.\n\nnearly deterministic clustering of the training documents (in the E-step) which is used to determine\nthe word probabilities in each mixture component (in the M-step). A previously unseen document\nmay best fit one of the resulting mixture components, but will probably contain at least one word\nwhich did not occur in the training documents that were assigned to that component. Such words\nwill have a very small probability, which causes the perplexity of the new document to explode.\nAs $k$ increases, the documents of the training corpus are partitioned into finer collections and thus\ninduce more words with small probabilities.\n\nIn the mixture of unigrams, we can alleviate overfitting through the variational Bayesian smoothing scheme presented in Section 5.4. This ensures that all words will have some probability under every mixture component.\n\nIn the pLSI case, the hard clustering problem is alleviated by the fact that each document is allowed to exhibit a different proportion of topics. However, pLSI only refers to the training documents and a different overfitting problem arises that is due to the dimensionality of the $p(z|d)$ parameter. One reasonable approach to assigning probability to a previously unseen document is by marginalizing over $d$:\n\n$p(\\mathbf{w}) = \\sum_d \\prod_{n=1}^N \\sum_z p(w_n \\mid z) p(z \\mid d) p(d).$\n\nEssentially, we are integrating over the empirical distribution on the topic simplex (see Figure 4).\n\nThis method of inference, though theoretically sound, causes the model to overfit. The document-\nspecific topic distribution has some components which are close to zero for those topics that do not\nappear in the document. Thus, certain words will have very small probability in the estimates of\neach mixture component. When determining the probability of a new document through marginal-\nization, only those training documents which exhibit a similar proportion of topics will contribute\nto the likelihood. For a given training document\u2019s topic proportions, any word which has small\nprobability in all the constituent topics will cause the perplexity to explode. As *k* gets larger, the\nchance that a training document will exhibit topics that cover all the words in the new document\ndecreases and thus the perplexity grows. Note that pLSI does not overfit as quickly (with respect to\n*k*) as the mixture of unigrams.\n\nThis overfitting problem essentially stems from the restriction that each future document exhibit\nthe same topic proportions as were seen in one or more of the training documents. Given this\nconstraint, we are not free to choose the most likely proportions of topics for the new document. An\nalternative approach is the \u201cfolding-in\u201d heuristic suggested by Hofmann (1999), where one ignores\nthe $p(z|d)$ parameters and refits $p(z|d_{new})$. Note that this gives the pLSI model an unfair advantage\nby allowing it to refit $k-1$ parameters to the test data.\n\nLDA suffers from neither of these problems. As in pLSI, each document can exhibit a different\nproportion of underlying topics. However, LDA can easily assign probability to a new document;\nno heuristics are needed for a new document to be endowed with a different set of topic proportions\nthan were associated with documents in the training corpus.\n\nFigure 9 presents the perplexity for each model on both corpora for different values of *k*. The pLSI model and mixture of unigrams are suitably corrected for overfitting. The latent variable models perform better than the simple unigram model. LDA consistently performs better than the other models."
        },
        {
            "text": "In the pLSI case, the hard clustering problem is alleviated by the fact that each document is allowed to exhibit a different proportion of topics. However, pLSI only refers to the training documents and a different overfitting problem arises that is due to the dimensionality of the $p(z|d)$ parameter. One reasonable approach to assigning probability to a previously unseen document is by marginalizing over $d$:",
            "page": 20,
            "x": 87,
            "y": 232,
            "width": 435,
            "height": 68,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-note",
            "chunk_id": "1936bf23-f77f-4e0c-877f-1978385f6dd5",
            "group_text": "3. Note that we simply use perplexity as a figure of merit for comparing models. The models that we compare are all unigram (\u201cbag-of-words\u201d) models, which\u2014as we have discussed in the Introduction\u2014are of interest in the information retrieval context. We are *not* attempting to do language modeling in this paper\u2014an enterprise that would require us to examine trigram or other higher-order models. We note in passing, however, that extensions of LDA could be considered that involve Dirichlet-multinomial over trigrams instead of unigrams. We leave the exploration of such extensions to language modeling to future work.\n\nFigure 9: Perplexity results on the nematode (Top) and AP (Bottom) corpora for LDA, the unigram\nmodel, mixture of unigrams, and pLSI.\n\nnearly deterministic clustering of the training documents (in the E-step) which is used to determine\nthe word probabilities in each mixture component (in the M-step). A previously unseen document\nmay best fit one of the resulting mixture components, but will probably contain at least one word\nwhich did not occur in the training documents that were assigned to that component. Such words\nwill have a very small probability, which causes the perplexity of the new document to explode.\nAs $k$ increases, the documents of the training corpus are partitioned into finer collections and thus\ninduce more words with small probabilities.\n\nIn the mixture of unigrams, we can alleviate overfitting through the variational Bayesian smoothing scheme presented in Section 5.4. This ensures that all words will have some probability under every mixture component.\n\nIn the pLSI case, the hard clustering problem is alleviated by the fact that each document is allowed to exhibit a different proportion of topics. However, pLSI only refers to the training documents and a different overfitting problem arises that is due to the dimensionality of the $p(z|d)$ parameter. One reasonable approach to assigning probability to a previously unseen document is by marginalizing over $d$:\n\n$p(\\mathbf{w}) = \\sum_d \\prod_{n=1}^N \\sum_z p(w_n \\mid z) p(z \\mid d) p(d).$\n\nEssentially, we are integrating over the empirical distribution on the topic simplex (see Figure 4).\n\nThis method of inference, though theoretically sound, causes the model to overfit. The document-\nspecific topic distribution has some components which are close to zero for those topics that do not\nappear in the document. Thus, certain words will have very small probability in the estimates of\neach mixture component. When determining the probability of a new document through marginal-\nization, only those training documents which exhibit a similar proportion of topics will contribute\nto the likelihood. For a given training document\u2019s topic proportions, any word which has small\nprobability in all the constituent topics will cause the perplexity to explode. As *k* gets larger, the\nchance that a training document will exhibit topics that cover all the words in the new document\ndecreases and thus the perplexity grows. Note that pLSI does not overfit as quickly (with respect to\n*k*) as the mixture of unigrams.\n\nThis overfitting problem essentially stems from the restriction that each future document exhibit\nthe same topic proportions as were seen in one or more of the training documents. Given this\nconstraint, we are not free to choose the most likely proportions of topics for the new document. An\nalternative approach is the \u201cfolding-in\u201d heuristic suggested by Hofmann (1999), where one ignores\nthe $p(z|d)$ parameters and refits $p(z|d_{new})$. Note that this gives the pLSI model an unfair advantage\nby allowing it to refit $k-1$ parameters to the test data.\n\nLDA suffers from neither of these problems. As in pLSI, each document can exhibit a different\nproportion of underlying topics. However, LDA can easily assign probability to a new document;\nno heuristics are needed for a new document to be endowed with a different set of topic proportions\nthan were associated with documents in the training corpus.\n\nFigure 9 presents the perplexity for each model on both corpora for different values of *k*. The pLSI model and mixture of unigrams are suitably corrected for overfitting. The latent variable models perform better than the simple unigram model. LDA consistently performs better than the other models."
        },
        {
            "text": "$p(\\mathbf{w}) = \\sum_d \\prod_{n=1}^N \\sum_z p(w_n \\mid z) p(z \\mid d) p(d).$",
            "page": 20,
            "x": 220,
            "y": 308,
            "width": 169,
            "height": 34,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-note",
            "chunk_id": "3893d457-cdda-4c34-9683-86178396884b",
            "group_text": "3. Note that we simply use perplexity as a figure of merit for comparing models. The models that we compare are all unigram (\u201cbag-of-words\u201d) models, which\u2014as we have discussed in the Introduction\u2014are of interest in the information retrieval context. We are *not* attempting to do language modeling in this paper\u2014an enterprise that would require us to examine trigram or other higher-order models. We note in passing, however, that extensions of LDA could be considered that involve Dirichlet-multinomial over trigrams instead of unigrams. We leave the exploration of such extensions to language modeling to future work.\n\nFigure 9: Perplexity results on the nematode (Top) and AP (Bottom) corpora for LDA, the unigram\nmodel, mixture of unigrams, and pLSI.\n\nnearly deterministic clustering of the training documents (in the E-step) which is used to determine\nthe word probabilities in each mixture component (in the M-step). A previously unseen document\nmay best fit one of the resulting mixture components, but will probably contain at least one word\nwhich did not occur in the training documents that were assigned to that component. Such words\nwill have a very small probability, which causes the perplexity of the new document to explode.\nAs $k$ increases, the documents of the training corpus are partitioned into finer collections and thus\ninduce more words with small probabilities.\n\nIn the mixture of unigrams, we can alleviate overfitting through the variational Bayesian smoothing scheme presented in Section 5.4. This ensures that all words will have some probability under every mixture component.\n\nIn the pLSI case, the hard clustering problem is alleviated by the fact that each document is allowed to exhibit a different proportion of topics. However, pLSI only refers to the training documents and a different overfitting problem arises that is due to the dimensionality of the $p(z|d)$ parameter. One reasonable approach to assigning probability to a previously unseen document is by marginalizing over $d$:\n\n$p(\\mathbf{w}) = \\sum_d \\prod_{n=1}^N \\sum_z p(w_n \\mid z) p(z \\mid d) p(d).$\n\nEssentially, we are integrating over the empirical distribution on the topic simplex (see Figure 4).\n\nThis method of inference, though theoretically sound, causes the model to overfit. The document-\nspecific topic distribution has some components which are close to zero for those topics that do not\nappear in the document. Thus, certain words will have very small probability in the estimates of\neach mixture component. When determining the probability of a new document through marginal-\nization, only those training documents which exhibit a similar proportion of topics will contribute\nto the likelihood. For a given training document\u2019s topic proportions, any word which has small\nprobability in all the constituent topics will cause the perplexity to explode. As *k* gets larger, the\nchance that a training document will exhibit topics that cover all the words in the new document\ndecreases and thus the perplexity grows. Note that pLSI does not overfit as quickly (with respect to\n*k*) as the mixture of unigrams.\n\nThis overfitting problem essentially stems from the restriction that each future document exhibit\nthe same topic proportions as were seen in one or more of the training documents. Given this\nconstraint, we are not free to choose the most likely proportions of topics for the new document. An\nalternative approach is the \u201cfolding-in\u201d heuristic suggested by Hofmann (1999), where one ignores\nthe $p(z|d)$ parameters and refits $p(z|d_{new})$. Note that this gives the pLSI model an unfair advantage\nby allowing it to refit $k-1$ parameters to the test data.\n\nLDA suffers from neither of these problems. As in pLSI, each document can exhibit a different\nproportion of underlying topics. However, LDA can easily assign probability to a new document;\nno heuristics are needed for a new document to be endowed with a different set of topic proportions\nthan were associated with documents in the training corpus.\n\nFigure 9 presents the perplexity for each model on both corpora for different values of *k*. The pLSI model and mixture of unigrams are suitably corrected for overfitting. The latent variable models perform better than the simple unigram model. LDA consistently performs better than the other models."
        },
        {
            "text": "Essentially, we are integrating over the empirical distribution on the topic simplex (see Figure 4).",
            "page": 20,
            "x": 87,
            "y": 354,
            "width": 426,
            "height": 15,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-note",
            "chunk_id": "d911477a-ad0c-4542-950a-8c9764ead840",
            "group_text": "3. Note that we simply use perplexity as a figure of merit for comparing models. The models that we compare are all unigram (\u201cbag-of-words\u201d) models, which\u2014as we have discussed in the Introduction\u2014are of interest in the information retrieval context. We are *not* attempting to do language modeling in this paper\u2014an enterprise that would require us to examine trigram or other higher-order models. We note in passing, however, that extensions of LDA could be considered that involve Dirichlet-multinomial over trigrams instead of unigrams. We leave the exploration of such extensions to language modeling to future work.\n\nFigure 9: Perplexity results on the nematode (Top) and AP (Bottom) corpora for LDA, the unigram\nmodel, mixture of unigrams, and pLSI.\n\nnearly deterministic clustering of the training documents (in the E-step) which is used to determine\nthe word probabilities in each mixture component (in the M-step). A previously unseen document\nmay best fit one of the resulting mixture components, but will probably contain at least one word\nwhich did not occur in the training documents that were assigned to that component. Such words\nwill have a very small probability, which causes the perplexity of the new document to explode.\nAs $k$ increases, the documents of the training corpus are partitioned into finer collections and thus\ninduce more words with small probabilities.\n\nIn the mixture of unigrams, we can alleviate overfitting through the variational Bayesian smoothing scheme presented in Section 5.4. This ensures that all words will have some probability under every mixture component.\n\nIn the pLSI case, the hard clustering problem is alleviated by the fact that each document is allowed to exhibit a different proportion of topics. However, pLSI only refers to the training documents and a different overfitting problem arises that is due to the dimensionality of the $p(z|d)$ parameter. One reasonable approach to assigning probability to a previously unseen document is by marginalizing over $d$:\n\n$p(\\mathbf{w}) = \\sum_d \\prod_{n=1}^N \\sum_z p(w_n \\mid z) p(z \\mid d) p(d).$\n\nEssentially, we are integrating over the empirical distribution on the topic simplex (see Figure 4).\n\nThis method of inference, though theoretically sound, causes the model to overfit. The document-\nspecific topic distribution has some components which are close to zero for those topics that do not\nappear in the document. Thus, certain words will have very small probability in the estimates of\neach mixture component. When determining the probability of a new document through marginal-\nization, only those training documents which exhibit a similar proportion of topics will contribute\nto the likelihood. For a given training document\u2019s topic proportions, any word which has small\nprobability in all the constituent topics will cause the perplexity to explode. As *k* gets larger, the\nchance that a training document will exhibit topics that cover all the words in the new document\ndecreases and thus the perplexity grows. Note that pLSI does not overfit as quickly (with respect to\n*k*) as the mixture of unigrams.\n\nThis overfitting problem essentially stems from the restriction that each future document exhibit\nthe same topic proportions as were seen in one or more of the training documents. Given this\nconstraint, we are not free to choose the most likely proportions of topics for the new document. An\nalternative approach is the \u201cfolding-in\u201d heuristic suggested by Hofmann (1999), where one ignores\nthe $p(z|d)$ parameters and refits $p(z|d_{new})$. Note that this gives the pLSI model an unfair advantage\nby allowing it to refit $k-1$ parameters to the test data.\n\nLDA suffers from neither of these problems. As in pLSI, each document can exhibit a different\nproportion of underlying topics. However, LDA can easily assign probability to a new document;\nno heuristics are needed for a new document to be endowed with a different set of topic proportions\nthan were associated with documents in the training corpus.\n\nFigure 9 presents the perplexity for each model on both corpora for different values of *k*. The pLSI model and mixture of unigrams are suitably corrected for overfitting. The latent variable models perform better than the simple unigram model. LDA consistently performs better than the other models."
        },
        {
            "text": "This method of inference, though theoretically sound, causes the model to overfit. The document-\nspecific topic distribution has some components which are close to zero for those topics that do not\nappear in the document. Thus, certain words will have very small probability in the estimates of\neach mixture component. When determining the probability of a new document through marginal-\nization, only those training documents which exhibit a similar proportion of topics will contribute\nto the likelihood. For a given training document\u2019s topic proportions, any word which has small\nprobability in all the constituent topics will cause the perplexity to explode. As *k* gets larger, the\nchance that a training document will exhibit topics that cover all the words in the new document\ndecreases and thus the perplexity grows. Note that pLSI does not overfit as quickly (with respect to\n*k*) as the mixture of unigrams.",
            "page": 20,
            "x": 87,
            "y": 371,
            "width": 436,
            "height": 135,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-note",
            "chunk_id": "ddf85078-0a6c-451f-8132-b1e28031dc4c",
            "group_text": "3. Note that we simply use perplexity as a figure of merit for comparing models. The models that we compare are all unigram (\u201cbag-of-words\u201d) models, which\u2014as we have discussed in the Introduction\u2014are of interest in the information retrieval context. We are *not* attempting to do language modeling in this paper\u2014an enterprise that would require us to examine trigram or other higher-order models. We note in passing, however, that extensions of LDA could be considered that involve Dirichlet-multinomial over trigrams instead of unigrams. We leave the exploration of such extensions to language modeling to future work.\n\nFigure 9: Perplexity results on the nematode (Top) and AP (Bottom) corpora for LDA, the unigram\nmodel, mixture of unigrams, and pLSI.\n\nnearly deterministic clustering of the training documents (in the E-step) which is used to determine\nthe word probabilities in each mixture component (in the M-step). A previously unseen document\nmay best fit one of the resulting mixture components, but will probably contain at least one word\nwhich did not occur in the training documents that were assigned to that component. Such words\nwill have a very small probability, which causes the perplexity of the new document to explode.\nAs $k$ increases, the documents of the training corpus are partitioned into finer collections and thus\ninduce more words with small probabilities.\n\nIn the mixture of unigrams, we can alleviate overfitting through the variational Bayesian smoothing scheme presented in Section 5.4. This ensures that all words will have some probability under every mixture component.\n\nIn the pLSI case, the hard clustering problem is alleviated by the fact that each document is allowed to exhibit a different proportion of topics. However, pLSI only refers to the training documents and a different overfitting problem arises that is due to the dimensionality of the $p(z|d)$ parameter. One reasonable approach to assigning probability to a previously unseen document is by marginalizing over $d$:\n\n$p(\\mathbf{w}) = \\sum_d \\prod_{n=1}^N \\sum_z p(w_n \\mid z) p(z \\mid d) p(d).$\n\nEssentially, we are integrating over the empirical distribution on the topic simplex (see Figure 4).\n\nThis method of inference, though theoretically sound, causes the model to overfit. The document-\nspecific topic distribution has some components which are close to zero for those topics that do not\nappear in the document. Thus, certain words will have very small probability in the estimates of\neach mixture component. When determining the probability of a new document through marginal-\nization, only those training documents which exhibit a similar proportion of topics will contribute\nto the likelihood. For a given training document\u2019s topic proportions, any word which has small\nprobability in all the constituent topics will cause the perplexity to explode. As *k* gets larger, the\nchance that a training document will exhibit topics that cover all the words in the new document\ndecreases and thus the perplexity grows. Note that pLSI does not overfit as quickly (with respect to\n*k*) as the mixture of unigrams.\n\nThis overfitting problem essentially stems from the restriction that each future document exhibit\nthe same topic proportions as were seen in one or more of the training documents. Given this\nconstraint, we are not free to choose the most likely proportions of topics for the new document. An\nalternative approach is the \u201cfolding-in\u201d heuristic suggested by Hofmann (1999), where one ignores\nthe $p(z|d)$ parameters and refits $p(z|d_{new})$. Note that this gives the pLSI model an unfair advantage\nby allowing it to refit $k-1$ parameters to the test data.\n\nLDA suffers from neither of these problems. As in pLSI, each document can exhibit a different\nproportion of underlying topics. However, LDA can easily assign probability to a new document;\nno heuristics are needed for a new document to be endowed with a different set of topic proportions\nthan were associated with documents in the training corpus.\n\nFigure 9 presents the perplexity for each model on both corpora for different values of *k*. The pLSI model and mixture of unigrams are suitably corrected for overfitting. The latent variable models perform better than the simple unigram model. LDA consistently performs better than the other models."
        },
        {
            "text": "This overfitting problem essentially stems from the restriction that each future document exhibit\nthe same topic proportions as were seen in one or more of the training documents. Given this\nconstraint, we are not free to choose the most likely proportions of topics for the new document. An\nalternative approach is the \u201cfolding-in\u201d heuristic suggested by Hofmann (1999), where one ignores\nthe $p(z|d)$ parameters and refits $p(z|d_{new})$. Note that this gives the pLSI model an unfair advantage\nby allowing it to refit $k-1$ parameters to the test data.",
            "page": 20,
            "x": 88,
            "y": 510,
            "width": 434,
            "height": 81,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-note",
            "chunk_id": "44f6bf2c-6d73-46df-8761-33e40d9ab023",
            "group_text": "3. Note that we simply use perplexity as a figure of merit for comparing models. The models that we compare are all unigram (\u201cbag-of-words\u201d) models, which\u2014as we have discussed in the Introduction\u2014are of interest in the information retrieval context. We are *not* attempting to do language modeling in this paper\u2014an enterprise that would require us to examine trigram or other higher-order models. We note in passing, however, that extensions of LDA could be considered that involve Dirichlet-multinomial over trigrams instead of unigrams. We leave the exploration of such extensions to language modeling to future work.\n\nFigure 9: Perplexity results on the nematode (Top) and AP (Bottom) corpora for LDA, the unigram\nmodel, mixture of unigrams, and pLSI.\n\nnearly deterministic clustering of the training documents (in the E-step) which is used to determine\nthe word probabilities in each mixture component (in the M-step). A previously unseen document\nmay best fit one of the resulting mixture components, but will probably contain at least one word\nwhich did not occur in the training documents that were assigned to that component. Such words\nwill have a very small probability, which causes the perplexity of the new document to explode.\nAs $k$ increases, the documents of the training corpus are partitioned into finer collections and thus\ninduce more words with small probabilities.\n\nIn the mixture of unigrams, we can alleviate overfitting through the variational Bayesian smoothing scheme presented in Section 5.4. This ensures that all words will have some probability under every mixture component.\n\nIn the pLSI case, the hard clustering problem is alleviated by the fact that each document is allowed to exhibit a different proportion of topics. However, pLSI only refers to the training documents and a different overfitting problem arises that is due to the dimensionality of the $p(z|d)$ parameter. One reasonable approach to assigning probability to a previously unseen document is by marginalizing over $d$:\n\n$p(\\mathbf{w}) = \\sum_d \\prod_{n=1}^N \\sum_z p(w_n \\mid z) p(z \\mid d) p(d).$\n\nEssentially, we are integrating over the empirical distribution on the topic simplex (see Figure 4).\n\nThis method of inference, though theoretically sound, causes the model to overfit. The document-\nspecific topic distribution has some components which are close to zero for those topics that do not\nappear in the document. Thus, certain words will have very small probability in the estimates of\neach mixture component. When determining the probability of a new document through marginal-\nization, only those training documents which exhibit a similar proportion of topics will contribute\nto the likelihood. For a given training document\u2019s topic proportions, any word which has small\nprobability in all the constituent topics will cause the perplexity to explode. As *k* gets larger, the\nchance that a training document will exhibit topics that cover all the words in the new document\ndecreases and thus the perplexity grows. Note that pLSI does not overfit as quickly (with respect to\n*k*) as the mixture of unigrams.\n\nThis overfitting problem essentially stems from the restriction that each future document exhibit\nthe same topic proportions as were seen in one or more of the training documents. Given this\nconstraint, we are not free to choose the most likely proportions of topics for the new document. An\nalternative approach is the \u201cfolding-in\u201d heuristic suggested by Hofmann (1999), where one ignores\nthe $p(z|d)$ parameters and refits $p(z|d_{new})$. Note that this gives the pLSI model an unfair advantage\nby allowing it to refit $k-1$ parameters to the test data.\n\nLDA suffers from neither of these problems. As in pLSI, each document can exhibit a different\nproportion of underlying topics. However, LDA can easily assign probability to a new document;\nno heuristics are needed for a new document to be endowed with a different set of topic proportions\nthan were associated with documents in the training corpus.\n\nFigure 9 presents the perplexity for each model on both corpora for different values of *k*. The pLSI model and mixture of unigrams are suitably corrected for overfitting. The latent variable models perform better than the simple unigram model. LDA consistently performs better than the other models."
        },
        {
            "text": "LDA suffers from neither of these problems. As in pLSI, each document can exhibit a different\nproportion of underlying topics. However, LDA can easily assign probability to a new document;\nno heuristics are needed for a new document to be endowed with a different set of topic proportions\nthan were associated with documents in the training corpus.",
            "page": 20,
            "x": 88,
            "y": 594,
            "width": 433,
            "height": 53,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-note",
            "chunk_id": "2c10dcf5-cac7-4301-9298-a1f35501d2ec",
            "group_text": "3. Note that we simply use perplexity as a figure of merit for comparing models. The models that we compare are all unigram (\u201cbag-of-words\u201d) models, which\u2014as we have discussed in the Introduction\u2014are of interest in the information retrieval context. We are *not* attempting to do language modeling in this paper\u2014an enterprise that would require us to examine trigram or other higher-order models. We note in passing, however, that extensions of LDA could be considered that involve Dirichlet-multinomial over trigrams instead of unigrams. We leave the exploration of such extensions to language modeling to future work.\n\nFigure 9: Perplexity results on the nematode (Top) and AP (Bottom) corpora for LDA, the unigram\nmodel, mixture of unigrams, and pLSI.\n\nnearly deterministic clustering of the training documents (in the E-step) which is used to determine\nthe word probabilities in each mixture component (in the M-step). A previously unseen document\nmay best fit one of the resulting mixture components, but will probably contain at least one word\nwhich did not occur in the training documents that were assigned to that component. Such words\nwill have a very small probability, which causes the perplexity of the new document to explode.\nAs $k$ increases, the documents of the training corpus are partitioned into finer collections and thus\ninduce more words with small probabilities.\n\nIn the mixture of unigrams, we can alleviate overfitting through the variational Bayesian smoothing scheme presented in Section 5.4. This ensures that all words will have some probability under every mixture component.\n\nIn the pLSI case, the hard clustering problem is alleviated by the fact that each document is allowed to exhibit a different proportion of topics. However, pLSI only refers to the training documents and a different overfitting problem arises that is due to the dimensionality of the $p(z|d)$ parameter. One reasonable approach to assigning probability to a previously unseen document is by marginalizing over $d$:\n\n$p(\\mathbf{w}) = \\sum_d \\prod_{n=1}^N \\sum_z p(w_n \\mid z) p(z \\mid d) p(d).$\n\nEssentially, we are integrating over the empirical distribution on the topic simplex (see Figure 4).\n\nThis method of inference, though theoretically sound, causes the model to overfit. The document-\nspecific topic distribution has some components which are close to zero for those topics that do not\nappear in the document. Thus, certain words will have very small probability in the estimates of\neach mixture component. When determining the probability of a new document through marginal-\nization, only those training documents which exhibit a similar proportion of topics will contribute\nto the likelihood. For a given training document\u2019s topic proportions, any word which has small\nprobability in all the constituent topics will cause the perplexity to explode. As *k* gets larger, the\nchance that a training document will exhibit topics that cover all the words in the new document\ndecreases and thus the perplexity grows. Note that pLSI does not overfit as quickly (with respect to\n*k*) as the mixture of unigrams.\n\nThis overfitting problem essentially stems from the restriction that each future document exhibit\nthe same topic proportions as were seen in one or more of the training documents. Given this\nconstraint, we are not free to choose the most likely proportions of topics for the new document. An\nalternative approach is the \u201cfolding-in\u201d heuristic suggested by Hofmann (1999), where one ignores\nthe $p(z|d)$ parameters and refits $p(z|d_{new})$. Note that this gives the pLSI model an unfair advantage\nby allowing it to refit $k-1$ parameters to the test data.\n\nLDA suffers from neither of these problems. As in pLSI, each document can exhibit a different\nproportion of underlying topics. However, LDA can easily assign probability to a new document;\nno heuristics are needed for a new document to be endowed with a different set of topic proportions\nthan were associated with documents in the training corpus.\n\nFigure 9 presents the perplexity for each model on both corpora for different values of *k*. The pLSI model and mixture of unigrams are suitably corrected for overfitting. The latent variable models perform better than the simple unigram model. LDA consistently performs better than the other models."
        },
        {
            "text": "Figure 9 presents the perplexity for each model on both corpora for different values of *k*. The pLSI model and mixture of unigrams are suitably corrected for overfitting. The latent variable models perform better than the simple unigram model. LDA consistently performs better than the other models.",
            "page": 20,
            "x": 88,
            "y": 651,
            "width": 433,
            "height": 54,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-note",
            "chunk_id": "3ea24969-c62e-4bab-87b0-20170bc7cab2",
            "group_text": "3. Note that we simply use perplexity as a figure of merit for comparing models. The models that we compare are all unigram (\u201cbag-of-words\u201d) models, which\u2014as we have discussed in the Introduction\u2014are of interest in the information retrieval context. We are *not* attempting to do language modeling in this paper\u2014an enterprise that would require us to examine trigram or other higher-order models. We note in passing, however, that extensions of LDA could be considered that involve Dirichlet-multinomial over trigrams instead of unigrams. We leave the exploration of such extensions to language modeling to future work.\n\nFigure 9: Perplexity results on the nematode (Top) and AP (Bottom) corpora for LDA, the unigram\nmodel, mixture of unigrams, and pLSI.\n\nnearly deterministic clustering of the training documents (in the E-step) which is used to determine\nthe word probabilities in each mixture component (in the M-step). A previously unseen document\nmay best fit one of the resulting mixture components, but will probably contain at least one word\nwhich did not occur in the training documents that were assigned to that component. Such words\nwill have a very small probability, which causes the perplexity of the new document to explode.\nAs $k$ increases, the documents of the training corpus are partitioned into finer collections and thus\ninduce more words with small probabilities.\n\nIn the mixture of unigrams, we can alleviate overfitting through the variational Bayesian smoothing scheme presented in Section 5.4. This ensures that all words will have some probability under every mixture component.\n\nIn the pLSI case, the hard clustering problem is alleviated by the fact that each document is allowed to exhibit a different proportion of topics. However, pLSI only refers to the training documents and a different overfitting problem arises that is due to the dimensionality of the $p(z|d)$ parameter. One reasonable approach to assigning probability to a previously unseen document is by marginalizing over $d$:\n\n$p(\\mathbf{w}) = \\sum_d \\prod_{n=1}^N \\sum_z p(w_n \\mid z) p(z \\mid d) p(d).$\n\nEssentially, we are integrating over the empirical distribution on the topic simplex (see Figure 4).\n\nThis method of inference, though theoretically sound, causes the model to overfit. The document-\nspecific topic distribution has some components which are close to zero for those topics that do not\nappear in the document. Thus, certain words will have very small probability in the estimates of\neach mixture component. When determining the probability of a new document through marginal-\nization, only those training documents which exhibit a similar proportion of topics will contribute\nto the likelihood. For a given training document\u2019s topic proportions, any word which has small\nprobability in all the constituent topics will cause the perplexity to explode. As *k* gets larger, the\nchance that a training document will exhibit topics that cover all the words in the new document\ndecreases and thus the perplexity grows. Note that pLSI does not overfit as quickly (with respect to\n*k*) as the mixture of unigrams.\n\nThis overfitting problem essentially stems from the restriction that each future document exhibit\nthe same topic proportions as were seen in one or more of the training documents. Given this\nconstraint, we are not free to choose the most likely proportions of topics for the new document. An\nalternative approach is the \u201cfolding-in\u201d heuristic suggested by Hofmann (1999), where one ignores\nthe $p(z|d)$ parameters and refits $p(z|d_{new})$. Note that this gives the pLSI model an unfair advantage\nby allowing it to refit $k-1$ parameters to the test data.\n\nLDA suffers from neither of these problems. As in pLSI, each document can exhibit a different\nproportion of underlying topics. However, LDA can easily assign probability to a new document;\nno heuristics are needed for a new document to be endowed with a different set of topic proportions\nthan were associated with documents in the training corpus.\n\nFigure 9 presents the perplexity for each model on both corpora for different values of *k*. The pLSI model and mixture of unigrams are suitably corrected for overfitting. The latent variable models perform better than the simple unigram model. LDA consistently performs better than the other models."
        },
        {
            "text": "### 7.2 Document classification\n\nIn the text classification problem, we wish to classify a document into two or more mutually exclusive classes. As in any classification problem, we may wish to consider generative approaches or discriminative approaches. In particular, by using one LDA module for each class, we obtain a generative model for classification. It is also of interest to use LDA in the discriminative framework, and this is our focus in this section.",
            "page": 21,
            "x": 85,
            "y": 399,
            "width": 438,
            "height": 91,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "23-document",
            "chunk_id": "066a3dbe-831f-4efe-89a3-f12a79955532",
            "group_text": "### 7.2 Document classification\n\nIn the text classification problem, we wish to classify a document into two or more mutually exclusive classes. As in any classification problem, we may wish to consider generative approaches or discriminative approaches. In particular, by using one LDA module for each class, we obtain a generative model for classification. It is also of interest to use LDA in the discriminative framework, and this is our focus in this section.\n\nA challenging aspect of the document classification problem is the choice of features. Treating individual words as features yields a rich but very large feature set (Joachims, 1999). One way to reduce this feature set is to use an LDA model for dimensionality reduction. In particular, LDA reduces any document to a fixed set of real-valued features\u2014the posterior Dirichlet parameters $\\gamma^*(\\mathbf{w})$ associated with the document. It is of interest to see how much discriminatory information we lose in reducing the document description to these parameters.\n\nWe conducted two binary classification experiments using the Reuters-21578 dataset. The dataset contains 8000 documents and 15,818 words.\n\nIn these experiments, we estimated the parameters of an LDA model on all the documents, without reference to their true class label. We then trained a support vector machine (SVM) on the low-dimensional representations provided by LDA and compared this SVM to an SVM trained on all the word features.\n\nUsing the SVMLight software package (Joachims, 1999), we compared an SVM trained on all\nthe word features with those trained on features induced by a 50-topic LDA model. Note that we\nreduce the feature space by 99.6 percent in this case.\n\nFigure 10 shows our results. We see that there is little reduction in classification performance in using the LDA-based features; indeed, in almost all cases the performance is improved with the LDA features. Although these results need further substantiation, they suggest that the topic-based representation provided by LDA may be useful as a fast filtering algorithm for feature selection in text classification."
        },
        {
            "text": "A challenging aspect of the document classification problem is the choice of features. Treating individual words as features yields a rich but very large feature set (Joachims, 1999). One way to reduce this feature set is to use an LDA model for dimensionality reduction. In particular, LDA reduces any document to a fixed set of real-valued features\u2014the posterior Dirichlet parameters $\\gamma^*(\\mathbf{w})$ associated with the document. It is of interest to see how much discriminatory information we lose in reducing the document description to these parameters.",
            "page": 21,
            "x": 87,
            "y": 494,
            "width": 436,
            "height": 80,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "23-document",
            "chunk_id": "3270014e-d45d-4535-9933-c1812c679a65",
            "group_text": "### 7.2 Document classification\n\nIn the text classification problem, we wish to classify a document into two or more mutually exclusive classes. As in any classification problem, we may wish to consider generative approaches or discriminative approaches. In particular, by using one LDA module for each class, we obtain a generative model for classification. It is also of interest to use LDA in the discriminative framework, and this is our focus in this section.\n\nA challenging aspect of the document classification problem is the choice of features. Treating individual words as features yields a rich but very large feature set (Joachims, 1999). One way to reduce this feature set is to use an LDA model for dimensionality reduction. In particular, LDA reduces any document to a fixed set of real-valued features\u2014the posterior Dirichlet parameters $\\gamma^*(\\mathbf{w})$ associated with the document. It is of interest to see how much discriminatory information we lose in reducing the document description to these parameters.\n\nWe conducted two binary classification experiments using the Reuters-21578 dataset. The dataset contains 8000 documents and 15,818 words.\n\nIn these experiments, we estimated the parameters of an LDA model on all the documents, without reference to their true class label. We then trained a support vector machine (SVM) on the low-dimensional representations provided by LDA and compared this SVM to an SVM trained on all the word features.\n\nUsing the SVMLight software package (Joachims, 1999), we compared an SVM trained on all\nthe word features with those trained on features induced by a 50-topic LDA model. Note that we\nreduce the feature space by 99.6 percent in this case.\n\nFigure 10 shows our results. We see that there is little reduction in classification performance in using the LDA-based features; indeed, in almost all cases the performance is improved with the LDA features. Although these results need further substantiation, they suggest that the topic-based representation provided by LDA may be useful as a fast filtering algorithm for feature selection in text classification."
        },
        {
            "text": "We conducted two binary classification experiments using the Reuters-21578 dataset. The dataset contains 8000 documents and 15,818 words.",
            "page": 21,
            "x": 87,
            "y": 577,
            "width": 435,
            "height": 28,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "23-document",
            "chunk_id": "d224d019-a241-4aba-b7fc-28a42c195129",
            "group_text": "### 7.2 Document classification\n\nIn the text classification problem, we wish to classify a document into two or more mutually exclusive classes. As in any classification problem, we may wish to consider generative approaches or discriminative approaches. In particular, by using one LDA module for each class, we obtain a generative model for classification. It is also of interest to use LDA in the discriminative framework, and this is our focus in this section.\n\nA challenging aspect of the document classification problem is the choice of features. Treating individual words as features yields a rich but very large feature set (Joachims, 1999). One way to reduce this feature set is to use an LDA model for dimensionality reduction. In particular, LDA reduces any document to a fixed set of real-valued features\u2014the posterior Dirichlet parameters $\\gamma^*(\\mathbf{w})$ associated with the document. It is of interest to see how much discriminatory information we lose in reducing the document description to these parameters.\n\nWe conducted two binary classification experiments using the Reuters-21578 dataset. The dataset contains 8000 documents and 15,818 words.\n\nIn these experiments, we estimated the parameters of an LDA model on all the documents, without reference to their true class label. We then trained a support vector machine (SVM) on the low-dimensional representations provided by LDA and compared this SVM to an SVM trained on all the word features.\n\nUsing the SVMLight software package (Joachims, 1999), we compared an SVM trained on all\nthe word features with those trained on features induced by a 50-topic LDA model. Note that we\nreduce the feature space by 99.6 percent in this case.\n\nFigure 10 shows our results. We see that there is little reduction in classification performance in using the LDA-based features; indeed, in almost all cases the performance is improved with the LDA features. Although these results need further substantiation, they suggest that the topic-based representation provided by LDA may be useful as a fast filtering algorithm for feature selection in text classification."
        },
        {
            "text": "In these experiments, we estimated the parameters of an LDA model on all the documents, without reference to their true class label. We then trained a support vector machine (SVM) on the low-dimensional representations provided by LDA and compared this SVM to an SVM trained on all the word features.",
            "page": 21,
            "x": 88,
            "y": 608,
            "width": 434,
            "height": 53,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "23-document",
            "chunk_id": "20bbec51-d5a7-4579-b61d-5030e270e314",
            "group_text": "### 7.2 Document classification\n\nIn the text classification problem, we wish to classify a document into two or more mutually exclusive classes. As in any classification problem, we may wish to consider generative approaches or discriminative approaches. In particular, by using one LDA module for each class, we obtain a generative model for classification. It is also of interest to use LDA in the discriminative framework, and this is our focus in this section.\n\nA challenging aspect of the document classification problem is the choice of features. Treating individual words as features yields a rich but very large feature set (Joachims, 1999). One way to reduce this feature set is to use an LDA model for dimensionality reduction. In particular, LDA reduces any document to a fixed set of real-valued features\u2014the posterior Dirichlet parameters $\\gamma^*(\\mathbf{w})$ associated with the document. It is of interest to see how much discriminatory information we lose in reducing the document description to these parameters.\n\nWe conducted two binary classification experiments using the Reuters-21578 dataset. The dataset contains 8000 documents and 15,818 words.\n\nIn these experiments, we estimated the parameters of an LDA model on all the documents, without reference to their true class label. We then trained a support vector machine (SVM) on the low-dimensional representations provided by LDA and compared this SVM to an SVM trained on all the word features.\n\nUsing the SVMLight software package (Joachims, 1999), we compared an SVM trained on all\nthe word features with those trained on features induced by a 50-topic LDA model. Note that we\nreduce the feature space by 99.6 percent in this case.\n\nFigure 10 shows our results. We see that there is little reduction in classification performance in using the LDA-based features; indeed, in almost all cases the performance is improved with the LDA features. Although these results need further substantiation, they suggest that the topic-based representation provided by LDA may be useful as a fast filtering algorithm for feature selection in text classification."
        },
        {
            "text": "Using the SVMLight software package (Joachims, 1999), we compared an SVM trained on all\nthe word features with those trained on features induced by a 50-topic LDA model. Note that we\nreduce the feature space by 99.6 percent in this case.",
            "page": 21,
            "x": 88,
            "y": 665,
            "width": 434,
            "height": 41,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "23-document",
            "chunk_id": "f44393c8-b4b6-4675-9584-edef3dd2c312",
            "group_text": "### 7.2 Document classification\n\nIn the text classification problem, we wish to classify a document into two or more mutually exclusive classes. As in any classification problem, we may wish to consider generative approaches or discriminative approaches. In particular, by using one LDA module for each class, we obtain a generative model for classification. It is also of interest to use LDA in the discriminative framework, and this is our focus in this section.\n\nA challenging aspect of the document classification problem is the choice of features. Treating individual words as features yields a rich but very large feature set (Joachims, 1999). One way to reduce this feature set is to use an LDA model for dimensionality reduction. In particular, LDA reduces any document to a fixed set of real-valued features\u2014the posterior Dirichlet parameters $\\gamma^*(\\mathbf{w})$ associated with the document. It is of interest to see how much discriminatory information we lose in reducing the document description to these parameters.\n\nWe conducted two binary classification experiments using the Reuters-21578 dataset. The dataset contains 8000 documents and 15,818 words.\n\nIn these experiments, we estimated the parameters of an LDA model on all the documents, without reference to their true class label. We then trained a support vector machine (SVM) on the low-dimensional representations provided by LDA and compared this SVM to an SVM trained on all the word features.\n\nUsing the SVMLight software package (Joachims, 1999), we compared an SVM trained on all\nthe word features with those trained on features induced by a 50-topic LDA model. Note that we\nreduce the feature space by 99.6 percent in this case.\n\nFigure 10 shows our results. We see that there is little reduction in classification performance in using the LDA-based features; indeed, in almost all cases the performance is improved with the LDA features. Although these results need further substantiation, they suggest that the topic-based representation provided by LDA may be useful as a fast filtering algorithm for feature selection in text classification."
        },
        {
            "text": "Figure 10 shows our results. We see that there is little reduction in classification performance in using the LDA-based features; indeed, in almost all cases the performance is improved with the LDA features. Although these results need further substantiation, they suggest that the topic-based representation provided by LDA may be useful as a fast filtering algorithm for feature selection in text classification.",
            "page": 22,
            "x": 86,
            "y": 304,
            "width": 436,
            "height": 69,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "23-document",
            "chunk_id": "4cf892ff-9a90-4c1a-b58e-f30eea3ea29d",
            "group_text": "### 7.2 Document classification\n\nIn the text classification problem, we wish to classify a document into two or more mutually exclusive classes. As in any classification problem, we may wish to consider generative approaches or discriminative approaches. In particular, by using one LDA module for each class, we obtain a generative model for classification. It is also of interest to use LDA in the discriminative framework, and this is our focus in this section.\n\nA challenging aspect of the document classification problem is the choice of features. Treating individual words as features yields a rich but very large feature set (Joachims, 1999). One way to reduce this feature set is to use an LDA model for dimensionality reduction. In particular, LDA reduces any document to a fixed set of real-valued features\u2014the posterior Dirichlet parameters $\\gamma^*(\\mathbf{w})$ associated with the document. It is of interest to see how much discriminatory information we lose in reducing the document description to these parameters.\n\nWe conducted two binary classification experiments using the Reuters-21578 dataset. The dataset contains 8000 documents and 15,818 words.\n\nIn these experiments, we estimated the parameters of an LDA model on all the documents, without reference to their true class label. We then trained a support vector machine (SVM) on the low-dimensional representations provided by LDA and compared this SVM to an SVM trained on all the word features.\n\nUsing the SVMLight software package (Joachims, 1999), we compared an SVM trained on all\nthe word features with those trained on features induced by a 50-topic LDA model. Note that we\nreduce the feature space by 99.6 percent in this case.\n\nFigure 10 shows our results. We see that there is little reduction in classification performance in using the LDA-based features; indeed, in almost all cases the performance is improved with the LDA features. Although these results need further substantiation, they suggest that the topic-based representation provided by LDA may be useful as a fast filtering algorithm for feature selection in text classification."
        },
        {
            "text": "7.3 Collaborative filtering\n\nOur final experiment uses the EachMovie collaborative filtering data. In this data set, a collection of users indicates their preferred movie choices. A user and the movies chosen are analogous to a document and the words in the document (respectively).",
            "page": 22,
            "x": 87,
            "y": 387,
            "width": 435,
            "height": 60,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "24-collaborative",
            "chunk_id": "d7bc89af-e800-42c0-bcf3-6d1339887be3",
            "group_text": "7.3 Collaborative filtering\n\nOur final experiment uses the EachMovie collaborative filtering data. In this data set, a collection of users indicates their preferred movie choices. A user and the movies chosen are analogous to a document and the words in the document (respectively).\n\nThe collaborative filtering task is as follows. We train a model on a fully observed set of users. Then, for each unobserved user, we are shown all but one of the movies preferred by that user and are asked to predict what the held-out movie is. The different algorithms are evaluated according to the likelihood they assign to the held-out movie. More precisely, define the predictive perplexity on $M$ test users to be:\n\n$predictive\\text{-}perplexity(\\mathcal{D}_{\\text{test}}) = \\exp\\left\\{ -\\frac{\\sum_{d=1}^M \\log p(w_{d,N_d} \\mid \\mathbf{w}_{d,1:N_d-1})}{M} \\right\\}.$\n\nWe restricted the EachMovie dataset to users that positively rated at least 100 movies (a positive rating is at least four out of five stars). We divided this set of users into 3300 training users and 390 testing users.\n\nUnder the mixture of unigrams model, the probability of a movie given a set of observed movies\nis obtained from the posterior distribution over topics:\n\n$p(w|\\mathbf{w}_{\\text{obs}}) = \\sum_z p(w|z) p(z|\\mathbf{w}_{\\text{obs}}).$\n\nIn the pLSI model, the probability of a held-out movie is given by the same equation except that $p(z|\\mathbf{w}_{\\text{obs}})$ is computed by folding in the previously seen movies. Finally, in the LDA model, the\n\nprobability of a held-out movie is given by integrating over the posterior Dirichlet:\n\n$p(w|\\mathbf{w}_{\\text{obs}}) = \\int \\sum_{z} p(w|z) p(z|\\theta) p(\\theta|\\mathbf{w}_{\\text{obs}}) d\\theta,$\n\nwhere $p(\\boldsymbol{\\theta}|\\mathbf{w}_{\\text{obs}})$ is given by the variational inference method described in Section 5.2. Note that this quantity is efficient to compute. We can interchange the sum and integral sign, and compute a linear combination of $k$ Dirichlet expectations.\n\nWith a vocabulary of 1600 movies, we find the predictive perplexities illustrated in Figure 11. Again, the mixture of unigrams model and pLSI are corrected for overfitting, but the best predictive perplexities are obtained by the LDA model."
        },
        {
            "text": "The collaborative filtering task is as follows. We train a model on a fully observed set of users. Then, for each unobserved user, we are shown all but one of the movies preferred by that user and are asked to predict what the held-out movie is. The different algorithms are evaluated according to the likelihood they assign to the held-out movie. More precisely, define the predictive perplexity on $M$ test users to be:",
            "page": 22,
            "x": 87,
            "y": 448,
            "width": 435,
            "height": 66,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "24-collaborative",
            "chunk_id": "6196a08b-f8d8-4281-b518-a1a10c748d29",
            "group_text": "7.3 Collaborative filtering\n\nOur final experiment uses the EachMovie collaborative filtering data. In this data set, a collection of users indicates their preferred movie choices. A user and the movies chosen are analogous to a document and the words in the document (respectively).\n\nThe collaborative filtering task is as follows. We train a model on a fully observed set of users. Then, for each unobserved user, we are shown all but one of the movies preferred by that user and are asked to predict what the held-out movie is. The different algorithms are evaluated according to the likelihood they assign to the held-out movie. More precisely, define the predictive perplexity on $M$ test users to be:\n\n$predictive\\text{-}perplexity(\\mathcal{D}_{\\text{test}}) = \\exp\\left\\{ -\\frac{\\sum_{d=1}^M \\log p(w_{d,N_d} \\mid \\mathbf{w}_{d,1:N_d-1})}{M} \\right\\}.$\n\nWe restricted the EachMovie dataset to users that positively rated at least 100 movies (a positive rating is at least four out of five stars). We divided this set of users into 3300 training users and 390 testing users.\n\nUnder the mixture of unigrams model, the probability of a movie given a set of observed movies\nis obtained from the posterior distribution over topics:\n\n$p(w|\\mathbf{w}_{\\text{obs}}) = \\sum_z p(w|z) p(z|\\mathbf{w}_{\\text{obs}}).$\n\nIn the pLSI model, the probability of a held-out movie is given by the same equation except that $p(z|\\mathbf{w}_{\\text{obs}})$ is computed by folding in the previously seen movies. Finally, in the LDA model, the\n\nprobability of a held-out movie is given by integrating over the posterior Dirichlet:\n\n$p(w|\\mathbf{w}_{\\text{obs}}) = \\int \\sum_{z} p(w|z) p(z|\\theta) p(\\theta|\\mathbf{w}_{\\text{obs}}) d\\theta,$\n\nwhere $p(\\boldsymbol{\\theta}|\\mathbf{w}_{\\text{obs}})$ is given by the variational inference method described in Section 5.2. Note that this quantity is efficient to compute. We can interchange the sum and integral sign, and compute a linear combination of $k$ Dirichlet expectations.\n\nWith a vocabulary of 1600 movies, we find the predictive perplexities illustrated in Figure 11. Again, the mixture of unigrams model and pLSI are corrected for overfitting, but the best predictive perplexities are obtained by the LDA model."
        },
        {
            "text": "$predictive\\text{-}perplexity(\\mathcal{D}_{\\text{test}}) = \\exp\\left\\{ -\\frac{\\sum_{d=1}^M \\log p(w_{d,N_d} \\mid \\mathbf{w}_{d,1:N_d-1})}{M} \\right\\}.$",
            "page": 22,
            "x": 149,
            "y": 524,
            "width": 309,
            "height": 31,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "24-collaborative",
            "chunk_id": "078acbdc-ea59-4bd2-b2e4-510decfe9fb9",
            "group_text": "7.3 Collaborative filtering\n\nOur final experiment uses the EachMovie collaborative filtering data. In this data set, a collection of users indicates their preferred movie choices. A user and the movies chosen are analogous to a document and the words in the document (respectively).\n\nThe collaborative filtering task is as follows. We train a model on a fully observed set of users. Then, for each unobserved user, we are shown all but one of the movies preferred by that user and are asked to predict what the held-out movie is. The different algorithms are evaluated according to the likelihood they assign to the held-out movie. More precisely, define the predictive perplexity on $M$ test users to be:\n\n$predictive\\text{-}perplexity(\\mathcal{D}_{\\text{test}}) = \\exp\\left\\{ -\\frac{\\sum_{d=1}^M \\log p(w_{d,N_d} \\mid \\mathbf{w}_{d,1:N_d-1})}{M} \\right\\}.$\n\nWe restricted the EachMovie dataset to users that positively rated at least 100 movies (a positive rating is at least four out of five stars). We divided this set of users into 3300 training users and 390 testing users.\n\nUnder the mixture of unigrams model, the probability of a movie given a set of observed movies\nis obtained from the posterior distribution over topics:\n\n$p(w|\\mathbf{w}_{\\text{obs}}) = \\sum_z p(w|z) p(z|\\mathbf{w}_{\\text{obs}}).$\n\nIn the pLSI model, the probability of a held-out movie is given by the same equation except that $p(z|\\mathbf{w}_{\\text{obs}})$ is computed by folding in the previously seen movies. Finally, in the LDA model, the\n\nprobability of a held-out movie is given by integrating over the posterior Dirichlet:\n\n$p(w|\\mathbf{w}_{\\text{obs}}) = \\int \\sum_{z} p(w|z) p(z|\\theta) p(\\theta|\\mathbf{w}_{\\text{obs}}) d\\theta,$\n\nwhere $p(\\boldsymbol{\\theta}|\\mathbf{w}_{\\text{obs}})$ is given by the variational inference method described in Section 5.2. Note that this quantity is efficient to compute. We can interchange the sum and integral sign, and compute a linear combination of $k$ Dirichlet expectations.\n\nWith a vocabulary of 1600 movies, we find the predictive perplexities illustrated in Figure 11. Again, the mixture of unigrams model and pLSI are corrected for overfitting, but the best predictive perplexities are obtained by the LDA model."
        },
        {
            "text": "We restricted the EachMovie dataset to users that positively rated at least 100 movies (a positive rating is at least four out of five stars). We divided this set of users into 3300 training users and 390 testing users.",
            "page": 22,
            "x": 88,
            "y": 564,
            "width": 434,
            "height": 40,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "24-collaborative",
            "chunk_id": "e3228297-2728-41c6-976f-99ae766203bc",
            "group_text": "7.3 Collaborative filtering\n\nOur final experiment uses the EachMovie collaborative filtering data. In this data set, a collection of users indicates their preferred movie choices. A user and the movies chosen are analogous to a document and the words in the document (respectively).\n\nThe collaborative filtering task is as follows. We train a model on a fully observed set of users. Then, for each unobserved user, we are shown all but one of the movies preferred by that user and are asked to predict what the held-out movie is. The different algorithms are evaluated according to the likelihood they assign to the held-out movie. More precisely, define the predictive perplexity on $M$ test users to be:\n\n$predictive\\text{-}perplexity(\\mathcal{D}_{\\text{test}}) = \\exp\\left\\{ -\\frac{\\sum_{d=1}^M \\log p(w_{d,N_d} \\mid \\mathbf{w}_{d,1:N_d-1})}{M} \\right\\}.$\n\nWe restricted the EachMovie dataset to users that positively rated at least 100 movies (a positive rating is at least four out of five stars). We divided this set of users into 3300 training users and 390 testing users.\n\nUnder the mixture of unigrams model, the probability of a movie given a set of observed movies\nis obtained from the posterior distribution over topics:\n\n$p(w|\\mathbf{w}_{\\text{obs}}) = \\sum_z p(w|z) p(z|\\mathbf{w}_{\\text{obs}}).$\n\nIn the pLSI model, the probability of a held-out movie is given by the same equation except that $p(z|\\mathbf{w}_{\\text{obs}})$ is computed by folding in the previously seen movies. Finally, in the LDA model, the\n\nprobability of a held-out movie is given by integrating over the posterior Dirichlet:\n\n$p(w|\\mathbf{w}_{\\text{obs}}) = \\int \\sum_{z} p(w|z) p(z|\\theta) p(\\theta|\\mathbf{w}_{\\text{obs}}) d\\theta,$\n\nwhere $p(\\boldsymbol{\\theta}|\\mathbf{w}_{\\text{obs}})$ is given by the variational inference method described in Section 5.2. Note that this quantity is efficient to compute. We can interchange the sum and integral sign, and compute a linear combination of $k$ Dirichlet expectations.\n\nWith a vocabulary of 1600 movies, we find the predictive perplexities illustrated in Figure 11. Again, the mixture of unigrams model and pLSI are corrected for overfitting, but the best predictive perplexities are obtained by the LDA model."
        },
        {
            "text": "Under the mixture of unigrams model, the probability of a movie given a set of observed movies\nis obtained from the posterior distribution over topics:",
            "page": 22,
            "x": 88,
            "y": 606,
            "width": 433,
            "height": 27,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "24-collaborative",
            "chunk_id": "babed3c5-0edd-4c31-8997-6594e35d6f9e",
            "group_text": "7.3 Collaborative filtering\n\nOur final experiment uses the EachMovie collaborative filtering data. In this data set, a collection of users indicates their preferred movie choices. A user and the movies chosen are analogous to a document and the words in the document (respectively).\n\nThe collaborative filtering task is as follows. We train a model on a fully observed set of users. Then, for each unobserved user, we are shown all but one of the movies preferred by that user and are asked to predict what the held-out movie is. The different algorithms are evaluated according to the likelihood they assign to the held-out movie. More precisely, define the predictive perplexity on $M$ test users to be:\n\n$predictive\\text{-}perplexity(\\mathcal{D}_{\\text{test}}) = \\exp\\left\\{ -\\frac{\\sum_{d=1}^M \\log p(w_{d,N_d} \\mid \\mathbf{w}_{d,1:N_d-1})}{M} \\right\\}.$\n\nWe restricted the EachMovie dataset to users that positively rated at least 100 movies (a positive rating is at least four out of five stars). We divided this set of users into 3300 training users and 390 testing users.\n\nUnder the mixture of unigrams model, the probability of a movie given a set of observed movies\nis obtained from the posterior distribution over topics:\n\n$p(w|\\mathbf{w}_{\\text{obs}}) = \\sum_z p(w|z) p(z|\\mathbf{w}_{\\text{obs}}).$\n\nIn the pLSI model, the probability of a held-out movie is given by the same equation except that $p(z|\\mathbf{w}_{\\text{obs}})$ is computed by folding in the previously seen movies. Finally, in the LDA model, the\n\nprobability of a held-out movie is given by integrating over the posterior Dirichlet:\n\n$p(w|\\mathbf{w}_{\\text{obs}}) = \\int \\sum_{z} p(w|z) p(z|\\theta) p(\\theta|\\mathbf{w}_{\\text{obs}}) d\\theta,$\n\nwhere $p(\\boldsymbol{\\theta}|\\mathbf{w}_{\\text{obs}})$ is given by the variational inference method described in Section 5.2. Note that this quantity is efficient to compute. We can interchange the sum and integral sign, and compute a linear combination of $k$ Dirichlet expectations.\n\nWith a vocabulary of 1600 movies, we find the predictive perplexities illustrated in Figure 11. Again, the mixture of unigrams model and pLSI are corrected for overfitting, but the best predictive perplexities are obtained by the LDA model."
        },
        {
            "text": "$p(w|\\mathbf{w}_{\\text{obs}}) = \\sum_z p(w|z) p(z|\\mathbf{w}_{\\text{obs}}).$",
            "page": 22,
            "x": 230,
            "y": 641,
            "width": 148,
            "height": 28,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "24-collaborative",
            "chunk_id": "ccdbb404-a8a5-45fe-9796-ea3b124001b4",
            "group_text": "7.3 Collaborative filtering\n\nOur final experiment uses the EachMovie collaborative filtering data. In this data set, a collection of users indicates their preferred movie choices. A user and the movies chosen are analogous to a document and the words in the document (respectively).\n\nThe collaborative filtering task is as follows. We train a model on a fully observed set of users. Then, for each unobserved user, we are shown all but one of the movies preferred by that user and are asked to predict what the held-out movie is. The different algorithms are evaluated according to the likelihood they assign to the held-out movie. More precisely, define the predictive perplexity on $M$ test users to be:\n\n$predictive\\text{-}perplexity(\\mathcal{D}_{\\text{test}}) = \\exp\\left\\{ -\\frac{\\sum_{d=1}^M \\log p(w_{d,N_d} \\mid \\mathbf{w}_{d,1:N_d-1})}{M} \\right\\}.$\n\nWe restricted the EachMovie dataset to users that positively rated at least 100 movies (a positive rating is at least four out of five stars). We divided this set of users into 3300 training users and 390 testing users.\n\nUnder the mixture of unigrams model, the probability of a movie given a set of observed movies\nis obtained from the posterior distribution over topics:\n\n$p(w|\\mathbf{w}_{\\text{obs}}) = \\sum_z p(w|z) p(z|\\mathbf{w}_{\\text{obs}}).$\n\nIn the pLSI model, the probability of a held-out movie is given by the same equation except that $p(z|\\mathbf{w}_{\\text{obs}})$ is computed by folding in the previously seen movies. Finally, in the LDA model, the\n\nprobability of a held-out movie is given by integrating over the posterior Dirichlet:\n\n$p(w|\\mathbf{w}_{\\text{obs}}) = \\int \\sum_{z} p(w|z) p(z|\\theta) p(\\theta|\\mathbf{w}_{\\text{obs}}) d\\theta,$\n\nwhere $p(\\boldsymbol{\\theta}|\\mathbf{w}_{\\text{obs}})$ is given by the variational inference method described in Section 5.2. Note that this quantity is efficient to compute. We can interchange the sum and integral sign, and compute a linear combination of $k$ Dirichlet expectations.\n\nWith a vocabulary of 1600 movies, we find the predictive perplexities illustrated in Figure 11. Again, the mixture of unigrams model and pLSI are corrected for overfitting, but the best predictive perplexities are obtained by the LDA model."
        },
        {
            "text": "In the pLSI model, the probability of a held-out movie is given by the same equation except that $p(z|\\mathbf{w}_{\\text{obs}})$ is computed by folding in the previously seen movies. Finally, in the LDA model, the",
            "page": 22,
            "x": 88,
            "y": 678,
            "width": 434,
            "height": 29,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "24-collaborative",
            "chunk_id": "5efaae99-2431-4ae7-9f49-fb0deeb5b148",
            "group_text": "7.3 Collaborative filtering\n\nOur final experiment uses the EachMovie collaborative filtering data. In this data set, a collection of users indicates their preferred movie choices. A user and the movies chosen are analogous to a document and the words in the document (respectively).\n\nThe collaborative filtering task is as follows. We train a model on a fully observed set of users. Then, for each unobserved user, we are shown all but one of the movies preferred by that user and are asked to predict what the held-out movie is. The different algorithms are evaluated according to the likelihood they assign to the held-out movie. More precisely, define the predictive perplexity on $M$ test users to be:\n\n$predictive\\text{-}perplexity(\\mathcal{D}_{\\text{test}}) = \\exp\\left\\{ -\\frac{\\sum_{d=1}^M \\log p(w_{d,N_d} \\mid \\mathbf{w}_{d,1:N_d-1})}{M} \\right\\}.$\n\nWe restricted the EachMovie dataset to users that positively rated at least 100 movies (a positive rating is at least four out of five stars). We divided this set of users into 3300 training users and 390 testing users.\n\nUnder the mixture of unigrams model, the probability of a movie given a set of observed movies\nis obtained from the posterior distribution over topics:\n\n$p(w|\\mathbf{w}_{\\text{obs}}) = \\sum_z p(w|z) p(z|\\mathbf{w}_{\\text{obs}}).$\n\nIn the pLSI model, the probability of a held-out movie is given by the same equation except that $p(z|\\mathbf{w}_{\\text{obs}})$ is computed by folding in the previously seen movies. Finally, in the LDA model, the\n\nprobability of a held-out movie is given by integrating over the posterior Dirichlet:\n\n$p(w|\\mathbf{w}_{\\text{obs}}) = \\int \\sum_{z} p(w|z) p(z|\\theta) p(\\theta|\\mathbf{w}_{\\text{obs}}) d\\theta,$\n\nwhere $p(\\boldsymbol{\\theta}|\\mathbf{w}_{\\text{obs}})$ is given by the variational inference method described in Section 5.2. Note that this quantity is efficient to compute. We can interchange the sum and integral sign, and compute a linear combination of $k$ Dirichlet expectations.\n\nWith a vocabulary of 1600 movies, we find the predictive perplexities illustrated in Figure 11. Again, the mixture of unigrams model and pLSI are corrected for overfitting, but the best predictive perplexities are obtained by the LDA model."
        },
        {
            "text": "probability of a held-out movie is given by integrating over the posterior Dirichlet:",
            "page": 23,
            "x": 86,
            "y": 89,
            "width": 365,
            "height": 17,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "24-collaborative",
            "chunk_id": "bc8a562e-a0c4-4f64-8f4d-e3f5f6f4fa1f",
            "group_text": "7.3 Collaborative filtering\n\nOur final experiment uses the EachMovie collaborative filtering data. In this data set, a collection of users indicates their preferred movie choices. A user and the movies chosen are analogous to a document and the words in the document (respectively).\n\nThe collaborative filtering task is as follows. We train a model on a fully observed set of users. Then, for each unobserved user, we are shown all but one of the movies preferred by that user and are asked to predict what the held-out movie is. The different algorithms are evaluated according to the likelihood they assign to the held-out movie. More precisely, define the predictive perplexity on $M$ test users to be:\n\n$predictive\\text{-}perplexity(\\mathcal{D}_{\\text{test}}) = \\exp\\left\\{ -\\frac{\\sum_{d=1}^M \\log p(w_{d,N_d} \\mid \\mathbf{w}_{d,1:N_d-1})}{M} \\right\\}.$\n\nWe restricted the EachMovie dataset to users that positively rated at least 100 movies (a positive rating is at least four out of five stars). We divided this set of users into 3300 training users and 390 testing users.\n\nUnder the mixture of unigrams model, the probability of a movie given a set of observed movies\nis obtained from the posterior distribution over topics:\n\n$p(w|\\mathbf{w}_{\\text{obs}}) = \\sum_z p(w|z) p(z|\\mathbf{w}_{\\text{obs}}).$\n\nIn the pLSI model, the probability of a held-out movie is given by the same equation except that $p(z|\\mathbf{w}_{\\text{obs}})$ is computed by folding in the previously seen movies. Finally, in the LDA model, the\n\nprobability of a held-out movie is given by integrating over the posterior Dirichlet:\n\n$p(w|\\mathbf{w}_{\\text{obs}}) = \\int \\sum_{z} p(w|z) p(z|\\theta) p(\\theta|\\mathbf{w}_{\\text{obs}}) d\\theta,$\n\nwhere $p(\\boldsymbol{\\theta}|\\mathbf{w}_{\\text{obs}})$ is given by the variational inference method described in Section 5.2. Note that this quantity is efficient to compute. We can interchange the sum and integral sign, and compute a linear combination of $k$ Dirichlet expectations.\n\nWith a vocabulary of 1600 movies, we find the predictive perplexities illustrated in Figure 11. Again, the mixture of unigrams model and pLSI are corrected for overfitting, but the best predictive perplexities are obtained by the LDA model."
        },
        {
            "text": "$p(w|\\mathbf{w}_{\\text{obs}}) = \\int \\sum_{z} p(w|z) p(z|\\theta) p(\\theta|\\mathbf{w}_{\\text{obs}}) d\\theta,$",
            "page": 23,
            "x": 204,
            "y": 113,
            "width": 202,
            "height": 30,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "24-collaborative",
            "chunk_id": "37338e5d-c8a8-45f0-9020-ddc02e48c8cc",
            "group_text": "7.3 Collaborative filtering\n\nOur final experiment uses the EachMovie collaborative filtering data. In this data set, a collection of users indicates their preferred movie choices. A user and the movies chosen are analogous to a document and the words in the document (respectively).\n\nThe collaborative filtering task is as follows. We train a model on a fully observed set of users. Then, for each unobserved user, we are shown all but one of the movies preferred by that user and are asked to predict what the held-out movie is. The different algorithms are evaluated according to the likelihood they assign to the held-out movie. More precisely, define the predictive perplexity on $M$ test users to be:\n\n$predictive\\text{-}perplexity(\\mathcal{D}_{\\text{test}}) = \\exp\\left\\{ -\\frac{\\sum_{d=1}^M \\log p(w_{d,N_d} \\mid \\mathbf{w}_{d,1:N_d-1})}{M} \\right\\}.$\n\nWe restricted the EachMovie dataset to users that positively rated at least 100 movies (a positive rating is at least four out of five stars). We divided this set of users into 3300 training users and 390 testing users.\n\nUnder the mixture of unigrams model, the probability of a movie given a set of observed movies\nis obtained from the posterior distribution over topics:\n\n$p(w|\\mathbf{w}_{\\text{obs}}) = \\sum_z p(w|z) p(z|\\mathbf{w}_{\\text{obs}}).$\n\nIn the pLSI model, the probability of a held-out movie is given by the same equation except that $p(z|\\mathbf{w}_{\\text{obs}})$ is computed by folding in the previously seen movies. Finally, in the LDA model, the\n\nprobability of a held-out movie is given by integrating over the posterior Dirichlet:\n\n$p(w|\\mathbf{w}_{\\text{obs}}) = \\int \\sum_{z} p(w|z) p(z|\\theta) p(\\theta|\\mathbf{w}_{\\text{obs}}) d\\theta,$\n\nwhere $p(\\boldsymbol{\\theta}|\\mathbf{w}_{\\text{obs}})$ is given by the variational inference method described in Section 5.2. Note that this quantity is efficient to compute. We can interchange the sum and integral sign, and compute a linear combination of $k$ Dirichlet expectations.\n\nWith a vocabulary of 1600 movies, we find the predictive perplexities illustrated in Figure 11. Again, the mixture of unigrams model and pLSI are corrected for overfitting, but the best predictive perplexities are obtained by the LDA model."
        },
        {
            "text": "where $p(\\boldsymbol{\\theta}|\\mathbf{w}_{\\text{obs}})$ is given by the variational inference method described in Section 5.2. Note that this quantity is efficient to compute. We can interchange the sum and integral sign, and compute a linear combination of $k$ Dirichlet expectations.",
            "page": 23,
            "x": 86,
            "y": 151,
            "width": 437,
            "height": 40,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "24-collaborative",
            "chunk_id": "ef859ea4-a29d-4c37-8ada-ae8c665bf43c",
            "group_text": "7.3 Collaborative filtering\n\nOur final experiment uses the EachMovie collaborative filtering data. In this data set, a collection of users indicates their preferred movie choices. A user and the movies chosen are analogous to a document and the words in the document (respectively).\n\nThe collaborative filtering task is as follows. We train a model on a fully observed set of users. Then, for each unobserved user, we are shown all but one of the movies preferred by that user and are asked to predict what the held-out movie is. The different algorithms are evaluated according to the likelihood they assign to the held-out movie. More precisely, define the predictive perplexity on $M$ test users to be:\n\n$predictive\\text{-}perplexity(\\mathcal{D}_{\\text{test}}) = \\exp\\left\\{ -\\frac{\\sum_{d=1}^M \\log p(w_{d,N_d} \\mid \\mathbf{w}_{d,1:N_d-1})}{M} \\right\\}.$\n\nWe restricted the EachMovie dataset to users that positively rated at least 100 movies (a positive rating is at least four out of five stars). We divided this set of users into 3300 training users and 390 testing users.\n\nUnder the mixture of unigrams model, the probability of a movie given a set of observed movies\nis obtained from the posterior distribution over topics:\n\n$p(w|\\mathbf{w}_{\\text{obs}}) = \\sum_z p(w|z) p(z|\\mathbf{w}_{\\text{obs}}).$\n\nIn the pLSI model, the probability of a held-out movie is given by the same equation except that $p(z|\\mathbf{w}_{\\text{obs}})$ is computed by folding in the previously seen movies. Finally, in the LDA model, the\n\nprobability of a held-out movie is given by integrating over the posterior Dirichlet:\n\n$p(w|\\mathbf{w}_{\\text{obs}}) = \\int \\sum_{z} p(w|z) p(z|\\theta) p(\\theta|\\mathbf{w}_{\\text{obs}}) d\\theta,$\n\nwhere $p(\\boldsymbol{\\theta}|\\mathbf{w}_{\\text{obs}})$ is given by the variational inference method described in Section 5.2. Note that this quantity is efficient to compute. We can interchange the sum and integral sign, and compute a linear combination of $k$ Dirichlet expectations.\n\nWith a vocabulary of 1600 movies, we find the predictive perplexities illustrated in Figure 11. Again, the mixture of unigrams model and pLSI are corrected for overfitting, but the best predictive perplexities are obtained by the LDA model."
        },
        {
            "text": "With a vocabulary of 1600 movies, we find the predictive perplexities illustrated in Figure 11. Again, the mixture of unigrams model and pLSI are corrected for overfitting, but the best predictive perplexities are obtained by the LDA model.",
            "page": 23,
            "x": 87,
            "y": 193,
            "width": 434,
            "height": 40,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "24-collaborative",
            "chunk_id": "1478c2b5-a398-43a5-8eb7-442711fcb271",
            "group_text": "7.3 Collaborative filtering\n\nOur final experiment uses the EachMovie collaborative filtering data. In this data set, a collection of users indicates their preferred movie choices. A user and the movies chosen are analogous to a document and the words in the document (respectively).\n\nThe collaborative filtering task is as follows. We train a model on a fully observed set of users. Then, for each unobserved user, we are shown all but one of the movies preferred by that user and are asked to predict what the held-out movie is. The different algorithms are evaluated according to the likelihood they assign to the held-out movie. More precisely, define the predictive perplexity on $M$ test users to be:\n\n$predictive\\text{-}perplexity(\\mathcal{D}_{\\text{test}}) = \\exp\\left\\{ -\\frac{\\sum_{d=1}^M \\log p(w_{d,N_d} \\mid \\mathbf{w}_{d,1:N_d-1})}{M} \\right\\}.$\n\nWe restricted the EachMovie dataset to users that positively rated at least 100 movies (a positive rating is at least four out of five stars). We divided this set of users into 3300 training users and 390 testing users.\n\nUnder the mixture of unigrams model, the probability of a movie given a set of observed movies\nis obtained from the posterior distribution over topics:\n\n$p(w|\\mathbf{w}_{\\text{obs}}) = \\sum_z p(w|z) p(z|\\mathbf{w}_{\\text{obs}}).$\n\nIn the pLSI model, the probability of a held-out movie is given by the same equation except that $p(z|\\mathbf{w}_{\\text{obs}})$ is computed by folding in the previously seen movies. Finally, in the LDA model, the\n\nprobability of a held-out movie is given by integrating over the posterior Dirichlet:\n\n$p(w|\\mathbf{w}_{\\text{obs}}) = \\int \\sum_{z} p(w|z) p(z|\\theta) p(\\theta|\\mathbf{w}_{\\text{obs}}) d\\theta,$\n\nwhere $p(\\boldsymbol{\\theta}|\\mathbf{w}_{\\text{obs}})$ is given by the variational inference method described in Section 5.2. Note that this quantity is efficient to compute. We can interchange the sum and integral sign, and compute a linear combination of $k$ Dirichlet expectations.\n\nWith a vocabulary of 1600 movies, we find the predictive perplexities illustrated in Figure 11. Again, the mixture of unigrams model and pLSI are corrected for overfitting, but the best predictive perplexities are obtained by the LDA model."
        },
        {
            "text": "8. Discussion  \nWe have described latent Dirichlet allocation, a flexible generative probabilistic model for collections of discrete data. LDA is based on a simple exchangeability assumption for the words and topics in a document; it is therefore realized by a straightforward application of de Finetti\u2019s representation theorem. We can view LDA as a dimensionality reduction technique, in the spirit of LSI, but with proper underlying generative probabilistic semantics that make sense for the type of data that it models.",
            "page": 23,
            "x": 87,
            "y": 251,
            "width": 435,
            "height": 101,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "25-discussion",
            "chunk_id": "0dd10bdc-e15a-4ff5-8072-9404586bd793",
            "group_text": "8. Discussion  \nWe have described latent Dirichlet allocation, a flexible generative probabilistic model for collections of discrete data. LDA is based on a simple exchangeability assumption for the words and topics in a document; it is therefore realized by a straightforward application of de Finetti\u2019s representation theorem. We can view LDA as a dimensionality reduction technique, in the spirit of LSI, but with proper underlying generative probabilistic semantics that make sense for the type of data that it models.\n\nExact inference is intractable for LDA, but any of a large suite of approximate inference algorithms can be used for inference and parameter estimation within the LDA framework. We have presented a simple convexity-based variational approach for inference, showing that it yields a fast algorithm resulting in reasonable comparative performance in terms of test set likelihood. Other approaches that might be considered include Laplace approximation, higher-order variational techniques, and Monte Carlo methods. In particular, Leisink and Kappen (2002) have presented a general methodology for converting low-order variational lower bounds into higher-order variational bounds. It is also possible to achieve higher accuracy by dispensing with the requirement of maintaining a bound, and indeed Minka and Lafferty (2002) have shown that improved inferential accuracy can be obtained for the LDA model via a higher-order variational technique known as expectation propagation. Finally, Griffiths and Steyvers (2002) have presented a Markov chain Monte Carlo algorithm for LDA.\n\nLDA is a simple model, and although we view it as a competitor to methods such as LSI and\npLSI in the setting of dimensionality reduction for document collections and other discrete cor-\npora, it is also intended to be illustrative of the way in which probabilistic models can be scaled\nup to provide useful inferential machinery in domains involving multiple levels of structure. In-\ndeed, the principal advantages of generative models such as LDA include their modularity and their\nextensibility. As a probabilistic module, LDA can be readily embedded in a more complex model\u2014\na property that is not possessed by LSI. In recent work we have used pairs of LDA modules to\nmodel relationships between images and their corresponding descriptive captions (Blei and Jordan,\n2002). Moreover, there are numerous possible extensions of LDA. For example, LDA is readily\nextended to continuous data or other non-multinomial data. As is the case for other mixture models,\nincluding finite mixture models and hidden Markov models, the \u201cemission\u201d probability $p(w_n|z_n)$\ncontributes only a likelihood value to the inference procedures for LDA, and other likelihoods are\nreadily substituted in its place. In particular, it is straightforward to develop a continuous variant of\nLDA in which Gaussian observables are used in place of multinomials. Another simple extension\n\nof LDA comes from allowing mixtures of Dirichlet distributions in the place of the single Dirichlet\nof LDA. This allows a richer structure in the latent topic space and in particular allows a form of\ndocument clustering that is different from the clustering that is achieved via shared topics. Finally,\na variety of extensions of LDA can be considered in which the distributions on the topic variables\nare elaborated. For example, we could arrange the topics in a time series, essentially relaxing the\nfull exchangeability assumption to one of partial exchangeability. We could also consider partially\nexchangeable models in which we condition on exogenous variables; thus, for example, the topic\ndistribution could be conditioned on features such as \u201cparagraph\u201d or \u201csentence,\u201d providing a more\npowerful text model that makes use of information obtained from a parser.\n\n**Acknowledgements**\n\nThis work was supported by the National Science Foundation (NSF grant IIS-9988642) and the Multidisciplinary Research Program of the Department of Defense (MURI N00014-00-1-0637). Andrew Y. Ng and David M. Blei were additionally supported by fellowships from the Microsoft Corporation."
        },
        {
            "text": "Exact inference is intractable for LDA, but any of a large suite of approximate inference algorithms can be used for inference and parameter estimation within the LDA framework. We have presented a simple convexity-based variational approach for inference, showing that it yields a fast algorithm resulting in reasonable comparative performance in terms of test set likelihood. Other approaches that might be considered include Laplace approximation, higher-order variational techniques, and Monte Carlo methods. In particular, Leisink and Kappen (2002) have presented a general methodology for converting low-order variational lower bounds into higher-order variational bounds. It is also possible to achieve higher accuracy by dispensing with the requirement of maintaining a bound, and indeed Minka and Lafferty (2002) have shown that improved inferential accuracy can be obtained for the LDA model via a higher-order variational technique known as expectation propagation. Finally, Griffiths and Steyvers (2002) have presented a Markov chain Monte Carlo algorithm for LDA.",
            "page": 23,
            "x": 88,
            "y": 354,
            "width": 435,
            "height": 160,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "25-discussion",
            "chunk_id": "5c422d53-df8b-423d-9c01-a063962ffc80",
            "group_text": "8. Discussion  \nWe have described latent Dirichlet allocation, a flexible generative probabilistic model for collections of discrete data. LDA is based on a simple exchangeability assumption for the words and topics in a document; it is therefore realized by a straightforward application of de Finetti\u2019s representation theorem. We can view LDA as a dimensionality reduction technique, in the spirit of LSI, but with proper underlying generative probabilistic semantics that make sense for the type of data that it models.\n\nExact inference is intractable for LDA, but any of a large suite of approximate inference algorithms can be used for inference and parameter estimation within the LDA framework. We have presented a simple convexity-based variational approach for inference, showing that it yields a fast algorithm resulting in reasonable comparative performance in terms of test set likelihood. Other approaches that might be considered include Laplace approximation, higher-order variational techniques, and Monte Carlo methods. In particular, Leisink and Kappen (2002) have presented a general methodology for converting low-order variational lower bounds into higher-order variational bounds. It is also possible to achieve higher accuracy by dispensing with the requirement of maintaining a bound, and indeed Minka and Lafferty (2002) have shown that improved inferential accuracy can be obtained for the LDA model via a higher-order variational technique known as expectation propagation. Finally, Griffiths and Steyvers (2002) have presented a Markov chain Monte Carlo algorithm for LDA.\n\nLDA is a simple model, and although we view it as a competitor to methods such as LSI and\npLSI in the setting of dimensionality reduction for document collections and other discrete cor-\npora, it is also intended to be illustrative of the way in which probabilistic models can be scaled\nup to provide useful inferential machinery in domains involving multiple levels of structure. In-\ndeed, the principal advantages of generative models such as LDA include their modularity and their\nextensibility. As a probabilistic module, LDA can be readily embedded in a more complex model\u2014\na property that is not possessed by LSI. In recent work we have used pairs of LDA modules to\nmodel relationships between images and their corresponding descriptive captions (Blei and Jordan,\n2002). Moreover, there are numerous possible extensions of LDA. For example, LDA is readily\nextended to continuous data or other non-multinomial data. As is the case for other mixture models,\nincluding finite mixture models and hidden Markov models, the \u201cemission\u201d probability $p(w_n|z_n)$\ncontributes only a likelihood value to the inference procedures for LDA, and other likelihoods are\nreadily substituted in its place. In particular, it is straightforward to develop a continuous variant of\nLDA in which Gaussian observables are used in place of multinomials. Another simple extension\n\nof LDA comes from allowing mixtures of Dirichlet distributions in the place of the single Dirichlet\nof LDA. This allows a richer structure in the latent topic space and in particular allows a form of\ndocument clustering that is different from the clustering that is achieved via shared topics. Finally,\na variety of extensions of LDA can be considered in which the distributions on the topic variables\nare elaborated. For example, we could arrange the topics in a time series, essentially relaxing the\nfull exchangeability assumption to one of partial exchangeability. We could also consider partially\nexchangeable models in which we condition on exogenous variables; thus, for example, the topic\ndistribution could be conditioned on features such as \u201cparagraph\u201d or \u201csentence,\u201d providing a more\npowerful text model that makes use of information obtained from a parser.\n\n**Acknowledgements**\n\nThis work was supported by the National Science Foundation (NSF grant IIS-9988642) and the Multidisciplinary Research Program of the Department of Defense (MURI N00014-00-1-0637). Andrew Y. Ng and David M. Blei were additionally supported by fellowships from the Microsoft Corporation."
        },
        {
            "text": "LDA is a simple model, and although we view it as a competitor to methods such as LSI and\npLSI in the setting of dimensionality reduction for document collections and other discrete cor-\npora, it is also intended to be illustrative of the way in which probabilistic models can be scaled\nup to provide useful inferential machinery in domains involving multiple levels of structure. In-\ndeed, the principal advantages of generative models such as LDA include their modularity and their\nextensibility. As a probabilistic module, LDA can be readily embedded in a more complex model\u2014\na property that is not possessed by LSI. In recent work we have used pairs of LDA modules to\nmodel relationships between images and their corresponding descriptive captions (Blei and Jordan,\n2002). Moreover, there are numerous possible extensions of LDA. For example, LDA is readily\nextended to continuous data or other non-multinomial data. As is the case for other mixture models,\nincluding finite mixture models and hidden Markov models, the \u201cemission\u201d probability $p(w_n|z_n)$\ncontributes only a likelihood value to the inference procedures for LDA, and other likelihoods are\nreadily substituted in its place. In particular, it is straightforward to develop a continuous variant of\nLDA in which Gaussian observables are used in place of multinomials. Another simple extension",
            "page": 23,
            "x": 88,
            "y": 517,
            "width": 434,
            "height": 188,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "25-discussion",
            "chunk_id": "7552c11e-a3a1-4388-8ad6-02eefd30c224",
            "group_text": "8. Discussion  \nWe have described latent Dirichlet allocation, a flexible generative probabilistic model for collections of discrete data. LDA is based on a simple exchangeability assumption for the words and topics in a document; it is therefore realized by a straightforward application of de Finetti\u2019s representation theorem. We can view LDA as a dimensionality reduction technique, in the spirit of LSI, but with proper underlying generative probabilistic semantics that make sense for the type of data that it models.\n\nExact inference is intractable for LDA, but any of a large suite of approximate inference algorithms can be used for inference and parameter estimation within the LDA framework. We have presented a simple convexity-based variational approach for inference, showing that it yields a fast algorithm resulting in reasonable comparative performance in terms of test set likelihood. Other approaches that might be considered include Laplace approximation, higher-order variational techniques, and Monte Carlo methods. In particular, Leisink and Kappen (2002) have presented a general methodology for converting low-order variational lower bounds into higher-order variational bounds. It is also possible to achieve higher accuracy by dispensing with the requirement of maintaining a bound, and indeed Minka and Lafferty (2002) have shown that improved inferential accuracy can be obtained for the LDA model via a higher-order variational technique known as expectation propagation. Finally, Griffiths and Steyvers (2002) have presented a Markov chain Monte Carlo algorithm for LDA.\n\nLDA is a simple model, and although we view it as a competitor to methods such as LSI and\npLSI in the setting of dimensionality reduction for document collections and other discrete cor-\npora, it is also intended to be illustrative of the way in which probabilistic models can be scaled\nup to provide useful inferential machinery in domains involving multiple levels of structure. In-\ndeed, the principal advantages of generative models such as LDA include their modularity and their\nextensibility. As a probabilistic module, LDA can be readily embedded in a more complex model\u2014\na property that is not possessed by LSI. In recent work we have used pairs of LDA modules to\nmodel relationships between images and their corresponding descriptive captions (Blei and Jordan,\n2002). Moreover, there are numerous possible extensions of LDA. For example, LDA is readily\nextended to continuous data or other non-multinomial data. As is the case for other mixture models,\nincluding finite mixture models and hidden Markov models, the \u201cemission\u201d probability $p(w_n|z_n)$\ncontributes only a likelihood value to the inference procedures for LDA, and other likelihoods are\nreadily substituted in its place. In particular, it is straightforward to develop a continuous variant of\nLDA in which Gaussian observables are used in place of multinomials. Another simple extension\n\nof LDA comes from allowing mixtures of Dirichlet distributions in the place of the single Dirichlet\nof LDA. This allows a richer structure in the latent topic space and in particular allows a form of\ndocument clustering that is different from the clustering that is achieved via shared topics. Finally,\na variety of extensions of LDA can be considered in which the distributions on the topic variables\nare elaborated. For example, we could arrange the topics in a time series, essentially relaxing the\nfull exchangeability assumption to one of partial exchangeability. We could also consider partially\nexchangeable models in which we condition on exogenous variables; thus, for example, the topic\ndistribution could be conditioned on features such as \u201cparagraph\u201d or \u201csentence,\u201d providing a more\npowerful text model that makes use of information obtained from a parser.\n\n**Acknowledgements**\n\nThis work was supported by the National Science Foundation (NSF grant IIS-9988642) and the Multidisciplinary Research Program of the Department of Defense (MURI N00014-00-1-0637). Andrew Y. Ng and David M. Blei were additionally supported by fellowships from the Microsoft Corporation."
        },
        {
            "text": "of LDA comes from allowing mixtures of Dirichlet distributions in the place of the single Dirichlet\nof LDA. This allows a richer structure in the latent topic space and in particular allows a form of\ndocument clustering that is different from the clustering that is achieved via shared topics. Finally,\na variety of extensions of LDA can be considered in which the distributions on the topic variables\nare elaborated. For example, we could arrange the topics in a time series, essentially relaxing the\nfull exchangeability assumption to one of partial exchangeability. We could also consider partially\nexchangeable models in which we condition on exogenous variables; thus, for example, the topic\ndistribution could be conditioned on features such as \u201cparagraph\u201d or \u201csentence,\u201d providing a more\npowerful text model that makes use of information obtained from a parser.",
            "page": 24,
            "x": 85,
            "y": 88,
            "width": 440,
            "height": 127,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "25-discussion",
            "chunk_id": "1365baf9-3c89-4616-a762-a492a9fdc7d7",
            "group_text": "8. Discussion  \nWe have described latent Dirichlet allocation, a flexible generative probabilistic model for collections of discrete data. LDA is based on a simple exchangeability assumption for the words and topics in a document; it is therefore realized by a straightforward application of de Finetti\u2019s representation theorem. We can view LDA as a dimensionality reduction technique, in the spirit of LSI, but with proper underlying generative probabilistic semantics that make sense for the type of data that it models.\n\nExact inference is intractable for LDA, but any of a large suite of approximate inference algorithms can be used for inference and parameter estimation within the LDA framework. We have presented a simple convexity-based variational approach for inference, showing that it yields a fast algorithm resulting in reasonable comparative performance in terms of test set likelihood. Other approaches that might be considered include Laplace approximation, higher-order variational techniques, and Monte Carlo methods. In particular, Leisink and Kappen (2002) have presented a general methodology for converting low-order variational lower bounds into higher-order variational bounds. It is also possible to achieve higher accuracy by dispensing with the requirement of maintaining a bound, and indeed Minka and Lafferty (2002) have shown that improved inferential accuracy can be obtained for the LDA model via a higher-order variational technique known as expectation propagation. Finally, Griffiths and Steyvers (2002) have presented a Markov chain Monte Carlo algorithm for LDA.\n\nLDA is a simple model, and although we view it as a competitor to methods such as LSI and\npLSI in the setting of dimensionality reduction for document collections and other discrete cor-\npora, it is also intended to be illustrative of the way in which probabilistic models can be scaled\nup to provide useful inferential machinery in domains involving multiple levels of structure. In-\ndeed, the principal advantages of generative models such as LDA include their modularity and their\nextensibility. As a probabilistic module, LDA can be readily embedded in a more complex model\u2014\na property that is not possessed by LSI. In recent work we have used pairs of LDA modules to\nmodel relationships between images and their corresponding descriptive captions (Blei and Jordan,\n2002). Moreover, there are numerous possible extensions of LDA. For example, LDA is readily\nextended to continuous data or other non-multinomial data. As is the case for other mixture models,\nincluding finite mixture models and hidden Markov models, the \u201cemission\u201d probability $p(w_n|z_n)$\ncontributes only a likelihood value to the inference procedures for LDA, and other likelihoods are\nreadily substituted in its place. In particular, it is straightforward to develop a continuous variant of\nLDA in which Gaussian observables are used in place of multinomials. Another simple extension\n\nof LDA comes from allowing mixtures of Dirichlet distributions in the place of the single Dirichlet\nof LDA. This allows a richer structure in the latent topic space and in particular allows a form of\ndocument clustering that is different from the clustering that is achieved via shared topics. Finally,\na variety of extensions of LDA can be considered in which the distributions on the topic variables\nare elaborated. For example, we could arrange the topics in a time series, essentially relaxing the\nfull exchangeability assumption to one of partial exchangeability. We could also consider partially\nexchangeable models in which we condition on exogenous variables; thus, for example, the topic\ndistribution could be conditioned on features such as \u201cparagraph\u201d or \u201csentence,\u201d providing a more\npowerful text model that makes use of information obtained from a parser.\n\n**Acknowledgements**\n\nThis work was supported by the National Science Foundation (NSF grant IIS-9988642) and the Multidisciplinary Research Program of the Department of Defense (MURI N00014-00-1-0637). Andrew Y. Ng and David M. Blei were additionally supported by fellowships from the Microsoft Corporation."
        },
        {
            "text": "**Acknowledgements**\n\nThis work was supported by the National Science Foundation (NSF grant IIS-9988642) and the Multidisciplinary Research Program of the Department of Defense (MURI N00014-00-1-0637). Andrew Y. Ng and David M. Blei were additionally supported by fellowships from the Microsoft Corporation.",
            "page": 24,
            "x": 86,
            "y": 229,
            "width": 438,
            "height": 79,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "25-discussion",
            "chunk_id": "77609c13-001f-4732-83d1-8fe0406114e3",
            "group_text": "8. Discussion  \nWe have described latent Dirichlet allocation, a flexible generative probabilistic model for collections of discrete data. LDA is based on a simple exchangeability assumption for the words and topics in a document; it is therefore realized by a straightforward application of de Finetti\u2019s representation theorem. We can view LDA as a dimensionality reduction technique, in the spirit of LSI, but with proper underlying generative probabilistic semantics that make sense for the type of data that it models.\n\nExact inference is intractable for LDA, but any of a large suite of approximate inference algorithms can be used for inference and parameter estimation within the LDA framework. We have presented a simple convexity-based variational approach for inference, showing that it yields a fast algorithm resulting in reasonable comparative performance in terms of test set likelihood. Other approaches that might be considered include Laplace approximation, higher-order variational techniques, and Monte Carlo methods. In particular, Leisink and Kappen (2002) have presented a general methodology for converting low-order variational lower bounds into higher-order variational bounds. It is also possible to achieve higher accuracy by dispensing with the requirement of maintaining a bound, and indeed Minka and Lafferty (2002) have shown that improved inferential accuracy can be obtained for the LDA model via a higher-order variational technique known as expectation propagation. Finally, Griffiths and Steyvers (2002) have presented a Markov chain Monte Carlo algorithm for LDA.\n\nLDA is a simple model, and although we view it as a competitor to methods such as LSI and\npLSI in the setting of dimensionality reduction for document collections and other discrete cor-\npora, it is also intended to be illustrative of the way in which probabilistic models can be scaled\nup to provide useful inferential machinery in domains involving multiple levels of structure. In-\ndeed, the principal advantages of generative models such as LDA include their modularity and their\nextensibility. As a probabilistic module, LDA can be readily embedded in a more complex model\u2014\na property that is not possessed by LSI. In recent work we have used pairs of LDA modules to\nmodel relationships between images and their corresponding descriptive captions (Blei and Jordan,\n2002). Moreover, there are numerous possible extensions of LDA. For example, LDA is readily\nextended to continuous data or other non-multinomial data. As is the case for other mixture models,\nincluding finite mixture models and hidden Markov models, the \u201cemission\u201d probability $p(w_n|z_n)$\ncontributes only a likelihood value to the inference procedures for LDA, and other likelihoods are\nreadily substituted in its place. In particular, it is straightforward to develop a continuous variant of\nLDA in which Gaussian observables are used in place of multinomials. Another simple extension\n\nof LDA comes from allowing mixtures of Dirichlet distributions in the place of the single Dirichlet\nof LDA. This allows a richer structure in the latent topic space and in particular allows a form of\ndocument clustering that is different from the clustering that is achieved via shared topics. Finally,\na variety of extensions of LDA can be considered in which the distributions on the topic variables\nare elaborated. For example, we could arrange the topics in a time series, essentially relaxing the\nfull exchangeability assumption to one of partial exchangeability. We could also consider partially\nexchangeable models in which we condition on exogenous variables; thus, for example, the topic\ndistribution could be conditioned on features such as \u201cparagraph\u201d or \u201csentence,\u201d providing a more\npowerful text model that makes use of information obtained from a parser.\n\n**Acknowledgements**\n\nThis work was supported by the National Science Foundation (NSF grant IIS-9988642) and the Multidisciplinary Research Program of the Department of Defense (MURI N00014-00-1-0637). Andrew Y. Ng and David M. Blei were additionally supported by fellowships from the Microsoft Corporation."
        },
        {
            "text": "References\n\nM. Abramowitz and I. Stegun, editors. _Handbook of Mathematical Functions_. Dover, New York, 1970.\n\nD. Aldous. Exchangeability and related topics. In _\u00c9cole d\u2019\u00e9t\u00e9 de probabilit\u00e9s de Saint-Flour, XIII\u20141983_, pages 1\u2013198. Springer, Berlin, 1985.\n\nH. Attias. A variational Bayesian framework for graphical models. In _Advances in Neural Information Processing Systems 12_, 2000.\n\nL. Avery. Caenorrhabditis genetic center bibliography. 2002. URL http://elegans.swmed.edu/wli/cgcbib.\n\nR. Baeza-Yates and B. Ribeiro-Neto. _Modern Information Retrieval_. ACM Press, New York, 1999.\n\nD. Blei and M. Jordan. Modeling annotated data. Technical Report UCB//CSD-02-1202, U.C. Berkeley Computer Science Division, 2002.\n\nB. de Finetti. _Theory of probability. Vol. 1-2_. John Wiley & Sons Ltd., Chichester, 1990. Reprint of the 1975 translation.\n\nS. Deerwester, S. Dumais, T. Landauer, G. Furnas, and R. Harshman. Indexing by latent semantic analysis. _Journal of the American Society of Information Science_, 41(6):391\u2013407, 1990.\n\nP. Diaconis. Recent progress on de Finetti\u2019s notions of exchangeability. In _Bayesian statistics, 3 (Valencia, 1987)_, pages 111\u2013125. Oxford Univ. Press, New York, 1988.\n\nJ. Dickey. Multiple hypergeometric functions: Probabilistic interpretations and statistical uses. _Journal of the American Statistical Association_, 78:628\u2013637, 1983.",
            "page": 24,
            "x": 84,
            "y": 324,
            "width": 442,
            "height": 383,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "26-reference",
            "chunk_id": "b18ce111-a191-4204-96ca-7ce841c87f7e",
            "group_text": "References\n\nM. Abramowitz and I. Stegun, editors. _Handbook of Mathematical Functions_. Dover, New York, 1970.\n\nD. Aldous. Exchangeability and related topics. In _\u00c9cole d\u2019\u00e9t\u00e9 de probabilit\u00e9s de Saint-Flour, XIII\u20141983_, pages 1\u2013198. Springer, Berlin, 1985.\n\nH. Attias. A variational Bayesian framework for graphical models. In _Advances in Neural Information Processing Systems 12_, 2000.\n\nL. Avery. Caenorrhabditis genetic center bibliography. 2002. URL http://elegans.swmed.edu/wli/cgcbib.\n\nR. Baeza-Yates and B. Ribeiro-Neto. _Modern Information Retrieval_. ACM Press, New York, 1999.\n\nD. Blei and M. Jordan. Modeling annotated data. Technical Report UCB//CSD-02-1202, U.C. Berkeley Computer Science Division, 2002.\n\nB. de Finetti. _Theory of probability. Vol. 1-2_. John Wiley & Sons Ltd., Chichester, 1990. Reprint of the 1975 translation.\n\nS. Deerwester, S. Dumais, T. Landauer, G. Furnas, and R. Harshman. Indexing by latent semantic analysis. _Journal of the American Society of Information Science_, 41(6):391\u2013407, 1990.\n\nP. Diaconis. Recent progress on de Finetti\u2019s notions of exchangeability. In _Bayesian statistics, 3 (Valencia, 1987)_, pages 111\u2013125. Oxford Univ. Press, New York, 1988.\n\nJ. Dickey. Multiple hypergeometric functions: Probabilistic interpretations and statistical uses. _Journal of the American Statistical Association_, 78:628\u2013637, 1983."
        },
        {
            "text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.",
            "page": 25,
            "x": 81,
            "y": 87,
            "width": 450,
            "height": 622,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "b1292f5b-e8b4-4947-932e-063d286104c2",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "A. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.",
            "page": 26,
            "x": 85,
            "y": 88,
            "width": 442,
            "height": 151,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "9453bb1c-274e-4450-9df1-3139d6acb137",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.",
            "page": 26,
            "x": 86,
            "y": 263,
            "width": 437,
            "height": 66,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "7aaa3c2c-82ba-464b-bf0c-d3c01c8103c5",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "A.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:",
            "page": 26,
            "x": 86,
            "y": 339,
            "width": 437,
            "height": 90,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "7a3b0c80-6240-45fc-9bf3-5ebe420d740e",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$",
            "page": 26,
            "x": 221,
            "y": 435,
            "width": 169,
            "height": 19,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "5caed1c2-37ff-4cca-a3cb-38805e2dfba4",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "where $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):",
            "page": 26,
            "x": 87,
            "y": 459,
            "width": 434,
            "height": 42,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "97cf1f3d-7cd7-49c0-82a7-508bc65e7d6a",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$",
            "page": 26,
            "x": 143,
            "y": 507,
            "width": 321,
            "height": 19,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "4d3cd6d6-96d6-444d-9156-6f4c0f0ea898",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "From this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:",
            "page": 26,
            "x": 87,
            "y": 532,
            "width": 437,
            "height": 54,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "81052e58-7104-40bb-9e03-680bf6dd5dfd",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$",
            "page": 26,
            "x": 226,
            "y": 593,
            "width": 158,
            "height": 20,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "b07ca29f-60dc-4bd1-8ce9-09cd19a71337",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "where $\\Psi$ is the digamma function, the first derivative of the log Gamma function.",
            "page": 26,
            "x": 88,
            "y": 618,
            "width": 354,
            "height": 13,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "3dec84e5-6c59-4ffa-a497-756060c33b45",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).",
            "page": 26,
            "x": 87,
            "y": 645,
            "width": 434,
            "height": 61,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "780ef1a9-936d-4428-8671-8ecc4ffdf4a4",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "The Newton-Raphson optimization technique finds a stationary point of a function by iterating:",
            "page": 27,
            "x": 103,
            "y": 88,
            "width": 419,
            "height": 19,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "3ae7820e-088f-46f7-9d1a-e6ce3095f5b9",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$",
            "page": 27,
            "x": 232,
            "y": 112,
            "width": 146,
            "height": 21,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "2be50803-493d-4fb4-bb3a-4688ce479f5d",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "where $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:",
            "page": 27,
            "x": 86,
            "y": 139,
            "width": 436,
            "height": 43,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "dc116304-c894-4645-a421-176089001e14",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$",
            "page": 27,
            "x": 257,
            "y": 188,
            "width": 266,
            "height": 22,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "f1f3c07d-ede7-4430-bf7a-6a2b13d9d2e9",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "where diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:",
            "page": 27,
            "x": 87,
            "y": 215,
            "width": 435,
            "height": 29,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "7b379342-5866-4386-86e2-0a4991e217fe",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$",
            "page": 27,
            "x": 205,
            "y": 251,
            "width": 200,
            "height": 37,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "8c644388-a6d4-4480-b759-6acf0bd3c362",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "Multiplying by the gradient, we obtain the $i$th component:",
            "page": 27,
            "x": 88,
            "y": 294,
            "width": 255,
            "height": 15,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "e082a828-cebf-4852-b2e7-3c7e7b6c388e",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$",
            "page": 27,
            "x": 264,
            "y": 316,
            "width": 120,
            "height": 29,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "e91d0218-b052-4df7-ab0b-107f2ff369ec",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "where",
            "page": 27,
            "x": 88,
            "y": 352,
            "width": 30,
            "height": 13,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "b0bf717b-78ce-4e9c-8386-d87c9e752e92",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$",
            "page": 27,
            "x": 256,
            "y": 360,
            "width": 262,
            "height": 38,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "5e13e670-fb60-4346-8817-5768ac6f31a3",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "Observe that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.",
            "page": 27,
            "x": 87,
            "y": 398,
            "width": 435,
            "height": 30,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "edcb6ad0-d89b-442e-b2af-79d784777704",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:",
            "page": 27,
            "x": 87,
            "y": 441,
            "width": 434,
            "height": 48,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "79b3f6c2-e4b1-4c36-b302-acf9314b2cfd",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$",
            "page": 27,
            "x": 230,
            "y": 498,
            "width": 293,
            "height": 34,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "33d27111-04c9-47bc-b578-5a0cac4fcb0a",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "as a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.",
            "page": 27,
            "x": 87,
            "y": 539,
            "width": 435,
            "height": 27,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "ae9d6029-08d6-4afa-a05a-8bc094e2b793",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "Following Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:",
            "page": 27,
            "x": 87,
            "y": 566,
            "width": 435,
            "height": 28,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "3c142e41-121b-4d4e-8916-b9ff9f8c125b",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$",
            "page": 27,
            "x": 126,
            "y": 605,
            "width": 397,
            "height": 105,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "990ba9a7-f7a6-43f6-aa76-6bb787d7f539",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "Thus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.",
            "page": 28,
            "x": 85,
            "y": 89,
            "width": 438,
            "height": 29,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "2c31c504-03a1-40fd-bc69-c260dfe44385",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "It can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:",
            "page": 28,
            "x": 86,
            "y": 118,
            "width": 437,
            "height": 55,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "1e4edeab-6a68-4f3f-9569-3a9d9f9eaa06",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$",
            "page": 28,
            "x": 163,
            "y": 179,
            "width": 360,
            "height": 21,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "a83173b0-1dcf-4743-b778-d706832a6c90",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "This shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:",
            "page": 28,
            "x": 86,
            "y": 205,
            "width": 436,
            "height": 55,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "1cd163d2-85ff-4987-8bc5-664f22534f4c",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $",
            "page": 28,
            "x": 157,
            "y": 267,
            "width": 366,
            "height": 36,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "9b584f46-347d-44bf-acd3-2ee9a6d856c9",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "Finally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:",
            "page": 28,
            "x": 87,
            "y": 310,
            "width": 435,
            "height": 30,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "117ea7f8-2ab2-4848-bafd-52aba926ccd6",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$",
            "page": 28,
            "x": 123,
            "y": 346,
            "width": 400,
            "height": 175,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "88d603c3-799a-4b60-bcc3-0dd631b5bc6a",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "where we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.",
            "page": 28,
            "x": 86,
            "y": 527,
            "width": 435,
            "height": 42,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "ccb24636-728a-452b-833f-3044cf1e699a",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "A.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.",
            "page": 28,
            "x": 87,
            "y": 581,
            "width": 434,
            "height": 46,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "914de18d-8138-4894-ad27-a71a301b43bf",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "We form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):",
            "page": 28,
            "x": 88,
            "y": 627,
            "width": 434,
            "height": 56,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "d0fefaa1-7039-4a59-8770-5500e5b08747",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$",
            "page": 28,
            "x": 133,
            "y": 690,
            "width": 344,
            "height": 22,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "3c658ae0-bdab-4ce0-a185-10d9fb3b7139",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "where we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:",
            "page": 29,
            "x": 85,
            "y": 89,
            "width": 439,
            "height": 43,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "1e94a80c-099b-45c2-a2e2-58788f657b7b",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$",
            "page": 29,
            "x": 185,
            "y": 129,
            "width": 242,
            "height": 30,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "f741d911-15dd-4817-bb9c-c07328f8b462",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "Setting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):",
            "page": 29,
            "x": 87,
            "y": 161,
            "width": 436,
            "height": 15,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "748ac37e-6928-403d-a557-99157c337636",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$",
            "page": 29,
            "x": 224,
            "y": 181,
            "width": 300,
            "height": 23,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "f0b60c81-a124-4dc9-891b-60f016f12c0f",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "A.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:",
            "page": 29,
            "x": 87,
            "y": 212,
            "width": 435,
            "height": 49,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "9e9c17df-2d86-4594-8b24-755ff1dec6b6",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$",
            "page": 29,
            "x": 146,
            "y": 268,
            "width": 317,
            "height": 69,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "87855a84-9017-46b8-886f-f3d9c7002a19",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "This simplifies to:",
            "page": 29,
            "x": 88,
            "y": 343,
            "width": 82,
            "height": 14,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "a43397a7-0d81-4c37-9ffe-31390faabacf",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$",
            "page": 29,
            "x": 126,
            "y": 367,
            "width": 358,
            "height": 33,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "74f91254-3105-4206-9090-c038023b3fbf",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "We take the derivative with respect to $\\gamma_i$:",
            "page": 29,
            "x": 88,
            "y": 408,
            "width": 179,
            "height": 15,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "fa05e9f3-4860-492c-9462-4e616f389240",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$",
            "page": 29,
            "x": 143,
            "y": 430,
            "width": 324,
            "height": 35,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "db85d3b6-ba54-45fb-883f-6f6c09002942",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "Setting this equation to zero yields a maximum at:",
            "page": 29,
            "x": 88,
            "y": 474,
            "width": 221,
            "height": 15,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "eec6ef60-e8c1-49e2-a000-27f6ea6cadd0",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$",
            "page": 29,
            "x": 262,
            "y": 496,
            "width": 261,
            "height": 20,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "b91bfc89-b8cf-47d5-81d5-d86d024b3421",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "Since Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.",
            "page": 29,
            "x": 88,
            "y": 522,
            "width": 433,
            "height": 29,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "93c8b479-32ca-40f4-92b9-ff05a9f80bbe",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.",
            "page": 29,
            "x": 87,
            "y": 564,
            "width": 434,
            "height": 86,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "f1089ba2-3281-48c2-9757-8428013edbe6",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "We have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse",
            "page": 29,
            "x": 88,
            "y": 651,
            "width": 434,
            "height": 54,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "a3f9624c-267a-4065-b2c7-32bca7467eb4",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "notation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.",
            "page": 30,
            "x": 85,
            "y": 90,
            "width": 438,
            "height": 27,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "9c4cb283-9297-4d04-95d1-8e8e13c0a7cf",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "Recall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.",
            "page": 30,
            "x": 86,
            "y": 118,
            "width": 437,
            "height": 69,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "fab25c43-c7b5-4c79-88ab-49874ad26d6b",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "A.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:",
            "page": 30,
            "x": 86,
            "y": 198,
            "width": 347,
            "height": 35,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "7af4ebe3-118e-47e7-a7d2-8616d39e30c7",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$",
            "page": 30,
            "x": 175,
            "y": 238,
            "width": 260,
            "height": 39,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "f5cdce00-e81c-4af4-b0e8-54a757095607",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "We take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:",
            "page": 30,
            "x": 87,
            "y": 284,
            "width": 285,
            "height": 16,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "ebaa8b9a-c0f0-47e1-b60c-9f40d826a752",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$",
            "page": 30,
            "x": 257,
            "y": 307,
            "width": 97,
            "height": 37,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "51dccd15-5ba6-4bd5-bb4d-e0869e64d3a0",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "A.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:",
            "page": 30,
            "x": 86,
            "y": 351,
            "width": 142,
            "height": 35,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "b1fd5bac-c8bf-4158-b2a8-24285d3f97ee",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$",
            "page": 30,
            "x": 117,
            "y": 391,
            "width": 376,
            "height": 39,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "7975fdec-5b60-4e6f-9b46-344877d027e9",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "Taking the derivative with respect to $\\alpha_i$ gives:",
            "page": 30,
            "x": 88,
            "y": 435,
            "width": 202,
            "height": 16,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "4b85cf80-e15e-4e99-9351-dee36cfd0b3f",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$",
            "page": 30,
            "x": 164,
            "y": 457,
            "width": 283,
            "height": 36,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "c2b55817-cfb6-4d79-b40f-a48c110146a8",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "This derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):",
            "page": 30,
            "x": 87,
            "y": 499,
            "width": 436,
            "height": 29,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "915f3676-dc87-4e6f-b328-ccab36e4ec1b",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$",
            "page": 30,
            "x": 213,
            "y": 535,
            "width": 185,
            "height": 32,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "dbb17d1e-5b72-417b-8cc4-f6f2c71a25c8",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "and thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.",
            "page": 30,
            "x": 88,
            "y": 572,
            "width": 421,
            "height": 16,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "c28e596d-3d4e-4257-a0a0-dadfd0dcd723",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        },
        {
            "text": "Finally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4.",
            "page": 30,
            "x": 88,
            "y": 587,
            "width": 434,
            "height": 28,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "27-dickey",
            "chunk_id": "5498221f-0ad9-4ea0-8625-d58c85732386",
            "group_text": "1. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. _Journal of the American Statistical Association_, 82:773\u2013781, 1987.\n\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. _Bayesian data analysis_. Chapman & Hall, London, 1995.\n\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In _Proceedings of the 24th Annual Conference of the Cognitive Science Society_, 2002.\n\nD. Harman. Overview of the first text retrieval conference (TREC-1). In _Proceedings of the First Text Retrieval Conference (TREC-1)_, pages 1\u201320, 1992.\n\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization methods. _Machine Learning_, 42:9\u201329, 2001.\n\nT. Hofmann. Probabilistic latent semantic indexing. _Proceedings of the Twenty-Second Annual International SIGIR Conference_, 1999.\n\nF. Jelinek. _Statistical Methods for Speech Recognition_. MIT Press, Cambridge, MA, 1997.\n\nT. Joachims. Making large-scale SVM learning practical. In _Advances in Kernel Methods - Support Vector Learning_. M.I.T. Press, 1999.\n\nM. Jordan, editor. _Learning in Graphical Models_. MIT Press, Cambridge, MA, 1999.\n\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. _Machine Learning_, 37:183\u2013233, 1999.\n\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). _Journal of the American Statistical Association_, 84 (407):717\u2013726, 1989.\n\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In _Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference_, 2002.\n\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\n\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In _Uncertainty in Artificial Intelligence (UAI)_, 2002.\n\nC. Morris. Parametric empirical Bayes inference: Theory and applications. _Journal of the American Statistical Association_, 78(381):47\u201365, 1983. With discussion.\n\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. _IJCAI-99 Workshop on Machine Learning for Information Filtering_, pages 61\u201367, 1999.\n\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. _Machine Learning_, 39(2/3):103\u2013134, 2000.\n\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159\u2013168, 1998.\n\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\n    and content-based recommendation in sparse-data environments.  In _Uncertainty in Artificial\n    Intelligence, Proceedings of the Seventeenth Conference_, 2001.\n\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001-\n004, M.I.T., 2001.\n\nG. Ronning.  Maximum likelihood estimation of Dirichlet distributions. _Journal of Statistcal Com-\nputation and Simulation_, 34(4):215\u2013221, 1989.\n\nG. Salton and M. McGill, editors.  _Introduction to Modern Information Retrieval_.  McGraw-Hill,\n    1983.\n\n**Appendix A. Inference and parameter estimation**\n\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by deriving a useful property of the Dirichlet distribution.\n\nA.1  Computing $\\mathbb{E}[\\log(\\theta_i \\mid \\alpha)]$\n\nThe need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA. This value can be easily computed from the natural parameterization of the exponential family representation of the Dirichlet distribution.\n\nRecall that a distribution is in the exponential family if it can be written in the form:\n\n$p(x|\\eta) = h(x) \\exp\\{\\eta^T T(x) - A(\\eta)\\},$\n\nwhere $\\eta$ is the natural parameter, $T(x)$ is the sufficient statistic, and $A(\\eta)$ is the log of the normalization factor.\n\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\n\n$p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) = \\exp \\left\\{ \\left( \\sum_{i=1}^k (\\alpha_i - 1) \\log \\theta_i \\right) + \\log \\Gamma \\left( \\sum_{i=1}^k \\alpha_i \\right) - \\sum_{i=1}^k \\log \\Gamma (\\alpha_i) \\right\\}.$\n\nFrom this form, we immediately see that the natural parameter of the Dirichlet is $\\eta_i = \\alpha_i - 1$ and the sufficient statistic is $T(\\theta_i) = \\log \\theta_i$. Furthermore, using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic, we obtain:\n\n$\\mathbb{E}[\\log \\theta_i \\mid \\alpha] = \\Psi(\\alpha_i) - \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)$\n\nwhere $\\Psi$ is the digamma function, the first derivative of the log Gamma function.\n\n**A.2 Newton-Raphson methods for a Hessian with special structure**\n\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\n\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\n\n$\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - H(\\alpha_{\\text{old}})^{-1} g(\\alpha_{\\text{old}})$\n\nwhere $H(\\alpha)$ and $g(\\alpha)$ are the Hessian matrix and gradient respectively at the point $\\alpha$. In general, this algorithm scales as O($N^3$) due to the matrix inversion.\n\nIf the Hessian matrix is of the form:\n\n$H = \\mathrm{diag}(h) + \\mathbf{1} z \\mathbf{1}^\\mathrm{T}, \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (10)$\n\nwhere diag$(h)$ is defined to be a diagonal matrix with the elements of the vector $h$ along the diagonal, then we can apply the matrix inversion lemma and obtain:\n\n$H^{-1} = \\operatorname{diag}(h)^{-1} - \\frac{\\operatorname{diag}(h)^{-1} \\mathbf{1} \\mathbf{1}^\\mathrm{T} \\operatorname{diag}(h)^{-1}}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}$\n\nMultiplying by the gradient, we obtain the $i$th component:\n\n$(H^{-1}g)_i = \\frac{g_i - c}{h_i}$\n\nwhere\n\n$c = \\frac{\\sum_{j=1}^k g_j / h_j}{z^{-1} + \\sum_{j=1}^k h_j^{-1}}.$\n\nObserve that this expression depends only on the $2k$ values $h_i$ and $g_i$ and thus yields a Newton-Raphson algorithm that has linear time complexity.\n\n**A.3 Variational inference**\n\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that this involves using the following *variational distribution*:\n\n$q(\\mathbf{\\theta}, \\mathbf{z} \\mid \\mathbf{\\gamma}, \\mathbf{\\phi}) = q(\\mathbf{\\theta} \\mid \\mathbf{\\gamma}) \\prod_{n=1}^{N} q(z_n \\mid \\phi_n)$\n\nas a surrogate for the posterior distribution $p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)$, where the *variational parameters* $\\gamma$ and $\\phi$ are set via an optimization procedure that we now describe.\n\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using Jensen\u2019s inequality. Omitting the parameters \u03b3 and \u03d5 for simplicity, we have:\n\n$\n\\begin{align*}\n\\log p(\\mathbf{w}|\\alpha, \\beta) \\quad &= \\quad \\log \\int \\sum_{\\mathbf{z}} p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta \\\\\n&= \\quad \\log \\int \\sum_{\\mathbf{z}} \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) q(\\theta, \\mathbf{z})}{q(\\theta, \\mathbf{z})} d\\theta \\\\\n&\\geq \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) d\\theta - \\int \\sum_{\\mathbf{z}} q(\\theta, \\mathbf{z}) \\log q(\\theta, \\mathbf{z}) d\\theta \\\\\n&= \\mathbb{E}_q [\\log p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)] - \\mathbb{E}_q [\\log q(\\theta, \\mathbf{z})].\n\\end{align*}\n\\hspace{10cm} (12)\n$\n\nThus we see that Jensen\u2019s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution $q(\\boldsymbol{\\theta}, \\mathbf{z} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\phi})$.\n\nIt can be easily verified that the difference between the left-hand side and the right-hand side\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\nposterior probability. That is, letting $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ denote the right-hand side of Eq. (12) (where we\nhave restored the dependence on the variational parameters $\\gamma$ and $\\phi$ in our notation), we have:\n\n$\\log p(\\mathbf{w}|\\alpha, \\beta) = \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) + \\mathrm{D}(q(\\theta, \\mathbf{z}|\\gamma, \\phi)\\;\\|\\;p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta)). \\hspace{2cm} (13)$\n\nThis shows that maximizing the lower bound $\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability, the optimization problem presented earlier in Eq. (5).\nWe now expand the lower bound by using the factorizations of $p$ and $q$:\n\n$ \\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\mathbb{E}_q[\\log p(\\theta | \\alpha)] + \\mathbb{E}_q[\\log p(\\mathbf{z} | \\theta)] + \\mathbb{E}_q[\\log p(\\mathbf{w} | \\mathbf{z}, \\beta)] \\\\\n\\qquad - \\mathbb{E}_q[\\log q(\\theta)] - \\mathbb{E}_q[\\log q(\\mathbf{z})]. \\hspace{3cm} (14) $\n\nFinally, we expand Eq. (14) in terms of the model parameters $(\\alpha, \\beta)$ and the variational parameters $(\\gamma, \\phi)$. Each of the five lines below expands one of the five terms in the bound:\n\n$\n\\mathcal{L}(\\gamma, \\phi; \\alpha, \\beta) = \\log \\Gamma \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} w_n^j \\log \\beta_{ij} \\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^k \\gamma_j \\right) + \\sum_{i=1}^k \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi \\left( \\sum_{j=1}^k \\gamma_j \\right) \\right) \\\\\n- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log \\phi_{ni},\n\\tag{15}\n$\n\nwhere we have made use of Eq. (8).\n    In the following two sections, we show how to maximize this lower bound with respect to the variational parameters $\\phi$ and $\\gamma$.\n\nA.3.1 Variational multinomial\n\nWe first maximize Eq. (15) with respect to $\\phi_{ni}$, the probability that the $n$th word is generated by latent topic $i$. Observe that this is a constrained maximization since $\\sum_{i=1}^k \\phi_{ni} = 1$.\n\nWe form the Lagrangian by isolating the terms which contain $\\phi_{ni}$ and adding the appropriate Lagrange multipliers. Let $\\beta_{i\\nu}$ be $p(w_n^\\nu = 1 | z_i^i = 1)$ for the appropriate $\\nu$. (Recall that each $w_n$ is a vector of size $V$ with exactly one component equal to one; we can select the unique $\\nu$ such that $w_n^\\nu = 1$):\n\n$\\mathcal{L}_{[\\phi_{ni}]} = \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right) \\right) + \\phi_{ni} \\log \\beta_{iv} - \\phi_{ni} \\log \\phi_{ni} + \\lambda_n \\left( \\sum_{j=1}^k \\phi_{ni} - 1 \\right),$\n\nwhere we have dropped the arguments of $\\mathcal{L}$ for simplicity, and where the subscript $\\phi_{ni}$ denotes that we have retained only those terms in $\\mathcal{L}$ that are a function of $\\phi_{ni}$. Taking derivatives with respect to $\\phi_{ni}$, we obtain:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{ni}} = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j) + \\log \\beta_{iv} - \\log \\phi_{ni} - 1 + \\lambda.$\n\nSetting this derivative to zero yields the maximizing value of the variational parameter $\\phi_{ni}$ (cf. Eq. 6):\n\n$\\phi_{ni} \\propto \\beta_{iv} \\exp\\left(\\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^k \\gamma_j\\right)\\right). \\hspace{3cm} (16)$\n\nA.3.2 Variational Dirichlet\n\nNext, we maximize Eq. (15) with respect to $\\gamma_i$, the $i$th component of the posterior Dirichlet parameter. The terms containing $\\gamma_i$ are:\n\n$ \n\\mathcal{L}_{[\\gamma]} = \\sum_{i=1}^{k} (\\alpha_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) + \\sum_{n=1}^{N} \\phi_{ni} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \n\\\\\n- \\log \\Gamma \\left( \\sum_{j=1}^{k} \\gamma_j \\right) + \\log \\Gamma(\\gamma_i) - \\sum_{i=1}^{k} (\\gamma_i - 1) \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right).\n$\n\nThis simplifies to:\n\n$L_{[\\gamma]} = \\sum_{i=1}^{k} \\left( \\Psi(\\gamma_i) - \\Psi\\left(\\sum_{j=1}^{k} \\gamma_j\\right) \\right) \\left( \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} - \\gamma_i \\right) - \\log \\Gamma\\left(\\sum_{j=1}^{k} \\gamma_j\\right) + \\log \\Gamma(\\gamma_i).$\n\nWe take the derivative with respect to $\\gamma_i$:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_i} = \\Psi'(\\gamma_i)\\left(\\alpha_i + \\sum_{n=1}^N \\phi_{ni} - \\gamma_i\\right) - \\Psi'\\left(\\sum_{j=1}^k \\gamma_j\\right) \\sum_{j=1}^k \\left(\\alpha_j + \\sum_{n=1}^N \\phi_{nj} - \\gamma_j\\right).$\n\nSetting this equation to zero yields a maximum at:\n\n$\\gamma_{i} = \\alpha_{i} + \\sum_{n=1}^{N} \\phi_{ni}.$\n\nSince Eq. (17) depends on the variational multinomial \u03d5, full variational inference requires alternating between Eqs. (16) and (17) until the bound converges.\n\n**A.4 Parameter estimation**\n\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model parameters $\\alpha$ and $\\beta$. We solve this problem by using the variational lower bound as a surrogate for the (intractable) marginal log likelihood, with the variational parameters $\\phi$ and $\\gamma$ fixed to the values found by variational inference. We then obtain (approximate) empirical Bayes estimates by maximizing this lower bound with respect to the model parameters.\n\nWe have thus far considered the log likelihood for a single document.  Given our assumption of exchangeability for the documents, the overall log likelihood of a corpus $\\mathcal{D} = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_M\\}$ is the sum of the log likelihoods for individual documents; moreover, the overall variational lower bound is the sum of the individual variational bounds.  In the remainder of this section, we abuse\n\nnotation by using $\\mathcal{L}$ for the total variational bound, indexing the document-specific terms in the individual bounds by $d$, and summing over all the documents.\n\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize the bound $L(\\gamma, \\phi; \\alpha, \\beta)$ with respect to the variational parameters $\\gamma$ and $\\phi$. In the M-step, which we describe in this section, we maximize the bound with respect to the model parameters $\\alpha$ and $\\beta$. The overall procedure can thus be viewed as coordinate ascent in $\\mathcal{L}$.\n\nA.4.1 CONDITIONAL MULTINOMIALS  \nTo maximize with respect to \u03b2, we isolate terms and add Lagrange multipliers:\n\n$\\mathcal{L}_{[\\beta]} = \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\sum_{i=1}^{k} \\sum_{j=1}^{V} \\phi_{dni} w_{dn}^j \\log \\beta_{ij} + \\sum_{i=1}^{k} \\lambda_i \\left( \\sum_{j=1}^{V} \\beta_{ij} - 1 \\right).$\n\nWe take the derivative with respect to $\\beta_{ij}$, set it to zero, and find:\n\n$\\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j.$\n\nA.4.2 Dirichlet\n\nThe terms which contain $\\alpha$ are:\n\n$L_{[\\alpha]} = \\sum_{d=1}^{M} \\left( \\log \\Gamma \\left( \\sum_{j=1}^{k} \\alpha_j \\right) - \\sum_{i=1}^{k} \\log \\Gamma(\\alpha_i) + \\sum_{i=1}^{k} \\left( (\\alpha_i - 1) \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^{k} \\gamma_{dj} \\right) \\right) \\right) \\right)$\n\nTaking the derivative with respect to $\\alpha_i$ gives:\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = M \\left( \\Psi \\left( \\sum_{j=1}^k \\alpha_j \\right) - \\Psi(\\alpha_i) \\right) + \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi \\left( \\sum_{j=1}^k \\gamma_{dj} \\right) \\right)$\n\nThis derivative depends on $\\alpha_j$, where $j \\neq i$, and we therefore must use an iterative method to find the maximal $\\alpha$. In particular, the Hessian is in the form found in Eq. (10):\n\n$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i \\alpha_j} = \\delta(i, j) M \\Psi'(\\alpha_i) - \\Psi'(\\sum_{j=1}^k \\alpha_j),$\n\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\n\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of \u03b7,\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4."
        }
    ]
}