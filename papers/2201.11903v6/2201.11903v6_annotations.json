{
    "annotations": [
        {
            "text": "Chain-of-Thought Prompting Elicits Reasoning\nin Large Language Models",
            "page": 1,
            "x": 127,
            "y": 94,
            "width": 358,
            "height": 45,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "0-title",
            "chunk_id": "4451aa4c-9f5c-41c6-afbc-b47e90a54675",
            "group_text": "Chain-of-Thought Prompting Elicits Reasoning\nin Large Language Models\n\nJason Wei        Xuezhi Wang        Dale Schuurmans        Maarten Bosma  \nBrian Ichter     Fei Xia            Ed H. Chi              Quoc V. Le  \n                                                     Denny Zhou\n\n                    Google Research, Brain Team  \n                {jasonwei,dennyzhou}@google.com"
        },
        {
            "text": "Jason Wei        Xuezhi Wang        Dale Schuurmans        Maarten Bosma  \nBrian Ichter     Fei Xia            Ed H. Chi              Quoc V. Le  \n                                                     Denny Zhou\n\n                    Google Research, Brain Team  \n                {jasonwei,dennyzhou}@google.com",
            "page": 1,
            "x": 147,
            "y": 175,
            "width": 319,
            "height": 68,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "0-title",
            "chunk_id": "dc001864-1cda-416f-96c8-21e814f97f6f",
            "group_text": "Chain-of-Thought Prompting Elicits Reasoning\nin Large Language Models\n\nJason Wei        Xuezhi Wang        Dale Schuurmans        Maarten Bosma  \nBrian Ichter     Fei Xia            Ed H. Chi              Quoc V. Le  \n                                                     Denny Zhou\n\n                    Google Research, Brain Team  \n                {jasonwei,dennyzhou}@google.com"
        },
        {
            "text": "## Abstract\n\nWe explore how generating a _chain of thought_\u2014a series of intermediate reasoning steps\u2014significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called _chain-of-thought prompting_, where a few chain of thought demonstrations are provided as exemplars in prompting.",
            "page": 1,
            "x": 139,
            "y": 274,
            "width": 333,
            "height": 93,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "1-abstract",
            "chunk_id": "ad6f3579-2f9a-4695-a67c-8b8771d3759e",
            "group_text": "## Abstract\n\nWe explore how generating a _chain of thought_\u2014a series of intermediate reasoning steps\u2014significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called _chain-of-thought prompting_, where a few chain of thought demonstrations are provided as exemplars in prompting.\n\nExperiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.\n\nStandard Prompting\n\nModel Input\n\nQ: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n\nA: The answer is 11.\n\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n\nModel Output\n\nA: The answer is 27.  \u274c\n\nFigure 1: Chain-of-thought prompting enables large language models to tackle complex arithmetic, commonsense, and symbolic reasoning tasks. Chain-of-thought reasoning processes are highlighted."
        },
        {
            "text": "Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
            "page": 1,
            "x": 140,
            "y": 369,
            "width": 332,
            "height": 67,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "1-abstract",
            "chunk_id": "356b323f-27b0-4af7-a3ac-d0c9593e2709",
            "group_text": "## Abstract\n\nWe explore how generating a _chain of thought_\u2014a series of intermediate reasoning steps\u2014significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called _chain-of-thought prompting_, where a few chain of thought demonstrations are provided as exemplars in prompting.\n\nExperiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.\n\nStandard Prompting\n\nModel Input\n\nQ: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n\nA: The answer is 11.\n\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n\nModel Output\n\nA: The answer is 27.  \u274c\n\nFigure 1: Chain-of-thought prompting enables large language models to tackle complex arithmetic, commonsense, and symbolic reasoning tasks. Chain-of-thought reasoning processes are highlighted."
        },
        {
            "text": "Standard Prompting\n\nModel Input\n\nQ: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n\nA: The answer is 11.\n\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?",
            "page": 1,
            "x": 115,
            "y": 451,
            "width": 189,
            "height": 120,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "1-abstract",
            "chunk_id": "2c430d9a-c789-4308-ad74-12240d9e5dea",
            "group_text": "## Abstract\n\nWe explore how generating a _chain of thought_\u2014a series of intermediate reasoning steps\u2014significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called _chain-of-thought prompting_, where a few chain of thought demonstrations are provided as exemplars in prompting.\n\nExperiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.\n\nStandard Prompting\n\nModel Input\n\nQ: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n\nA: The answer is 11.\n\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n\nModel Output\n\nA: The answer is 27.  \u274c\n\nFigure 1: Chain-of-thought prompting enables large language models to tackle complex arithmetic, commonsense, and symbolic reasoning tasks. Chain-of-thought reasoning processes are highlighted."
        },
        {
            "text": "Model Output\n\nA: The answer is 27.  \u274c",
            "page": 1,
            "x": 114,
            "y": 580,
            "width": 189,
            "height": 44,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "1-abstract",
            "chunk_id": "10372e01-ab47-42cb-a702-0bd8214d8d04",
            "group_text": "## Abstract\n\nWe explore how generating a _chain of thought_\u2014a series of intermediate reasoning steps\u2014significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called _chain-of-thought prompting_, where a few chain of thought demonstrations are provided as exemplars in prompting.\n\nExperiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.\n\nStandard Prompting\n\nModel Input\n\nQ: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n\nA: The answer is 11.\n\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n\nModel Output\n\nA: The answer is 27.  \u274c\n\nFigure 1: Chain-of-thought prompting enables large language models to tackle complex arithmetic, commonsense, and symbolic reasoning tasks. Chain-of-thought reasoning processes are highlighted."
        },
        {
            "text": "Figure 1: Chain-of-thought prompting enables large language models to tackle complex arithmetic, commonsense, and symbolic reasoning tasks. Chain-of-thought reasoning processes are highlighted.",
            "page": 1,
            "x": 104,
            "y": 647,
            "width": 404,
            "height": 26,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "1-abstract",
            "chunk_id": "13c21cd2-1c99-45d0-b814-730ff2f163a8",
            "group_text": "## Abstract\n\nWe explore how generating a _chain of thought_\u2014a series of intermediate reasoning steps\u2014significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called _chain-of-thought prompting_, where a few chain of thought demonstrations are provided as exemplars in prompting.\n\nExperiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.\n\nStandard Prompting\n\nModel Input\n\nQ: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n\nA: The answer is 11.\n\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n\nModel Output\n\nA: The answer is 27.  \u274c\n\nFigure 1: Chain-of-thought prompting enables large language models to tackle complex arithmetic, commonsense, and symbolic reasoning tasks. Chain-of-thought reasoning processes are highlighted."
        },
        {
            "text": "# 1 Introduction\n\nThe NLP landscape has recently been revolutionized by language models (Peters et al., 2018; Devlin et al., 2019; Brown et al., 2020, _inter alia_). Scaling up the size of language models has been shown to confer a range of benefits, such as improved performance and sample efficiency (Kaplan et al., 2020; Brown et al., 2020, _inter alia_). However, scaling up model size alone has not proved sufficient for achieving high performance on challenging tasks such as arithmetic, commonsense, and symbolic reasoning (Rae et al., 2021).",
            "page": 2,
            "x": 102,
            "y": 68,
            "width": 238,
            "height": 143,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "e8d8f2d9-7054-488b-af72-714011a23bc0",
            "group_text": "# 1 Introduction\n\nThe NLP landscape has recently been revolutionized by language models (Peters et al., 2018; Devlin et al., 2019; Brown et al., 2020, _inter alia_). Scaling up the size of language models has been shown to confer a range of benefits, such as improved performance and sample efficiency (Kaplan et al., 2020; Brown et al., 2020, _inter alia_). However, scaling up model size alone has not proved sufficient for achieving high performance on challenging tasks such as arithmetic, commonsense, and symbolic reasoning (Rae et al., 2021).\n\nThis work explores how the reasoning ability of large\nlanguage models can be unlocked by a simple method\nmotivated by two ideas. First, techniques for arithmetic\nreasoning can benefit from generating natural language\nrationales that lead to the final answer. Prior work has\ngiven models the ability to generate natural language inter-\nmediate steps by training from scratch (Ling et al., 2017)\nor finetuning a pretrained model (Cobbe et al., 2021), in\naddition to neuro-symbolic methods that use formal lan-\nguages instead of natural language (Roy and Roth, 2015;\nChiang and Chen, 2019; Amini et al., 2019; Chen et al.,\n2019). Second, large language models offer the exciting\nprospect of in-context few-shot learning via prompting.\nlanguage model checkpoint for each new task, one can sim-\ninput\u2013output exemplars demonstrating the task. Remarkably, this has be\nsimple question-answering tasks (Brown et al., 2020).\n\nmotivated by two ideas. First, techniques for arithmetic\nreasoning can benefit from generating natural language\nrationales that lead to the final answer. Prior work has\ngiven models the ability to generate natural language intermediate steps by training from scratch (Ling et al., 2017)\nor finetuning a pretrained model (Cobbe et al., 2021), in\naddition to neuro-symbolic methods that use formal languages instead of natural language (Roy and Roth, 2015;\nChiang and Chen, 2019; Amini et al., 2019; Chen et al.,\n2019). Second, large language models offer the exciting\nprospect of in-context few-shot learning via prompting.\nlanguage model checkpoint for each new task, one can\ninput\u2013output exemplars demonstrating the task. Remarkably, this has been successful for a range of simple question-answering tasks (Brown et al., 2020).\n\nMath Word Problems (GSM8K)\n\nFigure 2:   PaLM 540B uses chain-of-\nthought prompting to achieve new state-\nof-the-art performance on the GSM8K\nbenchmark of math word problems.\nFinetuned GPT-3 and prior best are from\nCobbe et al. (2021).\nThat is, instead of finetuning a separate\ncan simply \u201cprompt\u201d the model with a few\nThis\n\nBoth of the above ideas, however, have key limitations. For rationale-augmented training and\nfinetuning methods, it is costly to create a large set of high quality rationales, which is much more\ncomplicated than simple input\u2013output pairs used in normal machine learning. For the traditional few-\nshot prompting method used in Brown et al. (2020), it works poorly on tasks that require reasoning\nabilities, and often does not improve substantially with increasing language model scale (Rae et al.,\n2021). In this paper, we combine the strengths of these two ideas in a way that avoids their limitations.\nSpecifically, we explore the ability of language models to perform few-shot prompting for reasoning\ntasks, given a prompt that consists of triples: \u27e8input, _chain of thought_, output\u27e9. A _chain of thought_ is\na series of intermediate natural language reasoning steps that lead to the final output, and we refer to\nthis approach as _chain-of-thought prompting_. An example prompt is shown in Figure 1.\n\nWe present empirical evaluations on arithmetic, commonsense, and symbolic reasoning benchmarks, showing that chain-of-thought prompting outperforms standard prompting, sometimes to a striking degree. Figure 2 illustrates one such result\u2014on the GSM8K benchmark of math word problems (Cobbe et al., 2021), chain-of-thought prompting with PaLM 540B outperforms standard prompting by a large margin and achieves new state-of-the-art performance. A prompting only approach is important because it does not require a large training dataset and because a single model checkpoint can perform many tasks without loss of generality. This work underscores how large language models can learn via a few examples with natural language data about the task (c.f. automatically learning the patterns underlying inputs and outputs via a large training dataset)."
        },
        {
            "text": "This work explores how the reasoning ability of large\nlanguage models can be unlocked by a simple method\nmotivated by two ideas. First, techniques for arithmetic\nreasoning can benefit from generating natural language\nrationales that lead to the final answer. Prior work has\ngiven models the ability to generate natural language inter-\nmediate steps by training from scratch (Ling et al., 2017)\nor finetuning a pretrained model (Cobbe et al., 2021), in\naddition to neuro-symbolic methods that use formal lan-\nguages instead of natural language (Roy and Roth, 2015;\nChiang and Chen, 2019; Amini et al., 2019; Chen et al.,\n2019). Second, large language models offer the exciting\nprospect of in-context few-shot learning via prompting.\nlanguage model checkpoint for each new task, one can sim-\ninput\u2013output exemplars demonstrating the task. Remarkably, this has be\nsimple question-answering tasks (Brown et al., 2020).",
            "page": 2,
            "x": 103,
            "y": 212,
            "width": 290,
            "height": 177,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "b5bba78b-12b7-4d15-9e93-dec2afd5acfd",
            "group_text": "# 1 Introduction\n\nThe NLP landscape has recently been revolutionized by language models (Peters et al., 2018; Devlin et al., 2019; Brown et al., 2020, _inter alia_). Scaling up the size of language models has been shown to confer a range of benefits, such as improved performance and sample efficiency (Kaplan et al., 2020; Brown et al., 2020, _inter alia_). However, scaling up model size alone has not proved sufficient for achieving high performance on challenging tasks such as arithmetic, commonsense, and symbolic reasoning (Rae et al., 2021).\n\nThis work explores how the reasoning ability of large\nlanguage models can be unlocked by a simple method\nmotivated by two ideas. First, techniques for arithmetic\nreasoning can benefit from generating natural language\nrationales that lead to the final answer. Prior work has\ngiven models the ability to generate natural language inter-\nmediate steps by training from scratch (Ling et al., 2017)\nor finetuning a pretrained model (Cobbe et al., 2021), in\naddition to neuro-symbolic methods that use formal lan-\nguages instead of natural language (Roy and Roth, 2015;\nChiang and Chen, 2019; Amini et al., 2019; Chen et al.,\n2019). Second, large language models offer the exciting\nprospect of in-context few-shot learning via prompting.\nlanguage model checkpoint for each new task, one can sim-\ninput\u2013output exemplars demonstrating the task. Remarkably, this has be\nsimple question-answering tasks (Brown et al., 2020).\n\nmotivated by two ideas. First, techniques for arithmetic\nreasoning can benefit from generating natural language\nrationales that lead to the final answer. Prior work has\ngiven models the ability to generate natural language intermediate steps by training from scratch (Ling et al., 2017)\nor finetuning a pretrained model (Cobbe et al., 2021), in\naddition to neuro-symbolic methods that use formal languages instead of natural language (Roy and Roth, 2015;\nChiang and Chen, 2019; Amini et al., 2019; Chen et al.,\n2019). Second, large language models offer the exciting\nprospect of in-context few-shot learning via prompting.\nlanguage model checkpoint for each new task, one can\ninput\u2013output exemplars demonstrating the task. Remarkably, this has been successful for a range of simple question-answering tasks (Brown et al., 2020).\n\nMath Word Problems (GSM8K)\n\nFigure 2:   PaLM 540B uses chain-of-\nthought prompting to achieve new state-\nof-the-art performance on the GSM8K\nbenchmark of math word problems.\nFinetuned GPT-3 and prior best are from\nCobbe et al. (2021).\nThat is, instead of finetuning a separate\ncan simply \u201cprompt\u201d the model with a few\nThis\n\nBoth of the above ideas, however, have key limitations. For rationale-augmented training and\nfinetuning methods, it is costly to create a large set of high quality rationales, which is much more\ncomplicated than simple input\u2013output pairs used in normal machine learning. For the traditional few-\nshot prompting method used in Brown et al. (2020), it works poorly on tasks that require reasoning\nabilities, and often does not improve substantially with increasing language model scale (Rae et al.,\n2021). In this paper, we combine the strengths of these two ideas in a way that avoids their limitations.\nSpecifically, we explore the ability of language models to perform few-shot prompting for reasoning\ntasks, given a prompt that consists of triples: \u27e8input, _chain of thought_, output\u27e9. A _chain of thought_ is\na series of intermediate natural language reasoning steps that lead to the final output, and we refer to\nthis approach as _chain-of-thought prompting_. An example prompt is shown in Figure 1.\n\nWe present empirical evaluations on arithmetic, commonsense, and symbolic reasoning benchmarks, showing that chain-of-thought prompting outperforms standard prompting, sometimes to a striking degree. Figure 2 illustrates one such result\u2014on the GSM8K benchmark of math word problems (Cobbe et al., 2021), chain-of-thought prompting with PaLM 540B outperforms standard prompting by a large margin and achieves new state-of-the-art performance. A prompting only approach is important because it does not require a large training dataset and because a single model checkpoint can perform many tasks without loss of generality. This work underscores how large language models can learn via a few examples with natural language data about the task (c.f. automatically learning the patterns underlying inputs and outputs via a large training dataset)."
        },
        {
            "text": "motivated by two ideas. First, techniques for arithmetic\nreasoning can benefit from generating natural language\nrationales that lead to the final answer. Prior work has\ngiven models the ability to generate natural language intermediate steps by training from scratch (Ling et al., 2017)\nor finetuning a pretrained model (Cobbe et al., 2021), in\naddition to neuro-symbolic methods that use formal languages instead of natural language (Roy and Roth, 2015;\nChiang and Chen, 2019; Amini et al., 2019; Chen et al.,\n2019). Second, large language models offer the exciting\nprospect of in-context few-shot learning via prompting.\nlanguage model checkpoint for each new task, one can\ninput\u2013output exemplars demonstrating the task. Remarkably, this has been successful for a range of simple question-answering tasks (Brown et al., 2020).\n\nMath Word Problems (GSM8K)\n\nFigure 2:   PaLM 540B uses chain-of-\nthought prompting to achieve new state-\nof-the-art performance on the GSM8K\nbenchmark of math word problems.\nFinetuned GPT-3 and prior best are from\nCobbe et al. (2021).\nThat is, instead of finetuning a separate\ncan simply \u201cprompt\u201d the model with a few\nThis",
            "page": 2,
            "x": 105,
            "y": 242,
            "width": 403,
            "height": 152,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "3e242771-fa26-4695-a9b3-c65c6043c808",
            "group_text": "# 1 Introduction\n\nThe NLP landscape has recently been revolutionized by language models (Peters et al., 2018; Devlin et al., 2019; Brown et al., 2020, _inter alia_). Scaling up the size of language models has been shown to confer a range of benefits, such as improved performance and sample efficiency (Kaplan et al., 2020; Brown et al., 2020, _inter alia_). However, scaling up model size alone has not proved sufficient for achieving high performance on challenging tasks such as arithmetic, commonsense, and symbolic reasoning (Rae et al., 2021).\n\nThis work explores how the reasoning ability of large\nlanguage models can be unlocked by a simple method\nmotivated by two ideas. First, techniques for arithmetic\nreasoning can benefit from generating natural language\nrationales that lead to the final answer. Prior work has\ngiven models the ability to generate natural language inter-\nmediate steps by training from scratch (Ling et al., 2017)\nor finetuning a pretrained model (Cobbe et al., 2021), in\naddition to neuro-symbolic methods that use formal lan-\nguages instead of natural language (Roy and Roth, 2015;\nChiang and Chen, 2019; Amini et al., 2019; Chen et al.,\n2019). Second, large language models offer the exciting\nprospect of in-context few-shot learning via prompting.\nlanguage model checkpoint for each new task, one can sim-\ninput\u2013output exemplars demonstrating the task. Remarkably, this has be\nsimple question-answering tasks (Brown et al., 2020).\n\nmotivated by two ideas. First, techniques for arithmetic\nreasoning can benefit from generating natural language\nrationales that lead to the final answer. Prior work has\ngiven models the ability to generate natural language intermediate steps by training from scratch (Ling et al., 2017)\nor finetuning a pretrained model (Cobbe et al., 2021), in\naddition to neuro-symbolic methods that use formal languages instead of natural language (Roy and Roth, 2015;\nChiang and Chen, 2019; Amini et al., 2019; Chen et al.,\n2019). Second, large language models offer the exciting\nprospect of in-context few-shot learning via prompting.\nlanguage model checkpoint for each new task, one can\ninput\u2013output exemplars demonstrating the task. Remarkably, this has been successful for a range of simple question-answering tasks (Brown et al., 2020).\n\nMath Word Problems (GSM8K)\n\nFigure 2:   PaLM 540B uses chain-of-\nthought prompting to achieve new state-\nof-the-art performance on the GSM8K\nbenchmark of math word problems.\nFinetuned GPT-3 and prior best are from\nCobbe et al. (2021).\nThat is, instead of finetuning a separate\ncan simply \u201cprompt\u201d the model with a few\nThis\n\nBoth of the above ideas, however, have key limitations. For rationale-augmented training and\nfinetuning methods, it is costly to create a large set of high quality rationales, which is much more\ncomplicated than simple input\u2013output pairs used in normal machine learning. For the traditional few-\nshot prompting method used in Brown et al. (2020), it works poorly on tasks that require reasoning\nabilities, and often does not improve substantially with increasing language model scale (Rae et al.,\n2021). In this paper, we combine the strengths of these two ideas in a way that avoids their limitations.\nSpecifically, we explore the ability of language models to perform few-shot prompting for reasoning\ntasks, given a prompt that consists of triples: \u27e8input, _chain of thought_, output\u27e9. A _chain of thought_ is\na series of intermediate natural language reasoning steps that lead to the final output, and we refer to\nthis approach as _chain-of-thought prompting_. An example prompt is shown in Figure 1.\n\nWe present empirical evaluations on arithmetic, commonsense, and symbolic reasoning benchmarks, showing that chain-of-thought prompting outperforms standard prompting, sometimes to a striking degree. Figure 2 illustrates one such result\u2014on the GSM8K benchmark of math word problems (Cobbe et al., 2021), chain-of-thought prompting with PaLM 540B outperforms standard prompting by a large margin and achieves new state-of-the-art performance. A prompting only approach is important because it does not require a large training dataset and because a single model checkpoint can perform many tasks without loss of generality. This work underscores how large language models can learn via a few examples with natural language data about the task (c.f. automatically learning the patterns underlying inputs and outputs via a large training dataset)."
        },
        {
            "text": "Both of the above ideas, however, have key limitations. For rationale-augmented training and\nfinetuning methods, it is costly to create a large set of high quality rationales, which is much more\ncomplicated than simple input\u2013output pairs used in normal machine learning. For the traditional few-\nshot prompting method used in Brown et al. (2020), it works poorly on tasks that require reasoning\nabilities, and often does not improve substantially with increasing language model scale (Rae et al.,\n2021). In this paper, we combine the strengths of these two ideas in a way that avoids their limitations.\nSpecifically, we explore the ability of language models to perform few-shot prompting for reasoning\ntasks, given a prompt that consists of triples: \u27e8input, _chain of thought_, output\u27e9. A _chain of thought_ is\na series of intermediate natural language reasoning steps that lead to the final output, and we refer to\nthis approach as _chain-of-thought prompting_. An example prompt is shown in Figure 1.",
            "page": 2,
            "x": 105,
            "y": 392,
            "width": 403,
            "height": 113,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "cbb338fe-7387-4817-b5d7-cd2519fe7506",
            "group_text": "# 1 Introduction\n\nThe NLP landscape has recently been revolutionized by language models (Peters et al., 2018; Devlin et al., 2019; Brown et al., 2020, _inter alia_). Scaling up the size of language models has been shown to confer a range of benefits, such as improved performance and sample efficiency (Kaplan et al., 2020; Brown et al., 2020, _inter alia_). However, scaling up model size alone has not proved sufficient for achieving high performance on challenging tasks such as arithmetic, commonsense, and symbolic reasoning (Rae et al., 2021).\n\nThis work explores how the reasoning ability of large\nlanguage models can be unlocked by a simple method\nmotivated by two ideas. First, techniques for arithmetic\nreasoning can benefit from generating natural language\nrationales that lead to the final answer. Prior work has\ngiven models the ability to generate natural language inter-\nmediate steps by training from scratch (Ling et al., 2017)\nor finetuning a pretrained model (Cobbe et al., 2021), in\naddition to neuro-symbolic methods that use formal lan-\nguages instead of natural language (Roy and Roth, 2015;\nChiang and Chen, 2019; Amini et al., 2019; Chen et al.,\n2019). Second, large language models offer the exciting\nprospect of in-context few-shot learning via prompting.\nlanguage model checkpoint for each new task, one can sim-\ninput\u2013output exemplars demonstrating the task. Remarkably, this has be\nsimple question-answering tasks (Brown et al., 2020).\n\nmotivated by two ideas. First, techniques for arithmetic\nreasoning can benefit from generating natural language\nrationales that lead to the final answer. Prior work has\ngiven models the ability to generate natural language intermediate steps by training from scratch (Ling et al., 2017)\nor finetuning a pretrained model (Cobbe et al., 2021), in\naddition to neuro-symbolic methods that use formal languages instead of natural language (Roy and Roth, 2015;\nChiang and Chen, 2019; Amini et al., 2019; Chen et al.,\n2019). Second, large language models offer the exciting\nprospect of in-context few-shot learning via prompting.\nlanguage model checkpoint for each new task, one can\ninput\u2013output exemplars demonstrating the task. Remarkably, this has been successful for a range of simple question-answering tasks (Brown et al., 2020).\n\nMath Word Problems (GSM8K)\n\nFigure 2:   PaLM 540B uses chain-of-\nthought prompting to achieve new state-\nof-the-art performance on the GSM8K\nbenchmark of math word problems.\nFinetuned GPT-3 and prior best are from\nCobbe et al. (2021).\nThat is, instead of finetuning a separate\ncan simply \u201cprompt\u201d the model with a few\nThis\n\nBoth of the above ideas, however, have key limitations. For rationale-augmented training and\nfinetuning methods, it is costly to create a large set of high quality rationales, which is much more\ncomplicated than simple input\u2013output pairs used in normal machine learning. For the traditional few-\nshot prompting method used in Brown et al. (2020), it works poorly on tasks that require reasoning\nabilities, and often does not improve substantially with increasing language model scale (Rae et al.,\n2021). In this paper, we combine the strengths of these two ideas in a way that avoids their limitations.\nSpecifically, we explore the ability of language models to perform few-shot prompting for reasoning\ntasks, given a prompt that consists of triples: \u27e8input, _chain of thought_, output\u27e9. A _chain of thought_ is\na series of intermediate natural language reasoning steps that lead to the final output, and we refer to\nthis approach as _chain-of-thought prompting_. An example prompt is shown in Figure 1.\n\nWe present empirical evaluations on arithmetic, commonsense, and symbolic reasoning benchmarks, showing that chain-of-thought prompting outperforms standard prompting, sometimes to a striking degree. Figure 2 illustrates one such result\u2014on the GSM8K benchmark of math word problems (Cobbe et al., 2021), chain-of-thought prompting with PaLM 540B outperforms standard prompting by a large margin and achieves new state-of-the-art performance. A prompting only approach is important because it does not require a large training dataset and because a single model checkpoint can perform many tasks without loss of generality. This work underscores how large language models can learn via a few examples with natural language data about the task (c.f. automatically learning the patterns underlying inputs and outputs via a large training dataset)."
        },
        {
            "text": "We present empirical evaluations on arithmetic, commonsense, and symbolic reasoning benchmarks, showing that chain-of-thought prompting outperforms standard prompting, sometimes to a striking degree. Figure 2 illustrates one such result\u2014on the GSM8K benchmark of math word problems (Cobbe et al., 2021), chain-of-thought prompting with PaLM 540B outperforms standard prompting by a large margin and achieves new state-of-the-art performance. A prompting only approach is important because it does not require a large training dataset and because a single model checkpoint can perform many tasks without loss of generality. This work underscores how large language models can learn via a few examples with natural language data about the task (c.f. automatically learning the patterns underlying inputs and outputs via a large training dataset).",
            "page": 2,
            "x": 104,
            "y": 507,
            "width": 404,
            "height": 102,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "500f06f4-b8d3-43cc-a1f5-94a457152332",
            "group_text": "# 1 Introduction\n\nThe NLP landscape has recently been revolutionized by language models (Peters et al., 2018; Devlin et al., 2019; Brown et al., 2020, _inter alia_). Scaling up the size of language models has been shown to confer a range of benefits, such as improved performance and sample efficiency (Kaplan et al., 2020; Brown et al., 2020, _inter alia_). However, scaling up model size alone has not proved sufficient for achieving high performance on challenging tasks such as arithmetic, commonsense, and symbolic reasoning (Rae et al., 2021).\n\nThis work explores how the reasoning ability of large\nlanguage models can be unlocked by a simple method\nmotivated by two ideas. First, techniques for arithmetic\nreasoning can benefit from generating natural language\nrationales that lead to the final answer. Prior work has\ngiven models the ability to generate natural language inter-\nmediate steps by training from scratch (Ling et al., 2017)\nor finetuning a pretrained model (Cobbe et al., 2021), in\naddition to neuro-symbolic methods that use formal lan-\nguages instead of natural language (Roy and Roth, 2015;\nChiang and Chen, 2019; Amini et al., 2019; Chen et al.,\n2019). Second, large language models offer the exciting\nprospect of in-context few-shot learning via prompting.\nlanguage model checkpoint for each new task, one can sim-\ninput\u2013output exemplars demonstrating the task. Remarkably, this has be\nsimple question-answering tasks (Brown et al., 2020).\n\nmotivated by two ideas. First, techniques for arithmetic\nreasoning can benefit from generating natural language\nrationales that lead to the final answer. Prior work has\ngiven models the ability to generate natural language intermediate steps by training from scratch (Ling et al., 2017)\nor finetuning a pretrained model (Cobbe et al., 2021), in\naddition to neuro-symbolic methods that use formal languages instead of natural language (Roy and Roth, 2015;\nChiang and Chen, 2019; Amini et al., 2019; Chen et al.,\n2019). Second, large language models offer the exciting\nprospect of in-context few-shot learning via prompting.\nlanguage model checkpoint for each new task, one can\ninput\u2013output exemplars demonstrating the task. Remarkably, this has been successful for a range of simple question-answering tasks (Brown et al., 2020).\n\nMath Word Problems (GSM8K)\n\nFigure 2:   PaLM 540B uses chain-of-\nthought prompting to achieve new state-\nof-the-art performance on the GSM8K\nbenchmark of math word problems.\nFinetuned GPT-3 and prior best are from\nCobbe et al. (2021).\nThat is, instead of finetuning a separate\ncan simply \u201cprompt\u201d the model with a few\nThis\n\nBoth of the above ideas, however, have key limitations. For rationale-augmented training and\nfinetuning methods, it is costly to create a large set of high quality rationales, which is much more\ncomplicated than simple input\u2013output pairs used in normal machine learning. For the traditional few-\nshot prompting method used in Brown et al. (2020), it works poorly on tasks that require reasoning\nabilities, and often does not improve substantially with increasing language model scale (Rae et al.,\n2021). In this paper, we combine the strengths of these two ideas in a way that avoids their limitations.\nSpecifically, we explore the ability of language models to perform few-shot prompting for reasoning\ntasks, given a prompt that consists of triples: \u27e8input, _chain of thought_, output\u27e9. A _chain of thought_ is\na series of intermediate natural language reasoning steps that lead to the final output, and we refer to\nthis approach as _chain-of-thought prompting_. An example prompt is shown in Figure 1.\n\nWe present empirical evaluations on arithmetic, commonsense, and symbolic reasoning benchmarks, showing that chain-of-thought prompting outperforms standard prompting, sometimes to a striking degree. Figure 2 illustrates one such result\u2014on the GSM8K benchmark of math word problems (Cobbe et al., 2021), chain-of-thought prompting with PaLM 540B outperforms standard prompting by a large margin and achieves new state-of-the-art performance. A prompting only approach is important because it does not require a large training dataset and because a single model checkpoint can perform many tasks without loss of generality. This work underscores how large language models can learn via a few examples with natural language data about the task (c.f. automatically learning the patterns underlying inputs and outputs via a large training dataset)."
        },
        {
            "text": "## 2  Chain-of-Thought Prompting\n\nConsider one\u2019s own thought process when solving a complicated reasoning task such as a multi-step math word problem. It is typical to decompose the problem into intermediate steps and solve each before giving the final answer: _\u201cAfter Jane gives 2 flowers to her mom she has 10 . . . then after she gives 3 to her dad she will have 7 . . . so the answer is 7.\u201d_ The goal of this paper is to endow language models with the ability to generate a similar _chain of thought_\u2014a coherent series of intermediate reasoning steps that lead to the final answer for a problem. We will show that sufficiently large",
            "page": 2,
            "x": 103,
            "y": 624,
            "width": 405,
            "height": 101,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "3-chain",
            "chunk_id": "e6aabbdc-1afe-4e7f-8285-ae3d85ef4f4d",
            "group_text": "## 2  Chain-of-Thought Prompting\n\nConsider one\u2019s own thought process when solving a complicated reasoning task such as a multi-step math word problem. It is typical to decompose the problem into intermediate steps and solve each before giving the final answer: _\u201cAfter Jane gives 2 flowers to her mom she has 10 . . . then after she gives 3 to her dad she will have 7 . . . so the answer is 7.\u201d_ The goal of this paper is to endow language models with the ability to generate a similar _chain of thought_\u2014a coherent series of intermediate reasoning steps that lead to the final answer for a problem. We will show that sufficiently large\n\nlanguage models can generate chains of thought if demonstrations of chain-of-thought reasoning are provided in the exemplars for few-shot prompting.\n\nFigure 1 shows an example of a model producing a chain of thought to solve a math word problem that it would have otherwise gotten incorrect. The chain of thought in this case resembles a solution and can interpreted as one, but we still opt to call it a chain of thought to better capture the idea that it mimics a step-by-step thought process for arriving at the answer (and also, solutions/explanations typically come after the final answer (Narang et al., 2020; Wiegreffe et al., 2022; Lampinen et al., 2022, inter alia)).\n\nChain-of-thought prompting has several attractive properties as an approach for facilitating reasoning in language models.\n\n1. First, chain of thought, in principle, allows models to decompose multi-step problems into intermediate steps, which means that additional computation can be allocated to problems that require more reasoning steps.\n2. Second, a chain of thought provides an interpretable window into the behavior of the model, suggesting how it might have arrived at a particular answer and providing opportunities to debug where the reasoning path went wrong (although fully characterizing a model\u2019s computations that support an answer remains an open question).\n3. Third, chain-of-thought reasoning can be used for tasks such as math word problems, commonsense reasoning, and symbolic manipulation, and is potentially applicable (at least in principle) to any task that humans can solve via language.\n4. Finally, chain-of-thought reasoning can be readily elicited in sufficiently large off-the-shelf language models simply by including examples of chain of thought sequences into the exemplars of few-shot prompting.\n\nIn empirical experiments, we will observe the utility of chain-of-thought prompting for arithmetic reasoning (Section 3), commonsense reasoning (Section 4), and symbolic reasoning (Section 5)."
        },
        {
            "text": "language models can generate chains of thought if demonstrations of chain-of-thought reasoning are provided in the exemplars for few-shot prompting.",
            "page": 3,
            "x": 104,
            "y": 72,
            "width": 403,
            "height": 25,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "3-chain",
            "chunk_id": "a05370b6-77eb-4e3b-8138-878316348fe0",
            "group_text": "## 2  Chain-of-Thought Prompting\n\nConsider one\u2019s own thought process when solving a complicated reasoning task such as a multi-step math word problem. It is typical to decompose the problem into intermediate steps and solve each before giving the final answer: _\u201cAfter Jane gives 2 flowers to her mom she has 10 . . . then after she gives 3 to her dad she will have 7 . . . so the answer is 7.\u201d_ The goal of this paper is to endow language models with the ability to generate a similar _chain of thought_\u2014a coherent series of intermediate reasoning steps that lead to the final answer for a problem. We will show that sufficiently large\n\nlanguage models can generate chains of thought if demonstrations of chain-of-thought reasoning are provided in the exemplars for few-shot prompting.\n\nFigure 1 shows an example of a model producing a chain of thought to solve a math word problem that it would have otherwise gotten incorrect. The chain of thought in this case resembles a solution and can interpreted as one, but we still opt to call it a chain of thought to better capture the idea that it mimics a step-by-step thought process for arriving at the answer (and also, solutions/explanations typically come after the final answer (Narang et al., 2020; Wiegreffe et al., 2022; Lampinen et al., 2022, inter alia)).\n\nChain-of-thought prompting has several attractive properties as an approach for facilitating reasoning in language models.\n\n1. First, chain of thought, in principle, allows models to decompose multi-step problems into intermediate steps, which means that additional computation can be allocated to problems that require more reasoning steps.\n2. Second, a chain of thought provides an interpretable window into the behavior of the model, suggesting how it might have arrived at a particular answer and providing opportunities to debug where the reasoning path went wrong (although fully characterizing a model\u2019s computations that support an answer remains an open question).\n3. Third, chain-of-thought reasoning can be used for tasks such as math word problems, commonsense reasoning, and symbolic manipulation, and is potentially applicable (at least in principle) to any task that humans can solve via language.\n4. Finally, chain-of-thought reasoning can be readily elicited in sufficiently large off-the-shelf language models simply by including examples of chain of thought sequences into the exemplars of few-shot prompting.\n\nIn empirical experiments, we will observe the utility of chain-of-thought prompting for arithmetic reasoning (Section 3), commonsense reasoning (Section 4), and symbolic reasoning (Section 5)."
        },
        {
            "text": "Figure 1 shows an example of a model producing a chain of thought to solve a math word problem that it would have otherwise gotten incorrect. The chain of thought in this case resembles a solution and can interpreted as one, but we still opt to call it a chain of thought to better capture the idea that it mimics a step-by-step thought process for arriving at the answer (and also, solutions/explanations typically come after the final answer (Narang et al., 2020; Wiegreffe et al., 2022; Lampinen et al., 2022, inter alia)).",
            "page": 3,
            "x": 105,
            "y": 99,
            "width": 402,
            "height": 68,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "3-chain",
            "chunk_id": "3a43bae0-86f8-4669-b7c9-59c4338023b6",
            "group_text": "## 2  Chain-of-Thought Prompting\n\nConsider one\u2019s own thought process when solving a complicated reasoning task such as a multi-step math word problem. It is typical to decompose the problem into intermediate steps and solve each before giving the final answer: _\u201cAfter Jane gives 2 flowers to her mom she has 10 . . . then after she gives 3 to her dad she will have 7 . . . so the answer is 7.\u201d_ The goal of this paper is to endow language models with the ability to generate a similar _chain of thought_\u2014a coherent series of intermediate reasoning steps that lead to the final answer for a problem. We will show that sufficiently large\n\nlanguage models can generate chains of thought if demonstrations of chain-of-thought reasoning are provided in the exemplars for few-shot prompting.\n\nFigure 1 shows an example of a model producing a chain of thought to solve a math word problem that it would have otherwise gotten incorrect. The chain of thought in this case resembles a solution and can interpreted as one, but we still opt to call it a chain of thought to better capture the idea that it mimics a step-by-step thought process for arriving at the answer (and also, solutions/explanations typically come after the final answer (Narang et al., 2020; Wiegreffe et al., 2022; Lampinen et al., 2022, inter alia)).\n\nChain-of-thought prompting has several attractive properties as an approach for facilitating reasoning in language models.\n\n1. First, chain of thought, in principle, allows models to decompose multi-step problems into intermediate steps, which means that additional computation can be allocated to problems that require more reasoning steps.\n2. Second, a chain of thought provides an interpretable window into the behavior of the model, suggesting how it might have arrived at a particular answer and providing opportunities to debug where the reasoning path went wrong (although fully characterizing a model\u2019s computations that support an answer remains an open question).\n3. Third, chain-of-thought reasoning can be used for tasks such as math word problems, commonsense reasoning, and symbolic manipulation, and is potentially applicable (at least in principle) to any task that humans can solve via language.\n4. Finally, chain-of-thought reasoning can be readily elicited in sufficiently large off-the-shelf language models simply by including examples of chain of thought sequences into the exemplars of few-shot prompting.\n\nIn empirical experiments, we will observe the utility of chain-of-thought prompting for arithmetic reasoning (Section 3), commonsense reasoning (Section 4), and symbolic reasoning (Section 5)."
        },
        {
            "text": "Chain-of-thought prompting has several attractive properties as an approach for facilitating reasoning in language models.\n\n1. First, chain of thought, in principle, allows models to decompose multi-step problems into intermediate steps, which means that additional computation can be allocated to problems that require more reasoning steps.\n2. Second, a chain of thought provides an interpretable window into the behavior of the model, suggesting how it might have arrived at a particular answer and providing opportunities to debug where the reasoning path went wrong (although fully characterizing a model\u2019s computations that support an answer remains an open question).\n3. Third, chain-of-thought reasoning can be used for tasks such as math word problems, commonsense reasoning, and symbolic manipulation, and is potentially applicable (at least in principle) to any task that humans can solve via language.\n4. Finally, chain-of-thought reasoning can be readily elicited in sufficiently large off-the-shelf language models simply by including examples of chain of thought sequences into the exemplars of few-shot prompting.",
            "page": 3,
            "x": 105,
            "y": 170,
            "width": 403,
            "height": 181,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "3-chain",
            "chunk_id": "2f94ddab-68bf-4701-a6a7-53e81059e8a4",
            "group_text": "## 2  Chain-of-Thought Prompting\n\nConsider one\u2019s own thought process when solving a complicated reasoning task such as a multi-step math word problem. It is typical to decompose the problem into intermediate steps and solve each before giving the final answer: _\u201cAfter Jane gives 2 flowers to her mom she has 10 . . . then after she gives 3 to her dad she will have 7 . . . so the answer is 7.\u201d_ The goal of this paper is to endow language models with the ability to generate a similar _chain of thought_\u2014a coherent series of intermediate reasoning steps that lead to the final answer for a problem. We will show that sufficiently large\n\nlanguage models can generate chains of thought if demonstrations of chain-of-thought reasoning are provided in the exemplars for few-shot prompting.\n\nFigure 1 shows an example of a model producing a chain of thought to solve a math word problem that it would have otherwise gotten incorrect. The chain of thought in this case resembles a solution and can interpreted as one, but we still opt to call it a chain of thought to better capture the idea that it mimics a step-by-step thought process for arriving at the answer (and also, solutions/explanations typically come after the final answer (Narang et al., 2020; Wiegreffe et al., 2022; Lampinen et al., 2022, inter alia)).\n\nChain-of-thought prompting has several attractive properties as an approach for facilitating reasoning in language models.\n\n1. First, chain of thought, in principle, allows models to decompose multi-step problems into intermediate steps, which means that additional computation can be allocated to problems that require more reasoning steps.\n2. Second, a chain of thought provides an interpretable window into the behavior of the model, suggesting how it might have arrived at a particular answer and providing opportunities to debug where the reasoning path went wrong (although fully characterizing a model\u2019s computations that support an answer remains an open question).\n3. Third, chain-of-thought reasoning can be used for tasks such as math word problems, commonsense reasoning, and symbolic manipulation, and is potentially applicable (at least in principle) to any task that humans can solve via language.\n4. Finally, chain-of-thought reasoning can be readily elicited in sufficiently large off-the-shelf language models simply by including examples of chain of thought sequences into the exemplars of few-shot prompting.\n\nIn empirical experiments, we will observe the utility of chain-of-thought prompting for arithmetic reasoning (Section 3), commonsense reasoning (Section 4), and symbolic reasoning (Section 5)."
        },
        {
            "text": "In empirical experiments, we will observe the utility of chain-of-thought prompting for arithmetic reasoning (Section 3), commonsense reasoning (Section 4), and symbolic reasoning (Section 5).",
            "page": 3,
            "x": 104,
            "y": 354,
            "width": 402,
            "height": 27,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "3-chain",
            "chunk_id": "0910ca1c-d175-4875-a5ac-5518fe2404c5",
            "group_text": "## 2  Chain-of-Thought Prompting\n\nConsider one\u2019s own thought process when solving a complicated reasoning task such as a multi-step math word problem. It is typical to decompose the problem into intermediate steps and solve each before giving the final answer: _\u201cAfter Jane gives 2 flowers to her mom she has 10 . . . then after she gives 3 to her dad she will have 7 . . . so the answer is 7.\u201d_ The goal of this paper is to endow language models with the ability to generate a similar _chain of thought_\u2014a coherent series of intermediate reasoning steps that lead to the final answer for a problem. We will show that sufficiently large\n\nlanguage models can generate chains of thought if demonstrations of chain-of-thought reasoning are provided in the exemplars for few-shot prompting.\n\nFigure 1 shows an example of a model producing a chain of thought to solve a math word problem that it would have otherwise gotten incorrect. The chain of thought in this case resembles a solution and can interpreted as one, but we still opt to call it a chain of thought to better capture the idea that it mimics a step-by-step thought process for arriving at the answer (and also, solutions/explanations typically come after the final answer (Narang et al., 2020; Wiegreffe et al., 2022; Lampinen et al., 2022, inter alia)).\n\nChain-of-thought prompting has several attractive properties as an approach for facilitating reasoning in language models.\n\n1. First, chain of thought, in principle, allows models to decompose multi-step problems into intermediate steps, which means that additional computation can be allocated to problems that require more reasoning steps.\n2. Second, a chain of thought provides an interpretable window into the behavior of the model, suggesting how it might have arrived at a particular answer and providing opportunities to debug where the reasoning path went wrong (although fully characterizing a model\u2019s computations that support an answer remains an open question).\n3. Third, chain-of-thought reasoning can be used for tasks such as math word problems, commonsense reasoning, and symbolic manipulation, and is potentially applicable (at least in principle) to any task that humans can solve via language.\n4. Finally, chain-of-thought reasoning can be readily elicited in sufficiently large off-the-shelf language models simply by including examples of chain of thought sequences into the exemplars of few-shot prompting.\n\nIn empirical experiments, we will observe the utility of chain-of-thought prompting for arithmetic reasoning (Section 3), commonsense reasoning (Section 4), and symbolic reasoning (Section 5)."
        },
        {
            "text": "### 3  Arithmetic Reasoning\n\nWe begin by considering math word problems of the form in Figure 1, which measure the arithmetic reasoning ability of language models. Though simple for humans, arithmetic reasoning is a task where language models often struggle (Hendrycks et al., 2021; Patel et al., 2021, _inter alia_). Strikingly, chain-of-thought prompting when used with the 540B parameter language model performs comparably with task-specific finetuned models on several tasks, even achieving new state of the art on the challenging GSM8K benchmark (Cobbe et al., 2021).",
            "page": 3,
            "x": 104,
            "y": 391,
            "width": 404,
            "height": 95,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "4-arithmetic",
            "chunk_id": "a71a613f-1158-4b62-b7ca-be097437fdde",
            "group_text": "### 3  Arithmetic Reasoning\n\nWe begin by considering math word problems of the form in Figure 1, which measure the arithmetic reasoning ability of language models. Though simple for humans, arithmetic reasoning is a task where language models often struggle (Hendrycks et al., 2021; Patel et al., 2021, _inter alia_). Strikingly, chain-of-thought prompting when used with the 540B parameter language model performs comparably with task-specific finetuned models on several tasks, even achieving new state of the art on the challenging GSM8K benchmark (Cobbe et al., 2021)."
        },
        {
            "text": "3.1  Experimental Setup\n\nWe explore chain-of-thought prompting for various language models on multiple benchmarks.",
            "page": 3,
            "x": 104,
            "y": 498,
            "width": 380,
            "height": 35,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-experimental",
            "chunk_id": "11741339-7ada-4e41-bae5-22c8336e1673",
            "group_text": "3.1  Experimental Setup\n\nWe explore chain-of-thought prompting for various language models on multiple benchmarks.\n\n**Benchmarks.** We consider the following five math word problem benchmarks: **(1) the GSM8K** benchmark of math word problems (Cobbe et al., 2021), (2) the **SVAMP** dataset of math word problems with varying structures (Patel et al., 2021), (3) the **ASDiv** dataset of diverse math word problems (Miao et al., 2020), (4) the **AQuA** dataset of algebraic word problems, and (5) the **MAWPS** benchmark (Koncel-Kedziorski et al., 2016). Example problems are given in Appendix Table 12.\n\n**Standard prompting.** For the baseline, we consider standard few-shot prompting, popularized by Brown et al. (2020), in which a language model is given in-context exemplars of input\u2013output pairs before outputting a prediction for a test-time example. Exemplars are formatted as questions and answers. The model gives the answer directly, as shown in Figure 1 (left).\n\n**Chain-of-thought prompting.** Our proposed approach is to augment each exemplar in few-shot prompting with a chain of thought for an associated answer, as illustrated in Figure 1 (right). As most of the datasets only have an evaluation split, we manually composed a set of eight few-shot exemplars with chains of thought for prompting\u2014Figure 1 (right) shows one chain of thought exemplar, and the full set of exemplars is given in Appendix Table 20. (These particular exemplars did not undergo prompt engineering; robustness is studied in Section 3.4 and Appendix A.2.) To investigate whether chain-of-thought prompting in this form can successfully elicit successful reasoning across a range of\n\nFigure 3: Examples of \u27e8input, chain of thought, output\u27e9 triples for arithmetic, commonsense, and symbolic reasoning benchmarks. Chains of thought are highlighted. Full prompts in Appendix G.\n\nmath word problems, we used this single set of eight chain of thought exemplars for all benchmarks\nexcept AQuA, which is multiple choice instead of free response. For AQuA, we used four exemplars\nand solutions from the training set, as given in Appendix Table 21.\n\nLanguage models. We evaluate five large language models. The first is **GPT-3** (Brown et al., 2020), for which we use text-ada-001, text-babbage-001, text-curie-001, and text-davinci-002, which presumably correspond to InstructGPT models of 350M, 1.3B, 6.7B, and 175B parameters (Ouyang et al., 2022). The second is **LaMDA** (Thoppilan et al., 2022), which has models of 422M, 2B, 8B, 68B, and 137B parameters. The third is **PaLM**, which has models of 8B, 62B, and 540B parameters. The fourth is **UL2 20B** (Tay et al., 2022), and the fifth is **Codex** (Chen et al., 2021, code-davinci-002 in the OpenAI API). We sample from the models via greedy decoding (though follow-up work shows chain-of-thought prompting can be improved by taking the majority final answer over many sampled generations (Wang et al., 2022a)). For LaMDA, we report averaged results over five random seeds, where each seed had a different randomly shuffled order of exemplars. As LaMDA experiments did not show large variance among different seeds, to save compute we report results for a single exemplar order for all other models."
        },
        {
            "text": "**Benchmarks.** We consider the following five math word problem benchmarks: **(1) the GSM8K** benchmark of math word problems (Cobbe et al., 2021), (2) the **SVAMP** dataset of math word problems with varying structures (Patel et al., 2021), (3) the **ASDiv** dataset of diverse math word problems (Miao et al., 2020), (4) the **AQuA** dataset of algebraic word problems, and (5) the **MAWPS** benchmark (Koncel-Kedziorski et al., 2016). Example problems are given in Appendix Table 12.",
            "page": 3,
            "x": 105,
            "y": 535,
            "width": 401,
            "height": 58,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-experimental",
            "chunk_id": "0bb2712f-eaaf-4220-b43f-a99beb0b379b",
            "group_text": "3.1  Experimental Setup\n\nWe explore chain-of-thought prompting for various language models on multiple benchmarks.\n\n**Benchmarks.** We consider the following five math word problem benchmarks: **(1) the GSM8K** benchmark of math word problems (Cobbe et al., 2021), (2) the **SVAMP** dataset of math word problems with varying structures (Patel et al., 2021), (3) the **ASDiv** dataset of diverse math word problems (Miao et al., 2020), (4) the **AQuA** dataset of algebraic word problems, and (5) the **MAWPS** benchmark (Koncel-Kedziorski et al., 2016). Example problems are given in Appendix Table 12.\n\n**Standard prompting.** For the baseline, we consider standard few-shot prompting, popularized by Brown et al. (2020), in which a language model is given in-context exemplars of input\u2013output pairs before outputting a prediction for a test-time example. Exemplars are formatted as questions and answers. The model gives the answer directly, as shown in Figure 1 (left).\n\n**Chain-of-thought prompting.** Our proposed approach is to augment each exemplar in few-shot prompting with a chain of thought for an associated answer, as illustrated in Figure 1 (right). As most of the datasets only have an evaluation split, we manually composed a set of eight few-shot exemplars with chains of thought for prompting\u2014Figure 1 (right) shows one chain of thought exemplar, and the full set of exemplars is given in Appendix Table 20. (These particular exemplars did not undergo prompt engineering; robustness is studied in Section 3.4 and Appendix A.2.) To investigate whether chain-of-thought prompting in this form can successfully elicit successful reasoning across a range of\n\nFigure 3: Examples of \u27e8input, chain of thought, output\u27e9 triples for arithmetic, commonsense, and symbolic reasoning benchmarks. Chains of thought are highlighted. Full prompts in Appendix G.\n\nmath word problems, we used this single set of eight chain of thought exemplars for all benchmarks\nexcept AQuA, which is multiple choice instead of free response. For AQuA, we used four exemplars\nand solutions from the training set, as given in Appendix Table 21.\n\nLanguage models. We evaluate five large language models. The first is **GPT-3** (Brown et al., 2020), for which we use text-ada-001, text-babbage-001, text-curie-001, and text-davinci-002, which presumably correspond to InstructGPT models of 350M, 1.3B, 6.7B, and 175B parameters (Ouyang et al., 2022). The second is **LaMDA** (Thoppilan et al., 2022), which has models of 422M, 2B, 8B, 68B, and 137B parameters. The third is **PaLM**, which has models of 8B, 62B, and 540B parameters. The fourth is **UL2 20B** (Tay et al., 2022), and the fifth is **Codex** (Chen et al., 2021, code-davinci-002 in the OpenAI API). We sample from the models via greedy decoding (though follow-up work shows chain-of-thought prompting can be improved by taking the majority final answer over many sampled generations (Wang et al., 2022a)). For LaMDA, we report averaged results over five random seeds, where each seed had a different randomly shuffled order of exemplars. As LaMDA experiments did not show large variance among different seeds, to save compute we report results for a single exemplar order for all other models."
        },
        {
            "text": "**Standard prompting.** For the baseline, we consider standard few-shot prompting, popularized by Brown et al. (2020), in which a language model is given in-context exemplars of input\u2013output pairs before outputting a prediction for a test-time example. Exemplars are formatted as questions and answers. The model gives the answer directly, as shown in Figure 1 (left).",
            "page": 3,
            "x": 105,
            "y": 595,
            "width": 401,
            "height": 47,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-experimental",
            "chunk_id": "0aa8f595-b657-44f6-b23d-1b0cef0e581b",
            "group_text": "3.1  Experimental Setup\n\nWe explore chain-of-thought prompting for various language models on multiple benchmarks.\n\n**Benchmarks.** We consider the following five math word problem benchmarks: **(1) the GSM8K** benchmark of math word problems (Cobbe et al., 2021), (2) the **SVAMP** dataset of math word problems with varying structures (Patel et al., 2021), (3) the **ASDiv** dataset of diverse math word problems (Miao et al., 2020), (4) the **AQuA** dataset of algebraic word problems, and (5) the **MAWPS** benchmark (Koncel-Kedziorski et al., 2016). Example problems are given in Appendix Table 12.\n\n**Standard prompting.** For the baseline, we consider standard few-shot prompting, popularized by Brown et al. (2020), in which a language model is given in-context exemplars of input\u2013output pairs before outputting a prediction for a test-time example. Exemplars are formatted as questions and answers. The model gives the answer directly, as shown in Figure 1 (left).\n\n**Chain-of-thought prompting.** Our proposed approach is to augment each exemplar in few-shot prompting with a chain of thought for an associated answer, as illustrated in Figure 1 (right). As most of the datasets only have an evaluation split, we manually composed a set of eight few-shot exemplars with chains of thought for prompting\u2014Figure 1 (right) shows one chain of thought exemplar, and the full set of exemplars is given in Appendix Table 20. (These particular exemplars did not undergo prompt engineering; robustness is studied in Section 3.4 and Appendix A.2.) To investigate whether chain-of-thought prompting in this form can successfully elicit successful reasoning across a range of\n\nFigure 3: Examples of \u27e8input, chain of thought, output\u27e9 triples for arithmetic, commonsense, and symbolic reasoning benchmarks. Chains of thought are highlighted. Full prompts in Appendix G.\n\nmath word problems, we used this single set of eight chain of thought exemplars for all benchmarks\nexcept AQuA, which is multiple choice instead of free response. For AQuA, we used four exemplars\nand solutions from the training set, as given in Appendix Table 21.\n\nLanguage models. We evaluate five large language models. The first is **GPT-3** (Brown et al., 2020), for which we use text-ada-001, text-babbage-001, text-curie-001, and text-davinci-002, which presumably correspond to InstructGPT models of 350M, 1.3B, 6.7B, and 175B parameters (Ouyang et al., 2022). The second is **LaMDA** (Thoppilan et al., 2022), which has models of 422M, 2B, 8B, 68B, and 137B parameters. The third is **PaLM**, which has models of 8B, 62B, and 540B parameters. The fourth is **UL2 20B** (Tay et al., 2022), and the fifth is **Codex** (Chen et al., 2021, code-davinci-002 in the OpenAI API). We sample from the models via greedy decoding (though follow-up work shows chain-of-thought prompting can be improved by taking the majority final answer over many sampled generations (Wang et al., 2022a)). For LaMDA, we report averaged results over five random seeds, where each seed had a different randomly shuffled order of exemplars. As LaMDA experiments did not show large variance among different seeds, to save compute we report results for a single exemplar order for all other models."
        },
        {
            "text": "**Chain-of-thought prompting.** Our proposed approach is to augment each exemplar in few-shot prompting with a chain of thought for an associated answer, as illustrated in Figure 1 (right). As most of the datasets only have an evaluation split, we manually composed a set of eight few-shot exemplars with chains of thought for prompting\u2014Figure 1 (right) shows one chain of thought exemplar, and the full set of exemplars is given in Appendix Table 20. (These particular exemplars did not undergo prompt engineering; robustness is studied in Section 3.4 and Appendix A.2.) To investigate whether chain-of-thought prompting in this form can successfully elicit successful reasoning across a range of",
            "page": 3,
            "x": 105,
            "y": 644,
            "width": 402,
            "height": 80,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-experimental",
            "chunk_id": "07e094c9-b637-45f4-aa3b-44d15ef0f21a",
            "group_text": "3.1  Experimental Setup\n\nWe explore chain-of-thought prompting for various language models on multiple benchmarks.\n\n**Benchmarks.** We consider the following five math word problem benchmarks: **(1) the GSM8K** benchmark of math word problems (Cobbe et al., 2021), (2) the **SVAMP** dataset of math word problems with varying structures (Patel et al., 2021), (3) the **ASDiv** dataset of diverse math word problems (Miao et al., 2020), (4) the **AQuA** dataset of algebraic word problems, and (5) the **MAWPS** benchmark (Koncel-Kedziorski et al., 2016). Example problems are given in Appendix Table 12.\n\n**Standard prompting.** For the baseline, we consider standard few-shot prompting, popularized by Brown et al. (2020), in which a language model is given in-context exemplars of input\u2013output pairs before outputting a prediction for a test-time example. Exemplars are formatted as questions and answers. The model gives the answer directly, as shown in Figure 1 (left).\n\n**Chain-of-thought prompting.** Our proposed approach is to augment each exemplar in few-shot prompting with a chain of thought for an associated answer, as illustrated in Figure 1 (right). As most of the datasets only have an evaluation split, we manually composed a set of eight few-shot exemplars with chains of thought for prompting\u2014Figure 1 (right) shows one chain of thought exemplar, and the full set of exemplars is given in Appendix Table 20. (These particular exemplars did not undergo prompt engineering; robustness is studied in Section 3.4 and Appendix A.2.) To investigate whether chain-of-thought prompting in this form can successfully elicit successful reasoning across a range of\n\nFigure 3: Examples of \u27e8input, chain of thought, output\u27e9 triples for arithmetic, commonsense, and symbolic reasoning benchmarks. Chains of thought are highlighted. Full prompts in Appendix G.\n\nmath word problems, we used this single set of eight chain of thought exemplars for all benchmarks\nexcept AQuA, which is multiple choice instead of free response. For AQuA, we used four exemplars\nand solutions from the training set, as given in Appendix Table 21.\n\nLanguage models. We evaluate five large language models. The first is **GPT-3** (Brown et al., 2020), for which we use text-ada-001, text-babbage-001, text-curie-001, and text-davinci-002, which presumably correspond to InstructGPT models of 350M, 1.3B, 6.7B, and 175B parameters (Ouyang et al., 2022). The second is **LaMDA** (Thoppilan et al., 2022), which has models of 422M, 2B, 8B, 68B, and 137B parameters. The third is **PaLM**, which has models of 8B, 62B, and 540B parameters. The fourth is **UL2 20B** (Tay et al., 2022), and the fifth is **Codex** (Chen et al., 2021, code-davinci-002 in the OpenAI API). We sample from the models via greedy decoding (though follow-up work shows chain-of-thought prompting can be improved by taking the majority final answer over many sampled generations (Wang et al., 2022a)). For LaMDA, we report averaged results over five random seeds, where each seed had a different randomly shuffled order of exemplars. As LaMDA experiments did not show large variance among different seeds, to save compute we report results for a single exemplar order for all other models."
        },
        {
            "text": "Figure 3: Examples of \u27e8input, chain of thought, output\u27e9 triples for arithmetic, commonsense, and symbolic reasoning benchmarks. Chains of thought are highlighted. Full prompts in Appendix G.",
            "page": 4,
            "x": 104,
            "y": 393,
            "width": 402,
            "height": 28,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-experimental",
            "chunk_id": "9ee377fb-85ec-4ee7-9f46-8e673497e330",
            "group_text": "3.1  Experimental Setup\n\nWe explore chain-of-thought prompting for various language models on multiple benchmarks.\n\n**Benchmarks.** We consider the following five math word problem benchmarks: **(1) the GSM8K** benchmark of math word problems (Cobbe et al., 2021), (2) the **SVAMP** dataset of math word problems with varying structures (Patel et al., 2021), (3) the **ASDiv** dataset of diverse math word problems (Miao et al., 2020), (4) the **AQuA** dataset of algebraic word problems, and (5) the **MAWPS** benchmark (Koncel-Kedziorski et al., 2016). Example problems are given in Appendix Table 12.\n\n**Standard prompting.** For the baseline, we consider standard few-shot prompting, popularized by Brown et al. (2020), in which a language model is given in-context exemplars of input\u2013output pairs before outputting a prediction for a test-time example. Exemplars are formatted as questions and answers. The model gives the answer directly, as shown in Figure 1 (left).\n\n**Chain-of-thought prompting.** Our proposed approach is to augment each exemplar in few-shot prompting with a chain of thought for an associated answer, as illustrated in Figure 1 (right). As most of the datasets only have an evaluation split, we manually composed a set of eight few-shot exemplars with chains of thought for prompting\u2014Figure 1 (right) shows one chain of thought exemplar, and the full set of exemplars is given in Appendix Table 20. (These particular exemplars did not undergo prompt engineering; robustness is studied in Section 3.4 and Appendix A.2.) To investigate whether chain-of-thought prompting in this form can successfully elicit successful reasoning across a range of\n\nFigure 3: Examples of \u27e8input, chain of thought, output\u27e9 triples for arithmetic, commonsense, and symbolic reasoning benchmarks. Chains of thought are highlighted. Full prompts in Appendix G.\n\nmath word problems, we used this single set of eight chain of thought exemplars for all benchmarks\nexcept AQuA, which is multiple choice instead of free response. For AQuA, we used four exemplars\nand solutions from the training set, as given in Appendix Table 21.\n\nLanguage models. We evaluate five large language models. The first is **GPT-3** (Brown et al., 2020), for which we use text-ada-001, text-babbage-001, text-curie-001, and text-davinci-002, which presumably correspond to InstructGPT models of 350M, 1.3B, 6.7B, and 175B parameters (Ouyang et al., 2022). The second is **LaMDA** (Thoppilan et al., 2022), which has models of 422M, 2B, 8B, 68B, and 137B parameters. The third is **PaLM**, which has models of 8B, 62B, and 540B parameters. The fourth is **UL2 20B** (Tay et al., 2022), and the fifth is **Codex** (Chen et al., 2021, code-davinci-002 in the OpenAI API). We sample from the models via greedy decoding (though follow-up work shows chain-of-thought prompting can be improved by taking the majority final answer over many sampled generations (Wang et al., 2022a)). For LaMDA, we report averaged results over five random seeds, where each seed had a different randomly shuffled order of exemplars. As LaMDA experiments did not show large variance among different seeds, to save compute we report results for a single exemplar order for all other models."
        },
        {
            "text": "math word problems, we used this single set of eight chain of thought exemplars for all benchmarks\nexcept AQuA, which is multiple choice instead of free response. For AQuA, we used four exemplars\nand solutions from the training set, as given in Appendix Table 21.",
            "page": 4,
            "x": 105,
            "y": 435,
            "width": 401,
            "height": 35,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-experimental",
            "chunk_id": "b15ccd20-827f-416c-ac92-11fce712767c",
            "group_text": "3.1  Experimental Setup\n\nWe explore chain-of-thought prompting for various language models on multiple benchmarks.\n\n**Benchmarks.** We consider the following five math word problem benchmarks: **(1) the GSM8K** benchmark of math word problems (Cobbe et al., 2021), (2) the **SVAMP** dataset of math word problems with varying structures (Patel et al., 2021), (3) the **ASDiv** dataset of diverse math word problems (Miao et al., 2020), (4) the **AQuA** dataset of algebraic word problems, and (5) the **MAWPS** benchmark (Koncel-Kedziorski et al., 2016). Example problems are given in Appendix Table 12.\n\n**Standard prompting.** For the baseline, we consider standard few-shot prompting, popularized by Brown et al. (2020), in which a language model is given in-context exemplars of input\u2013output pairs before outputting a prediction for a test-time example. Exemplars are formatted as questions and answers. The model gives the answer directly, as shown in Figure 1 (left).\n\n**Chain-of-thought prompting.** Our proposed approach is to augment each exemplar in few-shot prompting with a chain of thought for an associated answer, as illustrated in Figure 1 (right). As most of the datasets only have an evaluation split, we manually composed a set of eight few-shot exemplars with chains of thought for prompting\u2014Figure 1 (right) shows one chain of thought exemplar, and the full set of exemplars is given in Appendix Table 20. (These particular exemplars did not undergo prompt engineering; robustness is studied in Section 3.4 and Appendix A.2.) To investigate whether chain-of-thought prompting in this form can successfully elicit successful reasoning across a range of\n\nFigure 3: Examples of \u27e8input, chain of thought, output\u27e9 triples for arithmetic, commonsense, and symbolic reasoning benchmarks. Chains of thought are highlighted. Full prompts in Appendix G.\n\nmath word problems, we used this single set of eight chain of thought exemplars for all benchmarks\nexcept AQuA, which is multiple choice instead of free response. For AQuA, we used four exemplars\nand solutions from the training set, as given in Appendix Table 21.\n\nLanguage models. We evaluate five large language models. The first is **GPT-3** (Brown et al., 2020), for which we use text-ada-001, text-babbage-001, text-curie-001, and text-davinci-002, which presumably correspond to InstructGPT models of 350M, 1.3B, 6.7B, and 175B parameters (Ouyang et al., 2022). The second is **LaMDA** (Thoppilan et al., 2022), which has models of 422M, 2B, 8B, 68B, and 137B parameters. The third is **PaLM**, which has models of 8B, 62B, and 540B parameters. The fourth is **UL2 20B** (Tay et al., 2022), and the fifth is **Codex** (Chen et al., 2021, code-davinci-002 in the OpenAI API). We sample from the models via greedy decoding (though follow-up work shows chain-of-thought prompting can be improved by taking the majority final answer over many sampled generations (Wang et al., 2022a)). For LaMDA, we report averaged results over five random seeds, where each seed had a different randomly shuffled order of exemplars. As LaMDA experiments did not show large variance among different seeds, to save compute we report results for a single exemplar order for all other models."
        },
        {
            "text": "Language models. We evaluate five large language models. The first is **GPT-3** (Brown et al., 2020), for which we use text-ada-001, text-babbage-001, text-curie-001, and text-davinci-002, which presumably correspond to InstructGPT models of 350M, 1.3B, 6.7B, and 175B parameters (Ouyang et al., 2022). The second is **LaMDA** (Thoppilan et al., 2022), which has models of 422M, 2B, 8B, 68B, and 137B parameters. The third is **PaLM**, which has models of 8B, 62B, and 540B parameters. The fourth is **UL2 20B** (Tay et al., 2022), and the fifth is **Codex** (Chen et al., 2021, code-davinci-002 in the OpenAI API). We sample from the models via greedy decoding (though follow-up work shows chain-of-thought prompting can be improved by taking the majority final answer over many sampled generations (Wang et al., 2022a)). For LaMDA, we report averaged results over five random seeds, where each seed had a different randomly shuffled order of exemplars. As LaMDA experiments did not show large variance among different seeds, to save compute we report results for a single exemplar order for all other models.",
            "page": 4,
            "x": 104,
            "y": 473,
            "width": 402,
            "height": 133,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-experimental",
            "chunk_id": "53a13f73-e9d1-4192-ae9b-8bc48a0ee215",
            "group_text": "3.1  Experimental Setup\n\nWe explore chain-of-thought prompting for various language models on multiple benchmarks.\n\n**Benchmarks.** We consider the following five math word problem benchmarks: **(1) the GSM8K** benchmark of math word problems (Cobbe et al., 2021), (2) the **SVAMP** dataset of math word problems with varying structures (Patel et al., 2021), (3) the **ASDiv** dataset of diverse math word problems (Miao et al., 2020), (4) the **AQuA** dataset of algebraic word problems, and (5) the **MAWPS** benchmark (Koncel-Kedziorski et al., 2016). Example problems are given in Appendix Table 12.\n\n**Standard prompting.** For the baseline, we consider standard few-shot prompting, popularized by Brown et al. (2020), in which a language model is given in-context exemplars of input\u2013output pairs before outputting a prediction for a test-time example. Exemplars are formatted as questions and answers. The model gives the answer directly, as shown in Figure 1 (left).\n\n**Chain-of-thought prompting.** Our proposed approach is to augment each exemplar in few-shot prompting with a chain of thought for an associated answer, as illustrated in Figure 1 (right). As most of the datasets only have an evaluation split, we manually composed a set of eight few-shot exemplars with chains of thought for prompting\u2014Figure 1 (right) shows one chain of thought exemplar, and the full set of exemplars is given in Appendix Table 20. (These particular exemplars did not undergo prompt engineering; robustness is studied in Section 3.4 and Appendix A.2.) To investigate whether chain-of-thought prompting in this form can successfully elicit successful reasoning across a range of\n\nFigure 3: Examples of \u27e8input, chain of thought, output\u27e9 triples for arithmetic, commonsense, and symbolic reasoning benchmarks. Chains of thought are highlighted. Full prompts in Appendix G.\n\nmath word problems, we used this single set of eight chain of thought exemplars for all benchmarks\nexcept AQuA, which is multiple choice instead of free response. For AQuA, we used four exemplars\nand solutions from the training set, as given in Appendix Table 21.\n\nLanguage models. We evaluate five large language models. The first is **GPT-3** (Brown et al., 2020), for which we use text-ada-001, text-babbage-001, text-curie-001, and text-davinci-002, which presumably correspond to InstructGPT models of 350M, 1.3B, 6.7B, and 175B parameters (Ouyang et al., 2022). The second is **LaMDA** (Thoppilan et al., 2022), which has models of 422M, 2B, 8B, 68B, and 137B parameters. The third is **PaLM**, which has models of 8B, 62B, and 540B parameters. The fourth is **UL2 20B** (Tay et al., 2022), and the fifth is **Codex** (Chen et al., 2021, code-davinci-002 in the OpenAI API). We sample from the models via greedy decoding (though follow-up work shows chain-of-thought prompting can be improved by taking the majority final answer over many sampled generations (Wang et al., 2022a)). For LaMDA, we report averaged results over five random seeds, where each seed had a different randomly shuffled order of exemplars. As LaMDA experiments did not show large variance among different seeds, to save compute we report results for a single exemplar order for all other models."
        },
        {
            "text": "3.2  Results\n\nThe strongest results of chain-of-thought prompting are summarized in Figure 4, with all experimental outputs for each model collection, model size, and benchmark shown in Table 2 in the Appendix. There are three key takeaways. First, Figure 4 shows that chain-of-thought prompting is an emergent ability of model scale (Wei et al., 2022b). That is, chain-of-thought prompting does not positively impact performance for small models, and only yields performance gains when used with models of ~100B parameters. We qualitatively found that models of smaller scale produced fluent but illogical chains of thought, leading to lower performance than standard prompting.",
            "page": 4,
            "x": 104,
            "y": 620,
            "width": 403,
            "height": 102,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "6-results",
            "chunk_id": "2c6869c0-9edf-4a6c-a196-94a8c3252162",
            "group_text": "3.2  Results\n\nThe strongest results of chain-of-thought prompting are summarized in Figure 4, with all experimental outputs for each model collection, model size, and benchmark shown in Table 2 in the Appendix. There are three key takeaways. First, Figure 4 shows that chain-of-thought prompting is an emergent ability of model scale (Wei et al., 2022b). That is, chain-of-thought prompting does not positively impact performance for small models, and only yields performance gains when used with models of ~100B parameters. We qualitatively found that models of smaller scale produced fluent but illogical chains of thought, leading to lower performance than standard prompting.\n\nSecond, chain-of-thought prompting has larger\nperformance gains for more-complicated prob-\nlems. For instance, for GSM8K (the dataset\nwith the lowest baseline performance), perfor-\nmance more than doubled for the largest GPT\nand PaLM models. On the other hand, for Sin-\ngleOp, the easiest subset of MAWPS which only\nrequires a single step to solve, performance im-\nprovements were either negative or very small\n(see Appendix Table 3).\n\nThird, chain-of-thought prompting via GPT-3\n175B and PaLM 540B compares favorably to\nprior state of the art, which typically finetunes a\ntask-specific model on a labeled training dataset.\nFigure 4 shows how PaLM 540B uses chain-of-\nthought prompting to achieve new state of the art\non GSM8K, SVAMP, and MAWPS (though note\nthat standard prompting already passed the prior\nbest for SVAMP). On the other two datasets,\nAQuA and ASDiv, PaLM with chain-of-thought\nprompting reaches within 2% of the state of the\nart (Appendix Table 2).\n\nTo better understand why chain-of-thought\nprompting works, we manually examined model-\ngenerated chains of thought by LaMDA 137B\nfor GSM8K. Of 50 random examples where the\nmodel returned the correct final answer, all of\nthe generated chains of thought were also logi-\ncally and mathematically correct except two\nthat coincidentally arrived at the correct answer\n(see Appendix D.1, and Table 8 for examples\nof correct model-generated chains of thought).\nWe also randomly examined 50 random sam-\nples for which the model gave the wrong answer.\nThe summary of this analysis is that 46% of the\n\nFigure 4:   Chain-of-thought prompting enables large language models to solve challenging math problems.  Notably, chain-of-thought reasoning is an emergent ability of increasing model scale. Prior best numbers are from Cobbe et al. (2021) for GSM8K, Jie et al. (2022) for SVAMP, and Lan et al. (2021) for MAWPS.\n\nminor mistakes (calculator error, symbol map-\nping error, or one reasoning step missing), and that the other 54% of the chains of thought had major\nerrors in semantic understanding or coherence (see Appendix D.2). To provide a small insight into\nwhy scaling improves chain-of-thought reasoning ability, we performed a similar analysis of errors\nmade by PaLM 62B and whether those errors were fixed by scaling to PaLM 540B. The summary\nis that scaling PaLM to 540B fixes a large portion of one-step missing and semantic understanding\nerrors in the 62B model (see Appendix A.1)."
        },
        {
            "text": "Second, chain-of-thought prompting has larger\nperformance gains for more-complicated prob-\nlems. For instance, for GSM8K (the dataset\nwith the lowest baseline performance), perfor-\nmance more than doubled for the largest GPT\nand PaLM models. On the other hand, for Sin-\ngleOp, the easiest subset of MAWPS which only\nrequires a single step to solve, performance im-\nprovements were either negative or very small\n(see Appendix Table 3).",
            "page": 5,
            "x": 104,
            "y": 71,
            "width": 195,
            "height": 112,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "6-results",
            "chunk_id": "ae53e079-d963-449b-a539-84e02fe6d65e",
            "group_text": "3.2  Results\n\nThe strongest results of chain-of-thought prompting are summarized in Figure 4, with all experimental outputs for each model collection, model size, and benchmark shown in Table 2 in the Appendix. There are three key takeaways. First, Figure 4 shows that chain-of-thought prompting is an emergent ability of model scale (Wei et al., 2022b). That is, chain-of-thought prompting does not positively impact performance for small models, and only yields performance gains when used with models of ~100B parameters. We qualitatively found that models of smaller scale produced fluent but illogical chains of thought, leading to lower performance than standard prompting.\n\nSecond, chain-of-thought prompting has larger\nperformance gains for more-complicated prob-\nlems. For instance, for GSM8K (the dataset\nwith the lowest baseline performance), perfor-\nmance more than doubled for the largest GPT\nand PaLM models. On the other hand, for Sin-\ngleOp, the easiest subset of MAWPS which only\nrequires a single step to solve, performance im-\nprovements were either negative or very small\n(see Appendix Table 3).\n\nThird, chain-of-thought prompting via GPT-3\n175B and PaLM 540B compares favorably to\nprior state of the art, which typically finetunes a\ntask-specific model on a labeled training dataset.\nFigure 4 shows how PaLM 540B uses chain-of-\nthought prompting to achieve new state of the art\non GSM8K, SVAMP, and MAWPS (though note\nthat standard prompting already passed the prior\nbest for SVAMP). On the other two datasets,\nAQuA and ASDiv, PaLM with chain-of-thought\nprompting reaches within 2% of the state of the\nart (Appendix Table 2).\n\nTo better understand why chain-of-thought\nprompting works, we manually examined model-\ngenerated chains of thought by LaMDA 137B\nfor GSM8K. Of 50 random examples where the\nmodel returned the correct final answer, all of\nthe generated chains of thought were also logi-\ncally and mathematically correct except two\nthat coincidentally arrived at the correct answer\n(see Appendix D.1, and Table 8 for examples\nof correct model-generated chains of thought).\nWe also randomly examined 50 random sam-\nples for which the model gave the wrong answer.\nThe summary of this analysis is that 46% of the\n\nFigure 4:   Chain-of-thought prompting enables large language models to solve challenging math problems.  Notably, chain-of-thought reasoning is an emergent ability of increasing model scale. Prior best numbers are from Cobbe et al. (2021) for GSM8K, Jie et al. (2022) for SVAMP, and Lan et al. (2021) for MAWPS.\n\nminor mistakes (calculator error, symbol map-\nping error, or one reasoning step missing), and that the other 54% of the chains of thought had major\nerrors in semantic understanding or coherence (see Appendix D.2). To provide a small insight into\nwhy scaling improves chain-of-thought reasoning ability, we performed a similar analysis of errors\nmade by PaLM 62B and whether those errors were fixed by scaling to PaLM 540B. The summary\nis that scaling PaLM to 540B fixes a large portion of one-step missing and semantic understanding\nerrors in the 62B model (see Appendix A.1)."
        },
        {
            "text": "Third, chain-of-thought prompting via GPT-3\n175B and PaLM 540B compares favorably to\nprior state of the art, which typically finetunes a\ntask-specific model on a labeled training dataset.\nFigure 4 shows how PaLM 540B uses chain-of-\nthought prompting to achieve new state of the art\non GSM8K, SVAMP, and MAWPS (though note\nthat standard prompting already passed the prior\nbest for SVAMP). On the other two datasets,\nAQuA and ASDiv, PaLM with chain-of-thought\nprompting reaches within 2% of the state of the\nart (Appendix Table 2).",
            "page": 5,
            "x": 105,
            "y": 185,
            "width": 195,
            "height": 134,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "6-results",
            "chunk_id": "b648501f-6bae-47f2-9bab-beb11b8ed534",
            "group_text": "3.2  Results\n\nThe strongest results of chain-of-thought prompting are summarized in Figure 4, with all experimental outputs for each model collection, model size, and benchmark shown in Table 2 in the Appendix. There are three key takeaways. First, Figure 4 shows that chain-of-thought prompting is an emergent ability of model scale (Wei et al., 2022b). That is, chain-of-thought prompting does not positively impact performance for small models, and only yields performance gains when used with models of ~100B parameters. We qualitatively found that models of smaller scale produced fluent but illogical chains of thought, leading to lower performance than standard prompting.\n\nSecond, chain-of-thought prompting has larger\nperformance gains for more-complicated prob-\nlems. For instance, for GSM8K (the dataset\nwith the lowest baseline performance), perfor-\nmance more than doubled for the largest GPT\nand PaLM models. On the other hand, for Sin-\ngleOp, the easiest subset of MAWPS which only\nrequires a single step to solve, performance im-\nprovements were either negative or very small\n(see Appendix Table 3).\n\nThird, chain-of-thought prompting via GPT-3\n175B and PaLM 540B compares favorably to\nprior state of the art, which typically finetunes a\ntask-specific model on a labeled training dataset.\nFigure 4 shows how PaLM 540B uses chain-of-\nthought prompting to achieve new state of the art\non GSM8K, SVAMP, and MAWPS (though note\nthat standard prompting already passed the prior\nbest for SVAMP). On the other two datasets,\nAQuA and ASDiv, PaLM with chain-of-thought\nprompting reaches within 2% of the state of the\nart (Appendix Table 2).\n\nTo better understand why chain-of-thought\nprompting works, we manually examined model-\ngenerated chains of thought by LaMDA 137B\nfor GSM8K. Of 50 random examples where the\nmodel returned the correct final answer, all of\nthe generated chains of thought were also logi-\ncally and mathematically correct except two\nthat coincidentally arrived at the correct answer\n(see Appendix D.1, and Table 8 for examples\nof correct model-generated chains of thought).\nWe also randomly examined 50 random sam-\nples for which the model gave the wrong answer.\nThe summary of this analysis is that 46% of the\n\nFigure 4:   Chain-of-thought prompting enables large language models to solve challenging math problems.  Notably, chain-of-thought reasoning is an emergent ability of increasing model scale. Prior best numbers are from Cobbe et al. (2021) for GSM8K, Jie et al. (2022) for SVAMP, and Lan et al. (2021) for MAWPS.\n\nminor mistakes (calculator error, symbol map-\nping error, or one reasoning step missing), and that the other 54% of the chains of thought had major\nerrors in semantic understanding or coherence (see Appendix D.2). To provide a small insight into\nwhy scaling improves chain-of-thought reasoning ability, we performed a similar analysis of errors\nmade by PaLM 62B and whether those errors were fixed by scaling to PaLM 540B. The summary\nis that scaling PaLM to 540B fixes a large portion of one-step missing and semantic understanding\nerrors in the 62B model (see Appendix A.1)."
        },
        {
            "text": "To better understand why chain-of-thought\nprompting works, we manually examined model-\ngenerated chains of thought by LaMDA 137B\nfor GSM8K. Of 50 random examples where the\nmodel returned the correct final answer, all of\nthe generated chains of thought were also logi-\ncally and mathematically correct except two\nthat coincidentally arrived at the correct answer\n(see Appendix D.1, and Table 8 for examples\nof correct model-generated chains of thought).\nWe also randomly examined 50 random sam-\nples for which the model gave the wrong answer.\nThe summary of this analysis is that 46% of the",
            "page": 5,
            "x": 104,
            "y": 323,
            "width": 198,
            "height": 141,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "6-results",
            "chunk_id": "03d127af-da40-4eb7-b09c-6d4ac60b8736",
            "group_text": "3.2  Results\n\nThe strongest results of chain-of-thought prompting are summarized in Figure 4, with all experimental outputs for each model collection, model size, and benchmark shown in Table 2 in the Appendix. There are three key takeaways. First, Figure 4 shows that chain-of-thought prompting is an emergent ability of model scale (Wei et al., 2022b). That is, chain-of-thought prompting does not positively impact performance for small models, and only yields performance gains when used with models of ~100B parameters. We qualitatively found that models of smaller scale produced fluent but illogical chains of thought, leading to lower performance than standard prompting.\n\nSecond, chain-of-thought prompting has larger\nperformance gains for more-complicated prob-\nlems. For instance, for GSM8K (the dataset\nwith the lowest baseline performance), perfor-\nmance more than doubled for the largest GPT\nand PaLM models. On the other hand, for Sin-\ngleOp, the easiest subset of MAWPS which only\nrequires a single step to solve, performance im-\nprovements were either negative or very small\n(see Appendix Table 3).\n\nThird, chain-of-thought prompting via GPT-3\n175B and PaLM 540B compares favorably to\nprior state of the art, which typically finetunes a\ntask-specific model on a labeled training dataset.\nFigure 4 shows how PaLM 540B uses chain-of-\nthought prompting to achieve new state of the art\non GSM8K, SVAMP, and MAWPS (though note\nthat standard prompting already passed the prior\nbest for SVAMP). On the other two datasets,\nAQuA and ASDiv, PaLM with chain-of-thought\nprompting reaches within 2% of the state of the\nart (Appendix Table 2).\n\nTo better understand why chain-of-thought\nprompting works, we manually examined model-\ngenerated chains of thought by LaMDA 137B\nfor GSM8K. Of 50 random examples where the\nmodel returned the correct final answer, all of\nthe generated chains of thought were also logi-\ncally and mathematically correct except two\nthat coincidentally arrived at the correct answer\n(see Appendix D.1, and Table 8 for examples\nof correct model-generated chains of thought).\nWe also randomly examined 50 random sam-\nples for which the model gave the wrong answer.\nThe summary of this analysis is that 46% of the\n\nFigure 4:   Chain-of-thought prompting enables large language models to solve challenging math problems.  Notably, chain-of-thought reasoning is an emergent ability of increasing model scale. Prior best numbers are from Cobbe et al. (2021) for GSM8K, Jie et al. (2022) for SVAMP, and Lan et al. (2021) for MAWPS.\n\nminor mistakes (calculator error, symbol map-\nping error, or one reasoning step missing), and that the other 54% of the chains of thought had major\nerrors in semantic understanding or coherence (see Appendix D.2). To provide a small insight into\nwhy scaling improves chain-of-thought reasoning ability, we performed a similar analysis of errors\nmade by PaLM 62B and whether those errors were fixed by scaling to PaLM 540B. The summary\nis that scaling PaLM to 540B fixes a large portion of one-step missing and semantic understanding\nerrors in the 62B model (see Appendix A.1)."
        },
        {
            "text": "Figure 4:   Chain-of-thought prompting enables large language models to solve challenging math problems.  Notably, chain-of-thought reasoning is an emergent ability of increasing model scale. Prior best numbers are from Cobbe et al. (2021) for GSM8K, Jie et al. (2022) for SVAMP, and Lan et al. (2021) for MAWPS.",
            "page": 5,
            "x": 303,
            "y": 395,
            "width": 205,
            "height": 80,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "6-results",
            "chunk_id": "f10d8d2c-ed9d-4168-bd42-d7305b6d2575",
            "group_text": "3.2  Results\n\nThe strongest results of chain-of-thought prompting are summarized in Figure 4, with all experimental outputs for each model collection, model size, and benchmark shown in Table 2 in the Appendix. There are three key takeaways. First, Figure 4 shows that chain-of-thought prompting is an emergent ability of model scale (Wei et al., 2022b). That is, chain-of-thought prompting does not positively impact performance for small models, and only yields performance gains when used with models of ~100B parameters. We qualitatively found that models of smaller scale produced fluent but illogical chains of thought, leading to lower performance than standard prompting.\n\nSecond, chain-of-thought prompting has larger\nperformance gains for more-complicated prob-\nlems. For instance, for GSM8K (the dataset\nwith the lowest baseline performance), perfor-\nmance more than doubled for the largest GPT\nand PaLM models. On the other hand, for Sin-\ngleOp, the easiest subset of MAWPS which only\nrequires a single step to solve, performance im-\nprovements were either negative or very small\n(see Appendix Table 3).\n\nThird, chain-of-thought prompting via GPT-3\n175B and PaLM 540B compares favorably to\nprior state of the art, which typically finetunes a\ntask-specific model on a labeled training dataset.\nFigure 4 shows how PaLM 540B uses chain-of-\nthought prompting to achieve new state of the art\non GSM8K, SVAMP, and MAWPS (though note\nthat standard prompting already passed the prior\nbest for SVAMP). On the other two datasets,\nAQuA and ASDiv, PaLM with chain-of-thought\nprompting reaches within 2% of the state of the\nart (Appendix Table 2).\n\nTo better understand why chain-of-thought\nprompting works, we manually examined model-\ngenerated chains of thought by LaMDA 137B\nfor GSM8K. Of 50 random examples where the\nmodel returned the correct final answer, all of\nthe generated chains of thought were also logi-\ncally and mathematically correct except two\nthat coincidentally arrived at the correct answer\n(see Appendix D.1, and Table 8 for examples\nof correct model-generated chains of thought).\nWe also randomly examined 50 random sam-\nples for which the model gave the wrong answer.\nThe summary of this analysis is that 46% of the\n\nFigure 4:   Chain-of-thought prompting enables large language models to solve challenging math problems.  Notably, chain-of-thought reasoning is an emergent ability of increasing model scale. Prior best numbers are from Cobbe et al. (2021) for GSM8K, Jie et al. (2022) for SVAMP, and Lan et al. (2021) for MAWPS.\n\nminor mistakes (calculator error, symbol map-\nping error, or one reasoning step missing), and that the other 54% of the chains of thought had major\nerrors in semantic understanding or coherence (see Appendix D.2). To provide a small insight into\nwhy scaling improves chain-of-thought reasoning ability, we performed a similar analysis of errors\nmade by PaLM 62B and whether those errors were fixed by scaling to PaLM 540B. The summary\nis that scaling PaLM to 540B fixes a large portion of one-step missing and semantic understanding\nerrors in the 62B model (see Appendix A.1)."
        },
        {
            "text": "minor mistakes (calculator error, symbol map-\nping error, or one reasoning step missing), and that the other 54% of the chains of thought had major\nerrors in semantic understanding or coherence (see Appendix D.2). To provide a small insight into\nwhy scaling improves chain-of-thought reasoning ability, we performed a similar analysis of errors\nmade by PaLM 62B and whether those errors were fixed by scaling to PaLM 540B. The summary\nis that scaling PaLM to 540B fixes a large portion of one-step missing and semantic understanding\nerrors in the 62B model (see Appendix A.1).",
            "page": 5,
            "x": 104,
            "y": 472,
            "width": 403,
            "height": 82,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "6-results",
            "chunk_id": "1806edd8-1985-4f8e-b99d-c8e9c1bc1474",
            "group_text": "3.2  Results\n\nThe strongest results of chain-of-thought prompting are summarized in Figure 4, with all experimental outputs for each model collection, model size, and benchmark shown in Table 2 in the Appendix. There are three key takeaways. First, Figure 4 shows that chain-of-thought prompting is an emergent ability of model scale (Wei et al., 2022b). That is, chain-of-thought prompting does not positively impact performance for small models, and only yields performance gains when used with models of ~100B parameters. We qualitatively found that models of smaller scale produced fluent but illogical chains of thought, leading to lower performance than standard prompting.\n\nSecond, chain-of-thought prompting has larger\nperformance gains for more-complicated prob-\nlems. For instance, for GSM8K (the dataset\nwith the lowest baseline performance), perfor-\nmance more than doubled for the largest GPT\nand PaLM models. On the other hand, for Sin-\ngleOp, the easiest subset of MAWPS which only\nrequires a single step to solve, performance im-\nprovements were either negative or very small\n(see Appendix Table 3).\n\nThird, chain-of-thought prompting via GPT-3\n175B and PaLM 540B compares favorably to\nprior state of the art, which typically finetunes a\ntask-specific model on a labeled training dataset.\nFigure 4 shows how PaLM 540B uses chain-of-\nthought prompting to achieve new state of the art\non GSM8K, SVAMP, and MAWPS (though note\nthat standard prompting already passed the prior\nbest for SVAMP). On the other two datasets,\nAQuA and ASDiv, PaLM with chain-of-thought\nprompting reaches within 2% of the state of the\nart (Appendix Table 2).\n\nTo better understand why chain-of-thought\nprompting works, we manually examined model-\ngenerated chains of thought by LaMDA 137B\nfor GSM8K. Of 50 random examples where the\nmodel returned the correct final answer, all of\nthe generated chains of thought were also logi-\ncally and mathematically correct except two\nthat coincidentally arrived at the correct answer\n(see Appendix D.1, and Table 8 for examples\nof correct model-generated chains of thought).\nWe also randomly examined 50 random sam-\nples for which the model gave the wrong answer.\nThe summary of this analysis is that 46% of the\n\nFigure 4:   Chain-of-thought prompting enables large language models to solve challenging math problems.  Notably, chain-of-thought reasoning is an emergent ability of increasing model scale. Prior best numbers are from Cobbe et al. (2021) for GSM8K, Jie et al. (2022) for SVAMP, and Lan et al. (2021) for MAWPS.\n\nminor mistakes (calculator error, symbol map-\nping error, or one reasoning step missing), and that the other 54% of the chains of thought had major\nerrors in semantic understanding or coherence (see Appendix D.2). To provide a small insight into\nwhy scaling improves chain-of-thought reasoning ability, we performed a similar analysis of errors\nmade by PaLM 62B and whether those errors were fixed by scaling to PaLM 540B. The summary\nis that scaling PaLM to 540B fixes a large portion of one-step missing and semantic understanding\nerrors in the 62B model (see Appendix A.1)."
        },
        {
            "text": "3.3  Ablation Study\n\nThe observed benefits of using chain-of-thought prompting raises the natural question of whether the same performance improvements can be conferred via other types of prompting. Figure 5 shows an ablation study with three variations of chain of thought described below.",
            "page": 5,
            "x": 104,
            "y": 569,
            "width": 403,
            "height": 59,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-ablation",
            "chunk_id": "003c176e-c2f5-4525-8d76-fade847a2f16",
            "group_text": "3.3  Ablation Study\n\nThe observed benefits of using chain-of-thought prompting raises the natural question of whether the same performance improvements can be conferred via other types of prompting. Figure 5 shows an ablation study with three variations of chain of thought described below.\n\nEquation only. One reason for why chain-of-thought prompting might help is that it produces the mathematical equation to be evaluated, and so we test a variation where the model is prompted to output only a mathematical equation before giving the answer. Figure 5 shows that equation only prompting does not help much for GSM8K, which implies that the semantics of the questions in GSM8K are too challenging to directly translate into an equation without the natural language reasoning steps in chain of thought. For datasets of one-step or two-step problems, however, we find that equation only prompting does improve performance, since the equation can be easily derived from the question (see Appendix Table 6).\n\n**Variable compute only.** Another intuition is that chain of thought allows the model to spend more computation (i.e., intermediate tokens) on harder problems. To isolate the effect of variable computation from chain-of-thought reasoning, we test a configuration where the model is prompted to output a only sequence of dots (. . .) equal to the number of characters in the equation needed to solve the problem. This variant performs about the same as the baseline, which suggests that variable computation by itself is not the reason for the success of chain-of-thought prompting, and that there appears to be utility from expressing intermediate steps via natural language.\n\nChain of thought after answer. Another potential benefit of chain-of-thought prompting could simply be that such prompts allow the model to better access relevant knowledge acquired during pretraining. Therefore, we test an alternative configuration where the chain of thought prompt is only given after the answer, isolating whether the model actually depends on the produced chain of thought to give the final answer. This variant performs about the same as the baseline, which suggests that the sequential reasoning embodied in the chain of thought is useful for reasons beyond just activating knowledge."
        },
        {
            "text": "Equation only. One reason for why chain-of-thought prompting might help is that it produces the mathematical equation to be evaluated, and so we test a variation where the model is prompted to output only a mathematical equation before giving the answer. Figure 5 shows that equation only prompting does not help much for GSM8K, which implies that the semantics of the questions in GSM8K are too challenging to directly translate into an equation without the natural language reasoning steps in chain of thought. For datasets of one-step or two-step problems, however, we find that equation only prompting does improve performance, since the equation can be easily derived from the question (see Appendix Table 6).",
            "page": 5,
            "x": 104,
            "y": 630,
            "width": 402,
            "height": 91,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-ablation",
            "chunk_id": "0fb45586-147b-4cd3-b659-4ebd5f0fe978",
            "group_text": "3.3  Ablation Study\n\nThe observed benefits of using chain-of-thought prompting raises the natural question of whether the same performance improvements can be conferred via other types of prompting. Figure 5 shows an ablation study with three variations of chain of thought described below.\n\nEquation only. One reason for why chain-of-thought prompting might help is that it produces the mathematical equation to be evaluated, and so we test a variation where the model is prompted to output only a mathematical equation before giving the answer. Figure 5 shows that equation only prompting does not help much for GSM8K, which implies that the semantics of the questions in GSM8K are too challenging to directly translate into an equation without the natural language reasoning steps in chain of thought. For datasets of one-step or two-step problems, however, we find that equation only prompting does improve performance, since the equation can be easily derived from the question (see Appendix Table 6).\n\n**Variable compute only.** Another intuition is that chain of thought allows the model to spend more computation (i.e., intermediate tokens) on harder problems. To isolate the effect of variable computation from chain-of-thought reasoning, we test a configuration where the model is prompted to output a only sequence of dots (. . .) equal to the number of characters in the equation needed to solve the problem. This variant performs about the same as the baseline, which suggests that variable computation by itself is not the reason for the success of chain-of-thought prompting, and that there appears to be utility from expressing intermediate steps via natural language.\n\nChain of thought after answer. Another potential benefit of chain-of-thought prompting could simply be that such prompts allow the model to better access relevant knowledge acquired during pretraining. Therefore, we test an alternative configuration where the chain of thought prompt is only given after the answer, isolating whether the model actually depends on the produced chain of thought to give the final answer. This variant performs about the same as the baseline, which suggests that the sequential reasoning embodied in the chain of thought is useful for reasons beyond just activating knowledge."
        },
        {
            "text": "**Variable compute only.** Another intuition is that chain of thought allows the model to spend more computation (i.e., intermediate tokens) on harder problems. To isolate the effect of variable computation from chain-of-thought reasoning, we test a configuration where the model is prompted to output a only sequence of dots (. . .) equal to the number of characters in the equation needed to solve the problem. This variant performs about the same as the baseline, which suggests that variable computation by itself is not the reason for the success of chain-of-thought prompting, and that there appears to be utility from expressing intermediate steps via natural language.",
            "page": 6,
            "x": 104,
            "y": 70,
            "width": 254,
            "height": 124,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-ablation",
            "chunk_id": "aac7dc24-072d-4805-a2ed-6cf95435ce52",
            "group_text": "3.3  Ablation Study\n\nThe observed benefits of using chain-of-thought prompting raises the natural question of whether the same performance improvements can be conferred via other types of prompting. Figure 5 shows an ablation study with three variations of chain of thought described below.\n\nEquation only. One reason for why chain-of-thought prompting might help is that it produces the mathematical equation to be evaluated, and so we test a variation where the model is prompted to output only a mathematical equation before giving the answer. Figure 5 shows that equation only prompting does not help much for GSM8K, which implies that the semantics of the questions in GSM8K are too challenging to directly translate into an equation without the natural language reasoning steps in chain of thought. For datasets of one-step or two-step problems, however, we find that equation only prompting does improve performance, since the equation can be easily derived from the question (see Appendix Table 6).\n\n**Variable compute only.** Another intuition is that chain of thought allows the model to spend more computation (i.e., intermediate tokens) on harder problems. To isolate the effect of variable computation from chain-of-thought reasoning, we test a configuration where the model is prompted to output a only sequence of dots (. . .) equal to the number of characters in the equation needed to solve the problem. This variant performs about the same as the baseline, which suggests that variable computation by itself is not the reason for the success of chain-of-thought prompting, and that there appears to be utility from expressing intermediate steps via natural language.\n\nChain of thought after answer. Another potential benefit of chain-of-thought prompting could simply be that such prompts allow the model to better access relevant knowledge acquired during pretraining. Therefore, we test an alternative configuration where the chain of thought prompt is only given after the answer, isolating whether the model actually depends on the produced chain of thought to give the final answer. This variant performs about the same as the baseline, which suggests that the sequential reasoning embodied in the chain of thought is useful for reasons beyond just activating knowledge."
        },
        {
            "text": "Chain of thought after answer. Another potential benefit of chain-of-thought prompting could simply be that such prompts allow the model to better access relevant knowledge acquired during pretraining. Therefore, we test an alternative configuration where the chain of thought prompt is only given after the answer, isolating whether the model actually depends on the produced chain of thought to give the final answer. This variant performs about the same as the baseline, which suggests that the sequential reasoning embodied in the chain of thought is useful for reasons beyond just activating knowledge.",
            "page": 6,
            "x": 104,
            "y": 197,
            "width": 254,
            "height": 113,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-ablation",
            "chunk_id": "86993a70-25fa-451f-a1a8-0d4a4f7bd98b",
            "group_text": "3.3  Ablation Study\n\nThe observed benefits of using chain-of-thought prompting raises the natural question of whether the same performance improvements can be conferred via other types of prompting. Figure 5 shows an ablation study with three variations of chain of thought described below.\n\nEquation only. One reason for why chain-of-thought prompting might help is that it produces the mathematical equation to be evaluated, and so we test a variation where the model is prompted to output only a mathematical equation before giving the answer. Figure 5 shows that equation only prompting does not help much for GSM8K, which implies that the semantics of the questions in GSM8K are too challenging to directly translate into an equation without the natural language reasoning steps in chain of thought. For datasets of one-step or two-step problems, however, we find that equation only prompting does improve performance, since the equation can be easily derived from the question (see Appendix Table 6).\n\n**Variable compute only.** Another intuition is that chain of thought allows the model to spend more computation (i.e., intermediate tokens) on harder problems. To isolate the effect of variable computation from chain-of-thought reasoning, we test a configuration where the model is prompted to output a only sequence of dots (. . .) equal to the number of characters in the equation needed to solve the problem. This variant performs about the same as the baseline, which suggests that variable computation by itself is not the reason for the success of chain-of-thought prompting, and that there appears to be utility from expressing intermediate steps via natural language.\n\nChain of thought after answer. Another potential benefit of chain-of-thought prompting could simply be that such prompts allow the model to better access relevant knowledge acquired during pretraining. Therefore, we test an alternative configuration where the chain of thought prompt is only given after the answer, isolating whether the model actually depends on the produced chain of thought to give the final answer. This variant performs about the same as the baseline, which suggests that the sequential reasoning embodied in the chain of thought is useful for reasons beyond just activating knowledge."
        },
        {
            "text": "3.4  Robustness of Chain of Thought\n\nSensitivity to exemplars is a key consideration of prompt-\ning approaches\u2014for instance, varying the permutation of\nfew-shot exemplars can cause the accuracy of GPT-3 on\nSST-2 to range from near chance (54.3%) to near state of\nthe art (93.4%) (Zhao et al., 2021). In this final subsec-\ntion, we evaluate robustness to chains of thought written\nby different annotators. In addition to the results above,\nwhich used chains of thought written by an Annotator\nA, two other co-authors of this paper (Annotators B and\nC) independently wrote chains of thought for the same\nfew-shot exemplars (shown in Appendix H). Annotator A\nalso wrote another chain of thought that was more concise\nthan the original, following the style of solutions given in\nCobbe et al. (2021).\u00b9",
            "page": 6,
            "x": 103,
            "y": 326,
            "width": 236,
            "height": 180,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "8-robustness",
            "chunk_id": "35246994-efbf-483b-a72c-ac7114a9ebe3",
            "group_text": "3.4  Robustness of Chain of Thought\n\nSensitivity to exemplars is a key consideration of prompt-\ning approaches\u2014for instance, varying the permutation of\nfew-shot exemplars can cause the accuracy of GPT-3 on\nSST-2 to range from near chance (54.3%) to near state of\nthe art (93.4%) (Zhao et al., 2021). In this final subsec-\ntion, we evaluate robustness to chains of thought written\nby different annotators. In addition to the results above,\nwhich used chains of thought written by an Annotator\nA, two other co-authors of this paper (Annotators B and\nC) independently wrote chains of thought for the same\nfew-shot exemplars (shown in Appendix H). Annotator A\nalso wrote another chain of thought that was more concise\nthan the original, following the style of solutions given in\nCobbe et al. (2021).\u00b9\n\nFigure 6 shows these results for LaMDA 137B on GSM8K\nand MAWPS (ablation results for other datasets are given\nin Appendix Table 6 / Table 7). Although there is variance\namong different chain of thought annotations, as would be\nexpected when using exemplar-based prompting (Le Scao\nand Rush, 2021; Reynolds and McDonell, 2021; Zhao\net al., 2021), all sets of chain of thought prompts outper-\nform the standard baseline by a large margin. This result\nimplies that successful use of chain of thought does not\ndepend on a particular linguistic style.\n\nTo confirm that successful chain-of-thought prompting\nworks for other sets of exemplars, we also run experiments\nwith three sets of eight exemplars randomly sampled from the GSM8K training set, an independent\n\n1For instance, whereas original chain of thought uses several short sentences (\u201c*There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29.*\u201d), the concise chain of thought would read \u201c*5 * 4 = 20 new computers were added. So there are 9 + 20 = 29 new computers in the server room now*\u201d.\n\nsource (examples in this dataset already included reasoning steps like a chain of thought).\u00b2 Figure 6 shows that these prompts performed comparably with our manually written exemplars, also substantially outperforming standard prompting.\n\nIn addition to robustness to annotators, independently-written chains of thought, different exemplars, and various language models, we also find that chain-of-thought prompting for arithmetic reasoning is robust to different exemplar orders and varying numbers of exemplars (see Appendix A.2)."
        },
        {
            "text": "Figure 6 shows these results for LaMDA 137B on GSM8K\nand MAWPS (ablation results for other datasets are given\nin Appendix Table 6 / Table 7). Although there is variance\namong different chain of thought annotations, as would be\nexpected when using exemplar-based prompting (Le Scao\nand Rush, 2021; Reynolds and McDonell, 2021; Zhao\net al., 2021), all sets of chain of thought prompts outper-\nform the standard baseline by a large margin. This result\nimplies that successful use of chain of thought does not\ndepend on a particular linguistic style.",
            "page": 6,
            "x": 104,
            "y": 509,
            "width": 234,
            "height": 113,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "8-robustness",
            "chunk_id": "efa72fdd-aeda-4316-b2d4-ac559f0187f4",
            "group_text": "3.4  Robustness of Chain of Thought\n\nSensitivity to exemplars is a key consideration of prompt-\ning approaches\u2014for instance, varying the permutation of\nfew-shot exemplars can cause the accuracy of GPT-3 on\nSST-2 to range from near chance (54.3%) to near state of\nthe art (93.4%) (Zhao et al., 2021). In this final subsec-\ntion, we evaluate robustness to chains of thought written\nby different annotators. In addition to the results above,\nwhich used chains of thought written by an Annotator\nA, two other co-authors of this paper (Annotators B and\nC) independently wrote chains of thought for the same\nfew-shot exemplars (shown in Appendix H). Annotator A\nalso wrote another chain of thought that was more concise\nthan the original, following the style of solutions given in\nCobbe et al. (2021).\u00b9\n\nFigure 6 shows these results for LaMDA 137B on GSM8K\nand MAWPS (ablation results for other datasets are given\nin Appendix Table 6 / Table 7). Although there is variance\namong different chain of thought annotations, as would be\nexpected when using exemplar-based prompting (Le Scao\nand Rush, 2021; Reynolds and McDonell, 2021; Zhao\net al., 2021), all sets of chain of thought prompts outper-\nform the standard baseline by a large margin. This result\nimplies that successful use of chain of thought does not\ndepend on a particular linguistic style.\n\nTo confirm that successful chain-of-thought prompting\nworks for other sets of exemplars, we also run experiments\nwith three sets of eight exemplars randomly sampled from the GSM8K training set, an independent\n\n1For instance, whereas original chain of thought uses several short sentences (\u201c*There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29.*\u201d), the concise chain of thought would read \u201c*5 * 4 = 20 new computers were added. So there are 9 + 20 = 29 new computers in the server room now*\u201d.\n\nsource (examples in this dataset already included reasoning steps like a chain of thought).\u00b2 Figure 6 shows that these prompts performed comparably with our manually written exemplars, also substantially outperforming standard prompting.\n\nIn addition to robustness to annotators, independently-written chains of thought, different exemplars, and various language models, we also find that chain-of-thought prompting for arithmetic reasoning is robust to different exemplar orders and varying numbers of exemplars (see Appendix A.2)."
        },
        {
            "text": "To confirm that successful chain-of-thought prompting\nworks for other sets of exemplars, we also run experiments\nwith three sets of eight exemplars randomly sampled from the GSM8K training set, an independent",
            "page": 6,
            "x": 104,
            "y": 624,
            "width": 403,
            "height": 37,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "8-robustness",
            "chunk_id": "b235af32-678f-4ee9-9a7c-829251ed15ec",
            "group_text": "3.4  Robustness of Chain of Thought\n\nSensitivity to exemplars is a key consideration of prompt-\ning approaches\u2014for instance, varying the permutation of\nfew-shot exemplars can cause the accuracy of GPT-3 on\nSST-2 to range from near chance (54.3%) to near state of\nthe art (93.4%) (Zhao et al., 2021). In this final subsec-\ntion, we evaluate robustness to chains of thought written\nby different annotators. In addition to the results above,\nwhich used chains of thought written by an Annotator\nA, two other co-authors of this paper (Annotators B and\nC) independently wrote chains of thought for the same\nfew-shot exemplars (shown in Appendix H). Annotator A\nalso wrote another chain of thought that was more concise\nthan the original, following the style of solutions given in\nCobbe et al. (2021).\u00b9\n\nFigure 6 shows these results for LaMDA 137B on GSM8K\nand MAWPS (ablation results for other datasets are given\nin Appendix Table 6 / Table 7). Although there is variance\namong different chain of thought annotations, as would be\nexpected when using exemplar-based prompting (Le Scao\nand Rush, 2021; Reynolds and McDonell, 2021; Zhao\net al., 2021), all sets of chain of thought prompts outper-\nform the standard baseline by a large margin. This result\nimplies that successful use of chain of thought does not\ndepend on a particular linguistic style.\n\nTo confirm that successful chain-of-thought prompting\nworks for other sets of exemplars, we also run experiments\nwith three sets of eight exemplars randomly sampled from the GSM8K training set, an independent\n\n1For instance, whereas original chain of thought uses several short sentences (\u201c*There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29.*\u201d), the concise chain of thought would read \u201c*5 * 4 = 20 new computers were added. So there are 9 + 20 = 29 new computers in the server room now*\u201d.\n\nsource (examples in this dataset already included reasoning steps like a chain of thought).\u00b2 Figure 6 shows that these prompts performed comparably with our manually written exemplars, also substantially outperforming standard prompting.\n\nIn addition to robustness to annotators, independently-written chains of thought, different exemplars, and various language models, we also find that chain-of-thought prompting for arithmetic reasoning is robust to different exemplar orders and varying numbers of exemplars (see Appendix A.2)."
        },
        {
            "text": "1For instance, whereas original chain of thought uses several short sentences (\u201c*There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29.*\u201d), the concise chain of thought would read \u201c*5 * 4 = 20 new computers were added. So there are 9 + 20 = 29 new computers in the server room now*\u201d.",
            "page": 6,
            "x": 103,
            "y": 679,
            "width": 405,
            "height": 45,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "8-robustness",
            "chunk_id": "26960377-88db-478e-9c64-f744e7da6e9b",
            "group_text": "3.4  Robustness of Chain of Thought\n\nSensitivity to exemplars is a key consideration of prompt-\ning approaches\u2014for instance, varying the permutation of\nfew-shot exemplars can cause the accuracy of GPT-3 on\nSST-2 to range from near chance (54.3%) to near state of\nthe art (93.4%) (Zhao et al., 2021). In this final subsec-\ntion, we evaluate robustness to chains of thought written\nby different annotators. In addition to the results above,\nwhich used chains of thought written by an Annotator\nA, two other co-authors of this paper (Annotators B and\nC) independently wrote chains of thought for the same\nfew-shot exemplars (shown in Appendix H). Annotator A\nalso wrote another chain of thought that was more concise\nthan the original, following the style of solutions given in\nCobbe et al. (2021).\u00b9\n\nFigure 6 shows these results for LaMDA 137B on GSM8K\nand MAWPS (ablation results for other datasets are given\nin Appendix Table 6 / Table 7). Although there is variance\namong different chain of thought annotations, as would be\nexpected when using exemplar-based prompting (Le Scao\nand Rush, 2021; Reynolds and McDonell, 2021; Zhao\net al., 2021), all sets of chain of thought prompts outper-\nform the standard baseline by a large margin. This result\nimplies that successful use of chain of thought does not\ndepend on a particular linguistic style.\n\nTo confirm that successful chain-of-thought prompting\nworks for other sets of exemplars, we also run experiments\nwith three sets of eight exemplars randomly sampled from the GSM8K training set, an independent\n\n1For instance, whereas original chain of thought uses several short sentences (\u201c*There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29.*\u201d), the concise chain of thought would read \u201c*5 * 4 = 20 new computers were added. So there are 9 + 20 = 29 new computers in the server room now*\u201d.\n\nsource (examples in this dataset already included reasoning steps like a chain of thought).\u00b2 Figure 6 shows that these prompts performed comparably with our manually written exemplars, also substantially outperforming standard prompting.\n\nIn addition to robustness to annotators, independently-written chains of thought, different exemplars, and various language models, we also find that chain-of-thought prompting for arithmetic reasoning is robust to different exemplar orders and varying numbers of exemplars (see Appendix A.2)."
        },
        {
            "text": "source (examples in this dataset already included reasoning steps like a chain of thought).\u00b2 Figure 6 shows that these prompts performed comparably with our manually written exemplars, also substantially outperforming standard prompting.",
            "page": 7,
            "x": 104,
            "y": 71,
            "width": 404,
            "height": 36,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "8-robustness",
            "chunk_id": "144ad35b-8172-4c25-b676-7cb28f43ca93",
            "group_text": "3.4  Robustness of Chain of Thought\n\nSensitivity to exemplars is a key consideration of prompt-\ning approaches\u2014for instance, varying the permutation of\nfew-shot exemplars can cause the accuracy of GPT-3 on\nSST-2 to range from near chance (54.3%) to near state of\nthe art (93.4%) (Zhao et al., 2021). In this final subsec-\ntion, we evaluate robustness to chains of thought written\nby different annotators. In addition to the results above,\nwhich used chains of thought written by an Annotator\nA, two other co-authors of this paper (Annotators B and\nC) independently wrote chains of thought for the same\nfew-shot exemplars (shown in Appendix H). Annotator A\nalso wrote another chain of thought that was more concise\nthan the original, following the style of solutions given in\nCobbe et al. (2021).\u00b9\n\nFigure 6 shows these results for LaMDA 137B on GSM8K\nand MAWPS (ablation results for other datasets are given\nin Appendix Table 6 / Table 7). Although there is variance\namong different chain of thought annotations, as would be\nexpected when using exemplar-based prompting (Le Scao\nand Rush, 2021; Reynolds and McDonell, 2021; Zhao\net al., 2021), all sets of chain of thought prompts outper-\nform the standard baseline by a large margin. This result\nimplies that successful use of chain of thought does not\ndepend on a particular linguistic style.\n\nTo confirm that successful chain-of-thought prompting\nworks for other sets of exemplars, we also run experiments\nwith three sets of eight exemplars randomly sampled from the GSM8K training set, an independent\n\n1For instance, whereas original chain of thought uses several short sentences (\u201c*There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29.*\u201d), the concise chain of thought would read \u201c*5 * 4 = 20 new computers were added. So there are 9 + 20 = 29 new computers in the server room now*\u201d.\n\nsource (examples in this dataset already included reasoning steps like a chain of thought).\u00b2 Figure 6 shows that these prompts performed comparably with our manually written exemplars, also substantially outperforming standard prompting.\n\nIn addition to robustness to annotators, independently-written chains of thought, different exemplars, and various language models, we also find that chain-of-thought prompting for arithmetic reasoning is robust to different exemplar orders and varying numbers of exemplars (see Appendix A.2)."
        },
        {
            "text": "In addition to robustness to annotators, independently-written chains of thought, different exemplars, and various language models, we also find that chain-of-thought prompting for arithmetic reasoning is robust to different exemplar orders and varying numbers of exemplars (see Appendix A.2).",
            "page": 7,
            "x": 104,
            "y": 110,
            "width": 404,
            "height": 37,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "8-robustness",
            "chunk_id": "11262653-01f4-4501-a256-e0dd9e5044b6",
            "group_text": "3.4  Robustness of Chain of Thought\n\nSensitivity to exemplars is a key consideration of prompt-\ning approaches\u2014for instance, varying the permutation of\nfew-shot exemplars can cause the accuracy of GPT-3 on\nSST-2 to range from near chance (54.3%) to near state of\nthe art (93.4%) (Zhao et al., 2021). In this final subsec-\ntion, we evaluate robustness to chains of thought written\nby different annotators. In addition to the results above,\nwhich used chains of thought written by an Annotator\nA, two other co-authors of this paper (Annotators B and\nC) independently wrote chains of thought for the same\nfew-shot exemplars (shown in Appendix H). Annotator A\nalso wrote another chain of thought that was more concise\nthan the original, following the style of solutions given in\nCobbe et al. (2021).\u00b9\n\nFigure 6 shows these results for LaMDA 137B on GSM8K\nand MAWPS (ablation results for other datasets are given\nin Appendix Table 6 / Table 7). Although there is variance\namong different chain of thought annotations, as would be\nexpected when using exemplar-based prompting (Le Scao\nand Rush, 2021; Reynolds and McDonell, 2021; Zhao\net al., 2021), all sets of chain of thought prompts outper-\nform the standard baseline by a large margin. This result\nimplies that successful use of chain of thought does not\ndepend on a particular linguistic style.\n\nTo confirm that successful chain-of-thought prompting\nworks for other sets of exemplars, we also run experiments\nwith three sets of eight exemplars randomly sampled from the GSM8K training set, an independent\n\n1For instance, whereas original chain of thought uses several short sentences (\u201c*There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29.*\u201d), the concise chain of thought would read \u201c*5 * 4 = 20 new computers were added. So there are 9 + 20 = 29 new computers in the server room now*\u201d.\n\nsource (examples in this dataset already included reasoning steps like a chain of thought).\u00b2 Figure 6 shows that these prompts performed comparably with our manually written exemplars, also substantially outperforming standard prompting.\n\nIn addition to robustness to annotators, independently-written chains of thought, different exemplars, and various language models, we also find that chain-of-thought prompting for arithmetic reasoning is robust to different exemplar orders and varying numbers of exemplars (see Appendix A.2)."
        },
        {
            "text": "## 4  Commonsense Reasoning\n\nAlthough chain of thought is particularly suitable for math word problems, the language-based nature of chain of thought actually makes it applicable to a broad class of commonsense reasoning problems, which involve reasoning about physical and human interactions under the presumption of general background knowledge. Commonsense reasoning is key for interacting with the world and is still beyond the reach of current natural language understanding systems (Talmor et al., 2021).",
            "page": 7,
            "x": 103,
            "y": 159,
            "width": 404,
            "height": 83,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-commonsense",
            "chunk_id": "eb664152-16ed-47d7-bd5e-47d93528b14f",
            "group_text": "## 4  Commonsense Reasoning\n\nAlthough chain of thought is particularly suitable for math word problems, the language-based nature of chain of thought actually makes it applicable to a broad class of commonsense reasoning problems, which involve reasoning about physical and human interactions under the presumption of general background knowledge. Commonsense reasoning is key for interacting with the world and is still beyond the reach of current natural language understanding systems (Talmor et al., 2021).\n\n**Benchmarks.** We consider five datasets covering a diverse range of commonsense reasoning types. The popular **CSQA** (Talmor et al., 2019) asks commonsense questions about the world involving complex semantics that often require prior knowledge. **StrategyQA** (Geva et al., 2021) requires models to infer a multi-hop strategy to answer questions. We choose two specialized evaluation sets from the BIG-bench effort (BIG-bench collaboration, 2021): **Date** Understanding, which involves inferring a date from a given context, and **Sports** Understanding, which involves determining whether a sentence relating to sports is plausible or implausible. Finally, the **SayCan** dataset (Ahn et al., 2022) involves mapping a natural language instruction to a sequence of robot actions from a discrete set. Figure 3 shows examples with chain of thought annotations for all datasets.\n\n**Prompts.** We follow the same experimental setup as the prior section. For CSQA and StrategyQA, we randomly selected examples from the training set and manually composed chains of thought for them to use as few-shot exemplars. The two BIG-bench tasks do not have training sets, so we selected the first ten examples as exemplars in the evaluation set as few-shot exemplars and report numbers on the rest of the evaluation set. For SayCan, we use six examples from the training set used in Ahn et al. (2022) and also manually composed chains of thought.\n\n**Results.** Figure 7 highlights these results for PaLM (full results for LaMDA, GPT-3, and different model scales are shown in Table 4). For all tasks, scaling up model size improved the performance of standard prompting; chain-of-thought prompting led to further gains, with improvements appearing to be largest for PaLM 540B. With chain-of-thought prompting, PaLM 540B achieved strong performance relative to baselines, outperforming the prior state of the art on StrategyQA (75.6% vs 69.4%) and outperforming an unaided sports enthusiast on sports understanding (95.4% vs 84%). These results demonstrate that chain-of-thought prompting can also improve performance on tasks requiring a range of commonsense reasoning abilities (though note that gain was minimal on CSQA)."
        },
        {
            "text": "**Benchmarks.** We consider five datasets covering a diverse range of commonsense reasoning types. The popular **CSQA** (Talmor et al., 2019) asks commonsense questions about the world involving complex semantics that often require prior knowledge. **StrategyQA** (Geva et al., 2021) requires models to infer a multi-hop strategy to answer questions. We choose two specialized evaluation sets from the BIG-bench effort (BIG-bench collaboration, 2021): **Date** Understanding, which involves inferring a date from a given context, and **Sports** Understanding, which involves determining whether a sentence relating to sports is plausible or implausible. Finally, the **SayCan** dataset (Ahn et al., 2022) involves mapping a natural language instruction to a sequence of robot actions from a discrete set. Figure 3 shows examples with chain of thought annotations for all datasets.",
            "page": 7,
            "x": 104,
            "y": 245,
            "width": 403,
            "height": 101,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-commonsense",
            "chunk_id": "56ee2fbb-45be-4767-90f3-1dcc54bff685",
            "group_text": "## 4  Commonsense Reasoning\n\nAlthough chain of thought is particularly suitable for math word problems, the language-based nature of chain of thought actually makes it applicable to a broad class of commonsense reasoning problems, which involve reasoning about physical and human interactions under the presumption of general background knowledge. Commonsense reasoning is key for interacting with the world and is still beyond the reach of current natural language understanding systems (Talmor et al., 2021).\n\n**Benchmarks.** We consider five datasets covering a diverse range of commonsense reasoning types. The popular **CSQA** (Talmor et al., 2019) asks commonsense questions about the world involving complex semantics that often require prior knowledge. **StrategyQA** (Geva et al., 2021) requires models to infer a multi-hop strategy to answer questions. We choose two specialized evaluation sets from the BIG-bench effort (BIG-bench collaboration, 2021): **Date** Understanding, which involves inferring a date from a given context, and **Sports** Understanding, which involves determining whether a sentence relating to sports is plausible or implausible. Finally, the **SayCan** dataset (Ahn et al., 2022) involves mapping a natural language instruction to a sequence of robot actions from a discrete set. Figure 3 shows examples with chain of thought annotations for all datasets.\n\n**Prompts.** We follow the same experimental setup as the prior section. For CSQA and StrategyQA, we randomly selected examples from the training set and manually composed chains of thought for them to use as few-shot exemplars. The two BIG-bench tasks do not have training sets, so we selected the first ten examples as exemplars in the evaluation set as few-shot exemplars and report numbers on the rest of the evaluation set. For SayCan, we use six examples from the training set used in Ahn et al. (2022) and also manually composed chains of thought.\n\n**Results.** Figure 7 highlights these results for PaLM (full results for LaMDA, GPT-3, and different model scales are shown in Table 4). For all tasks, scaling up model size improved the performance of standard prompting; chain-of-thought prompting led to further gains, with improvements appearing to be largest for PaLM 540B. With chain-of-thought prompting, PaLM 540B achieved strong performance relative to baselines, outperforming the prior state of the art on StrategyQA (75.6% vs 69.4%) and outperforming an unaided sports enthusiast on sports understanding (95.4% vs 84%). These results demonstrate that chain-of-thought prompting can also improve performance on tasks requiring a range of commonsense reasoning abilities (though note that gain was minimal on CSQA)."
        },
        {
            "text": "**Prompts.** We follow the same experimental setup as the prior section. For CSQA and StrategyQA, we randomly selected examples from the training set and manually composed chains of thought for them to use as few-shot exemplars. The two BIG-bench tasks do not have training sets, so we selected the first ten examples as exemplars in the evaluation set as few-shot exemplars and report numbers on the rest of the evaluation set. For SayCan, we use six examples from the training set used in Ahn et al. (2022) and also manually composed chains of thought.",
            "page": 7,
            "x": 104,
            "y": 348,
            "width": 403,
            "height": 68,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-commonsense",
            "chunk_id": "f5d41750-9e80-4934-9e8c-0f80db94fdc8",
            "group_text": "## 4  Commonsense Reasoning\n\nAlthough chain of thought is particularly suitable for math word problems, the language-based nature of chain of thought actually makes it applicable to a broad class of commonsense reasoning problems, which involve reasoning about physical and human interactions under the presumption of general background knowledge. Commonsense reasoning is key for interacting with the world and is still beyond the reach of current natural language understanding systems (Talmor et al., 2021).\n\n**Benchmarks.** We consider five datasets covering a diverse range of commonsense reasoning types. The popular **CSQA** (Talmor et al., 2019) asks commonsense questions about the world involving complex semantics that often require prior knowledge. **StrategyQA** (Geva et al., 2021) requires models to infer a multi-hop strategy to answer questions. We choose two specialized evaluation sets from the BIG-bench effort (BIG-bench collaboration, 2021): **Date** Understanding, which involves inferring a date from a given context, and **Sports** Understanding, which involves determining whether a sentence relating to sports is plausible or implausible. Finally, the **SayCan** dataset (Ahn et al., 2022) involves mapping a natural language instruction to a sequence of robot actions from a discrete set. Figure 3 shows examples with chain of thought annotations for all datasets.\n\n**Prompts.** We follow the same experimental setup as the prior section. For CSQA and StrategyQA, we randomly selected examples from the training set and manually composed chains of thought for them to use as few-shot exemplars. The two BIG-bench tasks do not have training sets, so we selected the first ten examples as exemplars in the evaluation set as few-shot exemplars and report numbers on the rest of the evaluation set. For SayCan, we use six examples from the training set used in Ahn et al. (2022) and also manually composed chains of thought.\n\n**Results.** Figure 7 highlights these results for PaLM (full results for LaMDA, GPT-3, and different model scales are shown in Table 4). For all tasks, scaling up model size improved the performance of standard prompting; chain-of-thought prompting led to further gains, with improvements appearing to be largest for PaLM 540B. With chain-of-thought prompting, PaLM 540B achieved strong performance relative to baselines, outperforming the prior state of the art on StrategyQA (75.6% vs 69.4%) and outperforming an unaided sports enthusiast on sports understanding (95.4% vs 84%). These results demonstrate that chain-of-thought prompting can also improve performance on tasks requiring a range of commonsense reasoning abilities (though note that gain was minimal on CSQA)."
        },
        {
            "text": "**Results.** Figure 7 highlights these results for PaLM (full results for LaMDA, GPT-3, and different model scales are shown in Table 4). For all tasks, scaling up model size improved the performance of standard prompting; chain-of-thought prompting led to further gains, with improvements appearing to be largest for PaLM 540B. With chain-of-thought prompting, PaLM 540B achieved strong performance relative to baselines, outperforming the prior state of the art on StrategyQA (75.6% vs 69.4%) and outperforming an unaided sports enthusiast on sports understanding (95.4% vs 84%). These results demonstrate that chain-of-thought prompting can also improve performance on tasks requiring a range of commonsense reasoning abilities (though note that gain was minimal on CSQA).",
            "page": 7,
            "x": 104,
            "y": 419,
            "width": 403,
            "height": 92,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-commonsense",
            "chunk_id": "fef96ac5-fc23-42df-b905-d573bf8a6e3b",
            "group_text": "## 4  Commonsense Reasoning\n\nAlthough chain of thought is particularly suitable for math word problems, the language-based nature of chain of thought actually makes it applicable to a broad class of commonsense reasoning problems, which involve reasoning about physical and human interactions under the presumption of general background knowledge. Commonsense reasoning is key for interacting with the world and is still beyond the reach of current natural language understanding systems (Talmor et al., 2021).\n\n**Benchmarks.** We consider five datasets covering a diverse range of commonsense reasoning types. The popular **CSQA** (Talmor et al., 2019) asks commonsense questions about the world involving complex semantics that often require prior knowledge. **StrategyQA** (Geva et al., 2021) requires models to infer a multi-hop strategy to answer questions. We choose two specialized evaluation sets from the BIG-bench effort (BIG-bench collaboration, 2021): **Date** Understanding, which involves inferring a date from a given context, and **Sports** Understanding, which involves determining whether a sentence relating to sports is plausible or implausible. Finally, the **SayCan** dataset (Ahn et al., 2022) involves mapping a natural language instruction to a sequence of robot actions from a discrete set. Figure 3 shows examples with chain of thought annotations for all datasets.\n\n**Prompts.** We follow the same experimental setup as the prior section. For CSQA and StrategyQA, we randomly selected examples from the training set and manually composed chains of thought for them to use as few-shot exemplars. The two BIG-bench tasks do not have training sets, so we selected the first ten examples as exemplars in the evaluation set as few-shot exemplars and report numbers on the rest of the evaluation set. For SayCan, we use six examples from the training set used in Ahn et al. (2022) and also manually composed chains of thought.\n\n**Results.** Figure 7 highlights these results for PaLM (full results for LaMDA, GPT-3, and different model scales are shown in Table 4). For all tasks, scaling up model size improved the performance of standard prompting; chain-of-thought prompting led to further gains, with improvements appearing to be largest for PaLM 540B. With chain-of-thought prompting, PaLM 540B achieved strong performance relative to baselines, outperforming the prior state of the art on StrategyQA (75.6% vs 69.4%) and outperforming an unaided sports enthusiast on sports understanding (95.4% vs 84%). These results demonstrate that chain-of-thought prompting can also improve performance on tasks requiring a range of commonsense reasoning abilities (though note that gain was minimal on CSQA)."
        },
        {
            "text": "## 5  Symbolic Reasoning\n\nOur final experimental evaluation considers symbolic reasoning, which is simple for humans but potentially challenging for language models.  We show that chain-of-thought prompting not only enables language models to perform symbolic reasoning tasks that are challenging in the standard prompting setting, but also facilitates length generalization to inference-time inputs longer than those seen in the few-shot exemplars.",
            "page": 8,
            "x": 103,
            "y": 68,
            "width": 236,
            "height": 117,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "10-symbolic",
            "chunk_id": "af74d2f3-a8ee-43b1-8394-b79f60007ec6",
            "group_text": "## 5  Symbolic Reasoning\n\nOur final experimental evaluation considers symbolic reasoning, which is simple for humans but potentially challenging for language models.  We show that chain-of-thought prompting not only enables language models to perform symbolic reasoning tasks that are challenging in the standard prompting setting, but also facilitates length generalization to inference-time inputs longer than those seen in the few-shot exemplars.\n\nTasks.   We use the following two toy tasks.\n\n\u2022 Last letter concatenation.  This task asks the model to concatenate the last letters of words in a name (e.g., \u201cAmy Brown\u201d \u2192 \u201cyn\u201d). It is a more challenging version of first letter concatenation, which language models can already perform without chain of thought.\u00b3 We generate full names by randomly concatenating names from the top one-thousand first and last names from name census data (https://namecensus.com/).\n\n- \u2022 Coin flip. This task asks the model to answer whether a coin is still heads up after people either flip or don\u2019t flip the coin (e.g., \u201cA coin is heads up. Phoebe flips the coin. Osvaldo does not flip the coin. Is the coin still heads up?\u201d \u2192 \u201cno\u201d).\n\nAs the construction of these symbolic reasoning tasks is\nwell-defined, for each task we consider an *in-domain* test\nset for which examples had the same number of steps as\nthe training/few-shot exemplars, as well as an *out-of-domain* (OOD) test set, for which evaluation\nexamples had more steps than those in the exemplars. For last letter concatenation, the model only\nsees exemplars of names with two words, and then performs last letter concatenation on names with 3\nand 4 words.\\textsuperscript{4} We do the same for the number of potential flips in the coin flip task. Our experimental\nsetup uses the same methods and models as in the prior two sections. We again manually compose\nchains of thought for the few-shot exemplars for each task, which are given in Figure 3.\n\nResults.   The results of these in-domain and OOD evaluations are shown in Figure 8 for PaLM, with results for LaMDA shown in Appendix Table 5. With PaLM 540B, chain-of-thought prompting leads to almost 100% solve rates (note that standard prompting already solves coin flip with PaLM 540, though not for LaMDA 137B). Note that these in-domain evaluations are \u201ctoy tasks\u201d in the sense that perfect solution structures are already provided by the chains of thought in the few-shot exemplars; all the model has to do is repeat the same steps with the new symbols in the test-time example. And yet, small models still fail\u2014the ability to perform abstract manipulations on unseen symbols for these three tasks only arises at the scale of 100B model parameters.\n\nAs for the OOD evaluations, standard prompting fails for both tasks. With chain-of-thought prompting,\nlanguage models achieve upward scaling curves (though performance is lower than in the in-domain\nsetting). Hence, chain-of-thought prompting facilitates length generalization beyond seen chains of\nthought for language models of sufficient scale."
        },
        {
            "text": "Tasks.   We use the following two toy tasks.\n\n\u2022 Last letter concatenation.  This task asks the model to concatenate the last letters of words in a name (e.g., \u201cAmy Brown\u201d \u2192 \u201cyn\u201d). It is a more challenging version of first letter concatenation, which language models can already perform without chain of thought.\u00b3 We generate full names by randomly concatenating names from the top one-thousand first and last names from name census data (https://namecensus.com/).",
            "page": 8,
            "x": 104,
            "y": 193,
            "width": 234,
            "height": 107,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "10-symbolic",
            "chunk_id": "087253b4-d5df-41c9-8ea6-18d2893e4756",
            "group_text": "## 5  Symbolic Reasoning\n\nOur final experimental evaluation considers symbolic reasoning, which is simple for humans but potentially challenging for language models.  We show that chain-of-thought prompting not only enables language models to perform symbolic reasoning tasks that are challenging in the standard prompting setting, but also facilitates length generalization to inference-time inputs longer than those seen in the few-shot exemplars.\n\nTasks.   We use the following two toy tasks.\n\n\u2022 Last letter concatenation.  This task asks the model to concatenate the last letters of words in a name (e.g., \u201cAmy Brown\u201d \u2192 \u201cyn\u201d). It is a more challenging version of first letter concatenation, which language models can already perform without chain of thought.\u00b3 We generate full names by randomly concatenating names from the top one-thousand first and last names from name census data (https://namecensus.com/).\n\n- \u2022 Coin flip. This task asks the model to answer whether a coin is still heads up after people either flip or don\u2019t flip the coin (e.g., \u201cA coin is heads up. Phoebe flips the coin. Osvaldo does not flip the coin. Is the coin still heads up?\u201d \u2192 \u201cno\u201d).\n\nAs the construction of these symbolic reasoning tasks is\nwell-defined, for each task we consider an *in-domain* test\nset for which examples had the same number of steps as\nthe training/few-shot exemplars, as well as an *out-of-domain* (OOD) test set, for which evaluation\nexamples had more steps than those in the exemplars. For last letter concatenation, the model only\nsees exemplars of names with two words, and then performs last letter concatenation on names with 3\nand 4 words.\\textsuperscript{4} We do the same for the number of potential flips in the coin flip task. Our experimental\nsetup uses the same methods and models as in the prior two sections. We again manually compose\nchains of thought for the few-shot exemplars for each task, which are given in Figure 3.\n\nResults.   The results of these in-domain and OOD evaluations are shown in Figure 8 for PaLM, with results for LaMDA shown in Appendix Table 5. With PaLM 540B, chain-of-thought prompting leads to almost 100% solve rates (note that standard prompting already solves coin flip with PaLM 540, though not for LaMDA 137B). Note that these in-domain evaluations are \u201ctoy tasks\u201d in the sense that perfect solution structures are already provided by the chains of thought in the few-shot exemplars; all the model has to do is repeat the same steps with the new symbols in the test-time example. And yet, small models still fail\u2014the ability to perform abstract manipulations on unseen symbols for these three tasks only arises at the scale of 100B model parameters.\n\nAs for the OOD evaluations, standard prompting fails for both tasks. With chain-of-thought prompting,\nlanguage models achieve upward scaling curves (though performance is lower than in the in-domain\nsetting). Hence, chain-of-thought prompting facilitates length generalization beyond seen chains of\nthought for language models of sufficient scale."
        },
        {
            "text": "- \u2022 Coin flip. This task asks the model to answer whether a coin is still heads up after people either flip or don\u2019t flip the coin (e.g., \u201cA coin is heads up. Phoebe flips the coin. Osvaldo does not flip the coin. Is the coin still heads up?\u201d \u2192 \u201cno\u201d).",
            "page": 8,
            "x": 105,
            "y": 301,
            "width": 235,
            "height": 56,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "10-symbolic",
            "chunk_id": "b40548b5-3034-4da8-9d4a-f9aad500c7f7",
            "group_text": "## 5  Symbolic Reasoning\n\nOur final experimental evaluation considers symbolic reasoning, which is simple for humans but potentially challenging for language models.  We show that chain-of-thought prompting not only enables language models to perform symbolic reasoning tasks that are challenging in the standard prompting setting, but also facilitates length generalization to inference-time inputs longer than those seen in the few-shot exemplars.\n\nTasks.   We use the following two toy tasks.\n\n\u2022 Last letter concatenation.  This task asks the model to concatenate the last letters of words in a name (e.g., \u201cAmy Brown\u201d \u2192 \u201cyn\u201d). It is a more challenging version of first letter concatenation, which language models can already perform without chain of thought.\u00b3 We generate full names by randomly concatenating names from the top one-thousand first and last names from name census data (https://namecensus.com/).\n\n- \u2022 Coin flip. This task asks the model to answer whether a coin is still heads up after people either flip or don\u2019t flip the coin (e.g., \u201cA coin is heads up. Phoebe flips the coin. Osvaldo does not flip the coin. Is the coin still heads up?\u201d \u2192 \u201cno\u201d).\n\nAs the construction of these symbolic reasoning tasks is\nwell-defined, for each task we consider an *in-domain* test\nset for which examples had the same number of steps as\nthe training/few-shot exemplars, as well as an *out-of-domain* (OOD) test set, for which evaluation\nexamples had more steps than those in the exemplars. For last letter concatenation, the model only\nsees exemplars of names with two words, and then performs last letter concatenation on names with 3\nand 4 words.\\textsuperscript{4} We do the same for the number of potential flips in the coin flip task. Our experimental\nsetup uses the same methods and models as in the prior two sections. We again manually compose\nchains of thought for the few-shot exemplars for each task, which are given in Figure 3.\n\nResults.   The results of these in-domain and OOD evaluations are shown in Figure 8 for PaLM, with results for LaMDA shown in Appendix Table 5. With PaLM 540B, chain-of-thought prompting leads to almost 100% solve rates (note that standard prompting already solves coin flip with PaLM 540, though not for LaMDA 137B). Note that these in-domain evaluations are \u201ctoy tasks\u201d in the sense that perfect solution structures are already provided by the chains of thought in the few-shot exemplars; all the model has to do is repeat the same steps with the new symbols in the test-time example. And yet, small models still fail\u2014the ability to perform abstract manipulations on unseen symbols for these three tasks only arises at the scale of 100B model parameters.\n\nAs for the OOD evaluations, standard prompting fails for both tasks. With chain-of-thought prompting,\nlanguage models achieve upward scaling curves (though performance is lower than in the in-domain\nsetting). Hence, chain-of-thought prompting facilitates length generalization beyond seen chains of\nthought for language models of sufficient scale."
        },
        {
            "text": "As the construction of these symbolic reasoning tasks is\nwell-defined, for each task we consider an *in-domain* test\nset for which examples had the same number of steps as\nthe training/few-shot exemplars, as well as an *out-of-domain* (OOD) test set, for which evaluation\nexamples had more steps than those in the exemplars. For last letter concatenation, the model only\nsees exemplars of names with two words, and then performs last letter concatenation on names with 3\nand 4 words.\\textsuperscript{4} We do the same for the number of potential flips in the coin flip task. Our experimental\nsetup uses the same methods and models as in the prior two sections. We again manually compose\nchains of thought for the few-shot exemplars for each task, which are given in Figure 3.",
            "page": 8,
            "x": 104,
            "y": 361,
            "width": 403,
            "height": 101,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "10-symbolic",
            "chunk_id": "8b0f8a19-9536-4dd0-b7b5-af02b3109ee5",
            "group_text": "## 5  Symbolic Reasoning\n\nOur final experimental evaluation considers symbolic reasoning, which is simple for humans but potentially challenging for language models.  We show that chain-of-thought prompting not only enables language models to perform symbolic reasoning tasks that are challenging in the standard prompting setting, but also facilitates length generalization to inference-time inputs longer than those seen in the few-shot exemplars.\n\nTasks.   We use the following two toy tasks.\n\n\u2022 Last letter concatenation.  This task asks the model to concatenate the last letters of words in a name (e.g., \u201cAmy Brown\u201d \u2192 \u201cyn\u201d). It is a more challenging version of first letter concatenation, which language models can already perform without chain of thought.\u00b3 We generate full names by randomly concatenating names from the top one-thousand first and last names from name census data (https://namecensus.com/).\n\n- \u2022 Coin flip. This task asks the model to answer whether a coin is still heads up after people either flip or don\u2019t flip the coin (e.g., \u201cA coin is heads up. Phoebe flips the coin. Osvaldo does not flip the coin. Is the coin still heads up?\u201d \u2192 \u201cno\u201d).\n\nAs the construction of these symbolic reasoning tasks is\nwell-defined, for each task we consider an *in-domain* test\nset for which examples had the same number of steps as\nthe training/few-shot exemplars, as well as an *out-of-domain* (OOD) test set, for which evaluation\nexamples had more steps than those in the exemplars. For last letter concatenation, the model only\nsees exemplars of names with two words, and then performs last letter concatenation on names with 3\nand 4 words.\\textsuperscript{4} We do the same for the number of potential flips in the coin flip task. Our experimental\nsetup uses the same methods and models as in the prior two sections. We again manually compose\nchains of thought for the few-shot exemplars for each task, which are given in Figure 3.\n\nResults.   The results of these in-domain and OOD evaluations are shown in Figure 8 for PaLM, with results for LaMDA shown in Appendix Table 5. With PaLM 540B, chain-of-thought prompting leads to almost 100% solve rates (note that standard prompting already solves coin flip with PaLM 540, though not for LaMDA 137B). Note that these in-domain evaluations are \u201ctoy tasks\u201d in the sense that perfect solution structures are already provided by the chains of thought in the few-shot exemplars; all the model has to do is repeat the same steps with the new symbols in the test-time example. And yet, small models still fail\u2014the ability to perform abstract manipulations on unseen symbols for these three tasks only arises at the scale of 100B model parameters.\n\nAs for the OOD evaluations, standard prompting fails for both tasks. With chain-of-thought prompting,\nlanguage models achieve upward scaling curves (though performance is lower than in the in-domain\nsetting). Hence, chain-of-thought prompting facilitates length generalization beyond seen chains of\nthought for language models of sufficient scale."
        },
        {
            "text": "Results.   The results of these in-domain and OOD evaluations are shown in Figure 8 for PaLM, with results for LaMDA shown in Appendix Table 5. With PaLM 540B, chain-of-thought prompting leads to almost 100% solve rates (note that standard prompting already solves coin flip with PaLM 540, though not for LaMDA 137B). Note that these in-domain evaluations are \u201ctoy tasks\u201d in the sense that perfect solution structures are already provided by the chains of thought in the few-shot exemplars; all the model has to do is repeat the same steps with the new symbols in the test-time example. And yet, small models still fail\u2014the ability to perform abstract manipulations on unseen symbols for these three tasks only arises at the scale of 100B model parameters.",
            "page": 8,
            "x": 104,
            "y": 469,
            "width": 403,
            "height": 91,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "10-symbolic",
            "chunk_id": "2bad8134-cd10-44c5-bf8a-428c06a1045d",
            "group_text": "## 5  Symbolic Reasoning\n\nOur final experimental evaluation considers symbolic reasoning, which is simple for humans but potentially challenging for language models.  We show that chain-of-thought prompting not only enables language models to perform symbolic reasoning tasks that are challenging in the standard prompting setting, but also facilitates length generalization to inference-time inputs longer than those seen in the few-shot exemplars.\n\nTasks.   We use the following two toy tasks.\n\n\u2022 Last letter concatenation.  This task asks the model to concatenate the last letters of words in a name (e.g., \u201cAmy Brown\u201d \u2192 \u201cyn\u201d). It is a more challenging version of first letter concatenation, which language models can already perform without chain of thought.\u00b3 We generate full names by randomly concatenating names from the top one-thousand first and last names from name census data (https://namecensus.com/).\n\n- \u2022 Coin flip. This task asks the model to answer whether a coin is still heads up after people either flip or don\u2019t flip the coin (e.g., \u201cA coin is heads up. Phoebe flips the coin. Osvaldo does not flip the coin. Is the coin still heads up?\u201d \u2192 \u201cno\u201d).\n\nAs the construction of these symbolic reasoning tasks is\nwell-defined, for each task we consider an *in-domain* test\nset for which examples had the same number of steps as\nthe training/few-shot exemplars, as well as an *out-of-domain* (OOD) test set, for which evaluation\nexamples had more steps than those in the exemplars. For last letter concatenation, the model only\nsees exemplars of names with two words, and then performs last letter concatenation on names with 3\nand 4 words.\\textsuperscript{4} We do the same for the number of potential flips in the coin flip task. Our experimental\nsetup uses the same methods and models as in the prior two sections. We again manually compose\nchains of thought for the few-shot exemplars for each task, which are given in Figure 3.\n\nResults.   The results of these in-domain and OOD evaluations are shown in Figure 8 for PaLM, with results for LaMDA shown in Appendix Table 5. With PaLM 540B, chain-of-thought prompting leads to almost 100% solve rates (note that standard prompting already solves coin flip with PaLM 540, though not for LaMDA 137B). Note that these in-domain evaluations are \u201ctoy tasks\u201d in the sense that perfect solution structures are already provided by the chains of thought in the few-shot exemplars; all the model has to do is repeat the same steps with the new symbols in the test-time example. And yet, small models still fail\u2014the ability to perform abstract manipulations on unseen symbols for these three tasks only arises at the scale of 100B model parameters.\n\nAs for the OOD evaluations, standard prompting fails for both tasks. With chain-of-thought prompting,\nlanguage models achieve upward scaling curves (though performance is lower than in the in-domain\nsetting). Hence, chain-of-thought prompting facilitates length generalization beyond seen chains of\nthought for language models of sufficient scale."
        },
        {
            "text": "As for the OOD evaluations, standard prompting fails for both tasks. With chain-of-thought prompting,\nlanguage models achieve upward scaling curves (though performance is lower than in the in-domain\nsetting). Hence, chain-of-thought prompting facilitates length generalization beyond seen chains of\nthought for language models of sufficient scale.",
            "page": 8,
            "x": 105,
            "y": 563,
            "width": 403,
            "height": 47,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "10-symbolic",
            "chunk_id": "c06a542e-0c42-41fc-8b27-5f4a7a9d63eb",
            "group_text": "## 5  Symbolic Reasoning\n\nOur final experimental evaluation considers symbolic reasoning, which is simple for humans but potentially challenging for language models.  We show that chain-of-thought prompting not only enables language models to perform symbolic reasoning tasks that are challenging in the standard prompting setting, but also facilitates length generalization to inference-time inputs longer than those seen in the few-shot exemplars.\n\nTasks.   We use the following two toy tasks.\n\n\u2022 Last letter concatenation.  This task asks the model to concatenate the last letters of words in a name (e.g., \u201cAmy Brown\u201d \u2192 \u201cyn\u201d). It is a more challenging version of first letter concatenation, which language models can already perform without chain of thought.\u00b3 We generate full names by randomly concatenating names from the top one-thousand first and last names from name census data (https://namecensus.com/).\n\n- \u2022 Coin flip. This task asks the model to answer whether a coin is still heads up after people either flip or don\u2019t flip the coin (e.g., \u201cA coin is heads up. Phoebe flips the coin. Osvaldo does not flip the coin. Is the coin still heads up?\u201d \u2192 \u201cno\u201d).\n\nAs the construction of these symbolic reasoning tasks is\nwell-defined, for each task we consider an *in-domain* test\nset for which examples had the same number of steps as\nthe training/few-shot exemplars, as well as an *out-of-domain* (OOD) test set, for which evaluation\nexamples had more steps than those in the exemplars. For last letter concatenation, the model only\nsees exemplars of names with two words, and then performs last letter concatenation on names with 3\nand 4 words.\\textsuperscript{4} We do the same for the number of potential flips in the coin flip task. Our experimental\nsetup uses the same methods and models as in the prior two sections. We again manually compose\nchains of thought for the few-shot exemplars for each task, which are given in Figure 3.\n\nResults.   The results of these in-domain and OOD evaluations are shown in Figure 8 for PaLM, with results for LaMDA shown in Appendix Table 5. With PaLM 540B, chain-of-thought prompting leads to almost 100% solve rates (note that standard prompting already solves coin flip with PaLM 540, though not for LaMDA 137B). Note that these in-domain evaluations are \u201ctoy tasks\u201d in the sense that perfect solution structures are already provided by the chains of thought in the few-shot exemplars; all the model has to do is repeat the same steps with the new symbols in the test-time example. And yet, small models still fail\u2014the ability to perform abstract manipulations on unseen symbols for these three tasks only arises at the scale of 100B model parameters.\n\nAs for the OOD evaluations, standard prompting fails for both tasks. With chain-of-thought prompting,\nlanguage models achieve upward scaling curves (though performance is lower than in the in-domain\nsetting). Hence, chain-of-thought prompting facilitates length generalization beyond seen chains of\nthought for language models of sufficient scale."
        },
        {
            "text": "## 6 Discussion\n\nWe have explored chain-of-thought prompting as a simple mechanism for eliciting multi-step reasoning behavior in large language models. We first saw that chain-of-thought prompting improves performance by a large margin on arithmetic reasoning, yielding improvements that are much stronger than ablations and robust to different annotators, exemplars, and language models (Section 3). Next,",
            "page": 8,
            "x": 104,
            "y": 622,
            "width": 404,
            "height": 73,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-discussion",
            "chunk_id": "7e1e7954-b728-4ab5-ac03-ffd46f7d226b",
            "group_text": "## 6 Discussion\n\nWe have explored chain-of-thought prompting as a simple mechanism for eliciting multi-step reasoning behavior in large language models. We first saw that chain-of-thought prompting improves performance by a large margin on arithmetic reasoning, yielding improvements that are much stronger than ablations and robust to different annotators, exemplars, and language models (Section 3). Next,\n\n\u00b3We tested 10 common names using GPT-3 davinci and it got all but one correct.\n\u2074For names of length longer than 2 words, we concatenate multiple first and last names together.\n\nexperiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought reasoning makes it generally applicable (Section 4). Finally, we showed that for symbolic reasoning, chain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In all experiments, chain-of-thought reasoning is elicited simply by prompting an off-the-shelf language model. No language models were finetuned in the process of writing this paper.\n\nThe emergence of chain-of-thought reasoning as a result of model scale has been a prevailing theme (Wei et al., 2022b). For many reasoning tasks where standard prompting has a flat scaling curve, chain-of-thought prompting leads to dramatically increasing scaling curves. Chain-of-thought prompting appears to expand the set of tasks that large language models can perform successfully\u2014in other words, our work underscores that standard prompting only provides a lower bound on the capabilities of large language models. This observation likely raises more questions than it answers\u2014for instance, how much more can we expect reasoning ability to improve with a further increase in model scale? What other prompting methods might expand the range of tasks that language models can solve?\n\nAs for limitations, we first qualify that although chain of thought emulates the thought processes of human reasoners, this does not answer whether the neural network is actually \u201creasoning,\u201d which we leave as an open question. Second, although the cost of manually augmenting exemplars with chains of thought is minimal in the few-shot setting, such annotation costs could be prohibitive for finetuning (though this could potentially be surmounted with synthetic data generation, or zero-shot generalization). Third, there is no guarantee of correct reasoning paths, which can lead to both correct and incorrect answers; improving factual generations of language models is an open direction for future work (Rashkin et al., 2021; Ye and Durrett, 2022; Wiegreffe et al., 2022, _inter alia_). Finally, the emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in real-world applications; further research could explore how to induce reasoning in smaller models."
        },
        {
            "text": "\u00b3We tested 10 common names using GPT-3 davinci and it got all but one correct.\n\u2074For names of length longer than 2 words, we concatenate multiple first and last names together.",
            "page": 8,
            "x": 116,
            "y": 698,
            "width": 352,
            "height": 26,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-discussion",
            "chunk_id": "a74ab0d6-bd63-4b75-b434-d0f30a20b284",
            "group_text": "## 6 Discussion\n\nWe have explored chain-of-thought prompting as a simple mechanism for eliciting multi-step reasoning behavior in large language models. We first saw that chain-of-thought prompting improves performance by a large margin on arithmetic reasoning, yielding improvements that are much stronger than ablations and robust to different annotators, exemplars, and language models (Section 3). Next,\n\n\u00b3We tested 10 common names using GPT-3 davinci and it got all but one correct.\n\u2074For names of length longer than 2 words, we concatenate multiple first and last names together.\n\nexperiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought reasoning makes it generally applicable (Section 4). Finally, we showed that for symbolic reasoning, chain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In all experiments, chain-of-thought reasoning is elicited simply by prompting an off-the-shelf language model. No language models were finetuned in the process of writing this paper.\n\nThe emergence of chain-of-thought reasoning as a result of model scale has been a prevailing theme (Wei et al., 2022b). For many reasoning tasks where standard prompting has a flat scaling curve, chain-of-thought prompting leads to dramatically increasing scaling curves. Chain-of-thought prompting appears to expand the set of tasks that large language models can perform successfully\u2014in other words, our work underscores that standard prompting only provides a lower bound on the capabilities of large language models. This observation likely raises more questions than it answers\u2014for instance, how much more can we expect reasoning ability to improve with a further increase in model scale? What other prompting methods might expand the range of tasks that language models can solve?\n\nAs for limitations, we first qualify that although chain of thought emulates the thought processes of human reasoners, this does not answer whether the neural network is actually \u201creasoning,\u201d which we leave as an open question. Second, although the cost of manually augmenting exemplars with chains of thought is minimal in the few-shot setting, such annotation costs could be prohibitive for finetuning (though this could potentially be surmounted with synthetic data generation, or zero-shot generalization). Third, there is no guarantee of correct reasoning paths, which can lead to both correct and incorrect answers; improving factual generations of language models is an open direction for future work (Rashkin et al., 2021; Ye and Durrett, 2022; Wiegreffe et al., 2022, _inter alia_). Finally, the emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in real-world applications; further research could explore how to induce reasoning in smaller models."
        },
        {
            "text": "experiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought reasoning makes it generally applicable (Section 4). Finally, we showed that for symbolic reasoning, chain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In all experiments, chain-of-thought reasoning is elicited simply by prompting an off-the-shelf language model. No language models were finetuned in the process of writing this paper.",
            "page": 9,
            "x": 104,
            "y": 72,
            "width": 404,
            "height": 57,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-discussion",
            "chunk_id": "0f70d247-f639-4fae-97f6-6f7e830a3a90",
            "group_text": "## 6 Discussion\n\nWe have explored chain-of-thought prompting as a simple mechanism for eliciting multi-step reasoning behavior in large language models. We first saw that chain-of-thought prompting improves performance by a large margin on arithmetic reasoning, yielding improvements that are much stronger than ablations and robust to different annotators, exemplars, and language models (Section 3). Next,\n\n\u00b3We tested 10 common names using GPT-3 davinci and it got all but one correct.\n\u2074For names of length longer than 2 words, we concatenate multiple first and last names together.\n\nexperiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought reasoning makes it generally applicable (Section 4). Finally, we showed that for symbolic reasoning, chain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In all experiments, chain-of-thought reasoning is elicited simply by prompting an off-the-shelf language model. No language models were finetuned in the process of writing this paper.\n\nThe emergence of chain-of-thought reasoning as a result of model scale has been a prevailing theme (Wei et al., 2022b). For many reasoning tasks where standard prompting has a flat scaling curve, chain-of-thought prompting leads to dramatically increasing scaling curves. Chain-of-thought prompting appears to expand the set of tasks that large language models can perform successfully\u2014in other words, our work underscores that standard prompting only provides a lower bound on the capabilities of large language models. This observation likely raises more questions than it answers\u2014for instance, how much more can we expect reasoning ability to improve with a further increase in model scale? What other prompting methods might expand the range of tasks that language models can solve?\n\nAs for limitations, we first qualify that although chain of thought emulates the thought processes of human reasoners, this does not answer whether the neural network is actually \u201creasoning,\u201d which we leave as an open question. Second, although the cost of manually augmenting exemplars with chains of thought is minimal in the few-shot setting, such annotation costs could be prohibitive for finetuning (though this could potentially be surmounted with synthetic data generation, or zero-shot generalization). Third, there is no guarantee of correct reasoning paths, which can lead to both correct and incorrect answers; improving factual generations of language models is an open direction for future work (Rashkin et al., 2021; Ye and Durrett, 2022; Wiegreffe et al., 2022, _inter alia_). Finally, the emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in real-world applications; further research could explore how to induce reasoning in smaller models."
        },
        {
            "text": "The emergence of chain-of-thought reasoning as a result of model scale has been a prevailing theme (Wei et al., 2022b). For many reasoning tasks where standard prompting has a flat scaling curve, chain-of-thought prompting leads to dramatically increasing scaling curves. Chain-of-thought prompting appears to expand the set of tasks that large language models can perform successfully\u2014in other words, our work underscores that standard prompting only provides a lower bound on the capabilities of large language models. This observation likely raises more questions than it answers\u2014for instance, how much more can we expect reasoning ability to improve with a further increase in model scale? What other prompting methods might expand the range of tasks that language models can solve?",
            "page": 9,
            "x": 104,
            "y": 132,
            "width": 403,
            "height": 90,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-discussion",
            "chunk_id": "2dd52518-235b-4631-bc66-f6ad3621faa5",
            "group_text": "## 6 Discussion\n\nWe have explored chain-of-thought prompting as a simple mechanism for eliciting multi-step reasoning behavior in large language models. We first saw that chain-of-thought prompting improves performance by a large margin on arithmetic reasoning, yielding improvements that are much stronger than ablations and robust to different annotators, exemplars, and language models (Section 3). Next,\n\n\u00b3We tested 10 common names using GPT-3 davinci and it got all but one correct.\n\u2074For names of length longer than 2 words, we concatenate multiple first and last names together.\n\nexperiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought reasoning makes it generally applicable (Section 4). Finally, we showed that for symbolic reasoning, chain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In all experiments, chain-of-thought reasoning is elicited simply by prompting an off-the-shelf language model. No language models were finetuned in the process of writing this paper.\n\nThe emergence of chain-of-thought reasoning as a result of model scale has been a prevailing theme (Wei et al., 2022b). For many reasoning tasks where standard prompting has a flat scaling curve, chain-of-thought prompting leads to dramatically increasing scaling curves. Chain-of-thought prompting appears to expand the set of tasks that large language models can perform successfully\u2014in other words, our work underscores that standard prompting only provides a lower bound on the capabilities of large language models. This observation likely raises more questions than it answers\u2014for instance, how much more can we expect reasoning ability to improve with a further increase in model scale? What other prompting methods might expand the range of tasks that language models can solve?\n\nAs for limitations, we first qualify that although chain of thought emulates the thought processes of human reasoners, this does not answer whether the neural network is actually \u201creasoning,\u201d which we leave as an open question. Second, although the cost of manually augmenting exemplars with chains of thought is minimal in the few-shot setting, such annotation costs could be prohibitive for finetuning (though this could potentially be surmounted with synthetic data generation, or zero-shot generalization). Third, there is no guarantee of correct reasoning paths, which can lead to both correct and incorrect answers; improving factual generations of language models is an open direction for future work (Rashkin et al., 2021; Ye and Durrett, 2022; Wiegreffe et al., 2022, _inter alia_). Finally, the emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in real-world applications; further research could explore how to induce reasoning in smaller models."
        },
        {
            "text": "As for limitations, we first qualify that although chain of thought emulates the thought processes of human reasoners, this does not answer whether the neural network is actually \u201creasoning,\u201d which we leave as an open question. Second, although the cost of manually augmenting exemplars with chains of thought is minimal in the few-shot setting, such annotation costs could be prohibitive for finetuning (though this could potentially be surmounted with synthetic data generation, or zero-shot generalization). Third, there is no guarantee of correct reasoning paths, which can lead to both correct and incorrect answers; improving factual generations of language models is an open direction for future work (Rashkin et al., 2021; Ye and Durrett, 2022; Wiegreffe et al., 2022, _inter alia_). Finally, the emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in real-world applications; further research could explore how to induce reasoning in smaller models.",
            "page": 9,
            "x": 104,
            "y": 225,
            "width": 404,
            "height": 113,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-discussion",
            "chunk_id": "5d6d9ada-7375-47cd-974e-7117889b63f5",
            "group_text": "## 6 Discussion\n\nWe have explored chain-of-thought prompting as a simple mechanism for eliciting multi-step reasoning behavior in large language models. We first saw that chain-of-thought prompting improves performance by a large margin on arithmetic reasoning, yielding improvements that are much stronger than ablations and robust to different annotators, exemplars, and language models (Section 3). Next,\n\n\u00b3We tested 10 common names using GPT-3 davinci and it got all but one correct.\n\u2074For names of length longer than 2 words, we concatenate multiple first and last names together.\n\nexperiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought reasoning makes it generally applicable (Section 4). Finally, we showed that for symbolic reasoning, chain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In all experiments, chain-of-thought reasoning is elicited simply by prompting an off-the-shelf language model. No language models were finetuned in the process of writing this paper.\n\nThe emergence of chain-of-thought reasoning as a result of model scale has been a prevailing theme (Wei et al., 2022b). For many reasoning tasks where standard prompting has a flat scaling curve, chain-of-thought prompting leads to dramatically increasing scaling curves. Chain-of-thought prompting appears to expand the set of tasks that large language models can perform successfully\u2014in other words, our work underscores that standard prompting only provides a lower bound on the capabilities of large language models. This observation likely raises more questions than it answers\u2014for instance, how much more can we expect reasoning ability to improve with a further increase in model scale? What other prompting methods might expand the range of tasks that language models can solve?\n\nAs for limitations, we first qualify that although chain of thought emulates the thought processes of human reasoners, this does not answer whether the neural network is actually \u201creasoning,\u201d which we leave as an open question. Second, although the cost of manually augmenting exemplars with chains of thought is minimal in the few-shot setting, such annotation costs could be prohibitive for finetuning (though this could potentially be surmounted with synthetic data generation, or zero-shot generalization). Third, there is no guarantee of correct reasoning paths, which can lead to both correct and incorrect answers; improving factual generations of language models is an open direction for future work (Rashkin et al., 2021; Ye and Durrett, 2022; Wiegreffe et al., 2022, _inter alia_). Finally, the emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in real-world applications; further research could explore how to induce reasoning in smaller models."
        },
        {
            "text": "7  Related Work\n\nThis work is inspired by many research areas, which we detail in an extended related work section (Appendix C). Here we describe two directions and associated papers that are perhaps most relevant.",
            "page": 9,
            "x": 103,
            "y": 348,
            "width": 404,
            "height": 51,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-related",
            "chunk_id": "c98046f0-04a7-4d39-91b0-1a611a88161c",
            "group_text": "7  Related Work\n\nThis work is inspired by many research areas, which we detail in an extended related work section (Appendix C). Here we describe two directions and associated papers that are perhaps most relevant.\n\nThe first relevant direction is using intermediate steps to solve reasoning problems. Ling et al. (2017) pioneer the idea of using natural language rationales to solve math word problems through a series of intermediate steps. Their work is a remarkable contrast to the literature using formal languages to reason (Roy et al., 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Cobbe et al. (2021) extend Ling et al. (2017) by creating a larger dataset and using it to finetune a pretrained language model rather than training a model from scratch. In the domain of program synthesis, Nye et al. (2021) leverage language models to predict the final outputs of Python programs via first line-to-line predicting the intermediate computational results, and show that their step-by-step prediction method performs better than directly predicting the final outputs.\n\nNaturally, this paper also relates closely to the large body of recent work on prompting. Since the popularization of few-shot prompting as given by Brown et al. (2020), several general approaches have improved the prompting ability of models, such as automatically learning prompts (Lester et al., 2021) or giving models instructions describing a task (Wei et al., 2022a; Sanh et al., 2022; Ouyang et al., 2022). Whereas these approaches improve or augment the input part of the prompt (e.g., instructions that are prepended to inputs), our work takes the orthogonal direction of augmenting the outputs of language models with a chain of thought."
        },
        {
            "text": "The first relevant direction is using intermediate steps to solve reasoning problems. Ling et al. (2017) pioneer the idea of using natural language rationales to solve math word problems through a series of intermediate steps. Their work is a remarkable contrast to the literature using formal languages to reason (Roy et al., 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Cobbe et al. (2021) extend Ling et al. (2017) by creating a larger dataset and using it to finetune a pretrained language model rather than training a model from scratch. In the domain of program synthesis, Nye et al. (2021) leverage language models to predict the final outputs of Python programs via first line-to-line predicting the intermediate computational results, and show that their step-by-step prediction method performs better than directly predicting the final outputs.",
            "page": 9,
            "x": 104,
            "y": 401,
            "width": 403,
            "height": 101,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-related",
            "chunk_id": "00ad1971-90b5-44d1-88ba-aae1e2392eff",
            "group_text": "7  Related Work\n\nThis work is inspired by many research areas, which we detail in an extended related work section (Appendix C). Here we describe two directions and associated papers that are perhaps most relevant.\n\nThe first relevant direction is using intermediate steps to solve reasoning problems. Ling et al. (2017) pioneer the idea of using natural language rationales to solve math word problems through a series of intermediate steps. Their work is a remarkable contrast to the literature using formal languages to reason (Roy et al., 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Cobbe et al. (2021) extend Ling et al. (2017) by creating a larger dataset and using it to finetune a pretrained language model rather than training a model from scratch. In the domain of program synthesis, Nye et al. (2021) leverage language models to predict the final outputs of Python programs via first line-to-line predicting the intermediate computational results, and show that their step-by-step prediction method performs better than directly predicting the final outputs.\n\nNaturally, this paper also relates closely to the large body of recent work on prompting. Since the popularization of few-shot prompting as given by Brown et al. (2020), several general approaches have improved the prompting ability of models, such as automatically learning prompts (Lester et al., 2021) or giving models instructions describing a task (Wei et al., 2022a; Sanh et al., 2022; Ouyang et al., 2022). Whereas these approaches improve or augment the input part of the prompt (e.g., instructions that are prepended to inputs), our work takes the orthogonal direction of augmenting the outputs of language models with a chain of thought."
        },
        {
            "text": "Naturally, this paper also relates closely to the large body of recent work on prompting. Since the popularization of few-shot prompting as given by Brown et al. (2020), several general approaches have improved the prompting ability of models, such as automatically learning prompts (Lester et al., 2021) or giving models instructions describing a task (Wei et al., 2022a; Sanh et al., 2022; Ouyang et al., 2022). Whereas these approaches improve or augment the input part of the prompt (e.g., instructions that are prepended to inputs), our work takes the orthogonal direction of augmenting the outputs of language models with a chain of thought.",
            "page": 9,
            "x": 105,
            "y": 505,
            "width": 403,
            "height": 80,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-related",
            "chunk_id": "56c26303-4383-4f1f-9898-1614ced05bd5",
            "group_text": "7  Related Work\n\nThis work is inspired by many research areas, which we detail in an extended related work section (Appendix C). Here we describe two directions and associated papers that are perhaps most relevant.\n\nThe first relevant direction is using intermediate steps to solve reasoning problems. Ling et al. (2017) pioneer the idea of using natural language rationales to solve math word problems through a series of intermediate steps. Their work is a remarkable contrast to the literature using formal languages to reason (Roy et al., 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Cobbe et al. (2021) extend Ling et al. (2017) by creating a larger dataset and using it to finetune a pretrained language model rather than training a model from scratch. In the domain of program synthesis, Nye et al. (2021) leverage language models to predict the final outputs of Python programs via first line-to-line predicting the intermediate computational results, and show that their step-by-step prediction method performs better than directly predicting the final outputs.\n\nNaturally, this paper also relates closely to the large body of recent work on prompting. Since the popularization of few-shot prompting as given by Brown et al. (2020), several general approaches have improved the prompting ability of models, such as automatically learning prompts (Lester et al., 2021) or giving models instructions describing a task (Wei et al., 2022a; Sanh et al., 2022; Ouyang et al., 2022). Whereas these approaches improve or augment the input part of the prompt (e.g., instructions that are prepended to inputs), our work takes the orthogonal direction of augmenting the outputs of language models with a chain of thought."
        },
        {
            "text": "8  Conclusions\n\nWe have explored chain-of-thought prompting as a simple and broadly applicable method for enhancing reasoning in language models. Through experiments on arithmetic, symbolic, and commonsense reasoning, we find that chain-of-thought reasoning is an emergent property of model scale that allows sufficiently large language models to perform reasoning tasks that otherwise have flat scaling curves. Broadening the range of reasoning tasks that language models can perform will hopefully inspire further work on language-based approaches to reasoning.",
            "page": 9,
            "x": 104,
            "y": 597,
            "width": 404,
            "height": 94,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "13-conclusions",
            "chunk_id": "1ff9d08e-c612-498e-b416-ebc392b83375",
            "group_text": "8  Conclusions\n\nWe have explored chain-of-thought prompting as a simple and broadly applicable method for enhancing reasoning in language models. Through experiments on arithmetic, symbolic, and commonsense reasoning, we find that chain-of-thought reasoning is an emergent property of model scale that allows sufficiently large language models to perform reasoning tasks that otherwise have flat scaling curves. Broadening the range of reasoning tasks that language models can perform will hopefully inspire further work on language-based approaches to reasoning."
        }
    ]
}