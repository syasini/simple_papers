{
    "annotations": [
        {
            "text": "**Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks**",
            "page": 1,
            "x": 78,
            "y": 66,
            "width": 441,
            "height": 21,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "0-title",
            "chunk_id": "be9cf636-4a00-40cd-8c6c-300297bd77b0",
            "group_text": "**Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks**\n\n**Nils Reimers and Iryna Gurevych**  \nUbiquitous Knowledge Processing Lab (UKP-TUDA)  \nDepartment of Computer Science, Technische Universit\u00e4t Darmstadt  \nwww.ukp.tu-darmstadt.de"
        },
        {
            "text": "**Nils Reimers and Iryna Gurevych**  \nUbiquitous Knowledge Processing Lab (UKP-TUDA)  \nDepartment of Computer Science, Technische Universit\u00e4t Darmstadt  \nwww.ukp.tu-darmstadt.de",
            "page": 1,
            "x": 133,
            "y": 112,
            "width": 334,
            "height": 58,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "0-title",
            "chunk_id": "46700407-34f7-46f8-a990-3703fea35cae",
            "group_text": "**Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks**\n\n**Nils Reimers and Iryna Gurevych**  \nUbiquitous Knowledge Processing Lab (UKP-TUDA)  \nDepartment of Computer Science, Technische Universit\u00e4t Darmstadt  \nwww.ukp.tu-darmstadt.de"
        },
        {
            "text": "# Abstract\n\nBERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering.",
            "page": 1,
            "x": 85,
            "y": 221,
            "width": 191,
            "height": 183,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "1-abstract",
            "chunk_id": "f1a1e259-423f-4774-92a0-2c0833bd1cfa",
            "group_text": "# Abstract\n\nBERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering.\n\nIn this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT.\n\nWe evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.\u00b9"
        },
        {
            "text": "In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT.",
            "page": 1,
            "x": 85,
            "y": 409,
            "width": 190,
            "height": 121,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "1-abstract",
            "chunk_id": "ec7fa467-ae47-4289-952c-11821ebca5a7",
            "group_text": "# Abstract\n\nBERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering.\n\nIn this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT.\n\nWe evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.\u00b9"
        },
        {
            "text": "We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.\u00b9",
            "page": 1,
            "x": 86,
            "y": 534,
            "width": 189,
            "height": 50,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "1-abstract",
            "chunk_id": "ea67efb5-24ff-4136-8130-cb54b8c87ef9",
            "group_text": "# Abstract\n\nBERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering.\n\nIn this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT.\n\nWe evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.\u00b9"
        },
        {
            "text": "# 1 Introduction\n\nIn this publication, we present Senten-  \n(SBERT), a modification of the BERT netwo  \nusing siamese and triplet networks that is ab  \nderive semantically meaningful sentence embe-  \ndings. This enables BERT to be used for certain  \nnew tasks, which up-to-now were not applica-  \nfor BERT. These tasks include large-scale seman-",
            "page": 1,
            "x": 68,
            "y": 595,
            "width": 224,
            "height": 119,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "92272508-91c9-4147-9312-fd8654ea01fd",
            "group_text": "# 1 Introduction\n\nIn this publication, we present Senten-  \n(SBERT), a modification of the BERT netwo  \nusing siamese and triplet networks that is ab  \nderive semantically meaningful sentence embe-  \ndings. This enables BERT to be used for certain  \nnew tasks, which up-to-now were not applica-  \nfor BERT. These tasks include large-scale seman-\n\n\u00b9Code available: https://github.com/UKPLab/sentence-transformers\n\n\u00b2With *semantically meaningful* we mean that semantically similar sentences are close in vector space.\n\ntic similarity comparison, clustering, and informa-\ntion retrieval via semantic search.\n\nBERT set new state-of-the-art performance on various sentence classification and sentence-pair regression tasks. BERT uses a cross-encoder: Two sentences are passed to the transformer network and the target value is predicted. However, this setup is unsuitable for various pair regression tasks due to too many possible combinations. Finding in a collection of $n = 10\\,000$ sentences the pair with the highest similarity requires with BERT $n \\cdot (n-1)/2 = 49\\,995\\,000$ inference computations. On a modern V100 GPU, this requires about 65 hours. Similar, finding which of the over 40 million existent questions of Quora is the most similar for a new question could be modeled as a pair-wise comparison with BERT, however, answering a single query would require over 50 hours.\n\nA common method to address clustering and semantic search is to map each sentence to a vector space such that semantically similar sentences are close. Researchers have started to input individual sentences into BERT and to derive fixed-size sentence embeddings. The most commonly used approach is to average the BERT output layer (known as BERT embeddings) or by using the output of the first token (the [CLS] token). As we will show, this common practice yields rather bad sentence embeddings, often worse than averaging GloVe embeddings (Pennington et al., 2014).\n\nTo alleviate this issue, we developed SBERT.\nThe siamese network architecture enables that\nfixed-sized vectors for input sentences can be de-\nrived.  Using a similarity measure like cosine-\nsimilarity or Manhattan / Euclidean distance, se-\nmantically similar sentences can be found.  These\nsimilarity measures can be performed extremely\nefficient on modern hardware, allowing SBERT\nto be used for semantic similarity search as well\nas for clustering.  The complexity for finding the\n\nmost similar sentence pair in a collection of 10,000 sentences is reduced from 65 hours with BERT to the computation of 10,000 sentence embeddings (~5 seconds with SBERT) and computing cosine-similarity (~0.01 seconds). By using optimized index structures, finding the most similar Quora question can be reduced from 50 hours to a few milliseconds (Johnson et al., 2017).\n\nWe fine-tune SBERT on NLI data, which creates sentence embeddings that significantly outperform other state-of-the-art sentence embedding methods like InferSent (Conneau et al., 2017) and Universal Sentence Encoder (Cer et al., 2018). On seven Semantic Textual Similarity (STS) tasks, SBERT achieves an improvement of 11.7 points compared to InferSent and 5.5 points compared to Universal Sentence Encoder. On SentEval (Conneau and Kiela, 2018), an evaluation toolkit for sentence embeddings, we achieve an improvement of 2.1 and 2.6 points, respectively.\n\nSBERT can be adapted to a specific task. It sets new state-of-the-art performance on a challenging argument similarity dataset (Misra et al., 2016) and on a triplet dataset to distinguish sentences from different sections of a Wikipedia article (Dor et al., 2018).\n\nThe paper is structured in the following way: Section 3 presents SBERT, section 4 evaluates SBERT on common STS tasks and on the challenging Argument Facet Similarity (AFS) corpus (Misra et al., 2016). Section 5 evaluates SBERT on SentEval. In section 6, we perform an ablation study to test some design aspect of SBERT. In section 7, we compare the computational efficiency of SBERT sentence embeddings in contrast to other state-of-the-art sentence embedding methods."
        },
        {
            "text": "\u00b9Code available: https://github.com/UKPLab/sentence-transformers\n\n\u00b2With *semantically meaningful* we mean that semantically similar sentences are close in vector space.",
            "page": 1,
            "x": 69,
            "y": 723,
            "width": 223,
            "height": 42,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "03c37743-9778-43e7-a653-371000745072",
            "group_text": "# 1 Introduction\n\nIn this publication, we present Senten-  \n(SBERT), a modification of the BERT netwo  \nusing siamese and triplet networks that is ab  \nderive semantically meaningful sentence embe-  \ndings. This enables BERT to be used for certain  \nnew tasks, which up-to-now were not applica-  \nfor BERT. These tasks include large-scale seman-\n\n\u00b9Code available: https://github.com/UKPLab/sentence-transformers\n\n\u00b2With *semantically meaningful* we mean that semantically similar sentences are close in vector space.\n\ntic similarity comparison, clustering, and informa-\ntion retrieval via semantic search.\n\nBERT set new state-of-the-art performance on various sentence classification and sentence-pair regression tasks. BERT uses a cross-encoder: Two sentences are passed to the transformer network and the target value is predicted. However, this setup is unsuitable for various pair regression tasks due to too many possible combinations. Finding in a collection of $n = 10\\,000$ sentences the pair with the highest similarity requires with BERT $n \\cdot (n-1)/2 = 49\\,995\\,000$ inference computations. On a modern V100 GPU, this requires about 65 hours. Similar, finding which of the over 40 million existent questions of Quora is the most similar for a new question could be modeled as a pair-wise comparison with BERT, however, answering a single query would require over 50 hours.\n\nA common method to address clustering and semantic search is to map each sentence to a vector space such that semantically similar sentences are close. Researchers have started to input individual sentences into BERT and to derive fixed-size sentence embeddings. The most commonly used approach is to average the BERT output layer (known as BERT embeddings) or by using the output of the first token (the [CLS] token). As we will show, this common practice yields rather bad sentence embeddings, often worse than averaging GloVe embeddings (Pennington et al., 2014).\n\nTo alleviate this issue, we developed SBERT.\nThe siamese network architecture enables that\nfixed-sized vectors for input sentences can be de-\nrived.  Using a similarity measure like cosine-\nsimilarity or Manhattan / Euclidean distance, se-\nmantically similar sentences can be found.  These\nsimilarity measures can be performed extremely\nefficient on modern hardware, allowing SBERT\nto be used for semantic similarity search as well\nas for clustering.  The complexity for finding the\n\nmost similar sentence pair in a collection of 10,000 sentences is reduced from 65 hours with BERT to the computation of 10,000 sentence embeddings (~5 seconds with SBERT) and computing cosine-similarity (~0.01 seconds). By using optimized index structures, finding the most similar Quora question can be reduced from 50 hours to a few milliseconds (Johnson et al., 2017).\n\nWe fine-tune SBERT on NLI data, which creates sentence embeddings that significantly outperform other state-of-the-art sentence embedding methods like InferSent (Conneau et al., 2017) and Universal Sentence Encoder (Cer et al., 2018). On seven Semantic Textual Similarity (STS) tasks, SBERT achieves an improvement of 11.7 points compared to InferSent and 5.5 points compared to Universal Sentence Encoder. On SentEval (Conneau and Kiela, 2018), an evaluation toolkit for sentence embeddings, we achieve an improvement of 2.1 and 2.6 points, respectively.\n\nSBERT can be adapted to a specific task. It sets new state-of-the-art performance on a challenging argument similarity dataset (Misra et al., 2016) and on a triplet dataset to distinguish sentences from different sections of a Wikipedia article (Dor et al., 2018).\n\nThe paper is structured in the following way: Section 3 presents SBERT, section 4 evaluates SBERT on common STS tasks and on the challenging Argument Facet Similarity (AFS) corpus (Misra et al., 2016). Section 5 evaluates SBERT on SentEval. In section 6, we perform an ablation study to test some design aspect of SBERT. In section 7, we compare the computational efficiency of SBERT sentence embeddings in contrast to other state-of-the-art sentence embedding methods."
        },
        {
            "text": "tic similarity comparison, clustering, and informa-\ntion retrieval via semantic search.",
            "page": 1,
            "x": 304,
            "y": 224,
            "width": 222,
            "height": 26,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "a6fbe06e-db66-423f-8b24-0dc853a89b26",
            "group_text": "# 1 Introduction\n\nIn this publication, we present Senten-  \n(SBERT), a modification of the BERT netwo  \nusing siamese and triplet networks that is ab  \nderive semantically meaningful sentence embe-  \ndings. This enables BERT to be used for certain  \nnew tasks, which up-to-now were not applica-  \nfor BERT. These tasks include large-scale seman-\n\n\u00b9Code available: https://github.com/UKPLab/sentence-transformers\n\n\u00b2With *semantically meaningful* we mean that semantically similar sentences are close in vector space.\n\ntic similarity comparison, clustering, and informa-\ntion retrieval via semantic search.\n\nBERT set new state-of-the-art performance on various sentence classification and sentence-pair regression tasks. BERT uses a cross-encoder: Two sentences are passed to the transformer network and the target value is predicted. However, this setup is unsuitable for various pair regression tasks due to too many possible combinations. Finding in a collection of $n = 10\\,000$ sentences the pair with the highest similarity requires with BERT $n \\cdot (n-1)/2 = 49\\,995\\,000$ inference computations. On a modern V100 GPU, this requires about 65 hours. Similar, finding which of the over 40 million existent questions of Quora is the most similar for a new question could be modeled as a pair-wise comparison with BERT, however, answering a single query would require over 50 hours.\n\nA common method to address clustering and semantic search is to map each sentence to a vector space such that semantically similar sentences are close. Researchers have started to input individual sentences into BERT and to derive fixed-size sentence embeddings. The most commonly used approach is to average the BERT output layer (known as BERT embeddings) or by using the output of the first token (the [CLS] token). As we will show, this common practice yields rather bad sentence embeddings, often worse than averaging GloVe embeddings (Pennington et al., 2014).\n\nTo alleviate this issue, we developed SBERT.\nThe siamese network architecture enables that\nfixed-sized vectors for input sentences can be de-\nrived.  Using a similarity measure like cosine-\nsimilarity or Manhattan / Euclidean distance, se-\nmantically similar sentences can be found.  These\nsimilarity measures can be performed extremely\nefficient on modern hardware, allowing SBERT\nto be used for semantic similarity search as well\nas for clustering.  The complexity for finding the\n\nmost similar sentence pair in a collection of 10,000 sentences is reduced from 65 hours with BERT to the computation of 10,000 sentence embeddings (~5 seconds with SBERT) and computing cosine-similarity (~0.01 seconds). By using optimized index structures, finding the most similar Quora question can be reduced from 50 hours to a few milliseconds (Johnson et al., 2017).\n\nWe fine-tune SBERT on NLI data, which creates sentence embeddings that significantly outperform other state-of-the-art sentence embedding methods like InferSent (Conneau et al., 2017) and Universal Sentence Encoder (Cer et al., 2018). On seven Semantic Textual Similarity (STS) tasks, SBERT achieves an improvement of 11.7 points compared to InferSent and 5.5 points compared to Universal Sentence Encoder. On SentEval (Conneau and Kiela, 2018), an evaluation toolkit for sentence embeddings, we achieve an improvement of 2.1 and 2.6 points, respectively.\n\nSBERT can be adapted to a specific task. It sets new state-of-the-art performance on a challenging argument similarity dataset (Misra et al., 2016) and on a triplet dataset to distinguish sentences from different sections of a Wikipedia article (Dor et al., 2018).\n\nThe paper is structured in the following way: Section 3 presents SBERT, section 4 evaluates SBERT on common STS tasks and on the challenging Argument Facet Similarity (AFS) corpus (Misra et al., 2016). Section 5 evaluates SBERT on SentEval. In section 6, we perform an ablation study to test some design aspect of SBERT. In section 7, we compare the computational efficiency of SBERT sentence embeddings in contrast to other state-of-the-art sentence embedding methods."
        },
        {
            "text": "BERT set new state-of-the-art performance on various sentence classification and sentence-pair regression tasks. BERT uses a cross-encoder: Two sentences are passed to the transformer network and the target value is predicted. However, this setup is unsuitable for various pair regression tasks due to too many possible combinations. Finding in a collection of $n = 10\\,000$ sentences the pair with the highest similarity requires with BERT $n \\cdot (n-1)/2 = 49\\,995\\,000$ inference computations. On a modern V100 GPU, this requires about 65 hours. Similar, finding which of the over 40 million existent questions of Quora is the most similar for a new question could be modeled as a pair-wise comparison with BERT, however, answering a single query would require over 50 hours.",
            "page": 1,
            "x": 304,
            "y": 251,
            "width": 223,
            "height": 216,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "282635ca-272a-4691-bf6f-304a2f526cba",
            "group_text": "# 1 Introduction\n\nIn this publication, we present Senten-  \n(SBERT), a modification of the BERT netwo  \nusing siamese and triplet networks that is ab  \nderive semantically meaningful sentence embe-  \ndings. This enables BERT to be used for certain  \nnew tasks, which up-to-now were not applica-  \nfor BERT. These tasks include large-scale seman-\n\n\u00b9Code available: https://github.com/UKPLab/sentence-transformers\n\n\u00b2With *semantically meaningful* we mean that semantically similar sentences are close in vector space.\n\ntic similarity comparison, clustering, and informa-\ntion retrieval via semantic search.\n\nBERT set new state-of-the-art performance on various sentence classification and sentence-pair regression tasks. BERT uses a cross-encoder: Two sentences are passed to the transformer network and the target value is predicted. However, this setup is unsuitable for various pair regression tasks due to too many possible combinations. Finding in a collection of $n = 10\\,000$ sentences the pair with the highest similarity requires with BERT $n \\cdot (n-1)/2 = 49\\,995\\,000$ inference computations. On a modern V100 GPU, this requires about 65 hours. Similar, finding which of the over 40 million existent questions of Quora is the most similar for a new question could be modeled as a pair-wise comparison with BERT, however, answering a single query would require over 50 hours.\n\nA common method to address clustering and semantic search is to map each sentence to a vector space such that semantically similar sentences are close. Researchers have started to input individual sentences into BERT and to derive fixed-size sentence embeddings. The most commonly used approach is to average the BERT output layer (known as BERT embeddings) or by using the output of the first token (the [CLS] token). As we will show, this common practice yields rather bad sentence embeddings, often worse than averaging GloVe embeddings (Pennington et al., 2014).\n\nTo alleviate this issue, we developed SBERT.\nThe siamese network architecture enables that\nfixed-sized vectors for input sentences can be de-\nrived.  Using a similarity measure like cosine-\nsimilarity or Manhattan / Euclidean distance, se-\nmantically similar sentences can be found.  These\nsimilarity measures can be performed extremely\nefficient on modern hardware, allowing SBERT\nto be used for semantic similarity search as well\nas for clustering.  The complexity for finding the\n\nmost similar sentence pair in a collection of 10,000 sentences is reduced from 65 hours with BERT to the computation of 10,000 sentence embeddings (~5 seconds with SBERT) and computing cosine-similarity (~0.01 seconds). By using optimized index structures, finding the most similar Quora question can be reduced from 50 hours to a few milliseconds (Johnson et al., 2017).\n\nWe fine-tune SBERT on NLI data, which creates sentence embeddings that significantly outperform other state-of-the-art sentence embedding methods like InferSent (Conneau et al., 2017) and Universal Sentence Encoder (Cer et al., 2018). On seven Semantic Textual Similarity (STS) tasks, SBERT achieves an improvement of 11.7 points compared to InferSent and 5.5 points compared to Universal Sentence Encoder. On SentEval (Conneau and Kiela, 2018), an evaluation toolkit for sentence embeddings, we achieve an improvement of 2.1 and 2.6 points, respectively.\n\nSBERT can be adapted to a specific task. It sets new state-of-the-art performance on a challenging argument similarity dataset (Misra et al., 2016) and on a triplet dataset to distinguish sentences from different sections of a Wikipedia article (Dor et al., 2018).\n\nThe paper is structured in the following way: Section 3 presents SBERT, section 4 evaluates SBERT on common STS tasks and on the challenging Argument Facet Similarity (AFS) corpus (Misra et al., 2016). Section 5 evaluates SBERT on SentEval. In section 6, we perform an ablation study to test some design aspect of SBERT. In section 7, we compare the computational efficiency of SBERT sentence embeddings in contrast to other state-of-the-art sentence embedding methods."
        },
        {
            "text": "A common method to address clustering and semantic search is to map each sentence to a vector space such that semantically similar sentences are close. Researchers have started to input individual sentences into BERT and to derive fixed-size sentence embeddings. The most commonly used approach is to average the BERT output layer (known as BERT embeddings) or by using the output of the first token (the [CLS] token). As we will show, this common practice yields rather bad sentence embeddings, often worse than averaging GloVe embeddings (Pennington et al., 2014).",
            "page": 1,
            "x": 304,
            "y": 469,
            "width": 223,
            "height": 160,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "29f4dbe9-180f-4069-a22c-885f27ae8da2",
            "group_text": "# 1 Introduction\n\nIn this publication, we present Senten-  \n(SBERT), a modification of the BERT netwo  \nusing siamese and triplet networks that is ab  \nderive semantically meaningful sentence embe-  \ndings. This enables BERT to be used for certain  \nnew tasks, which up-to-now were not applica-  \nfor BERT. These tasks include large-scale seman-\n\n\u00b9Code available: https://github.com/UKPLab/sentence-transformers\n\n\u00b2With *semantically meaningful* we mean that semantically similar sentences are close in vector space.\n\ntic similarity comparison, clustering, and informa-\ntion retrieval via semantic search.\n\nBERT set new state-of-the-art performance on various sentence classification and sentence-pair regression tasks. BERT uses a cross-encoder: Two sentences are passed to the transformer network and the target value is predicted. However, this setup is unsuitable for various pair regression tasks due to too many possible combinations. Finding in a collection of $n = 10\\,000$ sentences the pair with the highest similarity requires with BERT $n \\cdot (n-1)/2 = 49\\,995\\,000$ inference computations. On a modern V100 GPU, this requires about 65 hours. Similar, finding which of the over 40 million existent questions of Quora is the most similar for a new question could be modeled as a pair-wise comparison with BERT, however, answering a single query would require over 50 hours.\n\nA common method to address clustering and semantic search is to map each sentence to a vector space such that semantically similar sentences are close. Researchers have started to input individual sentences into BERT and to derive fixed-size sentence embeddings. The most commonly used approach is to average the BERT output layer (known as BERT embeddings) or by using the output of the first token (the [CLS] token). As we will show, this common practice yields rather bad sentence embeddings, often worse than averaging GloVe embeddings (Pennington et al., 2014).\n\nTo alleviate this issue, we developed SBERT.\nThe siamese network architecture enables that\nfixed-sized vectors for input sentences can be de-\nrived.  Using a similarity measure like cosine-\nsimilarity or Manhattan / Euclidean distance, se-\nmantically similar sentences can be found.  These\nsimilarity measures can be performed extremely\nefficient on modern hardware, allowing SBERT\nto be used for semantic similarity search as well\nas for clustering.  The complexity for finding the\n\nmost similar sentence pair in a collection of 10,000 sentences is reduced from 65 hours with BERT to the computation of 10,000 sentence embeddings (~5 seconds with SBERT) and computing cosine-similarity (~0.01 seconds). By using optimized index structures, finding the most similar Quora question can be reduced from 50 hours to a few milliseconds (Johnson et al., 2017).\n\nWe fine-tune SBERT on NLI data, which creates sentence embeddings that significantly outperform other state-of-the-art sentence embedding methods like InferSent (Conneau et al., 2017) and Universal Sentence Encoder (Cer et al., 2018). On seven Semantic Textual Similarity (STS) tasks, SBERT achieves an improvement of 11.7 points compared to InferSent and 5.5 points compared to Universal Sentence Encoder. On SentEval (Conneau and Kiela, 2018), an evaluation toolkit for sentence embeddings, we achieve an improvement of 2.1 and 2.6 points, respectively.\n\nSBERT can be adapted to a specific task. It sets new state-of-the-art performance on a challenging argument similarity dataset (Misra et al., 2016) and on a triplet dataset to distinguish sentences from different sections of a Wikipedia article (Dor et al., 2018).\n\nThe paper is structured in the following way: Section 3 presents SBERT, section 4 evaluates SBERT on common STS tasks and on the challenging Argument Facet Similarity (AFS) corpus (Misra et al., 2016). Section 5 evaluates SBERT on SentEval. In section 6, we perform an ablation study to test some design aspect of SBERT. In section 7, we compare the computational efficiency of SBERT sentence embeddings in contrast to other state-of-the-art sentence embedding methods."
        },
        {
            "text": "To alleviate this issue, we developed SBERT.\nThe siamese network architecture enables that\nfixed-sized vectors for input sentences can be de-\nrived.  Using a similarity measure like cosine-\nsimilarity or Manhattan / Euclidean distance, se-\nmantically similar sentences can be found.  These\nsimilarity measures can be performed extremely\nefficient on modern hardware, allowing SBERT\nto be used for semantic similarity search as well\nas for clustering.  The complexity for finding the",
            "page": 1,
            "x": 305,
            "y": 630,
            "width": 223,
            "height": 136,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "ae6ff7ce-de56-47fa-b726-89352b1209bc",
            "group_text": "# 1 Introduction\n\nIn this publication, we present Senten-  \n(SBERT), a modification of the BERT netwo  \nusing siamese and triplet networks that is ab  \nderive semantically meaningful sentence embe-  \ndings. This enables BERT to be used for certain  \nnew tasks, which up-to-now were not applica-  \nfor BERT. These tasks include large-scale seman-\n\n\u00b9Code available: https://github.com/UKPLab/sentence-transformers\n\n\u00b2With *semantically meaningful* we mean that semantically similar sentences are close in vector space.\n\ntic similarity comparison, clustering, and informa-\ntion retrieval via semantic search.\n\nBERT set new state-of-the-art performance on various sentence classification and sentence-pair regression tasks. BERT uses a cross-encoder: Two sentences are passed to the transformer network and the target value is predicted. However, this setup is unsuitable for various pair regression tasks due to too many possible combinations. Finding in a collection of $n = 10\\,000$ sentences the pair with the highest similarity requires with BERT $n \\cdot (n-1)/2 = 49\\,995\\,000$ inference computations. On a modern V100 GPU, this requires about 65 hours. Similar, finding which of the over 40 million existent questions of Quora is the most similar for a new question could be modeled as a pair-wise comparison with BERT, however, answering a single query would require over 50 hours.\n\nA common method to address clustering and semantic search is to map each sentence to a vector space such that semantically similar sentences are close. Researchers have started to input individual sentences into BERT and to derive fixed-size sentence embeddings. The most commonly used approach is to average the BERT output layer (known as BERT embeddings) or by using the output of the first token (the [CLS] token). As we will show, this common practice yields rather bad sentence embeddings, often worse than averaging GloVe embeddings (Pennington et al., 2014).\n\nTo alleviate this issue, we developed SBERT.\nThe siamese network architecture enables that\nfixed-sized vectors for input sentences can be de-\nrived.  Using a similarity measure like cosine-\nsimilarity or Manhattan / Euclidean distance, se-\nmantically similar sentences can be found.  These\nsimilarity measures can be performed extremely\nefficient on modern hardware, allowing SBERT\nto be used for semantic similarity search as well\nas for clustering.  The complexity for finding the\n\nmost similar sentence pair in a collection of 10,000 sentences is reduced from 65 hours with BERT to the computation of 10,000 sentence embeddings (~5 seconds with SBERT) and computing cosine-similarity (~0.01 seconds). By using optimized index structures, finding the most similar Quora question can be reduced from 50 hours to a few milliseconds (Johnson et al., 2017).\n\nWe fine-tune SBERT on NLI data, which creates sentence embeddings that significantly outperform other state-of-the-art sentence embedding methods like InferSent (Conneau et al., 2017) and Universal Sentence Encoder (Cer et al., 2018). On seven Semantic Textual Similarity (STS) tasks, SBERT achieves an improvement of 11.7 points compared to InferSent and 5.5 points compared to Universal Sentence Encoder. On SentEval (Conneau and Kiela, 2018), an evaluation toolkit for sentence embeddings, we achieve an improvement of 2.1 and 2.6 points, respectively.\n\nSBERT can be adapted to a specific task. It sets new state-of-the-art performance on a challenging argument similarity dataset (Misra et al., 2016) and on a triplet dataset to distinguish sentences from different sections of a Wikipedia article (Dor et al., 2018).\n\nThe paper is structured in the following way: Section 3 presents SBERT, section 4 evaluates SBERT on common STS tasks and on the challenging Argument Facet Similarity (AFS) corpus (Misra et al., 2016). Section 5 evaluates SBERT on SentEval. In section 6, we perform an ablation study to test some design aspect of SBERT. In section 7, we compare the computational efficiency of SBERT sentence embeddings in contrast to other state-of-the-art sentence embedding methods."
        },
        {
            "text": "most similar sentence pair in a collection of 10,000 sentences is reduced from 65 hours with BERT to the computation of 10,000 sentence embeddings (~5 seconds with SBERT) and computing cosine-similarity (~0.01 seconds). By using optimized index structures, finding the most similar Quora question can be reduced from 50 hours to a few milliseconds (Johnson et al., 2017).",
            "page": 2,
            "x": 68,
            "y": 62,
            "width": 225,
            "height": 109,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "cb7476bd-cc88-47a4-ac0f-4cf7f676b929",
            "group_text": "# 1 Introduction\n\nIn this publication, we present Senten-  \n(SBERT), a modification of the BERT netwo  \nusing siamese and triplet networks that is ab  \nderive semantically meaningful sentence embe-  \ndings. This enables BERT to be used for certain  \nnew tasks, which up-to-now were not applica-  \nfor BERT. These tasks include large-scale seman-\n\n\u00b9Code available: https://github.com/UKPLab/sentence-transformers\n\n\u00b2With *semantically meaningful* we mean that semantically similar sentences are close in vector space.\n\ntic similarity comparison, clustering, and informa-\ntion retrieval via semantic search.\n\nBERT set new state-of-the-art performance on various sentence classification and sentence-pair regression tasks. BERT uses a cross-encoder: Two sentences are passed to the transformer network and the target value is predicted. However, this setup is unsuitable for various pair regression tasks due to too many possible combinations. Finding in a collection of $n = 10\\,000$ sentences the pair with the highest similarity requires with BERT $n \\cdot (n-1)/2 = 49\\,995\\,000$ inference computations. On a modern V100 GPU, this requires about 65 hours. Similar, finding which of the over 40 million existent questions of Quora is the most similar for a new question could be modeled as a pair-wise comparison with BERT, however, answering a single query would require over 50 hours.\n\nA common method to address clustering and semantic search is to map each sentence to a vector space such that semantically similar sentences are close. Researchers have started to input individual sentences into BERT and to derive fixed-size sentence embeddings. The most commonly used approach is to average the BERT output layer (known as BERT embeddings) or by using the output of the first token (the [CLS] token). As we will show, this common practice yields rather bad sentence embeddings, often worse than averaging GloVe embeddings (Pennington et al., 2014).\n\nTo alleviate this issue, we developed SBERT.\nThe siamese network architecture enables that\nfixed-sized vectors for input sentences can be de-\nrived.  Using a similarity measure like cosine-\nsimilarity or Manhattan / Euclidean distance, se-\nmantically similar sentences can be found.  These\nsimilarity measures can be performed extremely\nefficient on modern hardware, allowing SBERT\nto be used for semantic similarity search as well\nas for clustering.  The complexity for finding the\n\nmost similar sentence pair in a collection of 10,000 sentences is reduced from 65 hours with BERT to the computation of 10,000 sentence embeddings (~5 seconds with SBERT) and computing cosine-similarity (~0.01 seconds). By using optimized index structures, finding the most similar Quora question can be reduced from 50 hours to a few milliseconds (Johnson et al., 2017).\n\nWe fine-tune SBERT on NLI data, which creates sentence embeddings that significantly outperform other state-of-the-art sentence embedding methods like InferSent (Conneau et al., 2017) and Universal Sentence Encoder (Cer et al., 2018). On seven Semantic Textual Similarity (STS) tasks, SBERT achieves an improvement of 11.7 points compared to InferSent and 5.5 points compared to Universal Sentence Encoder. On SentEval (Conneau and Kiela, 2018), an evaluation toolkit for sentence embeddings, we achieve an improvement of 2.1 and 2.6 points, respectively.\n\nSBERT can be adapted to a specific task. It sets new state-of-the-art performance on a challenging argument similarity dataset (Misra et al., 2016) and on a triplet dataset to distinguish sentences from different sections of a Wikipedia article (Dor et al., 2018).\n\nThe paper is structured in the following way: Section 3 presents SBERT, section 4 evaluates SBERT on common STS tasks and on the challenging Argument Facet Similarity (AFS) corpus (Misra et al., 2016). Section 5 evaluates SBERT on SentEval. In section 6, we perform an ablation study to test some design aspect of SBERT. In section 7, we compare the computational efficiency of SBERT sentence embeddings in contrast to other state-of-the-art sentence embedding methods."
        },
        {
            "text": "We fine-tune SBERT on NLI data, which creates sentence embeddings that significantly outperform other state-of-the-art sentence embedding methods like InferSent (Conneau et al., 2017) and Universal Sentence Encoder (Cer et al., 2018). On seven Semantic Textual Similarity (STS) tasks, SBERT achieves an improvement of 11.7 points compared to InferSent and 5.5 points compared to Universal Sentence Encoder. On SentEval (Conneau and Kiela, 2018), an evaluation toolkit for sentence embeddings, we achieve an improvement of 2.1 and 2.6 points, respectively.",
            "page": 2,
            "x": 68,
            "y": 173,
            "width": 226,
            "height": 162,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "d4c318f9-6f52-4eb0-bb2e-6b1bf688f827",
            "group_text": "# 1 Introduction\n\nIn this publication, we present Senten-  \n(SBERT), a modification of the BERT netwo  \nusing siamese and triplet networks that is ab  \nderive semantically meaningful sentence embe-  \ndings. This enables BERT to be used for certain  \nnew tasks, which up-to-now were not applica-  \nfor BERT. These tasks include large-scale seman-\n\n\u00b9Code available: https://github.com/UKPLab/sentence-transformers\n\n\u00b2With *semantically meaningful* we mean that semantically similar sentences are close in vector space.\n\ntic similarity comparison, clustering, and informa-\ntion retrieval via semantic search.\n\nBERT set new state-of-the-art performance on various sentence classification and sentence-pair regression tasks. BERT uses a cross-encoder: Two sentences are passed to the transformer network and the target value is predicted. However, this setup is unsuitable for various pair regression tasks due to too many possible combinations. Finding in a collection of $n = 10\\,000$ sentences the pair with the highest similarity requires with BERT $n \\cdot (n-1)/2 = 49\\,995\\,000$ inference computations. On a modern V100 GPU, this requires about 65 hours. Similar, finding which of the over 40 million existent questions of Quora is the most similar for a new question could be modeled as a pair-wise comparison with BERT, however, answering a single query would require over 50 hours.\n\nA common method to address clustering and semantic search is to map each sentence to a vector space such that semantically similar sentences are close. Researchers have started to input individual sentences into BERT and to derive fixed-size sentence embeddings. The most commonly used approach is to average the BERT output layer (known as BERT embeddings) or by using the output of the first token (the [CLS] token). As we will show, this common practice yields rather bad sentence embeddings, often worse than averaging GloVe embeddings (Pennington et al., 2014).\n\nTo alleviate this issue, we developed SBERT.\nThe siamese network architecture enables that\nfixed-sized vectors for input sentences can be de-\nrived.  Using a similarity measure like cosine-\nsimilarity or Manhattan / Euclidean distance, se-\nmantically similar sentences can be found.  These\nsimilarity measures can be performed extremely\nefficient on modern hardware, allowing SBERT\nto be used for semantic similarity search as well\nas for clustering.  The complexity for finding the\n\nmost similar sentence pair in a collection of 10,000 sentences is reduced from 65 hours with BERT to the computation of 10,000 sentence embeddings (~5 seconds with SBERT) and computing cosine-similarity (~0.01 seconds). By using optimized index structures, finding the most similar Quora question can be reduced from 50 hours to a few milliseconds (Johnson et al., 2017).\n\nWe fine-tune SBERT on NLI data, which creates sentence embeddings that significantly outperform other state-of-the-art sentence embedding methods like InferSent (Conneau et al., 2017) and Universal Sentence Encoder (Cer et al., 2018). On seven Semantic Textual Similarity (STS) tasks, SBERT achieves an improvement of 11.7 points compared to InferSent and 5.5 points compared to Universal Sentence Encoder. On SentEval (Conneau and Kiela, 2018), an evaluation toolkit for sentence embeddings, we achieve an improvement of 2.1 and 2.6 points, respectively.\n\nSBERT can be adapted to a specific task. It sets new state-of-the-art performance on a challenging argument similarity dataset (Misra et al., 2016) and on a triplet dataset to distinguish sentences from different sections of a Wikipedia article (Dor et al., 2018).\n\nThe paper is structured in the following way: Section 3 presents SBERT, section 4 evaluates SBERT on common STS tasks and on the challenging Argument Facet Similarity (AFS) corpus (Misra et al., 2016). Section 5 evaluates SBERT on SentEval. In section 6, we perform an ablation study to test some design aspect of SBERT. In section 7, we compare the computational efficiency of SBERT sentence embeddings in contrast to other state-of-the-art sentence embedding methods."
        },
        {
            "text": "SBERT can be adapted to a specific task. It sets new state-of-the-art performance on a challenging argument similarity dataset (Misra et al., 2016) and on a triplet dataset to distinguish sentences from different sections of a Wikipedia article (Dor et al., 2018).",
            "page": 2,
            "x": 68,
            "y": 336,
            "width": 225,
            "height": 82,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "a9977e20-04a0-4ef3-98d4-a8d0ae0a1b30",
            "group_text": "# 1 Introduction\n\nIn this publication, we present Senten-  \n(SBERT), a modification of the BERT netwo  \nusing siamese and triplet networks that is ab  \nderive semantically meaningful sentence embe-  \ndings. This enables BERT to be used for certain  \nnew tasks, which up-to-now were not applica-  \nfor BERT. These tasks include large-scale seman-\n\n\u00b9Code available: https://github.com/UKPLab/sentence-transformers\n\n\u00b2With *semantically meaningful* we mean that semantically similar sentences are close in vector space.\n\ntic similarity comparison, clustering, and informa-\ntion retrieval via semantic search.\n\nBERT set new state-of-the-art performance on various sentence classification and sentence-pair regression tasks. BERT uses a cross-encoder: Two sentences are passed to the transformer network and the target value is predicted. However, this setup is unsuitable for various pair regression tasks due to too many possible combinations. Finding in a collection of $n = 10\\,000$ sentences the pair with the highest similarity requires with BERT $n \\cdot (n-1)/2 = 49\\,995\\,000$ inference computations. On a modern V100 GPU, this requires about 65 hours. Similar, finding which of the over 40 million existent questions of Quora is the most similar for a new question could be modeled as a pair-wise comparison with BERT, however, answering a single query would require over 50 hours.\n\nA common method to address clustering and semantic search is to map each sentence to a vector space such that semantically similar sentences are close. Researchers have started to input individual sentences into BERT and to derive fixed-size sentence embeddings. The most commonly used approach is to average the BERT output layer (known as BERT embeddings) or by using the output of the first token (the [CLS] token). As we will show, this common practice yields rather bad sentence embeddings, often worse than averaging GloVe embeddings (Pennington et al., 2014).\n\nTo alleviate this issue, we developed SBERT.\nThe siamese network architecture enables that\nfixed-sized vectors for input sentences can be de-\nrived.  Using a similarity measure like cosine-\nsimilarity or Manhattan / Euclidean distance, se-\nmantically similar sentences can be found.  These\nsimilarity measures can be performed extremely\nefficient on modern hardware, allowing SBERT\nto be used for semantic similarity search as well\nas for clustering.  The complexity for finding the\n\nmost similar sentence pair in a collection of 10,000 sentences is reduced from 65 hours with BERT to the computation of 10,000 sentence embeddings (~5 seconds with SBERT) and computing cosine-similarity (~0.01 seconds). By using optimized index structures, finding the most similar Quora question can be reduced from 50 hours to a few milliseconds (Johnson et al., 2017).\n\nWe fine-tune SBERT on NLI data, which creates sentence embeddings that significantly outperform other state-of-the-art sentence embedding methods like InferSent (Conneau et al., 2017) and Universal Sentence Encoder (Cer et al., 2018). On seven Semantic Textual Similarity (STS) tasks, SBERT achieves an improvement of 11.7 points compared to InferSent and 5.5 points compared to Universal Sentence Encoder. On SentEval (Conneau and Kiela, 2018), an evaluation toolkit for sentence embeddings, we achieve an improvement of 2.1 and 2.6 points, respectively.\n\nSBERT can be adapted to a specific task. It sets new state-of-the-art performance on a challenging argument similarity dataset (Misra et al., 2016) and on a triplet dataset to distinguish sentences from different sections of a Wikipedia article (Dor et al., 2018).\n\nThe paper is structured in the following way: Section 3 presents SBERT, section 4 evaluates SBERT on common STS tasks and on the challenging Argument Facet Similarity (AFS) corpus (Misra et al., 2016). Section 5 evaluates SBERT on SentEval. In section 6, we perform an ablation study to test some design aspect of SBERT. In section 7, we compare the computational efficiency of SBERT sentence embeddings in contrast to other state-of-the-art sentence embedding methods."
        },
        {
            "text": "The paper is structured in the following way: Section 3 presents SBERT, section 4 evaluates SBERT on common STS tasks and on the challenging Argument Facet Similarity (AFS) corpus (Misra et al., 2016). Section 5 evaluates SBERT on SentEval. In section 6, we perform an ablation study to test some design aspect of SBERT. In section 7, we compare the computational efficiency of SBERT sentence embeddings in contrast to other state-of-the-art sentence embedding methods.",
            "page": 2,
            "x": 68,
            "y": 418,
            "width": 225,
            "height": 137,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "1ea922dd-fd2b-42cc-b334-5191016aba1d",
            "group_text": "# 1 Introduction\n\nIn this publication, we present Senten-  \n(SBERT), a modification of the BERT netwo  \nusing siamese and triplet networks that is ab  \nderive semantically meaningful sentence embe-  \ndings. This enables BERT to be used for certain  \nnew tasks, which up-to-now were not applica-  \nfor BERT. These tasks include large-scale seman-\n\n\u00b9Code available: https://github.com/UKPLab/sentence-transformers\n\n\u00b2With *semantically meaningful* we mean that semantically similar sentences are close in vector space.\n\ntic similarity comparison, clustering, and informa-\ntion retrieval via semantic search.\n\nBERT set new state-of-the-art performance on various sentence classification and sentence-pair regression tasks. BERT uses a cross-encoder: Two sentences are passed to the transformer network and the target value is predicted. However, this setup is unsuitable for various pair regression tasks due to too many possible combinations. Finding in a collection of $n = 10\\,000$ sentences the pair with the highest similarity requires with BERT $n \\cdot (n-1)/2 = 49\\,995\\,000$ inference computations. On a modern V100 GPU, this requires about 65 hours. Similar, finding which of the over 40 million existent questions of Quora is the most similar for a new question could be modeled as a pair-wise comparison with BERT, however, answering a single query would require over 50 hours.\n\nA common method to address clustering and semantic search is to map each sentence to a vector space such that semantically similar sentences are close. Researchers have started to input individual sentences into BERT and to derive fixed-size sentence embeddings. The most commonly used approach is to average the BERT output layer (known as BERT embeddings) or by using the output of the first token (the [CLS] token). As we will show, this common practice yields rather bad sentence embeddings, often worse than averaging GloVe embeddings (Pennington et al., 2014).\n\nTo alleviate this issue, we developed SBERT.\nThe siamese network architecture enables that\nfixed-sized vectors for input sentences can be de-\nrived.  Using a similarity measure like cosine-\nsimilarity or Manhattan / Euclidean distance, se-\nmantically similar sentences can be found.  These\nsimilarity measures can be performed extremely\nefficient on modern hardware, allowing SBERT\nto be used for semantic similarity search as well\nas for clustering.  The complexity for finding the\n\nmost similar sentence pair in a collection of 10,000 sentences is reduced from 65 hours with BERT to the computation of 10,000 sentence embeddings (~5 seconds with SBERT) and computing cosine-similarity (~0.01 seconds). By using optimized index structures, finding the most similar Quora question can be reduced from 50 hours to a few milliseconds (Johnson et al., 2017).\n\nWe fine-tune SBERT on NLI data, which creates sentence embeddings that significantly outperform other state-of-the-art sentence embedding methods like InferSent (Conneau et al., 2017) and Universal Sentence Encoder (Cer et al., 2018). On seven Semantic Textual Similarity (STS) tasks, SBERT achieves an improvement of 11.7 points compared to InferSent and 5.5 points compared to Universal Sentence Encoder. On SentEval (Conneau and Kiela, 2018), an evaluation toolkit for sentence embeddings, we achieve an improvement of 2.1 and 2.6 points, respectively.\n\nSBERT can be adapted to a specific task. It sets new state-of-the-art performance on a challenging argument similarity dataset (Misra et al., 2016) and on a triplet dataset to distinguish sentences from different sections of a Wikipedia article (Dor et al., 2018).\n\nThe paper is structured in the following way: Section 3 presents SBERT, section 4 evaluates SBERT on common STS tasks and on the challenging Argument Facet Similarity (AFS) corpus (Misra et al., 2016). Section 5 evaluates SBERT on SentEval. In section 6, we perform an ablation study to test some design aspect of SBERT. In section 7, we compare the computational efficiency of SBERT sentence embeddings in contrast to other state-of-the-art sentence embedding methods."
        },
        {
            "text": "## 2 Related Work\n\nWe first introduce BERT, then, we discuss state-of-the-art sentence embedding methods.",
            "page": 2,
            "x": 68,
            "y": 564,
            "width": 224,
            "height": 51,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "3-related",
            "chunk_id": "2061d702-1436-4f0f-83c5-b9d01f591ae3",
            "group_text": "## 2 Related Work\n\nWe first introduce BERT, then, we discuss state-of-the-art sentence embedding methods.\n\nBERT (Devlin et al., 2018) is a pre-trained transformer network (Vaswani et al., 2017), which set for various NLP tasks new state-of-the-art results, including question answering, sentence classification, and sentence-pair regression. The input for BERT for sentence-pair regression consists of the two sentences, separated by a special [SEP] token. Multi-head attention over 12 (base-model) or 24 layers (large-model) is applied and the output is passed to a simple regression function to derive the final label. Using this setup, BERT set a\n\nnew state-of-the-art performance on the Semantic\nTextual Similarity (STS) benchmark (Cer et al.,\n2017). RoBERTa (Liu et al., 2019) showed, that\nthe performance of BERT can further improved by\nsmall adaptations to the pre-training process. We\nalso tested XLNet (Yang et al., 2019), but it led in\ngeneral to worse results than BERT.\n\nA large disadvantage of the BERT network structure is that no independent sentence embeddings are computed, which makes it difficult to derive sentence embeddings from BERT. To bypass this limitations, researchers passed single sentences through BERT and then derive a fixed sized vector by either averaging the outputs (similar to average word embeddings) or by using the output of the special CLS token (for example: May et al. (2019); Zhang et al. (2019); Qiao et al. (2019)). These two options are also provided by the popular bert-as-a-service-repository\u00b3. Up to our knowledge, there is so far no evaluation if these methods lead to useful sentence embeddings.\n\nSentence embeddings are a well studied area\nwith dozens of proposed methods. Skip-Thought\n(Kiros et al., 2015) trains an encoder-decoder ar-\nchitecture to predict the surrounding sentences.\nInferSent (Conneau et al., 2017) uses labeled\ndata of the Stanford Natural Language Inference\ndataset (Bowman et al., 2015) and the Multi-\nGenre NLI dataset (Williams et al., 2018) to train\na siamese BiLSTM network with max-pooling\nover the output. Conneau et al. showed, that\nInferSent consistently outperforms unsupervised\nmethods like SkipThought. Universal Sentence\nEncoder (Cer et al., 2018) trains a transformer\nnetwork and augments unsupervised learning with\ntraining on SNLI. Hill et al. (2016) showed, that\nthe task on which sentence embeddings are trained\nsignificantly impacts their quality. Previous work\n(Conneau et al., 2017; Cer et al., 2018) found that\nthe SNLI datasets are suitable for training sen-\ntence embeddings. Yang et al. (2018) presented\na method to train on conversations from Reddit\nusing siamese DAN and siamese transformer net-\nworks, which yielded good results on the STS\nbenchmark dataset.\n\nHumeau et al. (2019) addresses the run-time overhead of the cross-encoder from BERT and present a method (poly-encoders) to compute a score between $m$ context vectors and pre-\n\ncomputed candidate embeddings using attention.\nThis idea works for finding the highest scoring\nsentence in a larger collection. However, poly-\nencoders have the drawback that the score function\nis not symmetric and the computational overhead\nis too large for use-cases like clustering, which\nwould require $O(n^2)$ score computations.\n\nPrevious neural sentence embedding methods\nstarted the training from a random initialization.\nIn this publication, we use the pre-trained BERT\nand RoBERTa network and only fine-tune it to\nyield useful sentence embeddings. This reduces\nsignificantly the needed training time: SBERT can\nbe tuned in less than 20 minutes, while yielding\nbetter results than comparable sentence embed-\nding methods."
        },
        {
            "text": "BERT (Devlin et al., 2018) is a pre-trained transformer network (Vaswani et al., 2017), which set for various NLP tasks new state-of-the-art results, including question answering, sentence classification, and sentence-pair regression. The input for BERT for sentence-pair regression consists of the two sentences, separated by a special [SEP] token. Multi-head attention over 12 (base-model) or 24 layers (large-model) is applied and the output is passed to a simple regression function to derive the final label. Using this setup, BERT set a",
            "page": 2,
            "x": 68,
            "y": 616,
            "width": 225,
            "height": 151,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "3-related",
            "chunk_id": "7f00a906-4acb-4d01-9cb8-d96a3165b82e",
            "group_text": "## 2 Related Work\n\nWe first introduce BERT, then, we discuss state-of-the-art sentence embedding methods.\n\nBERT (Devlin et al., 2018) is a pre-trained transformer network (Vaswani et al., 2017), which set for various NLP tasks new state-of-the-art results, including question answering, sentence classification, and sentence-pair regression. The input for BERT for sentence-pair regression consists of the two sentences, separated by a special [SEP] token. Multi-head attention over 12 (base-model) or 24 layers (large-model) is applied and the output is passed to a simple regression function to derive the final label. Using this setup, BERT set a\n\nnew state-of-the-art performance on the Semantic\nTextual Similarity (STS) benchmark (Cer et al.,\n2017). RoBERTa (Liu et al., 2019) showed, that\nthe performance of BERT can further improved by\nsmall adaptations to the pre-training process. We\nalso tested XLNet (Yang et al., 2019), but it led in\ngeneral to worse results than BERT.\n\nA large disadvantage of the BERT network structure is that no independent sentence embeddings are computed, which makes it difficult to derive sentence embeddings from BERT. To bypass this limitations, researchers passed single sentences through BERT and then derive a fixed sized vector by either averaging the outputs (similar to average word embeddings) or by using the output of the special CLS token (for example: May et al. (2019); Zhang et al. (2019); Qiao et al. (2019)). These two options are also provided by the popular bert-as-a-service-repository\u00b3. Up to our knowledge, there is so far no evaluation if these methods lead to useful sentence embeddings.\n\nSentence embeddings are a well studied area\nwith dozens of proposed methods. Skip-Thought\n(Kiros et al., 2015) trains an encoder-decoder ar-\nchitecture to predict the surrounding sentences.\nInferSent (Conneau et al., 2017) uses labeled\ndata of the Stanford Natural Language Inference\ndataset (Bowman et al., 2015) and the Multi-\nGenre NLI dataset (Williams et al., 2018) to train\na siamese BiLSTM network with max-pooling\nover the output. Conneau et al. showed, that\nInferSent consistently outperforms unsupervised\nmethods like SkipThought. Universal Sentence\nEncoder (Cer et al., 2018) trains a transformer\nnetwork and augments unsupervised learning with\ntraining on SNLI. Hill et al. (2016) showed, that\nthe task on which sentence embeddings are trained\nsignificantly impacts their quality. Previous work\n(Conneau et al., 2017; Cer et al., 2018) found that\nthe SNLI datasets are suitable for training sen-\ntence embeddings. Yang et al. (2018) presented\na method to train on conversations from Reddit\nusing siamese DAN and siamese transformer net-\nworks, which yielded good results on the STS\nbenchmark dataset.\n\nHumeau et al. (2019) addresses the run-time overhead of the cross-encoder from BERT and present a method (poly-encoders) to compute a score between $m$ context vectors and pre-\n\ncomputed candidate embeddings using attention.\nThis idea works for finding the highest scoring\nsentence in a larger collection. However, poly-\nencoders have the drawback that the score function\nis not symmetric and the computational overhead\nis too large for use-cases like clustering, which\nwould require $O(n^2)$ score computations.\n\nPrevious neural sentence embedding methods\nstarted the training from a random initialization.\nIn this publication, we use the pre-trained BERT\nand RoBERTa network and only fine-tune it to\nyield useful sentence embeddings. This reduces\nsignificantly the needed training time: SBERT can\nbe tuned in less than 20 minutes, while yielding\nbetter results than comparable sentence embed-\nding methods."
        },
        {
            "text": "new state-of-the-art performance on the Semantic\nTextual Similarity (STS) benchmark (Cer et al.,\n2017). RoBERTa (Liu et al., 2019) showed, that\nthe performance of BERT can further improved by\nsmall adaptations to the pre-training process. We\nalso tested XLNet (Yang et al., 2019), but it led in\ngeneral to worse results than BERT.",
            "page": 2,
            "x": 303,
            "y": 64,
            "width": 225,
            "height": 94,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "3-related",
            "chunk_id": "30dd41c7-73ea-4a52-b565-2fce802048b0",
            "group_text": "## 2 Related Work\n\nWe first introduce BERT, then, we discuss state-of-the-art sentence embedding methods.\n\nBERT (Devlin et al., 2018) is a pre-trained transformer network (Vaswani et al., 2017), which set for various NLP tasks new state-of-the-art results, including question answering, sentence classification, and sentence-pair regression. The input for BERT for sentence-pair regression consists of the two sentences, separated by a special [SEP] token. Multi-head attention over 12 (base-model) or 24 layers (large-model) is applied and the output is passed to a simple regression function to derive the final label. Using this setup, BERT set a\n\nnew state-of-the-art performance on the Semantic\nTextual Similarity (STS) benchmark (Cer et al.,\n2017). RoBERTa (Liu et al., 2019) showed, that\nthe performance of BERT can further improved by\nsmall adaptations to the pre-training process. We\nalso tested XLNet (Yang et al., 2019), but it led in\ngeneral to worse results than BERT.\n\nA large disadvantage of the BERT network structure is that no independent sentence embeddings are computed, which makes it difficult to derive sentence embeddings from BERT. To bypass this limitations, researchers passed single sentences through BERT and then derive a fixed sized vector by either averaging the outputs (similar to average word embeddings) or by using the output of the special CLS token (for example: May et al. (2019); Zhang et al. (2019); Qiao et al. (2019)). These two options are also provided by the popular bert-as-a-service-repository\u00b3. Up to our knowledge, there is so far no evaluation if these methods lead to useful sentence embeddings.\n\nSentence embeddings are a well studied area\nwith dozens of proposed methods. Skip-Thought\n(Kiros et al., 2015) trains an encoder-decoder ar-\nchitecture to predict the surrounding sentences.\nInferSent (Conneau et al., 2017) uses labeled\ndata of the Stanford Natural Language Inference\ndataset (Bowman et al., 2015) and the Multi-\nGenre NLI dataset (Williams et al., 2018) to train\na siamese BiLSTM network with max-pooling\nover the output. Conneau et al. showed, that\nInferSent consistently outperforms unsupervised\nmethods like SkipThought. Universal Sentence\nEncoder (Cer et al., 2018) trains a transformer\nnetwork and augments unsupervised learning with\ntraining on SNLI. Hill et al. (2016) showed, that\nthe task on which sentence embeddings are trained\nsignificantly impacts their quality. Previous work\n(Conneau et al., 2017; Cer et al., 2018) found that\nthe SNLI datasets are suitable for training sen-\ntence embeddings. Yang et al. (2018) presented\na method to train on conversations from Reddit\nusing siamese DAN and siamese transformer net-\nworks, which yielded good results on the STS\nbenchmark dataset.\n\nHumeau et al. (2019) addresses the run-time overhead of the cross-encoder from BERT and present a method (poly-encoders) to compute a score between $m$ context vectors and pre-\n\ncomputed candidate embeddings using attention.\nThis idea works for finding the highest scoring\nsentence in a larger collection. However, poly-\nencoders have the drawback that the score function\nis not symmetric and the computational overhead\nis too large for use-cases like clustering, which\nwould require $O(n^2)$ score computations.\n\nPrevious neural sentence embedding methods\nstarted the training from a random initialization.\nIn this publication, we use the pre-trained BERT\nand RoBERTa network and only fine-tune it to\nyield useful sentence embeddings. This reduces\nsignificantly the needed training time: SBERT can\nbe tuned in less than 20 minutes, while yielding\nbetter results than comparable sentence embed-\nding methods."
        },
        {
            "text": "A large disadvantage of the BERT network structure is that no independent sentence embeddings are computed, which makes it difficult to derive sentence embeddings from BERT. To bypass this limitations, researchers passed single sentences through BERT and then derive a fixed sized vector by either averaging the outputs (similar to average word embeddings) or by using the output of the special CLS token (for example: May et al. (2019); Zhang et al. (2019); Qiao et al. (2019)). These two options are also provided by the popular bert-as-a-service-repository\u00b3. Up to our knowledge, there is so far no evaluation if these methods lead to useful sentence embeddings.",
            "page": 2,
            "x": 304,
            "y": 160,
            "width": 225,
            "height": 189,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "3-related",
            "chunk_id": "0776f589-c7f6-42b0-9fd8-97795e8af3db",
            "group_text": "## 2 Related Work\n\nWe first introduce BERT, then, we discuss state-of-the-art sentence embedding methods.\n\nBERT (Devlin et al., 2018) is a pre-trained transformer network (Vaswani et al., 2017), which set for various NLP tasks new state-of-the-art results, including question answering, sentence classification, and sentence-pair regression. The input for BERT for sentence-pair regression consists of the two sentences, separated by a special [SEP] token. Multi-head attention over 12 (base-model) or 24 layers (large-model) is applied and the output is passed to a simple regression function to derive the final label. Using this setup, BERT set a\n\nnew state-of-the-art performance on the Semantic\nTextual Similarity (STS) benchmark (Cer et al.,\n2017). RoBERTa (Liu et al., 2019) showed, that\nthe performance of BERT can further improved by\nsmall adaptations to the pre-training process. We\nalso tested XLNet (Yang et al., 2019), but it led in\ngeneral to worse results than BERT.\n\nA large disadvantage of the BERT network structure is that no independent sentence embeddings are computed, which makes it difficult to derive sentence embeddings from BERT. To bypass this limitations, researchers passed single sentences through BERT and then derive a fixed sized vector by either averaging the outputs (similar to average word embeddings) or by using the output of the special CLS token (for example: May et al. (2019); Zhang et al. (2019); Qiao et al. (2019)). These two options are also provided by the popular bert-as-a-service-repository\u00b3. Up to our knowledge, there is so far no evaluation if these methods lead to useful sentence embeddings.\n\nSentence embeddings are a well studied area\nwith dozens of proposed methods. Skip-Thought\n(Kiros et al., 2015) trains an encoder-decoder ar-\nchitecture to predict the surrounding sentences.\nInferSent (Conneau et al., 2017) uses labeled\ndata of the Stanford Natural Language Inference\ndataset (Bowman et al., 2015) and the Multi-\nGenre NLI dataset (Williams et al., 2018) to train\na siamese BiLSTM network with max-pooling\nover the output. Conneau et al. showed, that\nInferSent consistently outperforms unsupervised\nmethods like SkipThought. Universal Sentence\nEncoder (Cer et al., 2018) trains a transformer\nnetwork and augments unsupervised learning with\ntraining on SNLI. Hill et al. (2016) showed, that\nthe task on which sentence embeddings are trained\nsignificantly impacts their quality. Previous work\n(Conneau et al., 2017; Cer et al., 2018) found that\nthe SNLI datasets are suitable for training sen-\ntence embeddings. Yang et al. (2018) presented\na method to train on conversations from Reddit\nusing siamese DAN and siamese transformer net-\nworks, which yielded good results on the STS\nbenchmark dataset.\n\nHumeau et al. (2019) addresses the run-time overhead of the cross-encoder from BERT and present a method (poly-encoders) to compute a score between $m$ context vectors and pre-\n\ncomputed candidate embeddings using attention.\nThis idea works for finding the highest scoring\nsentence in a larger collection. However, poly-\nencoders have the drawback that the score function\nis not symmetric and the computational overhead\nis too large for use-cases like clustering, which\nwould require $O(n^2)$ score computations.\n\nPrevious neural sentence embedding methods\nstarted the training from a random initialization.\nIn this publication, we use the pre-trained BERT\nand RoBERTa network and only fine-tune it to\nyield useful sentence embeddings. This reduces\nsignificantly the needed training time: SBERT can\nbe tuned in less than 20 minutes, while yielding\nbetter results than comparable sentence embed-\nding methods."
        },
        {
            "text": "Sentence embeddings are a well studied area\nwith dozens of proposed methods. Skip-Thought\n(Kiros et al., 2015) trains an encoder-decoder ar-\nchitecture to predict the surrounding sentences.\nInferSent (Conneau et al., 2017) uses labeled\ndata of the Stanford Natural Language Inference\ndataset (Bowman et al., 2015) and the Multi-\nGenre NLI dataset (Williams et al., 2018) to train\na siamese BiLSTM network with max-pooling\nover the output. Conneau et al. showed, that\nInferSent consistently outperforms unsupervised\nmethods like SkipThought. Universal Sentence\nEncoder (Cer et al., 2018) trains a transformer\nnetwork and augments unsupervised learning with\ntraining on SNLI. Hill et al. (2016) showed, that\nthe task on which sentence embeddings are trained\nsignificantly impacts their quality. Previous work\n(Conneau et al., 2017; Cer et al., 2018) found that\nthe SNLI datasets are suitable for training sen-\ntence embeddings. Yang et al. (2018) presented\na method to train on conversations from Reddit\nusing siamese DAN and siamese transformer net-\nworks, which yielded good results on the STS\nbenchmark dataset.",
            "page": 2,
            "x": 303,
            "y": 351,
            "width": 226,
            "height": 325,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "3-related",
            "chunk_id": "a432f063-272a-4b87-9bf9-f1634a8f879e",
            "group_text": "## 2 Related Work\n\nWe first introduce BERT, then, we discuss state-of-the-art sentence embedding methods.\n\nBERT (Devlin et al., 2018) is a pre-trained transformer network (Vaswani et al., 2017), which set for various NLP tasks new state-of-the-art results, including question answering, sentence classification, and sentence-pair regression. The input for BERT for sentence-pair regression consists of the two sentences, separated by a special [SEP] token. Multi-head attention over 12 (base-model) or 24 layers (large-model) is applied and the output is passed to a simple regression function to derive the final label. Using this setup, BERT set a\n\nnew state-of-the-art performance on the Semantic\nTextual Similarity (STS) benchmark (Cer et al.,\n2017). RoBERTa (Liu et al., 2019) showed, that\nthe performance of BERT can further improved by\nsmall adaptations to the pre-training process. We\nalso tested XLNet (Yang et al., 2019), but it led in\ngeneral to worse results than BERT.\n\nA large disadvantage of the BERT network structure is that no independent sentence embeddings are computed, which makes it difficult to derive sentence embeddings from BERT. To bypass this limitations, researchers passed single sentences through BERT and then derive a fixed sized vector by either averaging the outputs (similar to average word embeddings) or by using the output of the special CLS token (for example: May et al. (2019); Zhang et al. (2019); Qiao et al. (2019)). These two options are also provided by the popular bert-as-a-service-repository\u00b3. Up to our knowledge, there is so far no evaluation if these methods lead to useful sentence embeddings.\n\nSentence embeddings are a well studied area\nwith dozens of proposed methods. Skip-Thought\n(Kiros et al., 2015) trains an encoder-decoder ar-\nchitecture to predict the surrounding sentences.\nInferSent (Conneau et al., 2017) uses labeled\ndata of the Stanford Natural Language Inference\ndataset (Bowman et al., 2015) and the Multi-\nGenre NLI dataset (Williams et al., 2018) to train\na siamese BiLSTM network with max-pooling\nover the output. Conneau et al. showed, that\nInferSent consistently outperforms unsupervised\nmethods like SkipThought. Universal Sentence\nEncoder (Cer et al., 2018) trains a transformer\nnetwork and augments unsupervised learning with\ntraining on SNLI. Hill et al. (2016) showed, that\nthe task on which sentence embeddings are trained\nsignificantly impacts their quality. Previous work\n(Conneau et al., 2017; Cer et al., 2018) found that\nthe SNLI datasets are suitable for training sen-\ntence embeddings. Yang et al. (2018) presented\na method to train on conversations from Reddit\nusing siamese DAN and siamese transformer net-\nworks, which yielded good results on the STS\nbenchmark dataset.\n\nHumeau et al. (2019) addresses the run-time overhead of the cross-encoder from BERT and present a method (poly-encoders) to compute a score between $m$ context vectors and pre-\n\ncomputed candidate embeddings using attention.\nThis idea works for finding the highest scoring\nsentence in a larger collection. However, poly-\nencoders have the drawback that the score function\nis not symmetric and the computational overhead\nis too large for use-cases like clustering, which\nwould require $O(n^2)$ score computations.\n\nPrevious neural sentence embedding methods\nstarted the training from a random initialization.\nIn this publication, we use the pre-trained BERT\nand RoBERTa network and only fine-tune it to\nyield useful sentence embeddings. This reduces\nsignificantly the needed training time: SBERT can\nbe tuned in less than 20 minutes, while yielding\nbetter results than comparable sentence embed-\nding methods."
        },
        {
            "text": "Humeau et al. (2019) addresses the run-time overhead of the cross-encoder from BERT and present a method (poly-encoders) to compute a score between $m$ context vectors and pre-",
            "page": 2,
            "x": 303,
            "y": 677,
            "width": 227,
            "height": 57,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "3-related",
            "chunk_id": "e565d4d9-511e-404f-b583-b3995f2845bc",
            "group_text": "## 2 Related Work\n\nWe first introduce BERT, then, we discuss state-of-the-art sentence embedding methods.\n\nBERT (Devlin et al., 2018) is a pre-trained transformer network (Vaswani et al., 2017), which set for various NLP tasks new state-of-the-art results, including question answering, sentence classification, and sentence-pair regression. The input for BERT for sentence-pair regression consists of the two sentences, separated by a special [SEP] token. Multi-head attention over 12 (base-model) or 24 layers (large-model) is applied and the output is passed to a simple regression function to derive the final label. Using this setup, BERT set a\n\nnew state-of-the-art performance on the Semantic\nTextual Similarity (STS) benchmark (Cer et al.,\n2017). RoBERTa (Liu et al., 2019) showed, that\nthe performance of BERT can further improved by\nsmall adaptations to the pre-training process. We\nalso tested XLNet (Yang et al., 2019), but it led in\ngeneral to worse results than BERT.\n\nA large disadvantage of the BERT network structure is that no independent sentence embeddings are computed, which makes it difficult to derive sentence embeddings from BERT. To bypass this limitations, researchers passed single sentences through BERT and then derive a fixed sized vector by either averaging the outputs (similar to average word embeddings) or by using the output of the special CLS token (for example: May et al. (2019); Zhang et al. (2019); Qiao et al. (2019)). These two options are also provided by the popular bert-as-a-service-repository\u00b3. Up to our knowledge, there is so far no evaluation if these methods lead to useful sentence embeddings.\n\nSentence embeddings are a well studied area\nwith dozens of proposed methods. Skip-Thought\n(Kiros et al., 2015) trains an encoder-decoder ar-\nchitecture to predict the surrounding sentences.\nInferSent (Conneau et al., 2017) uses labeled\ndata of the Stanford Natural Language Inference\ndataset (Bowman et al., 2015) and the Multi-\nGenre NLI dataset (Williams et al., 2018) to train\na siamese BiLSTM network with max-pooling\nover the output. Conneau et al. showed, that\nInferSent consistently outperforms unsupervised\nmethods like SkipThought. Universal Sentence\nEncoder (Cer et al., 2018) trains a transformer\nnetwork and augments unsupervised learning with\ntraining on SNLI. Hill et al. (2016) showed, that\nthe task on which sentence embeddings are trained\nsignificantly impacts their quality. Previous work\n(Conneau et al., 2017; Cer et al., 2018) found that\nthe SNLI datasets are suitable for training sen-\ntence embeddings. Yang et al. (2018) presented\na method to train on conversations from Reddit\nusing siamese DAN and siamese transformer net-\nworks, which yielded good results on the STS\nbenchmark dataset.\n\nHumeau et al. (2019) addresses the run-time overhead of the cross-encoder from BERT and present a method (poly-encoders) to compute a score between $m$ context vectors and pre-\n\ncomputed candidate embeddings using attention.\nThis idea works for finding the highest scoring\nsentence in a larger collection. However, poly-\nencoders have the drawback that the score function\nis not symmetric and the computational overhead\nis too large for use-cases like clustering, which\nwould require $O(n^2)$ score computations.\n\nPrevious neural sentence embedding methods\nstarted the training from a random initialization.\nIn this publication, we use the pre-trained BERT\nand RoBERTa network and only fine-tune it to\nyield useful sentence embeddings. This reduces\nsignificantly the needed training time: SBERT can\nbe tuned in less than 20 minutes, while yielding\nbetter results than comparable sentence embed-\nding methods."
        },
        {
            "text": "computed candidate embeddings using attention.\nThis idea works for finding the highest scoring\nsentence in a larger collection. However, poly-\nencoders have the drawback that the score function\nis not symmetric and the computational overhead\nis too large for use-cases like clustering, which\nwould require $O(n^2)$ score computations.",
            "page": 3,
            "x": 67,
            "y": 318,
            "width": 226,
            "height": 96,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "3-related",
            "chunk_id": "e9adb86f-57dc-408e-83cf-315392a2b239",
            "group_text": "## 2 Related Work\n\nWe first introduce BERT, then, we discuss state-of-the-art sentence embedding methods.\n\nBERT (Devlin et al., 2018) is a pre-trained transformer network (Vaswani et al., 2017), which set for various NLP tasks new state-of-the-art results, including question answering, sentence classification, and sentence-pair regression. The input for BERT for sentence-pair regression consists of the two sentences, separated by a special [SEP] token. Multi-head attention over 12 (base-model) or 24 layers (large-model) is applied and the output is passed to a simple regression function to derive the final label. Using this setup, BERT set a\n\nnew state-of-the-art performance on the Semantic\nTextual Similarity (STS) benchmark (Cer et al.,\n2017). RoBERTa (Liu et al., 2019) showed, that\nthe performance of BERT can further improved by\nsmall adaptations to the pre-training process. We\nalso tested XLNet (Yang et al., 2019), but it led in\ngeneral to worse results than BERT.\n\nA large disadvantage of the BERT network structure is that no independent sentence embeddings are computed, which makes it difficult to derive sentence embeddings from BERT. To bypass this limitations, researchers passed single sentences through BERT and then derive a fixed sized vector by either averaging the outputs (similar to average word embeddings) or by using the output of the special CLS token (for example: May et al. (2019); Zhang et al. (2019); Qiao et al. (2019)). These two options are also provided by the popular bert-as-a-service-repository\u00b3. Up to our knowledge, there is so far no evaluation if these methods lead to useful sentence embeddings.\n\nSentence embeddings are a well studied area\nwith dozens of proposed methods. Skip-Thought\n(Kiros et al., 2015) trains an encoder-decoder ar-\nchitecture to predict the surrounding sentences.\nInferSent (Conneau et al., 2017) uses labeled\ndata of the Stanford Natural Language Inference\ndataset (Bowman et al., 2015) and the Multi-\nGenre NLI dataset (Williams et al., 2018) to train\na siamese BiLSTM network with max-pooling\nover the output. Conneau et al. showed, that\nInferSent consistently outperforms unsupervised\nmethods like SkipThought. Universal Sentence\nEncoder (Cer et al., 2018) trains a transformer\nnetwork and augments unsupervised learning with\ntraining on SNLI. Hill et al. (2016) showed, that\nthe task on which sentence embeddings are trained\nsignificantly impacts their quality. Previous work\n(Conneau et al., 2017; Cer et al., 2018) found that\nthe SNLI datasets are suitable for training sen-\ntence embeddings. Yang et al. (2018) presented\na method to train on conversations from Reddit\nusing siamese DAN and siamese transformer net-\nworks, which yielded good results on the STS\nbenchmark dataset.\n\nHumeau et al. (2019) addresses the run-time overhead of the cross-encoder from BERT and present a method (poly-encoders) to compute a score between $m$ context vectors and pre-\n\ncomputed candidate embeddings using attention.\nThis idea works for finding the highest scoring\nsentence in a larger collection. However, poly-\nencoders have the drawback that the score function\nis not symmetric and the computational overhead\nis too large for use-cases like clustering, which\nwould require $O(n^2)$ score computations.\n\nPrevious neural sentence embedding methods\nstarted the training from a random initialization.\nIn this publication, we use the pre-trained BERT\nand RoBERTa network and only fine-tune it to\nyield useful sentence embeddings. This reduces\nsignificantly the needed training time: SBERT can\nbe tuned in less than 20 minutes, while yielding\nbetter results than comparable sentence embed-\nding methods."
        },
        {
            "text": "Previous neural sentence embedding methods\nstarted the training from a random initialization.\nIn this publication, we use the pre-trained BERT\nand RoBERTa network and only fine-tune it to\nyield useful sentence embeddings. This reduces\nsignificantly the needed training time: SBERT can\nbe tuned in less than 20 minutes, while yielding\nbetter results than comparable sentence embed-\nding methods.",
            "page": 3,
            "x": 68,
            "y": 415,
            "width": 225,
            "height": 123,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "3-related",
            "chunk_id": "5121ddd1-f881-4a63-840a-5080ba3ab83e",
            "group_text": "## 2 Related Work\n\nWe first introduce BERT, then, we discuss state-of-the-art sentence embedding methods.\n\nBERT (Devlin et al., 2018) is a pre-trained transformer network (Vaswani et al., 2017), which set for various NLP tasks new state-of-the-art results, including question answering, sentence classification, and sentence-pair regression. The input for BERT for sentence-pair regression consists of the two sentences, separated by a special [SEP] token. Multi-head attention over 12 (base-model) or 24 layers (large-model) is applied and the output is passed to a simple regression function to derive the final label. Using this setup, BERT set a\n\nnew state-of-the-art performance on the Semantic\nTextual Similarity (STS) benchmark (Cer et al.,\n2017). RoBERTa (Liu et al., 2019) showed, that\nthe performance of BERT can further improved by\nsmall adaptations to the pre-training process. We\nalso tested XLNet (Yang et al., 2019), but it led in\ngeneral to worse results than BERT.\n\nA large disadvantage of the BERT network structure is that no independent sentence embeddings are computed, which makes it difficult to derive sentence embeddings from BERT. To bypass this limitations, researchers passed single sentences through BERT and then derive a fixed sized vector by either averaging the outputs (similar to average word embeddings) or by using the output of the special CLS token (for example: May et al. (2019); Zhang et al. (2019); Qiao et al. (2019)). These two options are also provided by the popular bert-as-a-service-repository\u00b3. Up to our knowledge, there is so far no evaluation if these methods lead to useful sentence embeddings.\n\nSentence embeddings are a well studied area\nwith dozens of proposed methods. Skip-Thought\n(Kiros et al., 2015) trains an encoder-decoder ar-\nchitecture to predict the surrounding sentences.\nInferSent (Conneau et al., 2017) uses labeled\ndata of the Stanford Natural Language Inference\ndataset (Bowman et al., 2015) and the Multi-\nGenre NLI dataset (Williams et al., 2018) to train\na siamese BiLSTM network with max-pooling\nover the output. Conneau et al. showed, that\nInferSent consistently outperforms unsupervised\nmethods like SkipThought. Universal Sentence\nEncoder (Cer et al., 2018) trains a transformer\nnetwork and augments unsupervised learning with\ntraining on SNLI. Hill et al. (2016) showed, that\nthe task on which sentence embeddings are trained\nsignificantly impacts their quality. Previous work\n(Conneau et al., 2017; Cer et al., 2018) found that\nthe SNLI datasets are suitable for training sen-\ntence embeddings. Yang et al. (2018) presented\na method to train on conversations from Reddit\nusing siamese DAN and siamese transformer net-\nworks, which yielded good results on the STS\nbenchmark dataset.\n\nHumeau et al. (2019) addresses the run-time overhead of the cross-encoder from BERT and present a method (poly-encoders) to compute a score between $m$ context vectors and pre-\n\ncomputed candidate embeddings using attention.\nThis idea works for finding the highest scoring\nsentence in a larger collection. However, poly-\nencoders have the drawback that the score function\nis not symmetric and the computational overhead\nis too large for use-cases like clustering, which\nwould require $O(n^2)$ score computations.\n\nPrevious neural sentence embedding methods\nstarted the training from a random initialization.\nIn this publication, we use the pre-trained BERT\nand RoBERTa network and only fine-tune it to\nyield useful sentence embeddings. This reduces\nsignificantly the needed training time: SBERT can\nbe tuned in less than 20 minutes, while yielding\nbetter results than comparable sentence embed-\nding methods."
        },
        {
            "text": "### 3   Model\n\nSBERT adds a pooling operation to the output of BERT / RoBERTa to derive a fixed sized sentence embedding. We experiment with three pooling strategies: Using the output of the CLS-token, computing the mean of all output vectors (MEAN-strategy), and computing a max-over-time of the output vectors (MAX-strategy). The default configuration is MEAN.",
            "page": 3,
            "x": 67,
            "y": 548,
            "width": 226,
            "height": 135,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "4-model",
            "chunk_id": "f3131d73-49be-477e-b374-40c4a326d73a",
            "group_text": "### 3   Model\n\nSBERT adds a pooling operation to the output of BERT / RoBERTa to derive a fixed sized sentence embedding. We experiment with three pooling strategies: Using the output of the CLS-token, computing the mean of all output vectors (MEAN-strategy), and computing a max-over-time of the output vectors (MAX-strategy). The default configuration is MEAN.\n\nIn order to fine-tune BERT / RoBERTa, we create siamese and triplet networks (Schroff et al., 2015) to update the weights such that the produced sentence embeddings are semantically meaningful and can be compared with cosine-similarity.\n\nThe network structure depends on the available\n\ntraining data.  We experiment with the following\nstructures and objective functions.\n\n**Classification Objective Function.** We concatenate the sentence embeddings $u$ and $v$ with the element-wise difference $|u-v|$ and multiply it with the trainable weight $W_t \\in \\mathbb{R}^{3n \\times k}$.\n\n$o = \\mathrm{softmax}(W_t(u, v, |u - v|))$\n\nwhere $n$ is the dimension of the sentence embeddings and $k$ the number of labels. We optimize cross-entropy loss. This structure is depicted in Figure 1.\n\n**Regression Objective Function.** The cosine-similarity between the two sentence embeddings *u* and *v* is computed (Figure 2). We use mean-squared-error loss as the objective function.\n\n**Triplet Objective Function.** Given an anchor sentence $a$, a positive sentence $p$, and a negative sentence $n$, triplet loss tunes the network such that the distance between $a$ and $p$ is smaller than the distance between $a$ and $n$. Mathematically, we minimize the following loss function:\n\n$max(\\lVert s_a - s_p \\rVert - \\lVert s_a - s_n \\rVert + \\epsilon, 0)$\n\nwith $s_x$ the sentence embedding for $a/n/p$, $||\\cdot||$ a distance metric and margin $\\epsilon$. Margin $\\epsilon$ ensures that $s_p$ is at least $\\epsilon$ closer to $s_a$ than $s_n$. As metric we use Euclidean distance and we set $\\epsilon = 1$ in our experiments."
        },
        {
            "text": "In order to fine-tune BERT / RoBERTa, we create siamese and triplet networks (Schroff et al., 2015) to update the weights such that the produced sentence embeddings are semantically meaningful and can be compared with cosine-similarity.",
            "page": 3,
            "x": 68,
            "y": 684,
            "width": 226,
            "height": 67,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "4-model",
            "chunk_id": "bb1a13b8-0458-4a0e-978e-237250bfcb39",
            "group_text": "### 3   Model\n\nSBERT adds a pooling operation to the output of BERT / RoBERTa to derive a fixed sized sentence embedding. We experiment with three pooling strategies: Using the output of the CLS-token, computing the mean of all output vectors (MEAN-strategy), and computing a max-over-time of the output vectors (MAX-strategy). The default configuration is MEAN.\n\nIn order to fine-tune BERT / RoBERTa, we create siamese and triplet networks (Schroff et al., 2015) to update the weights such that the produced sentence embeddings are semantically meaningful and can be compared with cosine-similarity.\n\nThe network structure depends on the available\n\ntraining data.  We experiment with the following\nstructures and objective functions.\n\n**Classification Objective Function.** We concatenate the sentence embeddings $u$ and $v$ with the element-wise difference $|u-v|$ and multiply it with the trainable weight $W_t \\in \\mathbb{R}^{3n \\times k}$.\n\n$o = \\mathrm{softmax}(W_t(u, v, |u - v|))$\n\nwhere $n$ is the dimension of the sentence embeddings and $k$ the number of labels. We optimize cross-entropy loss. This structure is depicted in Figure 1.\n\n**Regression Objective Function.** The cosine-similarity between the two sentence embeddings *u* and *v* is computed (Figure 2). We use mean-squared-error loss as the objective function.\n\n**Triplet Objective Function.** Given an anchor sentence $a$, a positive sentence $p$, and a negative sentence $n$, triplet loss tunes the network such that the distance between $a$ and $p$ is smaller than the distance between $a$ and $n$. Mathematically, we minimize the following loss function:\n\n$max(\\lVert s_a - s_p \\rVert - \\lVert s_a - s_n \\rVert + \\epsilon, 0)$\n\nwith $s_x$ the sentence embedding for $a/n/p$, $||\\cdot||$ a distance metric and margin $\\epsilon$. Margin $\\epsilon$ ensures that $s_p$ is at least $\\epsilon$ closer to $s_a$ than $s_n$. As metric we use Euclidean distance and we set $\\epsilon = 1$ in our experiments."
        },
        {
            "text": "The network structure depends on the available",
            "page": 3,
            "x": 80,
            "y": 753,
            "width": 213,
            "height": 13,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "4-model",
            "chunk_id": "0ed4b051-02b8-4244-a70f-3f7a45f73c03",
            "group_text": "### 3   Model\n\nSBERT adds a pooling operation to the output of BERT / RoBERTa to derive a fixed sized sentence embedding. We experiment with three pooling strategies: Using the output of the CLS-token, computing the mean of all output vectors (MEAN-strategy), and computing a max-over-time of the output vectors (MAX-strategy). The default configuration is MEAN.\n\nIn order to fine-tune BERT / RoBERTa, we create siamese and triplet networks (Schroff et al., 2015) to update the weights such that the produced sentence embeddings are semantically meaningful and can be compared with cosine-similarity.\n\nThe network structure depends on the available\n\ntraining data.  We experiment with the following\nstructures and objective functions.\n\n**Classification Objective Function.** We concatenate the sentence embeddings $u$ and $v$ with the element-wise difference $|u-v|$ and multiply it with the trainable weight $W_t \\in \\mathbb{R}^{3n \\times k}$.\n\n$o = \\mathrm{softmax}(W_t(u, v, |u - v|))$\n\nwhere $n$ is the dimension of the sentence embeddings and $k$ the number of labels. We optimize cross-entropy loss. This structure is depicted in Figure 1.\n\n**Regression Objective Function.** The cosine-similarity between the two sentence embeddings *u* and *v* is computed (Figure 2). We use mean-squared-error loss as the objective function.\n\n**Triplet Objective Function.** Given an anchor sentence $a$, a positive sentence $p$, and a negative sentence $n$, triplet loss tunes the network such that the distance between $a$ and $p$ is smaller than the distance between $a$ and $n$. Mathematically, we minimize the following loss function:\n\n$max(\\lVert s_a - s_p \\rVert - \\lVert s_a - s_n \\rVert + \\epsilon, 0)$\n\nwith $s_x$ the sentence embedding for $a/n/p$, $||\\cdot||$ a distance metric and margin $\\epsilon$. Margin $\\epsilon$ ensures that $s_p$ is at least $\\epsilon$ closer to $s_a$ than $s_n$. As metric we use Euclidean distance and we set $\\epsilon = 1$ in our experiments."
        },
        {
            "text": "training data.  We experiment with the following\nstructures and objective functions.",
            "page": 3,
            "x": 304,
            "y": 305,
            "width": 223,
            "height": 26,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "4-model",
            "chunk_id": "32a8c6d4-f583-459f-9f4e-c884dd73cdab",
            "group_text": "### 3   Model\n\nSBERT adds a pooling operation to the output of BERT / RoBERTa to derive a fixed sized sentence embedding. We experiment with three pooling strategies: Using the output of the CLS-token, computing the mean of all output vectors (MEAN-strategy), and computing a max-over-time of the output vectors (MAX-strategy). The default configuration is MEAN.\n\nIn order to fine-tune BERT / RoBERTa, we create siamese and triplet networks (Schroff et al., 2015) to update the weights such that the produced sentence embeddings are semantically meaningful and can be compared with cosine-similarity.\n\nThe network structure depends on the available\n\ntraining data.  We experiment with the following\nstructures and objective functions.\n\n**Classification Objective Function.** We concatenate the sentence embeddings $u$ and $v$ with the element-wise difference $|u-v|$ and multiply it with the trainable weight $W_t \\in \\mathbb{R}^{3n \\times k}$.\n\n$o = \\mathrm{softmax}(W_t(u, v, |u - v|))$\n\nwhere $n$ is the dimension of the sentence embeddings and $k$ the number of labels. We optimize cross-entropy loss. This structure is depicted in Figure 1.\n\n**Regression Objective Function.** The cosine-similarity between the two sentence embeddings *u* and *v* is computed (Figure 2). We use mean-squared-error loss as the objective function.\n\n**Triplet Objective Function.** Given an anchor sentence $a$, a positive sentence $p$, and a negative sentence $n$, triplet loss tunes the network such that the distance between $a$ and $p$ is smaller than the distance between $a$ and $n$. Mathematically, we minimize the following loss function:\n\n$max(\\lVert s_a - s_p \\rVert - \\lVert s_a - s_n \\rVert + \\epsilon, 0)$\n\nwith $s_x$ the sentence embedding for $a/n/p$, $||\\cdot||$ a distance metric and margin $\\epsilon$. Margin $\\epsilon$ ensures that $s_p$ is at least $\\epsilon$ closer to $s_a$ than $s_n$. As metric we use Euclidean distance and we set $\\epsilon = 1$ in our experiments."
        },
        {
            "text": "**Classification Objective Function.** We concatenate the sentence embeddings $u$ and $v$ with the element-wise difference $|u-v|$ and multiply it with the trainable weight $W_t \\in \\mathbb{R}^{3n \\times k}$.",
            "page": 3,
            "x": 304,
            "y": 333,
            "width": 224,
            "height": 53,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "4-model",
            "chunk_id": "bd941715-9356-46a1-a3e2-67d9f4035418",
            "group_text": "### 3   Model\n\nSBERT adds a pooling operation to the output of BERT / RoBERTa to derive a fixed sized sentence embedding. We experiment with three pooling strategies: Using the output of the CLS-token, computing the mean of all output vectors (MEAN-strategy), and computing a max-over-time of the output vectors (MAX-strategy). The default configuration is MEAN.\n\nIn order to fine-tune BERT / RoBERTa, we create siamese and triplet networks (Schroff et al., 2015) to update the weights such that the produced sentence embeddings are semantically meaningful and can be compared with cosine-similarity.\n\nThe network structure depends on the available\n\ntraining data.  We experiment with the following\nstructures and objective functions.\n\n**Classification Objective Function.** We concatenate the sentence embeddings $u$ and $v$ with the element-wise difference $|u-v|$ and multiply it with the trainable weight $W_t \\in \\mathbb{R}^{3n \\times k}$.\n\n$o = \\mathrm{softmax}(W_t(u, v, |u - v|))$\n\nwhere $n$ is the dimension of the sentence embeddings and $k$ the number of labels. We optimize cross-entropy loss. This structure is depicted in Figure 1.\n\n**Regression Objective Function.** The cosine-similarity between the two sentence embeddings *u* and *v* is computed (Figure 2). We use mean-squared-error loss as the objective function.\n\n**Triplet Objective Function.** Given an anchor sentence $a$, a positive sentence $p$, and a negative sentence $n$, triplet loss tunes the network such that the distance between $a$ and $p$ is smaller than the distance between $a$ and $n$. Mathematically, we minimize the following loss function:\n\n$max(\\lVert s_a - s_p \\rVert - \\lVert s_a - s_n \\rVert + \\epsilon, 0)$\n\nwith $s_x$ the sentence embedding for $a/n/p$, $||\\cdot||$ a distance metric and margin $\\epsilon$. Margin $\\epsilon$ ensures that $s_p$ is at least $\\epsilon$ closer to $s_a$ than $s_n$. As metric we use Euclidean distance and we set $\\epsilon = 1$ in our experiments."
        },
        {
            "text": "$o = \\mathrm{softmax}(W_t(u, v, |u - v|))$",
            "page": 3,
            "x": 343,
            "y": 396,
            "width": 145,
            "height": 17,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "4-model",
            "chunk_id": "4307160f-caa6-413a-a3b1-d8f2ad0d818b",
            "group_text": "### 3   Model\n\nSBERT adds a pooling operation to the output of BERT / RoBERTa to derive a fixed sized sentence embedding. We experiment with three pooling strategies: Using the output of the CLS-token, computing the mean of all output vectors (MEAN-strategy), and computing a max-over-time of the output vectors (MAX-strategy). The default configuration is MEAN.\n\nIn order to fine-tune BERT / RoBERTa, we create siamese and triplet networks (Schroff et al., 2015) to update the weights such that the produced sentence embeddings are semantically meaningful and can be compared with cosine-similarity.\n\nThe network structure depends on the available\n\ntraining data.  We experiment with the following\nstructures and objective functions.\n\n**Classification Objective Function.** We concatenate the sentence embeddings $u$ and $v$ with the element-wise difference $|u-v|$ and multiply it with the trainable weight $W_t \\in \\mathbb{R}^{3n \\times k}$.\n\n$o = \\mathrm{softmax}(W_t(u, v, |u - v|))$\n\nwhere $n$ is the dimension of the sentence embeddings and $k$ the number of labels. We optimize cross-entropy loss. This structure is depicted in Figure 1.\n\n**Regression Objective Function.** The cosine-similarity between the two sentence embeddings *u* and *v* is computed (Figure 2). We use mean-squared-error loss as the objective function.\n\n**Triplet Objective Function.** Given an anchor sentence $a$, a positive sentence $p$, and a negative sentence $n$, triplet loss tunes the network such that the distance between $a$ and $p$ is smaller than the distance between $a$ and $n$. Mathematically, we minimize the following loss function:\n\n$max(\\lVert s_a - s_p \\rVert - \\lVert s_a - s_n \\rVert + \\epsilon, 0)$\n\nwith $s_x$ the sentence embedding for $a/n/p$, $||\\cdot||$ a distance metric and margin $\\epsilon$. Margin $\\epsilon$ ensures that $s_p$ is at least $\\epsilon$ closer to $s_a$ than $s_n$. As metric we use Euclidean distance and we set $\\epsilon = 1$ in our experiments."
        },
        {
            "text": "where $n$ is the dimension of the sentence embeddings and $k$ the number of labels. We optimize cross-entropy loss. This structure is depicted in Figure 1.",
            "page": 3,
            "x": 304,
            "y": 421,
            "width": 224,
            "height": 53,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "4-model",
            "chunk_id": "08ad4f25-bbde-472f-8ab7-6ef543457085",
            "group_text": "### 3   Model\n\nSBERT adds a pooling operation to the output of BERT / RoBERTa to derive a fixed sized sentence embedding. We experiment with three pooling strategies: Using the output of the CLS-token, computing the mean of all output vectors (MEAN-strategy), and computing a max-over-time of the output vectors (MAX-strategy). The default configuration is MEAN.\n\nIn order to fine-tune BERT / RoBERTa, we create siamese and triplet networks (Schroff et al., 2015) to update the weights such that the produced sentence embeddings are semantically meaningful and can be compared with cosine-similarity.\n\nThe network structure depends on the available\n\ntraining data.  We experiment with the following\nstructures and objective functions.\n\n**Classification Objective Function.** We concatenate the sentence embeddings $u$ and $v$ with the element-wise difference $|u-v|$ and multiply it with the trainable weight $W_t \\in \\mathbb{R}^{3n \\times k}$.\n\n$o = \\mathrm{softmax}(W_t(u, v, |u - v|))$\n\nwhere $n$ is the dimension of the sentence embeddings and $k$ the number of labels. We optimize cross-entropy loss. This structure is depicted in Figure 1.\n\n**Regression Objective Function.** The cosine-similarity between the two sentence embeddings *u* and *v* is computed (Figure 2). We use mean-squared-error loss as the objective function.\n\n**Triplet Objective Function.** Given an anchor sentence $a$, a positive sentence $p$, and a negative sentence $n$, triplet loss tunes the network such that the distance between $a$ and $p$ is smaller than the distance between $a$ and $n$. Mathematically, we minimize the following loss function:\n\n$max(\\lVert s_a - s_p \\rVert - \\lVert s_a - s_n \\rVert + \\epsilon, 0)$\n\nwith $s_x$ the sentence embedding for $a/n/p$, $||\\cdot||$ a distance metric and margin $\\epsilon$. Margin $\\epsilon$ ensures that $s_p$ is at least $\\epsilon$ closer to $s_a$ than $s_n$. As metric we use Euclidean distance and we set $\\epsilon = 1$ in our experiments."
        },
        {
            "text": "**Regression Objective Function.** The cosine-similarity between the two sentence embeddings *u* and *v* is computed (Figure 2). We use mean-squared-error loss as the objective function.",
            "page": 3,
            "x": 304,
            "y": 475,
            "width": 223,
            "height": 54,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "4-model",
            "chunk_id": "022da1df-2cea-4c93-80cc-6e90e47be67a",
            "group_text": "### 3   Model\n\nSBERT adds a pooling operation to the output of BERT / RoBERTa to derive a fixed sized sentence embedding. We experiment with three pooling strategies: Using the output of the CLS-token, computing the mean of all output vectors (MEAN-strategy), and computing a max-over-time of the output vectors (MAX-strategy). The default configuration is MEAN.\n\nIn order to fine-tune BERT / RoBERTa, we create siamese and triplet networks (Schroff et al., 2015) to update the weights such that the produced sentence embeddings are semantically meaningful and can be compared with cosine-similarity.\n\nThe network structure depends on the available\n\ntraining data.  We experiment with the following\nstructures and objective functions.\n\n**Classification Objective Function.** We concatenate the sentence embeddings $u$ and $v$ with the element-wise difference $|u-v|$ and multiply it with the trainable weight $W_t \\in \\mathbb{R}^{3n \\times k}$.\n\n$o = \\mathrm{softmax}(W_t(u, v, |u - v|))$\n\nwhere $n$ is the dimension of the sentence embeddings and $k$ the number of labels. We optimize cross-entropy loss. This structure is depicted in Figure 1.\n\n**Regression Objective Function.** The cosine-similarity between the two sentence embeddings *u* and *v* is computed (Figure 2). We use mean-squared-error loss as the objective function.\n\n**Triplet Objective Function.** Given an anchor sentence $a$, a positive sentence $p$, and a negative sentence $n$, triplet loss tunes the network such that the distance between $a$ and $p$ is smaller than the distance between $a$ and $n$. Mathematically, we minimize the following loss function:\n\n$max(\\lVert s_a - s_p \\rVert - \\lVert s_a - s_n \\rVert + \\epsilon, 0)$\n\nwith $s_x$ the sentence embedding for $a/n/p$, $||\\cdot||$ a distance metric and margin $\\epsilon$. Margin $\\epsilon$ ensures that $s_p$ is at least $\\epsilon$ closer to $s_a$ than $s_n$. As metric we use Euclidean distance and we set $\\epsilon = 1$ in our experiments."
        },
        {
            "text": "**Triplet Objective Function.** Given an anchor sentence $a$, a positive sentence $p$, and a negative sentence $n$, triplet loss tunes the network such that the distance between $a$ and $p$ is smaller than the distance between $a$ and $n$. Mathematically, we minimize the following loss function:",
            "page": 3,
            "x": 304,
            "y": 530,
            "width": 224,
            "height": 80,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "4-model",
            "chunk_id": "2d49e55a-1d90-4beb-b6e4-08783310d8cc",
            "group_text": "### 3   Model\n\nSBERT adds a pooling operation to the output of BERT / RoBERTa to derive a fixed sized sentence embedding. We experiment with three pooling strategies: Using the output of the CLS-token, computing the mean of all output vectors (MEAN-strategy), and computing a max-over-time of the output vectors (MAX-strategy). The default configuration is MEAN.\n\nIn order to fine-tune BERT / RoBERTa, we create siamese and triplet networks (Schroff et al., 2015) to update the weights such that the produced sentence embeddings are semantically meaningful and can be compared with cosine-similarity.\n\nThe network structure depends on the available\n\ntraining data.  We experiment with the following\nstructures and objective functions.\n\n**Classification Objective Function.** We concatenate the sentence embeddings $u$ and $v$ with the element-wise difference $|u-v|$ and multiply it with the trainable weight $W_t \\in \\mathbb{R}^{3n \\times k}$.\n\n$o = \\mathrm{softmax}(W_t(u, v, |u - v|))$\n\nwhere $n$ is the dimension of the sentence embeddings and $k$ the number of labels. We optimize cross-entropy loss. This structure is depicted in Figure 1.\n\n**Regression Objective Function.** The cosine-similarity between the two sentence embeddings *u* and *v* is computed (Figure 2). We use mean-squared-error loss as the objective function.\n\n**Triplet Objective Function.** Given an anchor sentence $a$, a positive sentence $p$, and a negative sentence $n$, triplet loss tunes the network such that the distance between $a$ and $p$ is smaller than the distance between $a$ and $n$. Mathematically, we minimize the following loss function:\n\n$max(\\lVert s_a - s_p \\rVert - \\lVert s_a - s_n \\rVert + \\epsilon, 0)$\n\nwith $s_x$ the sentence embedding for $a/n/p$, $||\\cdot||$ a distance metric and margin $\\epsilon$. Margin $\\epsilon$ ensures that $s_p$ is at least $\\epsilon$ closer to $s_a$ than $s_n$. As metric we use Euclidean distance and we set $\\epsilon = 1$ in our experiments."
        },
        {
            "text": "$max(\\lVert s_a - s_p \\rVert - \\lVert s_a - s_n \\rVert + \\epsilon, 0)$",
            "page": 3,
            "x": 332,
            "y": 619,
            "width": 168,
            "height": 18,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "4-model",
            "chunk_id": "f40ac798-180b-489c-bc44-461bf6b2ab71",
            "group_text": "### 3   Model\n\nSBERT adds a pooling operation to the output of BERT / RoBERTa to derive a fixed sized sentence embedding. We experiment with three pooling strategies: Using the output of the CLS-token, computing the mean of all output vectors (MEAN-strategy), and computing a max-over-time of the output vectors (MAX-strategy). The default configuration is MEAN.\n\nIn order to fine-tune BERT / RoBERTa, we create siamese and triplet networks (Schroff et al., 2015) to update the weights such that the produced sentence embeddings are semantically meaningful and can be compared with cosine-similarity.\n\nThe network structure depends on the available\n\ntraining data.  We experiment with the following\nstructures and objective functions.\n\n**Classification Objective Function.** We concatenate the sentence embeddings $u$ and $v$ with the element-wise difference $|u-v|$ and multiply it with the trainable weight $W_t \\in \\mathbb{R}^{3n \\times k}$.\n\n$o = \\mathrm{softmax}(W_t(u, v, |u - v|))$\n\nwhere $n$ is the dimension of the sentence embeddings and $k$ the number of labels. We optimize cross-entropy loss. This structure is depicted in Figure 1.\n\n**Regression Objective Function.** The cosine-similarity between the two sentence embeddings *u* and *v* is computed (Figure 2). We use mean-squared-error loss as the objective function.\n\n**Triplet Objective Function.** Given an anchor sentence $a$, a positive sentence $p$, and a negative sentence $n$, triplet loss tunes the network such that the distance between $a$ and $p$ is smaller than the distance between $a$ and $n$. Mathematically, we minimize the following loss function:\n\n$max(\\lVert s_a - s_p \\rVert - \\lVert s_a - s_n \\rVert + \\epsilon, 0)$\n\nwith $s_x$ the sentence embedding for $a/n/p$, $||\\cdot||$ a distance metric and margin $\\epsilon$. Margin $\\epsilon$ ensures that $s_p$ is at least $\\epsilon$ closer to $s_a$ than $s_n$. As metric we use Euclidean distance and we set $\\epsilon = 1$ in our experiments."
        },
        {
            "text": "with $s_x$ the sentence embedding for $a/n/p$, $||\\cdot||$ a distance metric and margin $\\epsilon$. Margin $\\epsilon$ ensures that $s_p$ is at least $\\epsilon$ closer to $s_a$ than $s_n$. As metric we use Euclidean distance and we set $\\epsilon = 1$ in our experiments.",
            "page": 3,
            "x": 304,
            "y": 644,
            "width": 225,
            "height": 68,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "4-model",
            "chunk_id": "1d3062fc-da98-4686-8ae0-1f90bf3228e6",
            "group_text": "### 3   Model\n\nSBERT adds a pooling operation to the output of BERT / RoBERTa to derive a fixed sized sentence embedding. We experiment with three pooling strategies: Using the output of the CLS-token, computing the mean of all output vectors (MEAN-strategy), and computing a max-over-time of the output vectors (MAX-strategy). The default configuration is MEAN.\n\nIn order to fine-tune BERT / RoBERTa, we create siamese and triplet networks (Schroff et al., 2015) to update the weights such that the produced sentence embeddings are semantically meaningful and can be compared with cosine-similarity.\n\nThe network structure depends on the available\n\ntraining data.  We experiment with the following\nstructures and objective functions.\n\n**Classification Objective Function.** We concatenate the sentence embeddings $u$ and $v$ with the element-wise difference $|u-v|$ and multiply it with the trainable weight $W_t \\in \\mathbb{R}^{3n \\times k}$.\n\n$o = \\mathrm{softmax}(W_t(u, v, |u - v|))$\n\nwhere $n$ is the dimension of the sentence embeddings and $k$ the number of labels. We optimize cross-entropy loss. This structure is depicted in Figure 1.\n\n**Regression Objective Function.** The cosine-similarity between the two sentence embeddings *u* and *v* is computed (Figure 2). We use mean-squared-error loss as the objective function.\n\n**Triplet Objective Function.** Given an anchor sentence $a$, a positive sentence $p$, and a negative sentence $n$, triplet loss tunes the network such that the distance between $a$ and $p$ is smaller than the distance between $a$ and $n$. Mathematically, we minimize the following loss function:\n\n$max(\\lVert s_a - s_p \\rVert - \\lVert s_a - s_n \\rVert + \\epsilon, 0)$\n\nwith $s_x$ the sentence embedding for $a/n/p$, $||\\cdot||$ a distance metric and margin $\\epsilon$. Margin $\\epsilon$ ensures that $s_p$ is at least $\\epsilon$ closer to $s_a$ than $s_n$. As metric we use Euclidean distance and we set $\\epsilon = 1$ in our experiments."
        },
        {
            "text": "### 3.1 Training Details\nWe train SBERT on the combination of the SNLI (Bowman et al., 2015) and the Multi-Genre NLI",
            "page": 3,
            "x": 304,
            "y": 721,
            "width": 224,
            "height": 46,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-training",
            "chunk_id": "6c05d5c5-edec-4852-9554-b2ce547611af",
            "group_text": "### 3.1 Training Details\nWe train SBERT on the combination of the SNLI (Bowman et al., 2015) and the Multi-Genre NLI\n\nTable 1: Spearman rank correlation $\\rho$ between the cosine similarity of sentence representations and the gold labels for various Textual Similarity (STS) tasks. Performance is reported by convention as $\\rho \\times 100$. STS12-STS16: SemEval 2012-2016, STSb: STSbenchmark, SICK-R: SICK relatedness dataset.\n\n(Williams et al., 2018) dataset. The SNLI is a collection of 570,000 sentence pairs annotated with the labels *contradiction*, *entailment*, and *neutral*. MultiNLI contains 430,000 sentence pairs and covers a range of genres of spoken and written text. We fine-tune SBERT with a 3-way softmax-classifier objective function for one epoch. We used a batch-size of 16, Adam optimizer with learning rate 2e\u22125, and a linear learning rate warm-up over 10% of the training data. Our default pooling strategy is MEAN."
        },
        {
            "text": "Table 1: Spearman rank correlation $\\rho$ between the cosine similarity of sentence representations and the gold labels for various Textual Similarity (STS) tasks. Performance is reported by convention as $\\rho \\times 100$. STS12-STS16: SemEval 2012-2016, STSb: STSbenchmark, SICK-R: SICK relatedness dataset.",
            "page": 4,
            "x": 68,
            "y": 171,
            "width": 460,
            "height": 39,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-training",
            "chunk_id": "1477d5f6-88a2-4cdc-a989-1c407cec42d9",
            "group_text": "### 3.1 Training Details\nWe train SBERT on the combination of the SNLI (Bowman et al., 2015) and the Multi-Genre NLI\n\nTable 1: Spearman rank correlation $\\rho$ between the cosine similarity of sentence representations and the gold labels for various Textual Similarity (STS) tasks. Performance is reported by convention as $\\rho \\times 100$. STS12-STS16: SemEval 2012-2016, STSb: STSbenchmark, SICK-R: SICK relatedness dataset.\n\n(Williams et al., 2018) dataset. The SNLI is a collection of 570,000 sentence pairs annotated with the labels *contradiction*, *entailment*, and *neutral*. MultiNLI contains 430,000 sentence pairs and covers a range of genres of spoken and written text. We fine-tune SBERT with a 3-way softmax-classifier objective function for one epoch. We used a batch-size of 16, Adam optimizer with learning rate 2e\u22125, and a linear learning rate warm-up over 10% of the training data. Our default pooling strategy is MEAN."
        },
        {
            "text": "(Williams et al., 2018) dataset. The SNLI is a collection of 570,000 sentence pairs annotated with the labels *contradiction*, *entailment*, and *neutral*. MultiNLI contains 430,000 sentence pairs and covers a range of genres of spoken and written text. We fine-tune SBERT with a 3-way softmax-classifier objective function for one epoch. We used a batch-size of 16, Adam optimizer with learning rate 2e\u22125, and a linear learning rate warm-up over 10% of the training data. Our default pooling strategy is MEAN.",
            "page": 4,
            "x": 68,
            "y": 229,
            "width": 225,
            "height": 152,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-training",
            "chunk_id": "9d046fbc-be2b-4c8b-a5e2-0ce1e0443f7a",
            "group_text": "### 3.1 Training Details\nWe train SBERT on the combination of the SNLI (Bowman et al., 2015) and the Multi-Genre NLI\n\nTable 1: Spearman rank correlation $\\rho$ between the cosine similarity of sentence representations and the gold labels for various Textual Similarity (STS) tasks. Performance is reported by convention as $\\rho \\times 100$. STS12-STS16: SemEval 2012-2016, STSb: STSbenchmark, SICK-R: SICK relatedness dataset.\n\n(Williams et al., 2018) dataset. The SNLI is a collection of 570,000 sentence pairs annotated with the labels *contradiction*, *entailment*, and *neutral*. MultiNLI contains 430,000 sentence pairs and covers a range of genres of spoken and written text. We fine-tune SBERT with a 3-way softmax-classifier objective function for one epoch. We used a batch-size of 16, Adam optimizer with learning rate 2e\u22125, and a linear learning rate warm-up over 10% of the training data. Our default pooling strategy is MEAN."
        },
        {
            "text": "4  Evaluation - Semantic Textual\n   Similarity\n\nWe evaluate the performance of SBERT for com-\nmon Semantic Textual Similarity (STS) tasks.\nState-of-the-art methods often learn a (complex)\nregression function that maps sentence embed-\ndings to a similarity score. However, these regres-\nsion functions work pair-wise and due to the com-\nbinatorial explosion those are often not scalable if\nthe collection of sentences reaches a certain size.\nInstead, we always use cosine-similarity to com-\npare the similarity between two sentence embed-\ndings. We ran our experiments also with nega-\ntive Manhattan and negative Euclidean distances\nas similarity measures, but the results for all ap-\nproaches remained roughly the same.",
            "page": 4,
            "x": 67,
            "y": 388,
            "width": 227,
            "height": 229,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "6-evaluation",
            "chunk_id": "060a8722-c4f0-4b5e-a83e-2fea55b3b551",
            "group_text": "4  Evaluation - Semantic Textual\n   Similarity\n\nWe evaluate the performance of SBERT for com-\nmon Semantic Textual Similarity (STS) tasks.\nState-of-the-art methods often learn a (complex)\nregression function that maps sentence embed-\ndings to a similarity score. However, these regres-\nsion functions work pair-wise and due to the com-\nbinatorial explosion those are often not scalable if\nthe collection of sentences reaches a certain size.\nInstead, we always use cosine-similarity to com-\npare the similarity between two sentence embed-\ndings. We ran our experiments also with nega-\ntive Manhattan and negative Euclidean distances\nas similarity measures, but the results for all ap-\nproaches remained roughly the same."
        },
        {
            "text": "4.1  Unsupervised STS  \nWe evaluate the performance of SBERT for STS without using any STS specific training data. We use the STS tasks 2012 - 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016), the STS benchmark (Cer et al., 2017), and the SICK-Relatedness dataset (Marelli et al., 2014). These datasets provide labels between 0 and 5 on the semantic relatedness of sentence pairs. We showed in (Reimers et al., 2016) that Pearson correlation is badly suited for",
            "page": 4,
            "x": 67,
            "y": 625,
            "width": 226,
            "height": 143,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-unsupervised",
            "chunk_id": "3234db5c-9f6c-4f66-9c15-124c8bfb7221",
            "group_text": "4.1  Unsupervised STS  \nWe evaluate the performance of SBERT for STS without using any STS specific training data. We use the STS tasks 2012 - 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016), the STS benchmark (Cer et al., 2017), and the SICK-Relatedness dataset (Marelli et al., 2014). These datasets provide labels between 0 and 5 on the semantic relatedness of sentence pairs. We showed in (Reimers et al., 2016) that Pearson correlation is badly suited for\n\nSTS. Instead, we compute the Spearman\u2019s rank\ncorrelation between the cosine-similarity of the\nsentence embeddings and the gold labels. The\nsetup for the other sentence embedding methods\nis equivalent, the similarity is computed by cosine-\nsimilarity. The results are depicted in Table 1.\n\nThe results shows that directly using the output\nof BERT leads to rather poor performances.  Aver-\naging the BERT embeddings achieves an aver-\nage correlation of only 54.81, and using the CLS-\ntoken output only achieves an average correlation\nof 29.19. Both are worse than computing average\nGloVe embeddings.\n\nUsing the described siamese network structure and fine-tuning mechanism substantially improves the correlation, outperforming both InferSent and Universal Sentence Encoder substantially. The only dataset where SBERT performs worse than Universal Sentence Encoder is SICK-R. Universal Sentence Encoder was trained on various datasets, including news, question-answer pages and discussion forums, which appears to be more suitable to the data of SICK-R. In contrast, SBERT was pre-trained only on Wikipedia (via BERT) and on NLI data.\n\nWhile RoBERTa was able to improve the performance for several supervised tasks, we only observe minor difference between SBERT and SRoBERTa for generating sentence embeddings."
        },
        {
            "text": "STS. Instead, we compute the Spearman\u2019s rank\ncorrelation between the cosine-similarity of the\nsentence embeddings and the gold labels. The\nsetup for the other sentence embedding methods\nis equivalent, the similarity is computed by cosine-\nsimilarity. The results are depicted in Table 1.",
            "page": 4,
            "x": 304,
            "y": 230,
            "width": 224,
            "height": 82,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-unsupervised",
            "chunk_id": "71326a32-361b-42fb-b1e3-9cd7316ea521",
            "group_text": "4.1  Unsupervised STS  \nWe evaluate the performance of SBERT for STS without using any STS specific training data. We use the STS tasks 2012 - 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016), the STS benchmark (Cer et al., 2017), and the SICK-Relatedness dataset (Marelli et al., 2014). These datasets provide labels between 0 and 5 on the semantic relatedness of sentence pairs. We showed in (Reimers et al., 2016) that Pearson correlation is badly suited for\n\nSTS. Instead, we compute the Spearman\u2019s rank\ncorrelation between the cosine-similarity of the\nsentence embeddings and the gold labels. The\nsetup for the other sentence embedding methods\nis equivalent, the similarity is computed by cosine-\nsimilarity. The results are depicted in Table 1.\n\nThe results shows that directly using the output\nof BERT leads to rather poor performances.  Aver-\naging the BERT embeddings achieves an aver-\nage correlation of only 54.81, and using the CLS-\ntoken output only achieves an average correlation\nof 29.19. Both are worse than computing average\nGloVe embeddings.\n\nUsing the described siamese network structure and fine-tuning mechanism substantially improves the correlation, outperforming both InferSent and Universal Sentence Encoder substantially. The only dataset where SBERT performs worse than Universal Sentence Encoder is SICK-R. Universal Sentence Encoder was trained on various datasets, including news, question-answer pages and discussion forums, which appears to be more suitable to the data of SICK-R. In contrast, SBERT was pre-trained only on Wikipedia (via BERT) and on NLI data.\n\nWhile RoBERTa was able to improve the performance for several supervised tasks, we only observe minor difference between SBERT and SRoBERTa for generating sentence embeddings."
        },
        {
            "text": "The results shows that directly using the output\nof BERT leads to rather poor performances.  Aver-\naging the BERT embeddings achieves an aver-\nage correlation of only 54.81, and using the CLS-\ntoken output only achieves an average correlation\nof 29.19. Both are worse than computing average\nGloVe embeddings.",
            "page": 4,
            "x": 304,
            "y": 314,
            "width": 224,
            "height": 94,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-unsupervised",
            "chunk_id": "db670d30-26a0-4e44-a1bb-c3586f01b4dc",
            "group_text": "4.1  Unsupervised STS  \nWe evaluate the performance of SBERT for STS without using any STS specific training data. We use the STS tasks 2012 - 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016), the STS benchmark (Cer et al., 2017), and the SICK-Relatedness dataset (Marelli et al., 2014). These datasets provide labels between 0 and 5 on the semantic relatedness of sentence pairs. We showed in (Reimers et al., 2016) that Pearson correlation is badly suited for\n\nSTS. Instead, we compute the Spearman\u2019s rank\ncorrelation between the cosine-similarity of the\nsentence embeddings and the gold labels. The\nsetup for the other sentence embedding methods\nis equivalent, the similarity is computed by cosine-\nsimilarity. The results are depicted in Table 1.\n\nThe results shows that directly using the output\nof BERT leads to rather poor performances.  Aver-\naging the BERT embeddings achieves an aver-\nage correlation of only 54.81, and using the CLS-\ntoken output only achieves an average correlation\nof 29.19. Both are worse than computing average\nGloVe embeddings.\n\nUsing the described siamese network structure and fine-tuning mechanism substantially improves the correlation, outperforming both InferSent and Universal Sentence Encoder substantially. The only dataset where SBERT performs worse than Universal Sentence Encoder is SICK-R. Universal Sentence Encoder was trained on various datasets, including news, question-answer pages and discussion forums, which appears to be more suitable to the data of SICK-R. In contrast, SBERT was pre-trained only on Wikipedia (via BERT) and on NLI data.\n\nWhile RoBERTa was able to improve the performance for several supervised tasks, we only observe minor difference between SBERT and SRoBERTa for generating sentence embeddings."
        },
        {
            "text": "Using the described siamese network structure and fine-tuning mechanism substantially improves the correlation, outperforming both InferSent and Universal Sentence Encoder substantially. The only dataset where SBERT performs worse than Universal Sentence Encoder is SICK-R. Universal Sentence Encoder was trained on various datasets, including news, question-answer pages and discussion forums, which appears to be more suitable to the data of SICK-R. In contrast, SBERT was pre-trained only on Wikipedia (via BERT) and on NLI data.",
            "page": 4,
            "x": 304,
            "y": 409,
            "width": 224,
            "height": 163,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-unsupervised",
            "chunk_id": "9b416cd7-d510-447d-8bc6-9173138e8e00",
            "group_text": "4.1  Unsupervised STS  \nWe evaluate the performance of SBERT for STS without using any STS specific training data. We use the STS tasks 2012 - 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016), the STS benchmark (Cer et al., 2017), and the SICK-Relatedness dataset (Marelli et al., 2014). These datasets provide labels between 0 and 5 on the semantic relatedness of sentence pairs. We showed in (Reimers et al., 2016) that Pearson correlation is badly suited for\n\nSTS. Instead, we compute the Spearman\u2019s rank\ncorrelation between the cosine-similarity of the\nsentence embeddings and the gold labels. The\nsetup for the other sentence embedding methods\nis equivalent, the similarity is computed by cosine-\nsimilarity. The results are depicted in Table 1.\n\nThe results shows that directly using the output\nof BERT leads to rather poor performances.  Aver-\naging the BERT embeddings achieves an aver-\nage correlation of only 54.81, and using the CLS-\ntoken output only achieves an average correlation\nof 29.19. Both are worse than computing average\nGloVe embeddings.\n\nUsing the described siamese network structure and fine-tuning mechanism substantially improves the correlation, outperforming both InferSent and Universal Sentence Encoder substantially. The only dataset where SBERT performs worse than Universal Sentence Encoder is SICK-R. Universal Sentence Encoder was trained on various datasets, including news, question-answer pages and discussion forums, which appears to be more suitable to the data of SICK-R. In contrast, SBERT was pre-trained only on Wikipedia (via BERT) and on NLI data.\n\nWhile RoBERTa was able to improve the performance for several supervised tasks, we only observe minor difference between SBERT and SRoBERTa for generating sentence embeddings."
        },
        {
            "text": "While RoBERTa was able to improve the performance for several supervised tasks, we only observe minor difference between SBERT and SRoBERTa for generating sentence embeddings.",
            "page": 4,
            "x": 304,
            "y": 572,
            "width": 224,
            "height": 56,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-unsupervised",
            "chunk_id": "bbd6ec7a-12f0-4167-95e8-21f6f9bb8e56",
            "group_text": "4.1  Unsupervised STS  \nWe evaluate the performance of SBERT for STS without using any STS specific training data. We use the STS tasks 2012 - 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016), the STS benchmark (Cer et al., 2017), and the SICK-Relatedness dataset (Marelli et al., 2014). These datasets provide labels between 0 and 5 on the semantic relatedness of sentence pairs. We showed in (Reimers et al., 2016) that Pearson correlation is badly suited for\n\nSTS. Instead, we compute the Spearman\u2019s rank\ncorrelation between the cosine-similarity of the\nsentence embeddings and the gold labels. The\nsetup for the other sentence embedding methods\nis equivalent, the similarity is computed by cosine-\nsimilarity. The results are depicted in Table 1.\n\nThe results shows that directly using the output\nof BERT leads to rather poor performances.  Aver-\naging the BERT embeddings achieves an aver-\nage correlation of only 54.81, and using the CLS-\ntoken output only achieves an average correlation\nof 29.19. Both are worse than computing average\nGloVe embeddings.\n\nUsing the described siamese network structure and fine-tuning mechanism substantially improves the correlation, outperforming both InferSent and Universal Sentence Encoder substantially. The only dataset where SBERT performs worse than Universal Sentence Encoder is SICK-R. Universal Sentence Encoder was trained on various datasets, including news, question-answer pages and discussion forums, which appears to be more suitable to the data of SICK-R. In contrast, SBERT was pre-trained only on Wikipedia (via BERT) and on NLI data.\n\nWhile RoBERTa was able to improve the performance for several supervised tasks, we only observe minor difference between SBERT and SRoBERTa for generating sentence embeddings."
        },
        {
            "text": "### 4.2  Supervised STS\n\nThe STS benchmark (STSb) (Cer et al., 2017) provides is a popular dataset to evaluate supervised STS systems. The data includes 8,628 sentence pairs from the three categories *captions*, *news*, and *forums*. It is divided into train (5,749), dev (1,500) and test (1,379). BERT set a new state-of-the-art performance on this dataset by passing both sentences to the network and using a simple regres-",
            "page": 4,
            "x": 303,
            "y": 637,
            "width": 225,
            "height": 130,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "8-supervised",
            "chunk_id": "0df7fdb6-8dc1-41a7-9536-2ec88d04a5b1",
            "group_text": "### 4.2  Supervised STS\n\nThe STS benchmark (STSb) (Cer et al., 2017) provides is a popular dataset to evaluate supervised STS systems. The data includes 8,628 sentence pairs from the three categories *captions*, *news*, and *forums*. It is divided into train (5,749), dev (1,500) and test (1,379). BERT set a new state-of-the-art performance on this dataset by passing both sentences to the network and using a simple regres-\n\nsion method for the output.\n\nTable 2: Evaluation on the STS benchmark test set. BERT systems were trained with 10 random seeds and 4 epochs. SBERT was fine-tuned on the STSb dataset, SBERT-NLI was pretrained on the NLI datasets, then fine-tuned on the STSb dataset.\n\nWe use the training set to fine-tune SBERT using the regression objective function. At prediction time, we compute the cosine-similarity between the sentence embeddings. All systems are trained with 10 random seeds to counter variances (Reimers and Gurevych, 2018).\n\nThe results are depicted in Table 2. We experimented with two setups: Only training on STSb, and first training on NLI, then training on STSb. We observe that the later strategy leads to a slight improvement of 1-2 points. This two-step approach had an especially large impact for the BERT cross-encoder, which improved the performance by 3-4 points. We do not observe a significant difference between BERT and RoBERTa."
        },
        {
            "text": "sion method for the output.",
            "page": 5,
            "x": 68,
            "y": 62,
            "width": 125,
            "height": 15,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "8-supervised",
            "chunk_id": "fbf6996a-670d-4d3d-8115-a38b5a07d514",
            "group_text": "### 4.2  Supervised STS\n\nThe STS benchmark (STSb) (Cer et al., 2017) provides is a popular dataset to evaluate supervised STS systems. The data includes 8,628 sentence pairs from the three categories *captions*, *news*, and *forums*. It is divided into train (5,749), dev (1,500) and test (1,379). BERT set a new state-of-the-art performance on this dataset by passing both sentences to the network and using a simple regres-\n\nsion method for the output.\n\nTable 2: Evaluation on the STS benchmark test set. BERT systems were trained with 10 random seeds and 4 epochs. SBERT was fine-tuned on the STSb dataset, SBERT-NLI was pretrained on the NLI datasets, then fine-tuned on the STSb dataset.\n\nWe use the training set to fine-tune SBERT using the regression objective function. At prediction time, we compute the cosine-similarity between the sentence embeddings. All systems are trained with 10 random seeds to counter variances (Reimers and Gurevych, 2018).\n\nThe results are depicted in Table 2. We experimented with two setups: Only training on STSb, and first training on NLI, then training on STSb. We observe that the later strategy leads to a slight improvement of 1-2 points. This two-step approach had an especially large impact for the BERT cross-encoder, which improved the performance by 3-4 points. We do not observe a significant difference between BERT and RoBERTa."
        },
        {
            "text": "Table 2: Evaluation on the STS benchmark test set. BERT systems were trained with 10 random seeds and 4 epochs. SBERT was fine-tuned on the STSb dataset, SBERT-NLI was pretrained on the NLI datasets, then fine-tuned on the STSb dataset.",
            "page": 5,
            "x": 68,
            "y": 320,
            "width": 225,
            "height": 61,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "8-supervised",
            "chunk_id": "e0c6eb06-a6c2-4143-8804-5cdc36109baa",
            "group_text": "### 4.2  Supervised STS\n\nThe STS benchmark (STSb) (Cer et al., 2017) provides is a popular dataset to evaluate supervised STS systems. The data includes 8,628 sentence pairs from the three categories *captions*, *news*, and *forums*. It is divided into train (5,749), dev (1,500) and test (1,379). BERT set a new state-of-the-art performance on this dataset by passing both sentences to the network and using a simple regres-\n\nsion method for the output.\n\nTable 2: Evaluation on the STS benchmark test set. BERT systems were trained with 10 random seeds and 4 epochs. SBERT was fine-tuned on the STSb dataset, SBERT-NLI was pretrained on the NLI datasets, then fine-tuned on the STSb dataset.\n\nWe use the training set to fine-tune SBERT using the regression objective function. At prediction time, we compute the cosine-similarity between the sentence embeddings. All systems are trained with 10 random seeds to counter variances (Reimers and Gurevych, 2018).\n\nThe results are depicted in Table 2. We experimented with two setups: Only training on STSb, and first training on NLI, then training on STSb. We observe that the later strategy leads to a slight improvement of 1-2 points. This two-step approach had an especially large impact for the BERT cross-encoder, which improved the performance by 3-4 points. We do not observe a significant difference between BERT and RoBERTa."
        },
        {
            "text": "We use the training set to fine-tune SBERT using the regression objective function. At prediction time, we compute the cosine-similarity between the sentence embeddings. All systems are trained with 10 random seeds to counter variances (Reimers and Gurevych, 2018).",
            "page": 5,
            "x": 68,
            "y": 396,
            "width": 224,
            "height": 82,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "8-supervised",
            "chunk_id": "17dcd2b3-534b-4f61-bd08-baf1e0752f27",
            "group_text": "### 4.2  Supervised STS\n\nThe STS benchmark (STSb) (Cer et al., 2017) provides is a popular dataset to evaluate supervised STS systems. The data includes 8,628 sentence pairs from the three categories *captions*, *news*, and *forums*. It is divided into train (5,749), dev (1,500) and test (1,379). BERT set a new state-of-the-art performance on this dataset by passing both sentences to the network and using a simple regres-\n\nsion method for the output.\n\nTable 2: Evaluation on the STS benchmark test set. BERT systems were trained with 10 random seeds and 4 epochs. SBERT was fine-tuned on the STSb dataset, SBERT-NLI was pretrained on the NLI datasets, then fine-tuned on the STSb dataset.\n\nWe use the training set to fine-tune SBERT using the regression objective function. At prediction time, we compute the cosine-similarity between the sentence embeddings. All systems are trained with 10 random seeds to counter variances (Reimers and Gurevych, 2018).\n\nThe results are depicted in Table 2. We experimented with two setups: Only training on STSb, and first training on NLI, then training on STSb. We observe that the later strategy leads to a slight improvement of 1-2 points. This two-step approach had an especially large impact for the BERT cross-encoder, which improved the performance by 3-4 points. We do not observe a significant difference between BERT and RoBERTa."
        },
        {
            "text": "The results are depicted in Table 2. We experimented with two setups: Only training on STSb, and first training on NLI, then training on STSb. We observe that the later strategy leads to a slight improvement of 1-2 points. This two-step approach had an especially large impact for the BERT cross-encoder, which improved the performance by 3-4 points. We do not observe a significant difference between BERT and RoBERTa.",
            "page": 5,
            "x": 68,
            "y": 479,
            "width": 224,
            "height": 122,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "8-supervised",
            "chunk_id": "99293606-bb80-41e0-82a2-70ffa074edc4",
            "group_text": "### 4.2  Supervised STS\n\nThe STS benchmark (STSb) (Cer et al., 2017) provides is a popular dataset to evaluate supervised STS systems. The data includes 8,628 sentence pairs from the three categories *captions*, *news*, and *forums*. It is divided into train (5,749), dev (1,500) and test (1,379). BERT set a new state-of-the-art performance on this dataset by passing both sentences to the network and using a simple regres-\n\nsion method for the output.\n\nTable 2: Evaluation on the STS benchmark test set. BERT systems were trained with 10 random seeds and 4 epochs. SBERT was fine-tuned on the STSb dataset, SBERT-NLI was pretrained on the NLI datasets, then fine-tuned on the STSb dataset.\n\nWe use the training set to fine-tune SBERT using the regression objective function. At prediction time, we compute the cosine-similarity between the sentence embeddings. All systems are trained with 10 random seeds to counter variances (Reimers and Gurevych, 2018).\n\nThe results are depicted in Table 2. We experimented with two setups: Only training on STSb, and first training on NLI, then training on STSb. We observe that the later strategy leads to a slight improvement of 1-2 points. This two-step approach had an especially large impact for the BERT cross-encoder, which improved the performance by 3-4 points. We do not observe a significant difference between BERT and RoBERTa."
        },
        {
            "text": "### 4.3 Argument Facet Similarity\nWe evaluate SBERT on the Argument Facet Similarity (AFS) corpus by Misra et al. (2016). The AFS corpus annotated 6,000 sentential argument pairs from social media dialogs on three controversial topics: _gun control_, _gay marriage_, and _death penalty_. The data was annotated on a scale from 0 (\u201cdifferent topic\u201d) to 5 (\u201ccompletely equivalent\u201d). The similarity notion in the AFS corpus is fairly different to the similarity notion in the STS datasets from SemEval. STS data is usually",
            "page": 5,
            "x": 68,
            "y": 610,
            "width": 225,
            "height": 157,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-argument",
            "chunk_id": "48752afe-3534-44fe-9102-0bcb67d5b420",
            "group_text": "### 4.3 Argument Facet Similarity\nWe evaluate SBERT on the Argument Facet Similarity (AFS) corpus by Misra et al. (2016). The AFS corpus annotated 6,000 sentential argument pairs from social media dialogs on three controversial topics: _gun control_, _gay marriage_, and _death penalty_. The data was annotated on a scale from 0 (\u201cdifferent topic\u201d) to 5 (\u201ccompletely equivalent\u201d). The similarity notion in the AFS corpus is fairly different to the similarity notion in the STS datasets from SemEval. STS data is usually\n\ndescriptive, while AFS data are argumentative excerpts from dialogs. To be considered similar, arguments must not only make similar claims, but also provide a similar reasoning. Further, the lexical gap between the sentences in AFS is much larger. Hence, simple unsupervised methods as well as state-of-the-art STS systems perform badly on this dataset (Reimers et al., 2019).\n\nWe evaluate SBERT on this dataset in two scenarios: 1) As proposed by Misra et al., we evaluate SBERT using 10-fold cross-validation. A drawback of this evaluation setup is that it is not clear how well approaches generalize to different topics. Hence, 2) we evaluate SBERT in a cross-topic setup. Two topics serve for training and the approach is evaluated on the left-out topic. We repeat this for all three topics and average the results.\n\nSBERT is fine-tuned using the Regression Objective Function. The similarity score is computed using cosine-similarity based on the sentence embeddings. We also provide the Pearson correlation $r$ to make the results comparable to Misra et al. However, we showed (Reimers et al., 2016) that Pearson correlation has some serious drawbacks and should be avoided for comparing STS systems. The results are depicted in Table 3.\n\nUnsupervised methods like tf-idf, average GloVe embeddings or InferSent perform rather badly on this dataset with low scores. Training SBERT in the 10-fold cross-validation setup gives a performance that is nearly on-par with BERT.\n\nHowever, in the cross-topic evaluation, we observe a performance drop of SBERT by about 7 points Spearman correlation. To be considered similar, arguments should address the same claims and provide the same reasoning. BERT is able to use attention to compare directly both sentences (e.g. word-by-word comparison), while SBERT must map individual sentences from an unseen topic to a vector space such that arguments with similar claims and reasons are close. This is a much more challenging task, which appears to require more than just two topics for training to work on-par with BERT."
        },
        {
            "text": "descriptive, while AFS data are argumentative excerpts from dialogs. To be considered similar, arguments must not only make similar claims, but also provide a similar reasoning. Further, the lexical gap between the sentences in AFS is much larger. Hence, simple unsupervised methods as well as state-of-the-art STS systems perform badly on this dataset (Reimers et al., 2019).",
            "page": 5,
            "x": 303,
            "y": 63,
            "width": 225,
            "height": 108,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-argument",
            "chunk_id": "f53cef8f-0c56-4dbe-8809-4fa6f6ea2007",
            "group_text": "### 4.3 Argument Facet Similarity\nWe evaluate SBERT on the Argument Facet Similarity (AFS) corpus by Misra et al. (2016). The AFS corpus annotated 6,000 sentential argument pairs from social media dialogs on three controversial topics: _gun control_, _gay marriage_, and _death penalty_. The data was annotated on a scale from 0 (\u201cdifferent topic\u201d) to 5 (\u201ccompletely equivalent\u201d). The similarity notion in the AFS corpus is fairly different to the similarity notion in the STS datasets from SemEval. STS data is usually\n\ndescriptive, while AFS data are argumentative excerpts from dialogs. To be considered similar, arguments must not only make similar claims, but also provide a similar reasoning. Further, the lexical gap between the sentences in AFS is much larger. Hence, simple unsupervised methods as well as state-of-the-art STS systems perform badly on this dataset (Reimers et al., 2019).\n\nWe evaluate SBERT on this dataset in two scenarios: 1) As proposed by Misra et al., we evaluate SBERT using 10-fold cross-validation. A drawback of this evaluation setup is that it is not clear how well approaches generalize to different topics. Hence, 2) we evaluate SBERT in a cross-topic setup. Two topics serve for training and the approach is evaluated on the left-out topic. We repeat this for all three topics and average the results.\n\nSBERT is fine-tuned using the Regression Objective Function. The similarity score is computed using cosine-similarity based on the sentence embeddings. We also provide the Pearson correlation $r$ to make the results comparable to Misra et al. However, we showed (Reimers et al., 2016) that Pearson correlation has some serious drawbacks and should be avoided for comparing STS systems. The results are depicted in Table 3.\n\nUnsupervised methods like tf-idf, average GloVe embeddings or InferSent perform rather badly on this dataset with low scores. Training SBERT in the 10-fold cross-validation setup gives a performance that is nearly on-par with BERT.\n\nHowever, in the cross-topic evaluation, we observe a performance drop of SBERT by about 7 points Spearman correlation. To be considered similar, arguments should address the same claims and provide the same reasoning. BERT is able to use attention to compare directly both sentences (e.g. word-by-word comparison), while SBERT must map individual sentences from an unseen topic to a vector space such that arguments with similar claims and reasons are close. This is a much more challenging task, which appears to require more than just two topics for training to work on-par with BERT."
        },
        {
            "text": "We evaluate SBERT on this dataset in two scenarios: 1) As proposed by Misra et al., we evaluate SBERT using 10-fold cross-validation. A drawback of this evaluation setup is that it is not clear how well approaches generalize to different topics. Hence, 2) we evaluate SBERT in a cross-topic setup. Two topics serve for training and the approach is evaluated on the left-out topic. We repeat this for all three topics and average the results.",
            "page": 5,
            "x": 304,
            "y": 173,
            "width": 224,
            "height": 122,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-argument",
            "chunk_id": "532b1657-7e00-4416-9b2b-7bfa7946f9b6",
            "group_text": "### 4.3 Argument Facet Similarity\nWe evaluate SBERT on the Argument Facet Similarity (AFS) corpus by Misra et al. (2016). The AFS corpus annotated 6,000 sentential argument pairs from social media dialogs on three controversial topics: _gun control_, _gay marriage_, and _death penalty_. The data was annotated on a scale from 0 (\u201cdifferent topic\u201d) to 5 (\u201ccompletely equivalent\u201d). The similarity notion in the AFS corpus is fairly different to the similarity notion in the STS datasets from SemEval. STS data is usually\n\ndescriptive, while AFS data are argumentative excerpts from dialogs. To be considered similar, arguments must not only make similar claims, but also provide a similar reasoning. Further, the lexical gap between the sentences in AFS is much larger. Hence, simple unsupervised methods as well as state-of-the-art STS systems perform badly on this dataset (Reimers et al., 2019).\n\nWe evaluate SBERT on this dataset in two scenarios: 1) As proposed by Misra et al., we evaluate SBERT using 10-fold cross-validation. A drawback of this evaluation setup is that it is not clear how well approaches generalize to different topics. Hence, 2) we evaluate SBERT in a cross-topic setup. Two topics serve for training and the approach is evaluated on the left-out topic. We repeat this for all three topics and average the results.\n\nSBERT is fine-tuned using the Regression Objective Function. The similarity score is computed using cosine-similarity based on the sentence embeddings. We also provide the Pearson correlation $r$ to make the results comparable to Misra et al. However, we showed (Reimers et al., 2016) that Pearson correlation has some serious drawbacks and should be avoided for comparing STS systems. The results are depicted in Table 3.\n\nUnsupervised methods like tf-idf, average GloVe embeddings or InferSent perform rather badly on this dataset with low scores. Training SBERT in the 10-fold cross-validation setup gives a performance that is nearly on-par with BERT.\n\nHowever, in the cross-topic evaluation, we observe a performance drop of SBERT by about 7 points Spearman correlation. To be considered similar, arguments should address the same claims and provide the same reasoning. BERT is able to use attention to compare directly both sentences (e.g. word-by-word comparison), while SBERT must map individual sentences from an unseen topic to a vector space such that arguments with similar claims and reasons are close. This is a much more challenging task, which appears to require more than just two topics for training to work on-par with BERT."
        },
        {
            "text": "SBERT is fine-tuned using the Regression Objective Function. The similarity score is computed using cosine-similarity based on the sentence embeddings. We also provide the Pearson correlation $r$ to make the results comparable to Misra et al. However, we showed (Reimers et al., 2016) that Pearson correlation has some serious drawbacks and should be avoided for comparing STS systems. The results are depicted in Table 3.",
            "page": 5,
            "x": 304,
            "y": 296,
            "width": 224,
            "height": 122,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-argument",
            "chunk_id": "b3ed4efb-3cfa-4364-b6b2-456165a474bb",
            "group_text": "### 4.3 Argument Facet Similarity\nWe evaluate SBERT on the Argument Facet Similarity (AFS) corpus by Misra et al. (2016). The AFS corpus annotated 6,000 sentential argument pairs from social media dialogs on three controversial topics: _gun control_, _gay marriage_, and _death penalty_. The data was annotated on a scale from 0 (\u201cdifferent topic\u201d) to 5 (\u201ccompletely equivalent\u201d). The similarity notion in the AFS corpus is fairly different to the similarity notion in the STS datasets from SemEval. STS data is usually\n\ndescriptive, while AFS data are argumentative excerpts from dialogs. To be considered similar, arguments must not only make similar claims, but also provide a similar reasoning. Further, the lexical gap between the sentences in AFS is much larger. Hence, simple unsupervised methods as well as state-of-the-art STS systems perform badly on this dataset (Reimers et al., 2019).\n\nWe evaluate SBERT on this dataset in two scenarios: 1) As proposed by Misra et al., we evaluate SBERT using 10-fold cross-validation. A drawback of this evaluation setup is that it is not clear how well approaches generalize to different topics. Hence, 2) we evaluate SBERT in a cross-topic setup. Two topics serve for training and the approach is evaluated on the left-out topic. We repeat this for all three topics and average the results.\n\nSBERT is fine-tuned using the Regression Objective Function. The similarity score is computed using cosine-similarity based on the sentence embeddings. We also provide the Pearson correlation $r$ to make the results comparable to Misra et al. However, we showed (Reimers et al., 2016) that Pearson correlation has some serious drawbacks and should be avoided for comparing STS systems. The results are depicted in Table 3.\n\nUnsupervised methods like tf-idf, average GloVe embeddings or InferSent perform rather badly on this dataset with low scores. Training SBERT in the 10-fold cross-validation setup gives a performance that is nearly on-par with BERT.\n\nHowever, in the cross-topic evaluation, we observe a performance drop of SBERT by about 7 points Spearman correlation. To be considered similar, arguments should address the same claims and provide the same reasoning. BERT is able to use attention to compare directly both sentences (e.g. word-by-word comparison), while SBERT must map individual sentences from an unseen topic to a vector space such that arguments with similar claims and reasons are close. This is a much more challenging task, which appears to require more than just two topics for training to work on-par with BERT."
        },
        {
            "text": "Unsupervised methods like tf-idf, average GloVe embeddings or InferSent perform rather badly on this dataset with low scores. Training SBERT in the 10-fold cross-validation setup gives a performance that is nearly on-par with BERT.",
            "page": 5,
            "x": 304,
            "y": 420,
            "width": 224,
            "height": 68,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-argument",
            "chunk_id": "59abcfb4-840b-4ac4-8988-e1d0a9401b1c",
            "group_text": "### 4.3 Argument Facet Similarity\nWe evaluate SBERT on the Argument Facet Similarity (AFS) corpus by Misra et al. (2016). The AFS corpus annotated 6,000 sentential argument pairs from social media dialogs on three controversial topics: _gun control_, _gay marriage_, and _death penalty_. The data was annotated on a scale from 0 (\u201cdifferent topic\u201d) to 5 (\u201ccompletely equivalent\u201d). The similarity notion in the AFS corpus is fairly different to the similarity notion in the STS datasets from SemEval. STS data is usually\n\ndescriptive, while AFS data are argumentative excerpts from dialogs. To be considered similar, arguments must not only make similar claims, but also provide a similar reasoning. Further, the lexical gap between the sentences in AFS is much larger. Hence, simple unsupervised methods as well as state-of-the-art STS systems perform badly on this dataset (Reimers et al., 2019).\n\nWe evaluate SBERT on this dataset in two scenarios: 1) As proposed by Misra et al., we evaluate SBERT using 10-fold cross-validation. A drawback of this evaluation setup is that it is not clear how well approaches generalize to different topics. Hence, 2) we evaluate SBERT in a cross-topic setup. Two topics serve for training and the approach is evaluated on the left-out topic. We repeat this for all three topics and average the results.\n\nSBERT is fine-tuned using the Regression Objective Function. The similarity score is computed using cosine-similarity based on the sentence embeddings. We also provide the Pearson correlation $r$ to make the results comparable to Misra et al. However, we showed (Reimers et al., 2016) that Pearson correlation has some serious drawbacks and should be avoided for comparing STS systems. The results are depicted in Table 3.\n\nUnsupervised methods like tf-idf, average GloVe embeddings or InferSent perform rather badly on this dataset with low scores. Training SBERT in the 10-fold cross-validation setup gives a performance that is nearly on-par with BERT.\n\nHowever, in the cross-topic evaluation, we observe a performance drop of SBERT by about 7 points Spearman correlation. To be considered similar, arguments should address the same claims and provide the same reasoning. BERT is able to use attention to compare directly both sentences (e.g. word-by-word comparison), while SBERT must map individual sentences from an unseen topic to a vector space such that arguments with similar claims and reasons are close. This is a much more challenging task, which appears to require more than just two topics for training to work on-par with BERT."
        },
        {
            "text": "However, in the cross-topic evaluation, we observe a performance drop of SBERT by about 7 points Spearman correlation. To be considered similar, arguments should address the same claims and provide the same reasoning. BERT is able to use attention to compare directly both sentences (e.g. word-by-word comparison), while SBERT must map individual sentences from an unseen topic to a vector space such that arguments with similar claims and reasons are close. This is a much more challenging task, which appears to require more than just two topics for training to work on-par with BERT.",
            "page": 5,
            "x": 304,
            "y": 489,
            "width": 224,
            "height": 176,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-argument",
            "chunk_id": "0504d6e8-c795-4ace-8886-743c6a0c62f2",
            "group_text": "### 4.3 Argument Facet Similarity\nWe evaluate SBERT on the Argument Facet Similarity (AFS) corpus by Misra et al. (2016). The AFS corpus annotated 6,000 sentential argument pairs from social media dialogs on three controversial topics: _gun control_, _gay marriage_, and _death penalty_. The data was annotated on a scale from 0 (\u201cdifferent topic\u201d) to 5 (\u201ccompletely equivalent\u201d). The similarity notion in the AFS corpus is fairly different to the similarity notion in the STS datasets from SemEval. STS data is usually\n\ndescriptive, while AFS data are argumentative excerpts from dialogs. To be considered similar, arguments must not only make similar claims, but also provide a similar reasoning. Further, the lexical gap between the sentences in AFS is much larger. Hence, simple unsupervised methods as well as state-of-the-art STS systems perform badly on this dataset (Reimers et al., 2019).\n\nWe evaluate SBERT on this dataset in two scenarios: 1) As proposed by Misra et al., we evaluate SBERT using 10-fold cross-validation. A drawback of this evaluation setup is that it is not clear how well approaches generalize to different topics. Hence, 2) we evaluate SBERT in a cross-topic setup. Two topics serve for training and the approach is evaluated on the left-out topic. We repeat this for all three topics and average the results.\n\nSBERT is fine-tuned using the Regression Objective Function. The similarity score is computed using cosine-similarity based on the sentence embeddings. We also provide the Pearson correlation $r$ to make the results comparable to Misra et al. However, we showed (Reimers et al., 2016) that Pearson correlation has some serious drawbacks and should be avoided for comparing STS systems. The results are depicted in Table 3.\n\nUnsupervised methods like tf-idf, average GloVe embeddings or InferSent perform rather badly on this dataset with low scores. Training SBERT in the 10-fold cross-validation setup gives a performance that is nearly on-par with BERT.\n\nHowever, in the cross-topic evaluation, we observe a performance drop of SBERT by about 7 points Spearman correlation. To be considered similar, arguments should address the same claims and provide the same reasoning. BERT is able to use attention to compare directly both sentences (e.g. word-by-word comparison), while SBERT must map individual sentences from an unseen topic to a vector space such that arguments with similar claims and reasons are close. This is a much more challenging task, which appears to require more than just two topics for training to work on-par with BERT."
        },
        {
            "text": "### 4.4 Wikipedia Sections Distinction\n\n[**Dor et al. (2018)**](https://example.com) use Wikipedia to create a the-\nmatically fine-grained train, dev and test set for\nsentence embeddings methods. Wikipedia arti-\ncles are separated into distinct sections focusing\non certain aspects. Dor et al. assume that sen-",
            "page": 5,
            "x": 303,
            "y": 677,
            "width": 225,
            "height": 90,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "10-wikipedia",
            "chunk_id": "5d20f6a6-6623-4984-b7dd-4eae2daa9712",
            "group_text": "### 4.4 Wikipedia Sections Distinction\n\n[**Dor et al. (2018)**](https://example.com) use Wikipedia to create a the-\nmatically fine-grained train, dev and test set for\nsentence embeddings methods. Wikipedia arti-\ncles are separated into distinct sections focusing\non certain aspects. Dor et al. assume that sen-\n\nTable 3: Average Pearson correlation $r$ and average Spearman\u2019s rank correlation $\\rho$ on the Argument Facet Similarity (AFS) corpus (Misra et al., 2016). Misra et al. proposes 10-fold cross-validation. We additionally evaluate in a cross-topic scenario: Methods are trained on two topics, and are evaluated on the third topic.\n\ntences in the same section are thematically closer\nthan sentences in different sections. They use this\nto create a large dataset of weakly labeled sen-\ntence triplets: The anchor and the positive exam-\nple come from the same section, while the neg-\native example comes from a different section of\nthe same article. For example, from the Alice\nArnold article: Anchor: *Arnold joined the BBC\nRadio Drama Company in 1988.*, positive: *Arnold\ngained media attention in May 2012.*, negative:\n*Balding and Arnold are keen amateur golfers.*\n\nWe use the dataset from Dor et al. We use the Triplet Objective, train SBERT for one epoch on the about 1.8 Million training triplets and evaluate it on the 222,957 test triplets. Test triplets are from a distinct set of Wikipedia articles. As evaluation metric, we use accuracy: Is the positive example closer to the anchor than the negative example?\n\nResults are presented in Table 4. Dor et al. fine-tuned a BiLSTM architecture with triplet loss to derive sentence embeddings for this dataset. As the table shows, SBERT clearly outperforms the BiLSTM approach by Dor et al."
        },
        {
            "text": "Table 3: Average Pearson correlation $r$ and average Spearman\u2019s rank correlation $\\rho$ on the Argument Facet Similarity (AFS) corpus (Misra et al., 2016). Misra et al. proposes 10-fold cross-validation. We additionally evaluate in a cross-topic scenario: Methods are trained on two topics, and are evaluated on the third topic.",
            "page": 6,
            "x": 68,
            "y": 233,
            "width": 225,
            "height": 74,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "10-wikipedia",
            "chunk_id": "8d6c4a69-0115-46bf-afc8-7c5fe7b23c74",
            "group_text": "### 4.4 Wikipedia Sections Distinction\n\n[**Dor et al. (2018)**](https://example.com) use Wikipedia to create a the-\nmatically fine-grained train, dev and test set for\nsentence embeddings methods. Wikipedia arti-\ncles are separated into distinct sections focusing\non certain aspects. Dor et al. assume that sen-\n\nTable 3: Average Pearson correlation $r$ and average Spearman\u2019s rank correlation $\\rho$ on the Argument Facet Similarity (AFS) corpus (Misra et al., 2016). Misra et al. proposes 10-fold cross-validation. We additionally evaluate in a cross-topic scenario: Methods are trained on two topics, and are evaluated on the third topic.\n\ntences in the same section are thematically closer\nthan sentences in different sections. They use this\nto create a large dataset of weakly labeled sen-\ntence triplets: The anchor and the positive exam-\nple come from the same section, while the neg-\native example comes from a different section of\nthe same article. For example, from the Alice\nArnold article: Anchor: *Arnold joined the BBC\nRadio Drama Company in 1988.*, positive: *Arnold\ngained media attention in May 2012.*, negative:\n*Balding and Arnold are keen amateur golfers.*\n\nWe use the dataset from Dor et al. We use the Triplet Objective, train SBERT for one epoch on the about 1.8 Million training triplets and evaluate it on the 222,957 test triplets. Test triplets are from a distinct set of Wikipedia articles. As evaluation metric, we use accuracy: Is the positive example closer to the anchor than the negative example?\n\nResults are presented in Table 4. Dor et al. fine-tuned a BiLSTM architecture with triplet loss to derive sentence embeddings for this dataset. As the table shows, SBERT clearly outperforms the BiLSTM approach by Dor et al."
        },
        {
            "text": "tences in the same section are thematically closer\nthan sentences in different sections. They use this\nto create a large dataset of weakly labeled sen-\ntence triplets: The anchor and the positive exam-\nple come from the same section, while the neg-\native example comes from a different section of\nthe same article. For example, from the Alice\nArnold article: Anchor: *Arnold joined the BBC\nRadio Drama Company in 1988.*, positive: *Arnold\ngained media attention in May 2012.*, negative:\n*Balding and Arnold are keen amateur golfers.*",
            "page": 6,
            "x": 68,
            "y": 328,
            "width": 224,
            "height": 149,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "10-wikipedia",
            "chunk_id": "66ab5d77-8c06-49c6-8582-493b6b6dbaf0",
            "group_text": "### 4.4 Wikipedia Sections Distinction\n\n[**Dor et al. (2018)**](https://example.com) use Wikipedia to create a the-\nmatically fine-grained train, dev and test set for\nsentence embeddings methods. Wikipedia arti-\ncles are separated into distinct sections focusing\non certain aspects. Dor et al. assume that sen-\n\nTable 3: Average Pearson correlation $r$ and average Spearman\u2019s rank correlation $\\rho$ on the Argument Facet Similarity (AFS) corpus (Misra et al., 2016). Misra et al. proposes 10-fold cross-validation. We additionally evaluate in a cross-topic scenario: Methods are trained on two topics, and are evaluated on the third topic.\n\ntences in the same section are thematically closer\nthan sentences in different sections. They use this\nto create a large dataset of weakly labeled sen-\ntence triplets: The anchor and the positive exam-\nple come from the same section, while the neg-\native example comes from a different section of\nthe same article. For example, from the Alice\nArnold article: Anchor: *Arnold joined the BBC\nRadio Drama Company in 1988.*, positive: *Arnold\ngained media attention in May 2012.*, negative:\n*Balding and Arnold are keen amateur golfers.*\n\nWe use the dataset from Dor et al. We use the Triplet Objective, train SBERT for one epoch on the about 1.8 Million training triplets and evaluate it on the 222,957 test triplets. Test triplets are from a distinct set of Wikipedia articles. As evaluation metric, we use accuracy: Is the positive example closer to the anchor than the negative example?\n\nResults are presented in Table 4. Dor et al. fine-tuned a BiLSTM architecture with triplet loss to derive sentence embeddings for this dataset. As the table shows, SBERT clearly outperforms the BiLSTM approach by Dor et al."
        },
        {
            "text": "We use the dataset from Dor et al. We use the Triplet Objective, train SBERT for one epoch on the about 1.8 Million training triplets and evaluate it on the 222,957 test triplets. Test triplets are from a distinct set of Wikipedia articles. As evaluation metric, we use accuracy: Is the positive example closer to the anchor than the negative example?\n\nResults are presented in Table 4. Dor et al. fine-tuned a BiLSTM architecture with triplet loss to derive sentence embeddings for this dataset. As the table shows, SBERT clearly outperforms the BiLSTM approach by Dor et al.",
            "page": 6,
            "x": 69,
            "y": 478,
            "width": 224,
            "height": 162,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "10-wikipedia",
            "chunk_id": "52953c9d-b0dd-40a1-9c49-46f18397c427",
            "group_text": "### 4.4 Wikipedia Sections Distinction\n\n[**Dor et al. (2018)**](https://example.com) use Wikipedia to create a the-\nmatically fine-grained train, dev and test set for\nsentence embeddings methods. Wikipedia arti-\ncles are separated into distinct sections focusing\non certain aspects. Dor et al. assume that sen-\n\nTable 3: Average Pearson correlation $r$ and average Spearman\u2019s rank correlation $\\rho$ on the Argument Facet Similarity (AFS) corpus (Misra et al., 2016). Misra et al. proposes 10-fold cross-validation. We additionally evaluate in a cross-topic scenario: Methods are trained on two topics, and are evaluated on the third topic.\n\ntences in the same section are thematically closer\nthan sentences in different sections. They use this\nto create a large dataset of weakly labeled sen-\ntence triplets: The anchor and the positive exam-\nple come from the same section, while the neg-\native example comes from a different section of\nthe same article. For example, from the Alice\nArnold article: Anchor: *Arnold joined the BBC\nRadio Drama Company in 1988.*, positive: *Arnold\ngained media attention in May 2012.*, negative:\n*Balding and Arnold are keen amateur golfers.*\n\nWe use the dataset from Dor et al. We use the Triplet Objective, train SBERT for one epoch on the about 1.8 Million training triplets and evaluate it on the 222,957 test triplets. Test triplets are from a distinct set of Wikipedia articles. As evaluation metric, we use accuracy: Is the positive example closer to the anchor than the negative example?\n\nResults are presented in Table 4. Dor et al. fine-tuned a BiLSTM architecture with triplet loss to derive sentence embeddings for this dataset. As the table shows, SBERT clearly outperforms the BiLSTM approach by Dor et al."
        },
        {
            "text": "## 5 Evaluation - SentEval\n\nSentEval (Conneau and Kiela, 2018) is a popular toolkit to evaluate the quality of sentence embeddings. Sentence embeddings are used as features for a logistic regression classifier. The logistic regression classifier is trained on various tasks in a 10-fold cross-validation setup and the prediction accuracy is computed for the test-fold.",
            "page": 6,
            "x": 68,
            "y": 648,
            "width": 225,
            "height": 119,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-evaluation",
            "chunk_id": "a14a24d2-f6ef-4460-8f47-67d03936c3b7",
            "group_text": "## 5 Evaluation - SentEval\n\nSentEval (Conneau and Kiela, 2018) is a popular toolkit to evaluate the quality of sentence embeddings. Sentence embeddings are used as features for a logistic regression classifier. The logistic regression classifier is trained on various tasks in a 10-fold cross-validation setup and the prediction accuracy is computed for the test-fold.\n\nTable 4: Evaluation on the Wikipedia section triplets dataset (Dor et al., 2018). SBERT trained with triplet loss for one epoch.\n\nThe purpose of SBERT sentence embeddings are not to be used for transfer learning for other tasks. Here, we think fine-tuning BERT as described by Devlin et al. (2018) for new tasks is the more suitable method, as it updates all layers of the BERT network. However, SentEval can still give an impression on the quality of our sentence embeddings for various tasks.\n\n- \u2022 **MR**: Sentiment prediction for movie reviews snippets on a five start scale (Pang and Lee, 2005).\n\n- \u2022 **CR**: Sentiment prediction of customer product reviews (Hu and Liu, 2004).\n\n- \u2022 **SUBJ**: Subjectivity prediction of sentences from movie reviews and plot summaries (Pang and Lee, 2004).\n\n- \u2022 **MPQA**: Phrase level opinion polarity classification from newswire (Wiebe et al., 2005).\n\n- \u2022 **SST**: Stanford Sentiment Treebank with binary labels (Socher et al., 2013).\n\n- \u2022 **TREC**: Fine grained question-type classification from TREC (Li and Roth, 2002).\n\n- \u2022 **MRPC**: Microsoft Research Paraphrase Corpus from parallel news sources (Dolan et al., 2004).\n\nThe results can be found in Table 5. SBERT is able to achieve the best performance in 5 out of 7 tasks. The average performance increases by about 2 percentage points compared to InferSent as well as the Universal Sentence Encoder. Even though transfer learning is not the purpose of SBERT, it outperforms other state-of-the-art sentence embeddings methods on this task.\n\nTable 5: Evaluation of SBERT sentence embeddings using the SentEval toolkit. SentEval evaluates sentence embeddings on different sentence classification tasks by training a logistic regression classifier using the sentence embeddings as features. Scores are based on a 10-fold cross-validation.\n\nIt appears that the sentence embeddings from SBERT capture well sentiment information: We observe large improvements for all sentiment tasks (MR, CR, and SST) from SentEval in comparison to InferSent and Universal Sentence Encoder.\n\nThe only dataset where SBERT is significantly worse than Universal Sentence Encoder is the TREC dataset. Universal Sentence Encoder was pre-trained on question-answering data, which appears to be beneficial for the question-type classification task of the TREC dataset.\n\nAverage BERT embeddings or using the CLS-\ntoken output from a BERT network achieved bad\nresults for various STS tasks (Table 1), worse than\naverage GloVe embeddings. However, for Sent-\nEval, average BERT embeddings and the BERT\nCLS-token output achieves decent results (Ta-\nble 5), outperforming average GloVe embeddings.\nThe reason for this are the different setups. For\nthe STS tasks, we used cosine-similarity to es-\ntimate the similarities between sentence embed-\ndings. Cosine-similarity treats all dimensions\nequally. In contrast, SentEval fits a logistic regres-\nsion classifier to the sentence embeddings. This\nallows that certain dimensions can have higher or\nlower impact on the classification result.\n\nWe conclude that average BERT embeddings /\nCLS-token output from BERT return sentence em-\nbeddings that are infeasible to be used with cosine-\nsimilarity or with Manhatten / Euclidean distance.\nFor transfer learning, they yield slightly worse\nresults than InferSent or Universal Sentence En-\ncoder. However, using the described fine-tuning\nsetup with a siamese network structure on NLI\ndatasets yields sentence embeddings that achieve\na new state-of-the-art for the SentEval toolkit."
        },
        {
            "text": "Table 4: Evaluation on the Wikipedia section triplets dataset (Dor et al., 2018). SBERT trained with triplet loss for one epoch.",
            "page": 6,
            "x": 304,
            "y": 152,
            "width": 224,
            "height": 37,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-evaluation",
            "chunk_id": "c5a8cef7-6c07-4e8d-b291-e82a82383099",
            "group_text": "## 5 Evaluation - SentEval\n\nSentEval (Conneau and Kiela, 2018) is a popular toolkit to evaluate the quality of sentence embeddings. Sentence embeddings are used as features for a logistic regression classifier. The logistic regression classifier is trained on various tasks in a 10-fold cross-validation setup and the prediction accuracy is computed for the test-fold.\n\nTable 4: Evaluation on the Wikipedia section triplets dataset (Dor et al., 2018). SBERT trained with triplet loss for one epoch.\n\nThe purpose of SBERT sentence embeddings are not to be used for transfer learning for other tasks. Here, we think fine-tuning BERT as described by Devlin et al. (2018) for new tasks is the more suitable method, as it updates all layers of the BERT network. However, SentEval can still give an impression on the quality of our sentence embeddings for various tasks.\n\n- \u2022 **MR**: Sentiment prediction for movie reviews snippets on a five start scale (Pang and Lee, 2005).\n\n- \u2022 **CR**: Sentiment prediction of customer product reviews (Hu and Liu, 2004).\n\n- \u2022 **SUBJ**: Subjectivity prediction of sentences from movie reviews and plot summaries (Pang and Lee, 2004).\n\n- \u2022 **MPQA**: Phrase level opinion polarity classification from newswire (Wiebe et al., 2005).\n\n- \u2022 **SST**: Stanford Sentiment Treebank with binary labels (Socher et al., 2013).\n\n- \u2022 **TREC**: Fine grained question-type classification from TREC (Li and Roth, 2002).\n\n- \u2022 **MRPC**: Microsoft Research Paraphrase Corpus from parallel news sources (Dolan et al., 2004).\n\nThe results can be found in Table 5. SBERT is able to achieve the best performance in 5 out of 7 tasks. The average performance increases by about 2 percentage points compared to InferSent as well as the Universal Sentence Encoder. Even though transfer learning is not the purpose of SBERT, it outperforms other state-of-the-art sentence embeddings methods on this task.\n\nTable 5: Evaluation of SBERT sentence embeddings using the SentEval toolkit. SentEval evaluates sentence embeddings on different sentence classification tasks by training a logistic regression classifier using the sentence embeddings as features. Scores are based on a 10-fold cross-validation.\n\nIt appears that the sentence embeddings from SBERT capture well sentiment information: We observe large improvements for all sentiment tasks (MR, CR, and SST) from SentEval in comparison to InferSent and Universal Sentence Encoder.\n\nThe only dataset where SBERT is significantly worse than Universal Sentence Encoder is the TREC dataset. Universal Sentence Encoder was pre-trained on question-answering data, which appears to be beneficial for the question-type classification task of the TREC dataset.\n\nAverage BERT embeddings or using the CLS-\ntoken output from a BERT network achieved bad\nresults for various STS tasks (Table 1), worse than\naverage GloVe embeddings. However, for Sent-\nEval, average BERT embeddings and the BERT\nCLS-token output achieves decent results (Ta-\nble 5), outperforming average GloVe embeddings.\nThe reason for this are the different setups. For\nthe STS tasks, we used cosine-similarity to es-\ntimate the similarities between sentence embed-\ndings. Cosine-similarity treats all dimensions\nequally. In contrast, SentEval fits a logistic regres-\nsion classifier to the sentence embeddings. This\nallows that certain dimensions can have higher or\nlower impact on the classification result.\n\nWe conclude that average BERT embeddings /\nCLS-token output from BERT return sentence em-\nbeddings that are infeasible to be used with cosine-\nsimilarity or with Manhatten / Euclidean distance.\nFor transfer learning, they yield slightly worse\nresults than InferSent or Universal Sentence En-\ncoder. However, using the described fine-tuning\nsetup with a siamese network structure on NLI\ndatasets yields sentence embeddings that achieve\na new state-of-the-art for the SentEval toolkit."
        },
        {
            "text": "The purpose of SBERT sentence embeddings are not to be used for transfer learning for other tasks. Here, we think fine-tuning BERT as described by Devlin et al. (2018) for new tasks is the more suitable method, as it updates all layers of the BERT network. However, SentEval can still give an impression on the quality of our sentence embeddings for various tasks.",
            "page": 6,
            "x": 304,
            "y": 209,
            "width": 224,
            "height": 108,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-evaluation",
            "chunk_id": "2d5362e7-b175-4945-b395-578cbb00b50c",
            "group_text": "## 5 Evaluation - SentEval\n\nSentEval (Conneau and Kiela, 2018) is a popular toolkit to evaluate the quality of sentence embeddings. Sentence embeddings are used as features for a logistic regression classifier. The logistic regression classifier is trained on various tasks in a 10-fold cross-validation setup and the prediction accuracy is computed for the test-fold.\n\nTable 4: Evaluation on the Wikipedia section triplets dataset (Dor et al., 2018). SBERT trained with triplet loss for one epoch.\n\nThe purpose of SBERT sentence embeddings are not to be used for transfer learning for other tasks. Here, we think fine-tuning BERT as described by Devlin et al. (2018) for new tasks is the more suitable method, as it updates all layers of the BERT network. However, SentEval can still give an impression on the quality of our sentence embeddings for various tasks.\n\n- \u2022 **MR**: Sentiment prediction for movie reviews snippets on a five start scale (Pang and Lee, 2005).\n\n- \u2022 **CR**: Sentiment prediction of customer product reviews (Hu and Liu, 2004).\n\n- \u2022 **SUBJ**: Subjectivity prediction of sentences from movie reviews and plot summaries (Pang and Lee, 2004).\n\n- \u2022 **MPQA**: Phrase level opinion polarity classification from newswire (Wiebe et al., 2005).\n\n- \u2022 **SST**: Stanford Sentiment Treebank with binary labels (Socher et al., 2013).\n\n- \u2022 **TREC**: Fine grained question-type classification from TREC (Li and Roth, 2002).\n\n- \u2022 **MRPC**: Microsoft Research Paraphrase Corpus from parallel news sources (Dolan et al., 2004).\n\nThe results can be found in Table 5. SBERT is able to achieve the best performance in 5 out of 7 tasks. The average performance increases by about 2 percentage points compared to InferSent as well as the Universal Sentence Encoder. Even though transfer learning is not the purpose of SBERT, it outperforms other state-of-the-art sentence embeddings methods on this task.\n\nTable 5: Evaluation of SBERT sentence embeddings using the SentEval toolkit. SentEval evaluates sentence embeddings on different sentence classification tasks by training a logistic regression classifier using the sentence embeddings as features. Scores are based on a 10-fold cross-validation.\n\nIt appears that the sentence embeddings from SBERT capture well sentiment information: We observe large improvements for all sentiment tasks (MR, CR, and SST) from SentEval in comparison to InferSent and Universal Sentence Encoder.\n\nThe only dataset where SBERT is significantly worse than Universal Sentence Encoder is the TREC dataset. Universal Sentence Encoder was pre-trained on question-answering data, which appears to be beneficial for the question-type classification task of the TREC dataset.\n\nAverage BERT embeddings or using the CLS-\ntoken output from a BERT network achieved bad\nresults for various STS tasks (Table 1), worse than\naverage GloVe embeddings. However, for Sent-\nEval, average BERT embeddings and the BERT\nCLS-token output achieves decent results (Ta-\nble 5), outperforming average GloVe embeddings.\nThe reason for this are the different setups. For\nthe STS tasks, we used cosine-similarity to es-\ntimate the similarities between sentence embed-\ndings. Cosine-similarity treats all dimensions\nequally. In contrast, SentEval fits a logistic regres-\nsion classifier to the sentence embeddings. This\nallows that certain dimensions can have higher or\nlower impact on the classification result.\n\nWe conclude that average BERT embeddings /\nCLS-token output from BERT return sentence em-\nbeddings that are infeasible to be used with cosine-\nsimilarity or with Manhatten / Euclidean distance.\nFor transfer learning, they yield slightly worse\nresults than InferSent or Universal Sentence En-\ncoder. However, using the described fine-tuning\nsetup with a siamese network structure on NLI\ndatasets yields sentence embeddings that achieve\na new state-of-the-art for the SentEval toolkit."
        },
        {
            "text": "- \u2022 **MR**: Sentiment prediction for movie reviews snippets on a five start scale (Pang and Lee, 2005).\n\n- \u2022 **CR**: Sentiment prediction of customer product reviews (Hu and Liu, 2004).\n\n- \u2022 **SUBJ**: Subjectivity prediction of sentences from movie reviews and plot summaries (Pang and Lee, 2004).\n\n- \u2022 **MPQA**: Phrase level opinion polarity classification from newswire (Wiebe et al., 2005).\n\n- \u2022 **SST**: Stanford Sentiment Treebank with binary labels (Socher et al., 2013).\n\n- \u2022 **TREC**: Fine grained question-type classification from TREC (Li and Roth, 2002).\n\n- \u2022 **MRPC**: Microsoft Research Paraphrase Corpus from parallel news sources (Dolan et al., 2004).",
            "page": 6,
            "x": 305,
            "y": 319,
            "width": 225,
            "height": 332,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-evaluation",
            "chunk_id": "0577f70f-b9b1-450d-8c58-2a2d04e41360",
            "group_text": "## 5 Evaluation - SentEval\n\nSentEval (Conneau and Kiela, 2018) is a popular toolkit to evaluate the quality of sentence embeddings. Sentence embeddings are used as features for a logistic regression classifier. The logistic regression classifier is trained on various tasks in a 10-fold cross-validation setup and the prediction accuracy is computed for the test-fold.\n\nTable 4: Evaluation on the Wikipedia section triplets dataset (Dor et al., 2018). SBERT trained with triplet loss for one epoch.\n\nThe purpose of SBERT sentence embeddings are not to be used for transfer learning for other tasks. Here, we think fine-tuning BERT as described by Devlin et al. (2018) for new tasks is the more suitable method, as it updates all layers of the BERT network. However, SentEval can still give an impression on the quality of our sentence embeddings for various tasks.\n\n- \u2022 **MR**: Sentiment prediction for movie reviews snippets on a five start scale (Pang and Lee, 2005).\n\n- \u2022 **CR**: Sentiment prediction of customer product reviews (Hu and Liu, 2004).\n\n- \u2022 **SUBJ**: Subjectivity prediction of sentences from movie reviews and plot summaries (Pang and Lee, 2004).\n\n- \u2022 **MPQA**: Phrase level opinion polarity classification from newswire (Wiebe et al., 2005).\n\n- \u2022 **SST**: Stanford Sentiment Treebank with binary labels (Socher et al., 2013).\n\n- \u2022 **TREC**: Fine grained question-type classification from TREC (Li and Roth, 2002).\n\n- \u2022 **MRPC**: Microsoft Research Paraphrase Corpus from parallel news sources (Dolan et al., 2004).\n\nThe results can be found in Table 5. SBERT is able to achieve the best performance in 5 out of 7 tasks. The average performance increases by about 2 percentage points compared to InferSent as well as the Universal Sentence Encoder. Even though transfer learning is not the purpose of SBERT, it outperforms other state-of-the-art sentence embeddings methods on this task.\n\nTable 5: Evaluation of SBERT sentence embeddings using the SentEval toolkit. SentEval evaluates sentence embeddings on different sentence classification tasks by training a logistic regression classifier using the sentence embeddings as features. Scores are based on a 10-fold cross-validation.\n\nIt appears that the sentence embeddings from SBERT capture well sentiment information: We observe large improvements for all sentiment tasks (MR, CR, and SST) from SentEval in comparison to InferSent and Universal Sentence Encoder.\n\nThe only dataset where SBERT is significantly worse than Universal Sentence Encoder is the TREC dataset. Universal Sentence Encoder was pre-trained on question-answering data, which appears to be beneficial for the question-type classification task of the TREC dataset.\n\nAverage BERT embeddings or using the CLS-\ntoken output from a BERT network achieved bad\nresults for various STS tasks (Table 1), worse than\naverage GloVe embeddings. However, for Sent-\nEval, average BERT embeddings and the BERT\nCLS-token output achieves decent results (Ta-\nble 5), outperforming average GloVe embeddings.\nThe reason for this are the different setups. For\nthe STS tasks, we used cosine-similarity to es-\ntimate the similarities between sentence embed-\ndings. Cosine-similarity treats all dimensions\nequally. In contrast, SentEval fits a logistic regres-\nsion classifier to the sentence embeddings. This\nallows that certain dimensions can have higher or\nlower impact on the classification result.\n\nWe conclude that average BERT embeddings /\nCLS-token output from BERT return sentence em-\nbeddings that are infeasible to be used with cosine-\nsimilarity or with Manhatten / Euclidean distance.\nFor transfer learning, they yield slightly worse\nresults than InferSent or Universal Sentence En-\ncoder. However, using the described fine-tuning\nsetup with a siamese network structure on NLI\ndatasets yields sentence embeddings that achieve\na new state-of-the-art for the SentEval toolkit."
        },
        {
            "text": "The results can be found in Table 5. SBERT is able to achieve the best performance in 5 out of 7 tasks. The average performance increases by about 2 percentage points compared to InferSent as well as the Universal Sentence Encoder. Even though transfer learning is not the purpose of SBERT, it outperforms other state-of-the-art sentence embeddings methods on this task.",
            "page": 6,
            "x": 304,
            "y": 657,
            "width": 225,
            "height": 110,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-evaluation",
            "chunk_id": "2b44b183-69d4-4091-bad8-357d904ebea2",
            "group_text": "## 5 Evaluation - SentEval\n\nSentEval (Conneau and Kiela, 2018) is a popular toolkit to evaluate the quality of sentence embeddings. Sentence embeddings are used as features for a logistic regression classifier. The logistic regression classifier is trained on various tasks in a 10-fold cross-validation setup and the prediction accuracy is computed for the test-fold.\n\nTable 4: Evaluation on the Wikipedia section triplets dataset (Dor et al., 2018). SBERT trained with triplet loss for one epoch.\n\nThe purpose of SBERT sentence embeddings are not to be used for transfer learning for other tasks. Here, we think fine-tuning BERT as described by Devlin et al. (2018) for new tasks is the more suitable method, as it updates all layers of the BERT network. However, SentEval can still give an impression on the quality of our sentence embeddings for various tasks.\n\n- \u2022 **MR**: Sentiment prediction for movie reviews snippets on a five start scale (Pang and Lee, 2005).\n\n- \u2022 **CR**: Sentiment prediction of customer product reviews (Hu and Liu, 2004).\n\n- \u2022 **SUBJ**: Subjectivity prediction of sentences from movie reviews and plot summaries (Pang and Lee, 2004).\n\n- \u2022 **MPQA**: Phrase level opinion polarity classification from newswire (Wiebe et al., 2005).\n\n- \u2022 **SST**: Stanford Sentiment Treebank with binary labels (Socher et al., 2013).\n\n- \u2022 **TREC**: Fine grained question-type classification from TREC (Li and Roth, 2002).\n\n- \u2022 **MRPC**: Microsoft Research Paraphrase Corpus from parallel news sources (Dolan et al., 2004).\n\nThe results can be found in Table 5. SBERT is able to achieve the best performance in 5 out of 7 tasks. The average performance increases by about 2 percentage points compared to InferSent as well as the Universal Sentence Encoder. Even though transfer learning is not the purpose of SBERT, it outperforms other state-of-the-art sentence embeddings methods on this task.\n\nTable 5: Evaluation of SBERT sentence embeddings using the SentEval toolkit. SentEval evaluates sentence embeddings on different sentence classification tasks by training a logistic regression classifier using the sentence embeddings as features. Scores are based on a 10-fold cross-validation.\n\nIt appears that the sentence embeddings from SBERT capture well sentiment information: We observe large improvements for all sentiment tasks (MR, CR, and SST) from SentEval in comparison to InferSent and Universal Sentence Encoder.\n\nThe only dataset where SBERT is significantly worse than Universal Sentence Encoder is the TREC dataset. Universal Sentence Encoder was pre-trained on question-answering data, which appears to be beneficial for the question-type classification task of the TREC dataset.\n\nAverage BERT embeddings or using the CLS-\ntoken output from a BERT network achieved bad\nresults for various STS tasks (Table 1), worse than\naverage GloVe embeddings. However, for Sent-\nEval, average BERT embeddings and the BERT\nCLS-token output achieves decent results (Ta-\nble 5), outperforming average GloVe embeddings.\nThe reason for this are the different setups. For\nthe STS tasks, we used cosine-similarity to es-\ntimate the similarities between sentence embed-\ndings. Cosine-similarity treats all dimensions\nequally. In contrast, SentEval fits a logistic regres-\nsion classifier to the sentence embeddings. This\nallows that certain dimensions can have higher or\nlower impact on the classification result.\n\nWe conclude that average BERT embeddings /\nCLS-token output from BERT return sentence em-\nbeddings that are infeasible to be used with cosine-\nsimilarity or with Manhatten / Euclidean distance.\nFor transfer learning, they yield slightly worse\nresults than InferSent or Universal Sentence En-\ncoder. However, using the described fine-tuning\nsetup with a siamese network structure on NLI\ndatasets yields sentence embeddings that achieve\na new state-of-the-art for the SentEval toolkit."
        },
        {
            "text": "Table 5: Evaluation of SBERT sentence embeddings using the SentEval toolkit. SentEval evaluates sentence embeddings on different sentence classification tasks by training a logistic regression classifier using the sentence embeddings as features. Scores are based on a 10-fold cross-validation.",
            "page": 7,
            "x": 67,
            "y": 161,
            "width": 461,
            "height": 39,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-evaluation",
            "chunk_id": "bef65bf3-c8d3-405d-9e82-94a76c924f5f",
            "group_text": "## 5 Evaluation - SentEval\n\nSentEval (Conneau and Kiela, 2018) is a popular toolkit to evaluate the quality of sentence embeddings. Sentence embeddings are used as features for a logistic regression classifier. The logistic regression classifier is trained on various tasks in a 10-fold cross-validation setup and the prediction accuracy is computed for the test-fold.\n\nTable 4: Evaluation on the Wikipedia section triplets dataset (Dor et al., 2018). SBERT trained with triplet loss for one epoch.\n\nThe purpose of SBERT sentence embeddings are not to be used for transfer learning for other tasks. Here, we think fine-tuning BERT as described by Devlin et al. (2018) for new tasks is the more suitable method, as it updates all layers of the BERT network. However, SentEval can still give an impression on the quality of our sentence embeddings for various tasks.\n\n- \u2022 **MR**: Sentiment prediction for movie reviews snippets on a five start scale (Pang and Lee, 2005).\n\n- \u2022 **CR**: Sentiment prediction of customer product reviews (Hu and Liu, 2004).\n\n- \u2022 **SUBJ**: Subjectivity prediction of sentences from movie reviews and plot summaries (Pang and Lee, 2004).\n\n- \u2022 **MPQA**: Phrase level opinion polarity classification from newswire (Wiebe et al., 2005).\n\n- \u2022 **SST**: Stanford Sentiment Treebank with binary labels (Socher et al., 2013).\n\n- \u2022 **TREC**: Fine grained question-type classification from TREC (Li and Roth, 2002).\n\n- \u2022 **MRPC**: Microsoft Research Paraphrase Corpus from parallel news sources (Dolan et al., 2004).\n\nThe results can be found in Table 5. SBERT is able to achieve the best performance in 5 out of 7 tasks. The average performance increases by about 2 percentage points compared to InferSent as well as the Universal Sentence Encoder. Even though transfer learning is not the purpose of SBERT, it outperforms other state-of-the-art sentence embeddings methods on this task.\n\nTable 5: Evaluation of SBERT sentence embeddings using the SentEval toolkit. SentEval evaluates sentence embeddings on different sentence classification tasks by training a logistic regression classifier using the sentence embeddings as features. Scores are based on a 10-fold cross-validation.\n\nIt appears that the sentence embeddings from SBERT capture well sentiment information: We observe large improvements for all sentiment tasks (MR, CR, and SST) from SentEval in comparison to InferSent and Universal Sentence Encoder.\n\nThe only dataset where SBERT is significantly worse than Universal Sentence Encoder is the TREC dataset. Universal Sentence Encoder was pre-trained on question-answering data, which appears to be beneficial for the question-type classification task of the TREC dataset.\n\nAverage BERT embeddings or using the CLS-\ntoken output from a BERT network achieved bad\nresults for various STS tasks (Table 1), worse than\naverage GloVe embeddings. However, for Sent-\nEval, average BERT embeddings and the BERT\nCLS-token output achieves decent results (Ta-\nble 5), outperforming average GloVe embeddings.\nThe reason for this are the different setups. For\nthe STS tasks, we used cosine-similarity to es-\ntimate the similarities between sentence embed-\ndings. Cosine-similarity treats all dimensions\nequally. In contrast, SentEval fits a logistic regres-\nsion classifier to the sentence embeddings. This\nallows that certain dimensions can have higher or\nlower impact on the classification result.\n\nWe conclude that average BERT embeddings /\nCLS-token output from BERT return sentence em-\nbeddings that are infeasible to be used with cosine-\nsimilarity or with Manhatten / Euclidean distance.\nFor transfer learning, they yield slightly worse\nresults than InferSent or Universal Sentence En-\ncoder. However, using the described fine-tuning\nsetup with a siamese network structure on NLI\ndatasets yields sentence embeddings that achieve\na new state-of-the-art for the SentEval toolkit."
        },
        {
            "text": "It appears that the sentence embeddings from SBERT capture well sentiment information: We observe large improvements for all sentiment tasks (MR, CR, and SST) from SentEval in comparison to InferSent and Universal Sentence Encoder.",
            "page": 7,
            "x": 68,
            "y": 220,
            "width": 225,
            "height": 67,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-evaluation",
            "chunk_id": "55b35b40-12fb-47bc-a39d-69643af3d731",
            "group_text": "## 5 Evaluation - SentEval\n\nSentEval (Conneau and Kiela, 2018) is a popular toolkit to evaluate the quality of sentence embeddings. Sentence embeddings are used as features for a logistic regression classifier. The logistic regression classifier is trained on various tasks in a 10-fold cross-validation setup and the prediction accuracy is computed for the test-fold.\n\nTable 4: Evaluation on the Wikipedia section triplets dataset (Dor et al., 2018). SBERT trained with triplet loss for one epoch.\n\nThe purpose of SBERT sentence embeddings are not to be used for transfer learning for other tasks. Here, we think fine-tuning BERT as described by Devlin et al. (2018) for new tasks is the more suitable method, as it updates all layers of the BERT network. However, SentEval can still give an impression on the quality of our sentence embeddings for various tasks.\n\n- \u2022 **MR**: Sentiment prediction for movie reviews snippets on a five start scale (Pang and Lee, 2005).\n\n- \u2022 **CR**: Sentiment prediction of customer product reviews (Hu and Liu, 2004).\n\n- \u2022 **SUBJ**: Subjectivity prediction of sentences from movie reviews and plot summaries (Pang and Lee, 2004).\n\n- \u2022 **MPQA**: Phrase level opinion polarity classification from newswire (Wiebe et al., 2005).\n\n- \u2022 **SST**: Stanford Sentiment Treebank with binary labels (Socher et al., 2013).\n\n- \u2022 **TREC**: Fine grained question-type classification from TREC (Li and Roth, 2002).\n\n- \u2022 **MRPC**: Microsoft Research Paraphrase Corpus from parallel news sources (Dolan et al., 2004).\n\nThe results can be found in Table 5. SBERT is able to achieve the best performance in 5 out of 7 tasks. The average performance increases by about 2 percentage points compared to InferSent as well as the Universal Sentence Encoder. Even though transfer learning is not the purpose of SBERT, it outperforms other state-of-the-art sentence embeddings methods on this task.\n\nTable 5: Evaluation of SBERT sentence embeddings using the SentEval toolkit. SentEval evaluates sentence embeddings on different sentence classification tasks by training a logistic regression classifier using the sentence embeddings as features. Scores are based on a 10-fold cross-validation.\n\nIt appears that the sentence embeddings from SBERT capture well sentiment information: We observe large improvements for all sentiment tasks (MR, CR, and SST) from SentEval in comparison to InferSent and Universal Sentence Encoder.\n\nThe only dataset where SBERT is significantly worse than Universal Sentence Encoder is the TREC dataset. Universal Sentence Encoder was pre-trained on question-answering data, which appears to be beneficial for the question-type classification task of the TREC dataset.\n\nAverage BERT embeddings or using the CLS-\ntoken output from a BERT network achieved bad\nresults for various STS tasks (Table 1), worse than\naverage GloVe embeddings. However, for Sent-\nEval, average BERT embeddings and the BERT\nCLS-token output achieves decent results (Ta-\nble 5), outperforming average GloVe embeddings.\nThe reason for this are the different setups. For\nthe STS tasks, we used cosine-similarity to es-\ntimate the similarities between sentence embed-\ndings. Cosine-similarity treats all dimensions\nequally. In contrast, SentEval fits a logistic regres-\nsion classifier to the sentence embeddings. This\nallows that certain dimensions can have higher or\nlower impact on the classification result.\n\nWe conclude that average BERT embeddings /\nCLS-token output from BERT return sentence em-\nbeddings that are infeasible to be used with cosine-\nsimilarity or with Manhatten / Euclidean distance.\nFor transfer learning, they yield slightly worse\nresults than InferSent or Universal Sentence En-\ncoder. However, using the described fine-tuning\nsetup with a siamese network structure on NLI\ndatasets yields sentence embeddings that achieve\na new state-of-the-art for the SentEval toolkit."
        },
        {
            "text": "The only dataset where SBERT is significantly worse than Universal Sentence Encoder is the TREC dataset. Universal Sentence Encoder was pre-trained on question-answering data, which appears to be beneficial for the question-type classification task of the TREC dataset.",
            "page": 7,
            "x": 68,
            "y": 288,
            "width": 224,
            "height": 81,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-evaluation",
            "chunk_id": "da4359bb-532c-4234-bb26-e2d7b09452c9",
            "group_text": "## 5 Evaluation - SentEval\n\nSentEval (Conneau and Kiela, 2018) is a popular toolkit to evaluate the quality of sentence embeddings. Sentence embeddings are used as features for a logistic regression classifier. The logistic regression classifier is trained on various tasks in a 10-fold cross-validation setup and the prediction accuracy is computed for the test-fold.\n\nTable 4: Evaluation on the Wikipedia section triplets dataset (Dor et al., 2018). SBERT trained with triplet loss for one epoch.\n\nThe purpose of SBERT sentence embeddings are not to be used for transfer learning for other tasks. Here, we think fine-tuning BERT as described by Devlin et al. (2018) for new tasks is the more suitable method, as it updates all layers of the BERT network. However, SentEval can still give an impression on the quality of our sentence embeddings for various tasks.\n\n- \u2022 **MR**: Sentiment prediction for movie reviews snippets on a five start scale (Pang and Lee, 2005).\n\n- \u2022 **CR**: Sentiment prediction of customer product reviews (Hu and Liu, 2004).\n\n- \u2022 **SUBJ**: Subjectivity prediction of sentences from movie reviews and plot summaries (Pang and Lee, 2004).\n\n- \u2022 **MPQA**: Phrase level opinion polarity classification from newswire (Wiebe et al., 2005).\n\n- \u2022 **SST**: Stanford Sentiment Treebank with binary labels (Socher et al., 2013).\n\n- \u2022 **TREC**: Fine grained question-type classification from TREC (Li and Roth, 2002).\n\n- \u2022 **MRPC**: Microsoft Research Paraphrase Corpus from parallel news sources (Dolan et al., 2004).\n\nThe results can be found in Table 5. SBERT is able to achieve the best performance in 5 out of 7 tasks. The average performance increases by about 2 percentage points compared to InferSent as well as the Universal Sentence Encoder. Even though transfer learning is not the purpose of SBERT, it outperforms other state-of-the-art sentence embeddings methods on this task.\n\nTable 5: Evaluation of SBERT sentence embeddings using the SentEval toolkit. SentEval evaluates sentence embeddings on different sentence classification tasks by training a logistic regression classifier using the sentence embeddings as features. Scores are based on a 10-fold cross-validation.\n\nIt appears that the sentence embeddings from SBERT capture well sentiment information: We observe large improvements for all sentiment tasks (MR, CR, and SST) from SentEval in comparison to InferSent and Universal Sentence Encoder.\n\nThe only dataset where SBERT is significantly worse than Universal Sentence Encoder is the TREC dataset. Universal Sentence Encoder was pre-trained on question-answering data, which appears to be beneficial for the question-type classification task of the TREC dataset.\n\nAverage BERT embeddings or using the CLS-\ntoken output from a BERT network achieved bad\nresults for various STS tasks (Table 1), worse than\naverage GloVe embeddings. However, for Sent-\nEval, average BERT embeddings and the BERT\nCLS-token output achieves decent results (Ta-\nble 5), outperforming average GloVe embeddings.\nThe reason for this are the different setups. For\nthe STS tasks, we used cosine-similarity to es-\ntimate the similarities between sentence embed-\ndings. Cosine-similarity treats all dimensions\nequally. In contrast, SentEval fits a logistic regres-\nsion classifier to the sentence embeddings. This\nallows that certain dimensions can have higher or\nlower impact on the classification result.\n\nWe conclude that average BERT embeddings /\nCLS-token output from BERT return sentence em-\nbeddings that are infeasible to be used with cosine-\nsimilarity or with Manhatten / Euclidean distance.\nFor transfer learning, they yield slightly worse\nresults than InferSent or Universal Sentence En-\ncoder. However, using the described fine-tuning\nsetup with a siamese network structure on NLI\ndatasets yields sentence embeddings that achieve\na new state-of-the-art for the SentEval toolkit."
        },
        {
            "text": "Average BERT embeddings or using the CLS-\ntoken output from a BERT network achieved bad\nresults for various STS tasks (Table 1), worse than\naverage GloVe embeddings. However, for Sent-\nEval, average BERT embeddings and the BERT\nCLS-token output achieves decent results (Ta-\nble 5), outperforming average GloVe embeddings.\nThe reason for this are the different setups. For\nthe STS tasks, we used cosine-similarity to es-\ntimate the similarities between sentence embed-\ndings. Cosine-similarity treats all dimensions\nequally. In contrast, SentEval fits a logistic regres-\nsion classifier to the sentence embeddings. This\nallows that certain dimensions can have higher or\nlower impact on the classification result.",
            "page": 7,
            "x": 68,
            "y": 370,
            "width": 224,
            "height": 201,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-evaluation",
            "chunk_id": "fa0a8b6d-1d07-4744-9f7d-fafff12b6338",
            "group_text": "## 5 Evaluation - SentEval\n\nSentEval (Conneau and Kiela, 2018) is a popular toolkit to evaluate the quality of sentence embeddings. Sentence embeddings are used as features for a logistic regression classifier. The logistic regression classifier is trained on various tasks in a 10-fold cross-validation setup and the prediction accuracy is computed for the test-fold.\n\nTable 4: Evaluation on the Wikipedia section triplets dataset (Dor et al., 2018). SBERT trained with triplet loss for one epoch.\n\nThe purpose of SBERT sentence embeddings are not to be used for transfer learning for other tasks. Here, we think fine-tuning BERT as described by Devlin et al. (2018) for new tasks is the more suitable method, as it updates all layers of the BERT network. However, SentEval can still give an impression on the quality of our sentence embeddings for various tasks.\n\n- \u2022 **MR**: Sentiment prediction for movie reviews snippets on a five start scale (Pang and Lee, 2005).\n\n- \u2022 **CR**: Sentiment prediction of customer product reviews (Hu and Liu, 2004).\n\n- \u2022 **SUBJ**: Subjectivity prediction of sentences from movie reviews and plot summaries (Pang and Lee, 2004).\n\n- \u2022 **MPQA**: Phrase level opinion polarity classification from newswire (Wiebe et al., 2005).\n\n- \u2022 **SST**: Stanford Sentiment Treebank with binary labels (Socher et al., 2013).\n\n- \u2022 **TREC**: Fine grained question-type classification from TREC (Li and Roth, 2002).\n\n- \u2022 **MRPC**: Microsoft Research Paraphrase Corpus from parallel news sources (Dolan et al., 2004).\n\nThe results can be found in Table 5. SBERT is able to achieve the best performance in 5 out of 7 tasks. The average performance increases by about 2 percentage points compared to InferSent as well as the Universal Sentence Encoder. Even though transfer learning is not the purpose of SBERT, it outperforms other state-of-the-art sentence embeddings methods on this task.\n\nTable 5: Evaluation of SBERT sentence embeddings using the SentEval toolkit. SentEval evaluates sentence embeddings on different sentence classification tasks by training a logistic regression classifier using the sentence embeddings as features. Scores are based on a 10-fold cross-validation.\n\nIt appears that the sentence embeddings from SBERT capture well sentiment information: We observe large improvements for all sentiment tasks (MR, CR, and SST) from SentEval in comparison to InferSent and Universal Sentence Encoder.\n\nThe only dataset where SBERT is significantly worse than Universal Sentence Encoder is the TREC dataset. Universal Sentence Encoder was pre-trained on question-answering data, which appears to be beneficial for the question-type classification task of the TREC dataset.\n\nAverage BERT embeddings or using the CLS-\ntoken output from a BERT network achieved bad\nresults for various STS tasks (Table 1), worse than\naverage GloVe embeddings. However, for Sent-\nEval, average BERT embeddings and the BERT\nCLS-token output achieves decent results (Ta-\nble 5), outperforming average GloVe embeddings.\nThe reason for this are the different setups. For\nthe STS tasks, we used cosine-similarity to es-\ntimate the similarities between sentence embed-\ndings. Cosine-similarity treats all dimensions\nequally. In contrast, SentEval fits a logistic regres-\nsion classifier to the sentence embeddings. This\nallows that certain dimensions can have higher or\nlower impact on the classification result.\n\nWe conclude that average BERT embeddings /\nCLS-token output from BERT return sentence em-\nbeddings that are infeasible to be used with cosine-\nsimilarity or with Manhatten / Euclidean distance.\nFor transfer learning, they yield slightly worse\nresults than InferSent or Universal Sentence En-\ncoder. However, using the described fine-tuning\nsetup with a siamese network structure on NLI\ndatasets yields sentence embeddings that achieve\na new state-of-the-art for the SentEval toolkit."
        },
        {
            "text": "We conclude that average BERT embeddings /\nCLS-token output from BERT return sentence em-\nbeddings that are infeasible to be used with cosine-\nsimilarity or with Manhatten / Euclidean distance.\nFor transfer learning, they yield slightly worse\nresults than InferSent or Universal Sentence En-\ncoder. However, using the described fine-tuning\nsetup with a siamese network structure on NLI\ndatasets yields sentence embeddings that achieve\na new state-of-the-art for the SentEval toolkit.",
            "page": 7,
            "x": 68,
            "y": 573,
            "width": 225,
            "height": 135,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-evaluation",
            "chunk_id": "7182f713-64f1-4e1b-878a-a9f0b3ab338c",
            "group_text": "## 5 Evaluation - SentEval\n\nSentEval (Conneau and Kiela, 2018) is a popular toolkit to evaluate the quality of sentence embeddings. Sentence embeddings are used as features for a logistic regression classifier. The logistic regression classifier is trained on various tasks in a 10-fold cross-validation setup and the prediction accuracy is computed for the test-fold.\n\nTable 4: Evaluation on the Wikipedia section triplets dataset (Dor et al., 2018). SBERT trained with triplet loss for one epoch.\n\nThe purpose of SBERT sentence embeddings are not to be used for transfer learning for other tasks. Here, we think fine-tuning BERT as described by Devlin et al. (2018) for new tasks is the more suitable method, as it updates all layers of the BERT network. However, SentEval can still give an impression on the quality of our sentence embeddings for various tasks.\n\n- \u2022 **MR**: Sentiment prediction for movie reviews snippets on a five start scale (Pang and Lee, 2005).\n\n- \u2022 **CR**: Sentiment prediction of customer product reviews (Hu and Liu, 2004).\n\n- \u2022 **SUBJ**: Subjectivity prediction of sentences from movie reviews and plot summaries (Pang and Lee, 2004).\n\n- \u2022 **MPQA**: Phrase level opinion polarity classification from newswire (Wiebe et al., 2005).\n\n- \u2022 **SST**: Stanford Sentiment Treebank with binary labels (Socher et al., 2013).\n\n- \u2022 **TREC**: Fine grained question-type classification from TREC (Li and Roth, 2002).\n\n- \u2022 **MRPC**: Microsoft Research Paraphrase Corpus from parallel news sources (Dolan et al., 2004).\n\nThe results can be found in Table 5. SBERT is able to achieve the best performance in 5 out of 7 tasks. The average performance increases by about 2 percentage points compared to InferSent as well as the Universal Sentence Encoder. Even though transfer learning is not the purpose of SBERT, it outperforms other state-of-the-art sentence embeddings methods on this task.\n\nTable 5: Evaluation of SBERT sentence embeddings using the SentEval toolkit. SentEval evaluates sentence embeddings on different sentence classification tasks by training a logistic regression classifier using the sentence embeddings as features. Scores are based on a 10-fold cross-validation.\n\nIt appears that the sentence embeddings from SBERT capture well sentiment information: We observe large improvements for all sentiment tasks (MR, CR, and SST) from SentEval in comparison to InferSent and Universal Sentence Encoder.\n\nThe only dataset where SBERT is significantly worse than Universal Sentence Encoder is the TREC dataset. Universal Sentence Encoder was pre-trained on question-answering data, which appears to be beneficial for the question-type classification task of the TREC dataset.\n\nAverage BERT embeddings or using the CLS-\ntoken output from a BERT network achieved bad\nresults for various STS tasks (Table 1), worse than\naverage GloVe embeddings. However, for Sent-\nEval, average BERT embeddings and the BERT\nCLS-token output achieves decent results (Ta-\nble 5), outperforming average GloVe embeddings.\nThe reason for this are the different setups. For\nthe STS tasks, we used cosine-similarity to es-\ntimate the similarities between sentence embed-\ndings. Cosine-similarity treats all dimensions\nequally. In contrast, SentEval fits a logistic regres-\nsion classifier to the sentence embeddings. This\nallows that certain dimensions can have higher or\nlower impact on the classification result.\n\nWe conclude that average BERT embeddings /\nCLS-token output from BERT return sentence em-\nbeddings that are infeasible to be used with cosine-\nsimilarity or with Manhatten / Euclidean distance.\nFor transfer learning, they yield slightly worse\nresults than InferSent or Universal Sentence En-\ncoder. However, using the described fine-tuning\nsetup with a siamese network structure on NLI\ndatasets yields sentence embeddings that achieve\na new state-of-the-art for the SentEval toolkit."
        },
        {
            "text": "6  Ablation Study\n\nWe have demonstrated strong empirical results for\nthe quality of SBERT sentence embeddings.  In",
            "page": 7,
            "x": 68,
            "y": 717,
            "width": 225,
            "height": 51,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-ablation",
            "chunk_id": "6bc595a4-6c3b-4f67-8a03-342e06e1216e",
            "group_text": "6  Ablation Study\n\nWe have demonstrated strong empirical results for\nthe quality of SBERT sentence embeddings.  In\n\nthis section, we perform an ablation study of dif-\nferent aspects of SBERT in order to get a better\nunderstanding of their relative importance.\n\nWe evaluated different pooling strategies (MEAN, MAX, and CLS). For the classification objective function, we evaluate different concatenation methods. For each possible configuration, we train SBERT with 10 different random seeds and average the performances.\n\nThe objective function (classification vs. regres-\nsion) depends on the annotated dataset. For the\nclassification objective function, we train SBERT-\nbase on the SNLI and the Multi-NLI dataset. For\nthe regression objective function, we train on the\ntraining set of the STS benchmark dataset. Perfor-\nmances are measured on the development split of\nthe STS benchmark dataset. Results are shown in\nTable 6.\n\nTable 6: SBERT trained on NLI data with the classification objective function, on the STS benchmark (STSb) with the regression objective function. Configurations are evaluated on the development set of the STSb using cosine-similarity and Spearman\u2019s rank correlation. For the concatenation methods, we only report scores with MEAN pooling strategy.\n\nWhen trained with the classification objective\nfunction on NLI data, the pooling strategy has a\nrather minor impact. The impact of the concate-\nnation mode is much larger. InferSent (Conneau\n\net al., 2017) and Universal Sentence Encoder (Cer\net al., 2018) both use $(u, v, |u-v|, u*v)$ as input\nfor a softmax classifier. However, in our architec-\nture, adding the element-wise $u*v$ decreased the\nperformance.\n\nThe most important component is the element-\nwise difference $|u - v|$. Note, that the concate-\nnation mode is only relevant for training the soft-\nmax classifier. At inference, when predicting simi-\nlarities for the STS benchmark dataset, only the\nsentence embeddings $u$ and $v$ are used in combi-\nnation with cosine-similarity. The element-wise\ndifference measures the distance between the di-\nmensions of the two sentence embeddings, ensur-\ning that similar pairs are closer and dissimilar pairs\nare further apart.\n\nWhen trained with the regression objective function, we observe that the pooling strategy has a large impact. There, the MAX strategy perform significantly worse than MEAN or CLS-token strategy. This is in contrast to (Conneau et al., 2017), who found it beneficial for the BiLSTM-layer of InferSent to use MAX instead of MEAN pooling."
        },
        {
            "text": "this section, we perform an ablation study of dif-\nferent aspects of SBERT in order to get a better\nunderstanding of their relative importance.",
            "page": 7,
            "x": 304,
            "y": 221,
            "width": 224,
            "height": 40,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-ablation",
            "chunk_id": "01fb4e6f-3f10-4cf6-971a-9ddc417d1950",
            "group_text": "6  Ablation Study\n\nWe have demonstrated strong empirical results for\nthe quality of SBERT sentence embeddings.  In\n\nthis section, we perform an ablation study of dif-\nferent aspects of SBERT in order to get a better\nunderstanding of their relative importance.\n\nWe evaluated different pooling strategies (MEAN, MAX, and CLS). For the classification objective function, we evaluate different concatenation methods. For each possible configuration, we train SBERT with 10 different random seeds and average the performances.\n\nThe objective function (classification vs. regres-\nsion) depends on the annotated dataset. For the\nclassification objective function, we train SBERT-\nbase on the SNLI and the Multi-NLI dataset. For\nthe regression objective function, we train on the\ntraining set of the STS benchmark dataset. Perfor-\nmances are measured on the development split of\nthe STS benchmark dataset. Results are shown in\nTable 6.\n\nTable 6: SBERT trained on NLI data with the classification objective function, on the STS benchmark (STSb) with the regression objective function. Configurations are evaluated on the development set of the STSb using cosine-similarity and Spearman\u2019s rank correlation. For the concatenation methods, we only report scores with MEAN pooling strategy.\n\nWhen trained with the classification objective\nfunction on NLI data, the pooling strategy has a\nrather minor impact. The impact of the concate-\nnation mode is much larger. InferSent (Conneau\n\net al., 2017) and Universal Sentence Encoder (Cer\net al., 2018) both use $(u, v, |u-v|, u*v)$ as input\nfor a softmax classifier. However, in our architec-\nture, adding the element-wise $u*v$ decreased the\nperformance.\n\nThe most important component is the element-\nwise difference $|u - v|$. Note, that the concate-\nnation mode is only relevant for training the soft-\nmax classifier. At inference, when predicting simi-\nlarities for the STS benchmark dataset, only the\nsentence embeddings $u$ and $v$ are used in combi-\nnation with cosine-similarity. The element-wise\ndifference measures the distance between the di-\nmensions of the two sentence embeddings, ensur-\ning that similar pairs are closer and dissimilar pairs\nare further apart.\n\nWhen trained with the regression objective function, we observe that the pooling strategy has a large impact. There, the MAX strategy perform significantly worse than MEAN or CLS-token strategy. This is in contrast to (Conneau et al., 2017), who found it beneficial for the BiLSTM-layer of InferSent to use MAX instead of MEAN pooling."
        },
        {
            "text": "We evaluated different pooling strategies (MEAN, MAX, and CLS). For the classification objective function, we evaluate different concatenation methods. For each possible configuration, we train SBERT with 10 different random seeds and average the performances.",
            "page": 7,
            "x": 304,
            "y": 262,
            "width": 224,
            "height": 80,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-ablation",
            "chunk_id": "a1015fa8-f653-4d9d-9084-2a1485eaf6ef",
            "group_text": "6  Ablation Study\n\nWe have demonstrated strong empirical results for\nthe quality of SBERT sentence embeddings.  In\n\nthis section, we perform an ablation study of dif-\nferent aspects of SBERT in order to get a better\nunderstanding of their relative importance.\n\nWe evaluated different pooling strategies (MEAN, MAX, and CLS). For the classification objective function, we evaluate different concatenation methods. For each possible configuration, we train SBERT with 10 different random seeds and average the performances.\n\nThe objective function (classification vs. regres-\nsion) depends on the annotated dataset. For the\nclassification objective function, we train SBERT-\nbase on the SNLI and the Multi-NLI dataset. For\nthe regression objective function, we train on the\ntraining set of the STS benchmark dataset. Perfor-\nmances are measured on the development split of\nthe STS benchmark dataset. Results are shown in\nTable 6.\n\nTable 6: SBERT trained on NLI data with the classification objective function, on the STS benchmark (STSb) with the regression objective function. Configurations are evaluated on the development set of the STSb using cosine-similarity and Spearman\u2019s rank correlation. For the concatenation methods, we only report scores with MEAN pooling strategy.\n\nWhen trained with the classification objective\nfunction on NLI data, the pooling strategy has a\nrather minor impact. The impact of the concate-\nnation mode is much larger. InferSent (Conneau\n\net al., 2017) and Universal Sentence Encoder (Cer\net al., 2018) both use $(u, v, |u-v|, u*v)$ as input\nfor a softmax classifier. However, in our architec-\nture, adding the element-wise $u*v$ decreased the\nperformance.\n\nThe most important component is the element-\nwise difference $|u - v|$. Note, that the concate-\nnation mode is only relevant for training the soft-\nmax classifier. At inference, when predicting simi-\nlarities for the STS benchmark dataset, only the\nsentence embeddings $u$ and $v$ are used in combi-\nnation with cosine-similarity. The element-wise\ndifference measures the distance between the di-\nmensions of the two sentence embeddings, ensur-\ning that similar pairs are closer and dissimilar pairs\nare further apart.\n\nWhen trained with the regression objective function, we observe that the pooling strategy has a large impact. There, the MAX strategy perform significantly worse than MEAN or CLS-token strategy. This is in contrast to (Conneau et al., 2017), who found it beneficial for the BiLSTM-layer of InferSent to use MAX instead of MEAN pooling."
        },
        {
            "text": "The objective function (classification vs. regres-\nsion) depends on the annotated dataset. For the\nclassification objective function, we train SBERT-\nbase on the SNLI and the Multi-NLI dataset. For\nthe regression objective function, we train on the\ntraining set of the STS benchmark dataset. Perfor-\nmances are measured on the development split of\nthe STS benchmark dataset. Results are shown in\nTable 6.",
            "page": 7,
            "x": 304,
            "y": 343,
            "width": 225,
            "height": 120,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-ablation",
            "chunk_id": "69b5f3a4-dd78-4745-876e-b5aed21ee6fd",
            "group_text": "6  Ablation Study\n\nWe have demonstrated strong empirical results for\nthe quality of SBERT sentence embeddings.  In\n\nthis section, we perform an ablation study of dif-\nferent aspects of SBERT in order to get a better\nunderstanding of their relative importance.\n\nWe evaluated different pooling strategies (MEAN, MAX, and CLS). For the classification objective function, we evaluate different concatenation methods. For each possible configuration, we train SBERT with 10 different random seeds and average the performances.\n\nThe objective function (classification vs. regres-\nsion) depends on the annotated dataset. For the\nclassification objective function, we train SBERT-\nbase on the SNLI and the Multi-NLI dataset. For\nthe regression objective function, we train on the\ntraining set of the STS benchmark dataset. Perfor-\nmances are measured on the development split of\nthe STS benchmark dataset. Results are shown in\nTable 6.\n\nTable 6: SBERT trained on NLI data with the classification objective function, on the STS benchmark (STSb) with the regression objective function. Configurations are evaluated on the development set of the STSb using cosine-similarity and Spearman\u2019s rank correlation. For the concatenation methods, we only report scores with MEAN pooling strategy.\n\nWhen trained with the classification objective\nfunction on NLI data, the pooling strategy has a\nrather minor impact. The impact of the concate-\nnation mode is much larger. InferSent (Conneau\n\net al., 2017) and Universal Sentence Encoder (Cer\net al., 2018) both use $(u, v, |u-v|, u*v)$ as input\nfor a softmax classifier. However, in our architec-\nture, adding the element-wise $u*v$ decreased the\nperformance.\n\nThe most important component is the element-\nwise difference $|u - v|$. Note, that the concate-\nnation mode is only relevant for training the soft-\nmax classifier. At inference, when predicting simi-\nlarities for the STS benchmark dataset, only the\nsentence embeddings $u$ and $v$ are used in combi-\nnation with cosine-similarity. The element-wise\ndifference measures the distance between the di-\nmensions of the two sentence embeddings, ensur-\ning that similar pairs are closer and dissimilar pairs\nare further apart.\n\nWhen trained with the regression objective function, we observe that the pooling strategy has a large impact. There, the MAX strategy perform significantly worse than MEAN or CLS-token strategy. This is in contrast to (Conneau et al., 2017), who found it beneficial for the BiLSTM-layer of InferSent to use MAX instead of MEAN pooling."
        },
        {
            "text": "Table 6: SBERT trained on NLI data with the classification objective function, on the STS benchmark (STSb) with the regression objective function. Configurations are evaluated on the development set of the STSb using cosine-similarity and Spearman\u2019s rank correlation. For the concatenation methods, we only report scores with MEAN pooling strategy.",
            "page": 7,
            "x": 304,
            "y": 612,
            "width": 225,
            "height": 86,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-ablation",
            "chunk_id": "654f9316-4c71-4b9c-9c1f-f19fcdcc75ac",
            "group_text": "6  Ablation Study\n\nWe have demonstrated strong empirical results for\nthe quality of SBERT sentence embeddings.  In\n\nthis section, we perform an ablation study of dif-\nferent aspects of SBERT in order to get a better\nunderstanding of their relative importance.\n\nWe evaluated different pooling strategies (MEAN, MAX, and CLS). For the classification objective function, we evaluate different concatenation methods. For each possible configuration, we train SBERT with 10 different random seeds and average the performances.\n\nThe objective function (classification vs. regres-\nsion) depends on the annotated dataset. For the\nclassification objective function, we train SBERT-\nbase on the SNLI and the Multi-NLI dataset. For\nthe regression objective function, we train on the\ntraining set of the STS benchmark dataset. Perfor-\nmances are measured on the development split of\nthe STS benchmark dataset. Results are shown in\nTable 6.\n\nTable 6: SBERT trained on NLI data with the classification objective function, on the STS benchmark (STSb) with the regression objective function. Configurations are evaluated on the development set of the STSb using cosine-similarity and Spearman\u2019s rank correlation. For the concatenation methods, we only report scores with MEAN pooling strategy.\n\nWhen trained with the classification objective\nfunction on NLI data, the pooling strategy has a\nrather minor impact. The impact of the concate-\nnation mode is much larger. InferSent (Conneau\n\net al., 2017) and Universal Sentence Encoder (Cer\net al., 2018) both use $(u, v, |u-v|, u*v)$ as input\nfor a softmax classifier. However, in our architec-\nture, adding the element-wise $u*v$ decreased the\nperformance.\n\nThe most important component is the element-\nwise difference $|u - v|$. Note, that the concate-\nnation mode is only relevant for training the soft-\nmax classifier. At inference, when predicting simi-\nlarities for the STS benchmark dataset, only the\nsentence embeddings $u$ and $v$ are used in combi-\nnation with cosine-similarity. The element-wise\ndifference measures the distance between the di-\nmensions of the two sentence embeddings, ensur-\ning that similar pairs are closer and dissimilar pairs\nare further apart.\n\nWhen trained with the regression objective function, we observe that the pooling strategy has a large impact. There, the MAX strategy perform significantly worse than MEAN or CLS-token strategy. This is in contrast to (Conneau et al., 2017), who found it beneficial for the BiLSTM-layer of InferSent to use MAX instead of MEAN pooling."
        },
        {
            "text": "When trained with the classification objective\nfunction on NLI data, the pooling strategy has a\nrather minor impact. The impact of the concate-\nnation mode is much larger. InferSent (Conneau",
            "page": 7,
            "x": 304,
            "y": 711,
            "width": 224,
            "height": 56,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-ablation",
            "chunk_id": "3b3b2a2d-a680-4188-a284-19ec5b2dd3b8",
            "group_text": "6  Ablation Study\n\nWe have demonstrated strong empirical results for\nthe quality of SBERT sentence embeddings.  In\n\nthis section, we perform an ablation study of dif-\nferent aspects of SBERT in order to get a better\nunderstanding of their relative importance.\n\nWe evaluated different pooling strategies (MEAN, MAX, and CLS). For the classification objective function, we evaluate different concatenation methods. For each possible configuration, we train SBERT with 10 different random seeds and average the performances.\n\nThe objective function (classification vs. regres-\nsion) depends on the annotated dataset. For the\nclassification objective function, we train SBERT-\nbase on the SNLI and the Multi-NLI dataset. For\nthe regression objective function, we train on the\ntraining set of the STS benchmark dataset. Perfor-\nmances are measured on the development split of\nthe STS benchmark dataset. Results are shown in\nTable 6.\n\nTable 6: SBERT trained on NLI data with the classification objective function, on the STS benchmark (STSb) with the regression objective function. Configurations are evaluated on the development set of the STSb using cosine-similarity and Spearman\u2019s rank correlation. For the concatenation methods, we only report scores with MEAN pooling strategy.\n\nWhen trained with the classification objective\nfunction on NLI data, the pooling strategy has a\nrather minor impact. The impact of the concate-\nnation mode is much larger. InferSent (Conneau\n\net al., 2017) and Universal Sentence Encoder (Cer\net al., 2018) both use $(u, v, |u-v|, u*v)$ as input\nfor a softmax classifier. However, in our architec-\nture, adding the element-wise $u*v$ decreased the\nperformance.\n\nThe most important component is the element-\nwise difference $|u - v|$. Note, that the concate-\nnation mode is only relevant for training the soft-\nmax classifier. At inference, when predicting simi-\nlarities for the STS benchmark dataset, only the\nsentence embeddings $u$ and $v$ are used in combi-\nnation with cosine-similarity. The element-wise\ndifference measures the distance between the di-\nmensions of the two sentence embeddings, ensur-\ning that similar pairs are closer and dissimilar pairs\nare further apart.\n\nWhen trained with the regression objective function, we observe that the pooling strategy has a large impact. There, the MAX strategy perform significantly worse than MEAN or CLS-token strategy. This is in contrast to (Conneau et al., 2017), who found it beneficial for the BiLSTM-layer of InferSent to use MAX instead of MEAN pooling."
        },
        {
            "text": "et al., 2017) and Universal Sentence Encoder (Cer\net al., 2018) both use $(u, v, |u-v|, u*v)$ as input\nfor a softmax classifier. However, in our architec-\nture, adding the element-wise $u*v$ decreased the\nperformance.",
            "page": 8,
            "x": 67,
            "y": 62,
            "width": 226,
            "height": 68,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-ablation",
            "chunk_id": "aabe9c15-22d0-4b0e-9d38-6c1a85fb1d65",
            "group_text": "6  Ablation Study\n\nWe have demonstrated strong empirical results for\nthe quality of SBERT sentence embeddings.  In\n\nthis section, we perform an ablation study of dif-\nferent aspects of SBERT in order to get a better\nunderstanding of their relative importance.\n\nWe evaluated different pooling strategies (MEAN, MAX, and CLS). For the classification objective function, we evaluate different concatenation methods. For each possible configuration, we train SBERT with 10 different random seeds and average the performances.\n\nThe objective function (classification vs. regres-\nsion) depends on the annotated dataset. For the\nclassification objective function, we train SBERT-\nbase on the SNLI and the Multi-NLI dataset. For\nthe regression objective function, we train on the\ntraining set of the STS benchmark dataset. Perfor-\nmances are measured on the development split of\nthe STS benchmark dataset. Results are shown in\nTable 6.\n\nTable 6: SBERT trained on NLI data with the classification objective function, on the STS benchmark (STSb) with the regression objective function. Configurations are evaluated on the development set of the STSb using cosine-similarity and Spearman\u2019s rank correlation. For the concatenation methods, we only report scores with MEAN pooling strategy.\n\nWhen trained with the classification objective\nfunction on NLI data, the pooling strategy has a\nrather minor impact. The impact of the concate-\nnation mode is much larger. InferSent (Conneau\n\net al., 2017) and Universal Sentence Encoder (Cer\net al., 2018) both use $(u, v, |u-v|, u*v)$ as input\nfor a softmax classifier. However, in our architec-\nture, adding the element-wise $u*v$ decreased the\nperformance.\n\nThe most important component is the element-\nwise difference $|u - v|$. Note, that the concate-\nnation mode is only relevant for training the soft-\nmax classifier. At inference, when predicting simi-\nlarities for the STS benchmark dataset, only the\nsentence embeddings $u$ and $v$ are used in combi-\nnation with cosine-similarity. The element-wise\ndifference measures the distance between the di-\nmensions of the two sentence embeddings, ensur-\ning that similar pairs are closer and dissimilar pairs\nare further apart.\n\nWhen trained with the regression objective function, we observe that the pooling strategy has a large impact. There, the MAX strategy perform significantly worse than MEAN or CLS-token strategy. This is in contrast to (Conneau et al., 2017), who found it beneficial for the BiLSTM-layer of InferSent to use MAX instead of MEAN pooling."
        },
        {
            "text": "The most important component is the element-\nwise difference $|u - v|$. Note, that the concate-\nnation mode is only relevant for training the soft-\nmax classifier. At inference, when predicting simi-\nlarities for the STS benchmark dataset, only the\nsentence embeddings $u$ and $v$ are used in combi-\nnation with cosine-similarity. The element-wise\ndifference measures the distance between the di-\nmensions of the two sentence embeddings, ensur-\ning that similar pairs are closer and dissimilar pairs\nare further apart.",
            "page": 8,
            "x": 68,
            "y": 132,
            "width": 225,
            "height": 148,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-ablation",
            "chunk_id": "62c9d587-3cb4-442b-ba18-b7da3c69d358",
            "group_text": "6  Ablation Study\n\nWe have demonstrated strong empirical results for\nthe quality of SBERT sentence embeddings.  In\n\nthis section, we perform an ablation study of dif-\nferent aspects of SBERT in order to get a better\nunderstanding of their relative importance.\n\nWe evaluated different pooling strategies (MEAN, MAX, and CLS). For the classification objective function, we evaluate different concatenation methods. For each possible configuration, we train SBERT with 10 different random seeds and average the performances.\n\nThe objective function (classification vs. regres-\nsion) depends on the annotated dataset. For the\nclassification objective function, we train SBERT-\nbase on the SNLI and the Multi-NLI dataset. For\nthe regression objective function, we train on the\ntraining set of the STS benchmark dataset. Perfor-\nmances are measured on the development split of\nthe STS benchmark dataset. Results are shown in\nTable 6.\n\nTable 6: SBERT trained on NLI data with the classification objective function, on the STS benchmark (STSb) with the regression objective function. Configurations are evaluated on the development set of the STSb using cosine-similarity and Spearman\u2019s rank correlation. For the concatenation methods, we only report scores with MEAN pooling strategy.\n\nWhen trained with the classification objective\nfunction on NLI data, the pooling strategy has a\nrather minor impact. The impact of the concate-\nnation mode is much larger. InferSent (Conneau\n\net al., 2017) and Universal Sentence Encoder (Cer\net al., 2018) both use $(u, v, |u-v|, u*v)$ as input\nfor a softmax classifier. However, in our architec-\nture, adding the element-wise $u*v$ decreased the\nperformance.\n\nThe most important component is the element-\nwise difference $|u - v|$. Note, that the concate-\nnation mode is only relevant for training the soft-\nmax classifier. At inference, when predicting simi-\nlarities for the STS benchmark dataset, only the\nsentence embeddings $u$ and $v$ are used in combi-\nnation with cosine-similarity. The element-wise\ndifference measures the distance between the di-\nmensions of the two sentence embeddings, ensur-\ning that similar pairs are closer and dissimilar pairs\nare further apart.\n\nWhen trained with the regression objective function, we observe that the pooling strategy has a large impact. There, the MAX strategy perform significantly worse than MEAN or CLS-token strategy. This is in contrast to (Conneau et al., 2017), who found it beneficial for the BiLSTM-layer of InferSent to use MAX instead of MEAN pooling."
        },
        {
            "text": "When trained with the regression objective function, we observe that the pooling strategy has a large impact. There, the MAX strategy perform significantly worse than MEAN or CLS-token strategy. This is in contrast to (Conneau et al., 2017), who found it beneficial for the BiLSTM-layer of InferSent to use MAX instead of MEAN pooling.",
            "page": 8,
            "x": 68,
            "y": 282,
            "width": 226,
            "height": 98,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-ablation",
            "chunk_id": "28c292a2-ad54-4aa4-a475-bbf610679237",
            "group_text": "6  Ablation Study\n\nWe have demonstrated strong empirical results for\nthe quality of SBERT sentence embeddings.  In\n\nthis section, we perform an ablation study of dif-\nferent aspects of SBERT in order to get a better\nunderstanding of their relative importance.\n\nWe evaluated different pooling strategies (MEAN, MAX, and CLS). For the classification objective function, we evaluate different concatenation methods. For each possible configuration, we train SBERT with 10 different random seeds and average the performances.\n\nThe objective function (classification vs. regres-\nsion) depends on the annotated dataset. For the\nclassification objective function, we train SBERT-\nbase on the SNLI and the Multi-NLI dataset. For\nthe regression objective function, we train on the\ntraining set of the STS benchmark dataset. Perfor-\nmances are measured on the development split of\nthe STS benchmark dataset. Results are shown in\nTable 6.\n\nTable 6: SBERT trained on NLI data with the classification objective function, on the STS benchmark (STSb) with the regression objective function. Configurations are evaluated on the development set of the STSb using cosine-similarity and Spearman\u2019s rank correlation. For the concatenation methods, we only report scores with MEAN pooling strategy.\n\nWhen trained with the classification objective\nfunction on NLI data, the pooling strategy has a\nrather minor impact. The impact of the concate-\nnation mode is much larger. InferSent (Conneau\n\net al., 2017) and Universal Sentence Encoder (Cer\net al., 2018) both use $(u, v, |u-v|, u*v)$ as input\nfor a softmax classifier. However, in our architec-\nture, adding the element-wise $u*v$ decreased the\nperformance.\n\nThe most important component is the element-\nwise difference $|u - v|$. Note, that the concate-\nnation mode is only relevant for training the soft-\nmax classifier. At inference, when predicting simi-\nlarities for the STS benchmark dataset, only the\nsentence embeddings $u$ and $v$ are used in combi-\nnation with cosine-similarity. The element-wise\ndifference measures the distance between the di-\nmensions of the two sentence embeddings, ensur-\ning that similar pairs are closer and dissimilar pairs\nare further apart.\n\nWhen trained with the regression objective function, we observe that the pooling strategy has a large impact. There, the MAX strategy perform significantly worse than MEAN or CLS-token strategy. This is in contrast to (Conneau et al., 2017), who found it beneficial for the BiLSTM-layer of InferSent to use MAX instead of MEAN pooling."
        },
        {
            "text": "7  Computational Efficiency\n\nSentence embeddings need potentially be computed for Millions of sentences, hence, a high computation speed is desired. In this section, we compare SBERT to average GloVe embeddings, InferSent (Conneau et al., 2017), and Universal Sentence Encoder (Cer et al., 2018).",
            "page": 8,
            "x": 67,
            "y": 387,
            "width": 226,
            "height": 108,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "13-computational",
            "chunk_id": "797dfdc9-bbc3-48da-82ac-6e87b3c58360",
            "group_text": "7  Computational Efficiency\n\nSentence embeddings need potentially be computed for Millions of sentences, hence, a high computation speed is desired. In this section, we compare SBERT to average GloVe embeddings, InferSent (Conneau et al., 2017), and Universal Sentence Encoder (Cer et al., 2018).\n\nFor our comparison we use the sentences from the STS benchmark (Cer et al., 2017). We compute average GloVe embeddings using a simple for-loop with python dictionary lookups and NumPy. InferSent\u2074 is based on PyTorch. For Universal Sentence Encoder, we use the Tensor-Flow Hub version\u2075, which is based on Tensor-Flow. SBERT is based on PyTorch. For improved computation of sentence embeddings, we implemented a smart batching strategy: Sentences with similar lengths are grouped together and are only padded to the longest element in a mini-batch. This drastically reduces computational overhead from padding tokens.\n\nPerformances were measured on a server with  \nIntel i7-5820K CPU @ 3.30GHz, Nvidia Tesla\n\n\u2074https://github.com/facebookresearch/InferSent\n\u2075https://tfhub.dev/google/universal-sentence-encoder-large/3\n\nV100 GPU, CUDA 9.2 and cuDNN. The results\nare depicted in Table 7.\n\nOn CPU, InferSent is about 65% faster than SBERT. This is due to the much simpler network architecture. InferSent uses a single Bi-LSTM layer, while BERT uses 12 stacked transformer layers. However, an advantage of transformer networks is the computational efficiency on GPUs. There, SBERT with smart batching is about 9% faster than InferSent and about 55% faster than Universal Sentence Encoder. Smart batching achieves a speed-up of 89% on CPU and 48% on GPU. Average GloVe embeddings is obviously by a large margin the fastest method to compute sentence embeddings."
        },
        {
            "text": "For our comparison we use the sentences from the STS benchmark (Cer et al., 2017). We compute average GloVe embeddings using a simple for-loop with python dictionary lookups and NumPy. InferSent\u2074 is based on PyTorch. For Universal Sentence Encoder, we use the Tensor-Flow Hub version\u2075, which is based on Tensor-Flow. SBERT is based on PyTorch. For improved computation of sentence embeddings, we implemented a smart batching strategy: Sentences with similar lengths are grouped together and are only padded to the longest element in a mini-batch. This drastically reduces computational overhead from padding tokens.",
            "page": 8,
            "x": 67,
            "y": 496,
            "width": 226,
            "height": 189,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "13-computational",
            "chunk_id": "de2e52c4-0b57-4d40-98d2-87ca2112d9b1",
            "group_text": "7  Computational Efficiency\n\nSentence embeddings need potentially be computed for Millions of sentences, hence, a high computation speed is desired. In this section, we compare SBERT to average GloVe embeddings, InferSent (Conneau et al., 2017), and Universal Sentence Encoder (Cer et al., 2018).\n\nFor our comparison we use the sentences from the STS benchmark (Cer et al., 2017). We compute average GloVe embeddings using a simple for-loop with python dictionary lookups and NumPy. InferSent\u2074 is based on PyTorch. For Universal Sentence Encoder, we use the Tensor-Flow Hub version\u2075, which is based on Tensor-Flow. SBERT is based on PyTorch. For improved computation of sentence embeddings, we implemented a smart batching strategy: Sentences with similar lengths are grouped together and are only padded to the longest element in a mini-batch. This drastically reduces computational overhead from padding tokens.\n\nPerformances were measured on a server with  \nIntel i7-5820K CPU @ 3.30GHz, Nvidia Tesla\n\n\u2074https://github.com/facebookresearch/InferSent\n\u2075https://tfhub.dev/google/universal-sentence-encoder-large/3\n\nV100 GPU, CUDA 9.2 and cuDNN. The results\nare depicted in Table 7.\n\nOn CPU, InferSent is about 65% faster than SBERT. This is due to the much simpler network architecture. InferSent uses a single Bi-LSTM layer, while BERT uses 12 stacked transformer layers. However, an advantage of transformer networks is the computational efficiency on GPUs. There, SBERT with smart batching is about 9% faster than InferSent and about 55% faster than Universal Sentence Encoder. Smart batching achieves a speed-up of 89% on CPU and 48% on GPU. Average GloVe embeddings is obviously by a large margin the fastest method to compute sentence embeddings."
        },
        {
            "text": "Performances were measured on a server with  \nIntel i7-5820K CPU @ 3.30GHz, Nvidia Tesla",
            "page": 8,
            "x": 68,
            "y": 686,
            "width": 226,
            "height": 29,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "13-computational",
            "chunk_id": "38fcf376-1a00-49c2-a17e-ad57136fd6bf",
            "group_text": "7  Computational Efficiency\n\nSentence embeddings need potentially be computed for Millions of sentences, hence, a high computation speed is desired. In this section, we compare SBERT to average GloVe embeddings, InferSent (Conneau et al., 2017), and Universal Sentence Encoder (Cer et al., 2018).\n\nFor our comparison we use the sentences from the STS benchmark (Cer et al., 2017). We compute average GloVe embeddings using a simple for-loop with python dictionary lookups and NumPy. InferSent\u2074 is based on PyTorch. For Universal Sentence Encoder, we use the Tensor-Flow Hub version\u2075, which is based on Tensor-Flow. SBERT is based on PyTorch. For improved computation of sentence embeddings, we implemented a smart batching strategy: Sentences with similar lengths are grouped together and are only padded to the longest element in a mini-batch. This drastically reduces computational overhead from padding tokens.\n\nPerformances were measured on a server with  \nIntel i7-5820K CPU @ 3.30GHz, Nvidia Tesla\n\n\u2074https://github.com/facebookresearch/InferSent\n\u2075https://tfhub.dev/google/universal-sentence-encoder-large/3\n\nV100 GPU, CUDA 9.2 and cuDNN. The results\nare depicted in Table 7.\n\nOn CPU, InferSent is about 65% faster than SBERT. This is due to the much simpler network architecture. InferSent uses a single Bi-LSTM layer, while BERT uses 12 stacked transformer layers. However, an advantage of transformer networks is the computational efficiency on GPUs. There, SBERT with smart batching is about 9% faster than InferSent and about 55% faster than Universal Sentence Encoder. Smart batching achieves a speed-up of 89% on CPU and 48% on GPU. Average GloVe embeddings is obviously by a large margin the fastest method to compute sentence embeddings."
        },
        {
            "text": "\u2074https://github.com/facebookresearch/InferSent\n\u2075https://tfhub.dev/google/universal-sentence-encoder-large/3",
            "page": 8,
            "x": 68,
            "y": 722,
            "width": 218,
            "height": 45,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "13-computational",
            "chunk_id": "fd3d8a3a-a65f-4934-8909-fe671fba0f19",
            "group_text": "7  Computational Efficiency\n\nSentence embeddings need potentially be computed for Millions of sentences, hence, a high computation speed is desired. In this section, we compare SBERT to average GloVe embeddings, InferSent (Conneau et al., 2017), and Universal Sentence Encoder (Cer et al., 2018).\n\nFor our comparison we use the sentences from the STS benchmark (Cer et al., 2017). We compute average GloVe embeddings using a simple for-loop with python dictionary lookups and NumPy. InferSent\u2074 is based on PyTorch. For Universal Sentence Encoder, we use the Tensor-Flow Hub version\u2075, which is based on Tensor-Flow. SBERT is based on PyTorch. For improved computation of sentence embeddings, we implemented a smart batching strategy: Sentences with similar lengths are grouped together and are only padded to the longest element in a mini-batch. This drastically reduces computational overhead from padding tokens.\n\nPerformances were measured on a server with  \nIntel i7-5820K CPU @ 3.30GHz, Nvidia Tesla\n\n\u2074https://github.com/facebookresearch/InferSent\n\u2075https://tfhub.dev/google/universal-sentence-encoder-large/3\n\nV100 GPU, CUDA 9.2 and cuDNN. The results\nare depicted in Table 7.\n\nOn CPU, InferSent is about 65% faster than SBERT. This is due to the much simpler network architecture. InferSent uses a single Bi-LSTM layer, while BERT uses 12 stacked transformer layers. However, an advantage of transformer networks is the computational efficiency on GPUs. There, SBERT with smart batching is about 9% faster than InferSent and about 55% faster than Universal Sentence Encoder. Smart batching achieves a speed-up of 89% on CPU and 48% on GPU. Average GloVe embeddings is obviously by a large margin the fastest method to compute sentence embeddings."
        },
        {
            "text": "V100 GPU, CUDA 9.2 and cuDNN. The results\nare depicted in Table 7.",
            "page": 8,
            "x": 303,
            "y": 61,
            "width": 225,
            "height": 29,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "13-computational",
            "chunk_id": "831c12c5-7c36-4f82-8c6c-89a377e7e6cb",
            "group_text": "7  Computational Efficiency\n\nSentence embeddings need potentially be computed for Millions of sentences, hence, a high computation speed is desired. In this section, we compare SBERT to average GloVe embeddings, InferSent (Conneau et al., 2017), and Universal Sentence Encoder (Cer et al., 2018).\n\nFor our comparison we use the sentences from the STS benchmark (Cer et al., 2017). We compute average GloVe embeddings using a simple for-loop with python dictionary lookups and NumPy. InferSent\u2074 is based on PyTorch. For Universal Sentence Encoder, we use the Tensor-Flow Hub version\u2075, which is based on Tensor-Flow. SBERT is based on PyTorch. For improved computation of sentence embeddings, we implemented a smart batching strategy: Sentences with similar lengths are grouped together and are only padded to the longest element in a mini-batch. This drastically reduces computational overhead from padding tokens.\n\nPerformances were measured on a server with  \nIntel i7-5820K CPU @ 3.30GHz, Nvidia Tesla\n\n\u2074https://github.com/facebookresearch/InferSent\n\u2075https://tfhub.dev/google/universal-sentence-encoder-large/3\n\nV100 GPU, CUDA 9.2 and cuDNN. The results\nare depicted in Table 7.\n\nOn CPU, InferSent is about 65% faster than SBERT. This is due to the much simpler network architecture. InferSent uses a single Bi-LSTM layer, while BERT uses 12 stacked transformer layers. However, an advantage of transformer networks is the computational efficiency on GPUs. There, SBERT with smart batching is about 9% faster than InferSent and about 55% faster than Universal Sentence Encoder. Smart batching achieves a speed-up of 89% on CPU and 48% on GPU. Average GloVe embeddings is obviously by a large margin the fastest method to compute sentence embeddings."
        },
        {
            "text": "On CPU, InferSent is about 65% faster than SBERT. This is due to the much simpler network architecture. InferSent uses a single Bi-LSTM layer, while BERT uses 12 stacked transformer layers. However, an advantage of transformer networks is the computational efficiency on GPUs. There, SBERT with smart batching is about 9% faster than InferSent and about 55% faster than Universal Sentence Encoder. Smart batching achieves a speed-up of 89% on CPU and 48% on GPU. Average GloVe embeddings is obviously by a large margin the fastest method to compute sentence embeddings.",
            "page": 8,
            "x": 302,
            "y": 212,
            "width": 227,
            "height": 178,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "13-computational",
            "chunk_id": "67da07e0-b12d-4e2d-9347-7e29ebe2dbfa",
            "group_text": "7  Computational Efficiency\n\nSentence embeddings need potentially be computed for Millions of sentences, hence, a high computation speed is desired. In this section, we compare SBERT to average GloVe embeddings, InferSent (Conneau et al., 2017), and Universal Sentence Encoder (Cer et al., 2018).\n\nFor our comparison we use the sentences from the STS benchmark (Cer et al., 2017). We compute average GloVe embeddings using a simple for-loop with python dictionary lookups and NumPy. InferSent\u2074 is based on PyTorch. For Universal Sentence Encoder, we use the Tensor-Flow Hub version\u2075, which is based on Tensor-Flow. SBERT is based on PyTorch. For improved computation of sentence embeddings, we implemented a smart batching strategy: Sentences with similar lengths are grouped together and are only padded to the longest element in a mini-batch. This drastically reduces computational overhead from padding tokens.\n\nPerformances were measured on a server with  \nIntel i7-5820K CPU @ 3.30GHz, Nvidia Tesla\n\n\u2074https://github.com/facebookresearch/InferSent\n\u2075https://tfhub.dev/google/universal-sentence-encoder-large/3\n\nV100 GPU, CUDA 9.2 and cuDNN. The results\nare depicted in Table 7.\n\nOn CPU, InferSent is about 65% faster than SBERT. This is due to the much simpler network architecture. InferSent uses a single Bi-LSTM layer, while BERT uses 12 stacked transformer layers. However, an advantage of transformer networks is the computational efficiency on GPUs. There, SBERT with smart batching is about 9% faster than InferSent and about 55% faster than Universal Sentence Encoder. Smart batching achieves a speed-up of 89% on CPU and 48% on GPU. Average GloVe embeddings is obviously by a large margin the fastest method to compute sentence embeddings."
        },
        {
            "text": "## 8 Conclusion\n\nWe showed that BERT out-of-the-box maps sentences to a vector space that is rather unsuitable to be used with common similarity measures like cosine-similarity. The performance for seven STS tasks was below the performance of average GloVe embeddings.",
            "page": 8,
            "x": 302,
            "y": 399,
            "width": 226,
            "height": 107,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "14-conclusion",
            "chunk_id": "5d343321-015e-435d-9be4-8bbbef74a8c2",
            "group_text": "## 8 Conclusion\n\nWe showed that BERT out-of-the-box maps sentences to a vector space that is rather unsuitable to be used with common similarity measures like cosine-similarity. The performance for seven STS tasks was below the performance of average GloVe embeddings.\n\nTo overcome this shortcoming, we presented\nSentence-BERT (SBERT). SBERT fine-tunes\nBERT in a siamese / triplet network architec-\nture. We evaluated the quality on various com-\nmon benchmarks, where it could achieve a sig-\nnificant improvement over state-of-the-art sen-\ntence embeddings methods. Replacing BERT with\nRoBERTa did not yield a significant improvement\nin our experiments.\n\nSBERT is computationally efficient. On a GPU,\nit is about 9% faster than InferSent and about 55%\nfaster than Universal Sentence Encoder. SBERT\ncan be used for tasks which are computationally\nnot feasible to be modeled with BERT. For exam-\nple, clustering of 10,000 sentences with hierarchi-\ncal clustering requires with BERT about 65 hours,\nas around 50 Million sentence combinations must\nbe computed. With SBERT, we were able to re-\nduce the effort to about 5 seconds."
        },
        {
            "text": "To overcome this shortcoming, we presented\nSentence-BERT (SBERT). SBERT fine-tunes\nBERT in a siamese / triplet network architec-\nture. We evaluated the quality on various com-\nmon benchmarks, where it could achieve a sig-\nnificant improvement over state-of-the-art sen-\ntence embeddings methods. Replacing BERT with\nRoBERTa did not yield a significant improvement\nin our experiments.",
            "page": 8,
            "x": 303,
            "y": 507,
            "width": 226,
            "height": 122,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "14-conclusion",
            "chunk_id": "e75766ad-1bc0-4684-9a81-6ef24fe852fb",
            "group_text": "## 8 Conclusion\n\nWe showed that BERT out-of-the-box maps sentences to a vector space that is rather unsuitable to be used with common similarity measures like cosine-similarity. The performance for seven STS tasks was below the performance of average GloVe embeddings.\n\nTo overcome this shortcoming, we presented\nSentence-BERT (SBERT). SBERT fine-tunes\nBERT in a siamese / triplet network architec-\nture. We evaluated the quality on various com-\nmon benchmarks, where it could achieve a sig-\nnificant improvement over state-of-the-art sen-\ntence embeddings methods. Replacing BERT with\nRoBERTa did not yield a significant improvement\nin our experiments.\n\nSBERT is computationally efficient. On a GPU,\nit is about 9% faster than InferSent and about 55%\nfaster than Universal Sentence Encoder. SBERT\ncan be used for tasks which are computationally\nnot feasible to be modeled with BERT. For exam-\nple, clustering of 10,000 sentences with hierarchi-\ncal clustering requires with BERT about 65 hours,\nas around 50 Million sentence combinations must\nbe computed. With SBERT, we were able to re-\nduce the effort to about 5 seconds."
        },
        {
            "text": "SBERT is computationally efficient. On a GPU,\nit is about 9% faster than InferSent and about 55%\nfaster than Universal Sentence Encoder. SBERT\ncan be used for tasks which are computationally\nnot feasible to be modeled with BERT. For exam-\nple, clustering of 10,000 sentences with hierarchi-\ncal clustering requires with BERT about 65 hours,\nas around 50 Million sentence combinations must\nbe computed. With SBERT, we were able to re-\nduce the effort to about 5 seconds.",
            "page": 8,
            "x": 303,
            "y": 630,
            "width": 227,
            "height": 137,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "14-conclusion",
            "chunk_id": "130c6df5-169d-41d2-b3e5-afe9a7d306df",
            "group_text": "## 8 Conclusion\n\nWe showed that BERT out-of-the-box maps sentences to a vector space that is rather unsuitable to be used with common similarity measures like cosine-similarity. The performance for seven STS tasks was below the performance of average GloVe embeddings.\n\nTo overcome this shortcoming, we presented\nSentence-BERT (SBERT). SBERT fine-tunes\nBERT in a siamese / triplet network architec-\nture. We evaluated the quality on various com-\nmon benchmarks, where it could achieve a sig-\nnificant improvement over state-of-the-art sen-\ntence embeddings methods. Replacing BERT with\nRoBERTa did not yield a significant improvement\nin our experiments.\n\nSBERT is computationally efficient. On a GPU,\nit is about 9% faster than InferSent and about 55%\nfaster than Universal Sentence Encoder. SBERT\ncan be used for tasks which are computationally\nnot feasible to be modeled with BERT. For exam-\nple, clustering of 10,000 sentences with hierarchi-\ncal clustering requires with BERT about 65 hours,\nas around 50 Million sentence combinations must\nbe computed. With SBERT, we were able to re-\nduce the effort to about 5 seconds."
        }
    ]
}