{
    "annotations": [
        {
            "text": "# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "page": 1,
            "x": 111,
            "y": 66,
            "width": 374,
            "height": 38,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "0-title",
            "chunk_id": "2578f9d4-6580-4326-a632-520b757bfa22",
            "group_text": "# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n\n**Jacob Devlin**\u2003\u2003**Ming-Wei Chang**\u2003\u2003**Kenton Lee**\u2003\u2003**Kristina Toutanova**  \nGoogle AI Language  \n{jacobdevlin,mingweichang,kentonl,kristout}@google.com"
        },
        {
            "text": "**Jacob Devlin**\u2003\u2003**Ming-Wei Chang**\u2003\u2003**Kenton Lee**\u2003\u2003**Kristina Toutanova**  \nGoogle AI Language  \n{jacobdevlin,mingweichang,kentonl,kristout}@google.com",
            "page": 1,
            "x": 104,
            "y": 126,
            "width": 392,
            "height": 48,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "0-title",
            "chunk_id": "8b1e10b7-ee3d-4b77-9690-d63afe67b468",
            "group_text": "# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n\n**Jacob Devlin**\u2003\u2003**Ming-Wei Chang**\u2003\u2003**Kenton Lee**\u2003\u2003**Kristina Toutanova**  \nGoogle AI Language  \n{jacobdevlin,mingweichang,kentonl,kristout}@google.com"
        },
        {
            "text": "# Abstract\n\nWe introduce a new language representation model called **BERT**, which stands for **Bidirectional Encoder Representations** from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.",
            "page": 1,
            "x": 84,
            "y": 222,
            "width": 192,
            "height": 203,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "1-abstract",
            "chunk_id": "8fde519f-cb14-4bbe-8e20-23018ba5d719",
            "group_text": "# Abstract\n\nWe introduce a new language representation model called **BERT**, which stands for **Bidirectional Encoder Representations** from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\n\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
        },
        {
            "text": "BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
            "page": 1,
            "x": 86,
            "y": 426,
            "width": 190,
            "height": 122,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "1-abstract",
            "chunk_id": "f2585808-52fb-48ab-b724-e77fa97db540",
            "group_text": "# Abstract\n\nWe introduce a new language representation model called **BERT**, which stands for **Bidirectional Encoder Representations** from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\n\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
        },
        {
            "text": "# 1 Introduction\n\nLanguage model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016).",
            "page": 1,
            "x": 67,
            "y": 554,
            "width": 227,
            "height": 213,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "db8ad398-c236-4d9b-a14b-afc90089bf4e",
            "group_text": "# 1 Introduction\n\nLanguage model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for applying pre-trained language representations to downstream tasks: *feature-based* and *fine-tuning*. The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning *all* pre-trained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the fine-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying fine-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the fine-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations from Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\nword based only on its context.  Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\ntrain a deep bidirectional Transformer.  In addi-\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations.  The contributions\nof our paper are as follows:\n\n- \u2022 We demonstrate the importance of bidirectional pre-training for language representations. Unlike Radford et al. (2018), which uses unidirectional language models for pre-training, BERT uses masked language models to enable pre-trained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.\n\n- \u2022 We show that pre-trained representations reduce the need for many heavily-engineered task-specific architectures. BERT is the first fine-tuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.\n\n- \u2022 BERT advances the state of the art for eleven NLP tasks. The code and pre-trained models are available at https://github.com/google-research/bert."
        },
        {
            "text": "There are two existing strategies for applying pre-trained language representations to downstream tasks: *feature-based* and *fine-tuning*. The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning *all* pre-trained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
            "page": 1,
            "x": 303,
            "y": 222,
            "width": 225,
            "height": 204,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "0253fb2b-3cfb-4d01-8793-a78c3f4cb673",
            "group_text": "# 1 Introduction\n\nLanguage model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for applying pre-trained language representations to downstream tasks: *feature-based* and *fine-tuning*. The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning *all* pre-trained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the fine-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying fine-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the fine-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations from Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\nword based only on its context.  Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\ntrain a deep bidirectional Transformer.  In addi-\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations.  The contributions\nof our paper are as follows:\n\n- \u2022 We demonstrate the importance of bidirectional pre-training for language representations. Unlike Radford et al. (2018), which uses unidirectional language models for pre-training, BERT uses masked language models to enable pre-trained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.\n\n- \u2022 We show that pre-trained representations reduce the need for many heavily-engineered task-specific architectures. BERT is the first fine-tuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.\n\n- \u2022 BERT advances the state of the art for eleven NLP tasks. The code and pre-trained models are available at https://github.com/google-research/bert."
        },
        {
            "text": "We argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the fine-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying fine-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.",
            "page": 1,
            "x": 303,
            "y": 428,
            "width": 226,
            "height": 202,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "6542b7f7-facc-43cc-8ee8-f4cfc9a1662a",
            "group_text": "# 1 Introduction\n\nLanguage model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for applying pre-trained language representations to downstream tasks: *feature-based* and *fine-tuning*. The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning *all* pre-trained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the fine-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying fine-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the fine-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations from Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\nword based only on its context.  Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\ntrain a deep bidirectional Transformer.  In addi-\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations.  The contributions\nof our paper are as follows:\n\n- \u2022 We demonstrate the importance of bidirectional pre-training for language representations. Unlike Radford et al. (2018), which uses unidirectional language models for pre-training, BERT uses masked language models to enable pre-trained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.\n\n- \u2022 We show that pre-trained representations reduce the need for many heavily-engineered task-specific architectures. BERT is the first fine-tuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.\n\n- \u2022 BERT advances the state of the art for eleven NLP tasks. The code and pre-trained models are available at https://github.com/google-research/bert."
        },
        {
            "text": "In this paper, we improve the fine-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations from Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked",
            "page": 1,
            "x": 304,
            "y": 631,
            "width": 226,
            "height": 137,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "b0d80090-972f-4e1d-a89e-5d4d4990b16b",
            "group_text": "# 1 Introduction\n\nLanguage model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for applying pre-trained language representations to downstream tasks: *feature-based* and *fine-tuning*. The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning *all* pre-trained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the fine-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying fine-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the fine-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations from Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\nword based only on its context.  Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\ntrain a deep bidirectional Transformer.  In addi-\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations.  The contributions\nof our paper are as follows:\n\n- \u2022 We demonstrate the importance of bidirectional pre-training for language representations. Unlike Radford et al. (2018), which uses unidirectional language models for pre-training, BERT uses masked language models to enable pre-trained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.\n\n- \u2022 We show that pre-trained representations reduce the need for many heavily-engineered task-specific architectures. BERT is the first fine-tuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.\n\n- \u2022 BERT advances the state of the art for eleven NLP tasks. The code and pre-trained models are available at https://github.com/google-research/bert."
        },
        {
            "text": "word based only on its context.  Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\ntrain a deep bidirectional Transformer.  In addi-\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations.  The contributions\nof our paper are as follows:",
            "page": 2,
            "x": 66,
            "y": 61,
            "width": 228,
            "height": 125,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "3d2b4f70-6fe0-4582-83d5-f8948442370b",
            "group_text": "# 1 Introduction\n\nLanguage model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for applying pre-trained language representations to downstream tasks: *feature-based* and *fine-tuning*. The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning *all* pre-trained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the fine-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying fine-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the fine-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations from Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\nword based only on its context.  Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\ntrain a deep bidirectional Transformer.  In addi-\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations.  The contributions\nof our paper are as follows:\n\n- \u2022 We demonstrate the importance of bidirectional pre-training for language representations. Unlike Radford et al. (2018), which uses unidirectional language models for pre-training, BERT uses masked language models to enable pre-trained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.\n\n- \u2022 We show that pre-trained representations reduce the need for many heavily-engineered task-specific architectures. BERT is the first fine-tuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.\n\n- \u2022 BERT advances the state of the art for eleven NLP tasks. The code and pre-trained models are available at https://github.com/google-research/bert."
        },
        {
            "text": "- \u2022 We demonstrate the importance of bidirectional pre-training for language representations. Unlike Radford et al. (2018), which uses unidirectional language models for pre-training, BERT uses masked language models to enable pre-trained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.\n\n- \u2022 We show that pre-trained representations reduce the need for many heavily-engineered task-specific architectures. BERT is the first fine-tuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.\n\n- \u2022 BERT advances the state of the art for eleven NLP tasks. The code and pre-trained models are available at https://github.com/google-research/bert.",
            "page": 2,
            "x": 68,
            "y": 190,
            "width": 227,
            "height": 292,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "8e1319cc-f36c-474e-8522-21323e33aff2",
            "group_text": "# 1 Introduction\n\nLanguage model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for applying pre-trained language representations to downstream tasks: *feature-based* and *fine-tuning*. The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning *all* pre-trained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the fine-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying fine-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the fine-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations from Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\nword based only on its context.  Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\ntrain a deep bidirectional Transformer.  In addi-\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations.  The contributions\nof our paper are as follows:\n\n- \u2022 We demonstrate the importance of bidirectional pre-training for language representations. Unlike Radford et al. (2018), which uses unidirectional language models for pre-training, BERT uses masked language models to enable pre-trained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.\n\n- \u2022 We show that pre-trained representations reduce the need for many heavily-engineered task-specific architectures. BERT is the first fine-tuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.\n\n- \u2022 BERT advances the state of the art for eleven NLP tasks. The code and pre-trained models are available at https://github.com/google-research/bert."
        },
        {
            "text": "2   Related Work\n\nThere is a long history of pre-training general language representations, and we briefly review the most widely-used approaches in this section.",
            "page": 2,
            "x": 67,
            "y": 487,
            "width": 227,
            "height": 66,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "3-related",
            "chunk_id": "cb20cb42-425a-4f28-832d-a968181c7c76",
            "group_text": "2   Related Work\n\nThere is a long history of pre-training general language representations, and we briefly review the most widely-used approaches in this section."
        },
        {
            "text": "2.1  Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of words has been an active area of research for decades, including non-neural (Brown et al., 1992; Ando and Zhang, 2005; Blitzer et al., 2006) and neural (Mikolov et al., 2013; Pennington et al., 2014) methods. Pre-trained word embeddings are an integral part of modern NLP systems, offering significant improvements over embeddings learned from scratch (Turian et al., 2010). To pre-train word embedding vectors, left-to-right language modeling objectives have been used (Mnih and Hinton, 2009), as well as objectives to discriminate correct from incorrect words in left and right context (Mikolov et al., 2013).",
            "page": 2,
            "x": 66,
            "y": 557,
            "width": 228,
            "height": 211,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "4-unsupervised",
            "chunk_id": "e8318992-2283-43e0-91a8-7ac6dcadeb61",
            "group_text": "2.1  Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of words has been an active area of research for decades, including non-neural (Brown et al., 1992; Ando and Zhang, 2005; Blitzer et al., 2006) and neural (Mikolov et al., 2013; Pennington et al., 2014) methods. Pre-trained word embeddings are an integral part of modern NLP systems, offering significant improvements over embeddings learned from scratch (Turian et al., 2010). To pre-train word embedding vectors, left-to-right language modeling objectives have been used (Mnih and Hinton, 2009), as well as objectives to discriminate correct from incorrect words in left and right context (Mikolov et al., 2013).\n\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\n\nELMo and its predecessor (Peters et al., 2017, 2018a) generalize traditional word embedding research along a different dimension. They extract *context-sensitive* features from a left-to-right and a right-to-left language model. The contextual representation of each token is the concatenation of the left-to-right and right-to-left representations. When integrating contextual word embeddings with existing task-specific architectures, ELMo advances the state of the art for several major NLP benchmarks (Peters et al., 2018a) including question answering (Rajpurkar et al., 2016), sentiment analysis (Socher et al., 2013), and named entity recognition (Tjong Kim Sang and De Meulder, 2003). Melamud et al. (2016) proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs. Similar to ELMo, their model is feature-based and not deeply bidirectional. Fedus et al. (2018) shows that the cloze task can be used to improve the robustness of text generation models."
        },
        {
            "text": "These approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).",
            "page": 2,
            "x": 303,
            "y": 62,
            "width": 227,
            "height": 151,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "4-unsupervised",
            "chunk_id": "cd5c490a-f116-426a-a3f3-1b92f68c73ec",
            "group_text": "2.1  Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of words has been an active area of research for decades, including non-neural (Brown et al., 1992; Ando and Zhang, 2005; Blitzer et al., 2006) and neural (Mikolov et al., 2013; Pennington et al., 2014) methods. Pre-trained word embeddings are an integral part of modern NLP systems, offering significant improvements over embeddings learned from scratch (Turian et al., 2010). To pre-train word embedding vectors, left-to-right language modeling objectives have been used (Mnih and Hinton, 2009), as well as objectives to discriminate correct from incorrect words in left and right context (Mikolov et al., 2013).\n\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\n\nELMo and its predecessor (Peters et al., 2017, 2018a) generalize traditional word embedding research along a different dimension. They extract *context-sensitive* features from a left-to-right and a right-to-left language model. The contextual representation of each token is the concatenation of the left-to-right and right-to-left representations. When integrating contextual word embeddings with existing task-specific architectures, ELMo advances the state of the art for several major NLP benchmarks (Peters et al., 2018a) including question answering (Rajpurkar et al., 2016), sentiment analysis (Socher et al., 2013), and named entity recognition (Tjong Kim Sang and De Meulder, 2003). Melamud et al. (2016) proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs. Similar to ELMo, their model is feature-based and not deeply bidirectional. Fedus et al. (2018) shows that the cloze task can be used to improve the robustness of text generation models."
        },
        {
            "text": "ELMo and its predecessor (Peters et al., 2017, 2018a) generalize traditional word embedding research along a different dimension. They extract *context-sensitive* features from a left-to-right and a right-to-left language model. The contextual representation of each token is the concatenation of the left-to-right and right-to-left representations. When integrating contextual word embeddings with existing task-specific architectures, ELMo advances the state of the art for several major NLP benchmarks (Peters et al., 2018a) including question answering (Rajpurkar et al., 2016), sentiment analysis (Socher et al., 2013), and named entity recognition (Tjong Kim Sang and De Meulder, 2003). Melamud et al. (2016) proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs. Similar to ELMo, their model is feature-based and not deeply bidirectional. Fedus et al. (2018) shows that the cloze task can be used to improve the robustness of text generation models.",
            "page": 2,
            "x": 303,
            "y": 213,
            "width": 228,
            "height": 300,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "4-unsupervised",
            "chunk_id": "732311bc-6059-4f35-94d0-e5cecb1a792b",
            "group_text": "2.1  Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of words has been an active area of research for decades, including non-neural (Brown et al., 1992; Ando and Zhang, 2005; Blitzer et al., 2006) and neural (Mikolov et al., 2013; Pennington et al., 2014) methods. Pre-trained word embeddings are an integral part of modern NLP systems, offering significant improvements over embeddings learned from scratch (Turian et al., 2010). To pre-train word embedding vectors, left-to-right language modeling objectives have been used (Mnih and Hinton, 2009), as well as objectives to discriminate correct from incorrect words in left and right context (Mikolov et al., 2013).\n\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\n\nELMo and its predecessor (Peters et al., 2017, 2018a) generalize traditional word embedding research along a different dimension. They extract *context-sensitive* features from a left-to-right and a right-to-left language model. The contextual representation of each token is the concatenation of the left-to-right and right-to-left representations. When integrating contextual word embeddings with existing task-specific architectures, ELMo advances the state of the art for several major NLP benchmarks (Peters et al., 2018a) including question answering (Rajpurkar et al., 2016), sentiment analysis (Socher et al., 2013), and named entity recognition (Tjong Kim Sang and De Meulder, 2003). Melamud et al. (2016) proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs. Similar to ELMo, their model is feature-based and not deeply bidirectional. Fedus et al. (2018) shows that the cloze task can be used to improve the robustness of text generation models."
        },
        {
            "text": "## 2.2  Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the first works in this direction only pre-trained word embedding parameters from unlabeled text (Col-lobert and Weston, 2008).",
            "page": 2,
            "x": 302,
            "y": 524,
            "width": 228,
            "height": 77,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-unsupervised",
            "chunk_id": "de2bb12e-74cc-4703-ba2e-9ae35d3742e7",
            "group_text": "## 2.2  Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the first works in this direction only pre-trained word embedding parameters from unlabeled text (Col-lobert and Weston, 2008).\n\nMore recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018). The advantage of these approaches is that few parameters need to be learned from scratch. At least partly due to this advantage, OpenAI GPT (Radford et al., 2018) achieved previously state-of-the-art results on many sentence-level tasks from the GLUE benchmark (Wang et al., 2018a). Left-to-right language model-\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015)."
        },
        {
            "text": "More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018). The advantage of these approaches is that few parameters need to be learned from scratch. At least partly due to this advantage, OpenAI GPT (Radford et al., 2018) achieved previously state-of-the-art results on many sentence-level tasks from the GLUE benchmark (Wang et al., 2018a). Left-to-right language model-",
            "page": 2,
            "x": 303,
            "y": 603,
            "width": 228,
            "height": 166,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-unsupervised",
            "chunk_id": "e61c8373-a331-4661-8b5c-0398e34afc14",
            "group_text": "## 2.2  Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the first works in this direction only pre-trained word embedding parameters from unlabeled text (Col-lobert and Weston, 2008).\n\nMore recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018). The advantage of these approaches is that few parameters need to be learned from scratch. At least partly due to this advantage, OpenAI GPT (Radford et al., 2018) achieved previously state-of-the-art results on many sentence-level tasks from the GLUE benchmark (Wang et al., 2018a). Left-to-right language model-\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015)."
        },
        {
            "text": "ing and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).",
            "page": 3,
            "x": 67,
            "y": 337,
            "width": 225,
            "height": 44,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-unsupervised",
            "chunk_id": "5a825778-bf52-4443-aca5-b8fba63f7be5",
            "group_text": "## 2.2  Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the first works in this direction only pre-trained word embedding parameters from unlabeled text (Col-lobert and Weston, 2008).\n\nMore recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018). The advantage of these approaches is that few parameters need to be learned from scratch. At least partly due to this advantage, OpenAI GPT (Radford et al., 2018) achieved previously state-of-the-art results on many sentence-level tasks from the GLUE benchmark (Wang et al., 2018a). Left-to-right language model-\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015)."
        },
        {
            "text": "2.3  Transfer Learning from Supervised Data\nThere has also been work showing effective transfer from supervised tasks with large datasets, such as natural language inference (Conneau et al., 2017) and machine translation (McCann et al., 2017). Computer vision research has also demonstrated the importance of transfer learning from large pre-trained models, where an effective recipe is to fine-tune models pre-trained with ImageNet (Deng et al., 2009; Yosinski et al., 2014).",
            "page": 3,
            "x": 67,
            "y": 387,
            "width": 226,
            "height": 144,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "6-transfer",
            "chunk_id": "2ce5ddb8-a85d-405e-8827-74dfd0efde4b",
            "group_text": "2.3  Transfer Learning from Supervised Data\nThere has also been work showing effective transfer from supervised tasks with large datasets, such as natural language inference (Conneau et al., 2017) and machine translation (McCann et al., 2017). Computer vision research has also demonstrated the importance of transfer learning from large pre-trained models, where an effective recipe is to fine-tune models pre-trained with ImageNet (Deng et al., 2009; Yosinski et al., 2014)."
        },
        {
            "text": "3   BERT\n\nWe introduce BERT and its detailed implementation in this section. There are two steps in our framework: _pre-training_ and _fine-tuning_. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine-tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters. The question-answering example in Figure 1 will serve as a running example for this section.",
            "page": 3,
            "x": 67,
            "y": 538,
            "width": 226,
            "height": 200,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-bert",
            "chunk_id": "264eec10-c941-409d-83fc-3c1f2539a437",
            "group_text": "3   BERT\n\nWe introduce BERT and its detailed implementation in this section. There are two steps in our framework: _pre-training_ and _fine-tuning_. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine-tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters. The question-answering example in Figure 1 will serve as a running example for this section.\n\nA distinctive feature of BERT is its unified ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the final downstream architecture.\n\n**Model Architecture** BERT\u2019s model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al. (2017) and released in the `tensor2tensor` library.\u00b9 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \u201cThe Annotated Transformer.\u201d\u00b2\n\nIn this work, we denote the number of layers (i.e., Transformer blocks) as $L$, the hidden size as $H$, and the number of self-attention heads as $A$.\u00b3\nWe primarily report results on two model sizes:\n**BERT$_{BASE}$** (L=12, H=768, A=12, Total Parameters=110M) and **BERT$_{LARGE}$** (L=24, H=1024, A=16, Total Parameters=340M).\n\nBERT$_{BASE}$ was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left.$^4$\n\n1https://github.com/tensorflow/tensor2tensor  \n2http://nlp.seas.harvard.edu/2018/04/03/attention.html  \n3In all cases we set the feed-forward/filter size to be 4H, i.e., 3072 for the $H=768$ and 4096 for the $H=1024$.  \n4We note that in the literature the bidirectional Trans-\n\n**Input/Output Representations**  To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., ( Question, Answer )) in one token sequence. Throughout this work, a \u201csentence\u201d can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. A \u201csequence\u201d refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary. The first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as $E$, the final hidden vector of the special [CLS] token as $C \\in \\mathbb{R}^H$, and the final hidden vector for the $i^{th}$ input token as $T_i \\in \\mathbb{R}^H$.\n\nFor a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualization of this construction can be seen in Figure 2."
        },
        {
            "text": "A distinctive feature of BERT is its unified ar-\nchitecture across different tasks. There is mini-",
            "page": 3,
            "x": 67,
            "y": 739,
            "width": 226,
            "height": 27,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-bert",
            "chunk_id": "62ac7289-3312-4758-a3bb-155816d2b1d5",
            "group_text": "3   BERT\n\nWe introduce BERT and its detailed implementation in this section. There are two steps in our framework: _pre-training_ and _fine-tuning_. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine-tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters. The question-answering example in Figure 1 will serve as a running example for this section.\n\nA distinctive feature of BERT is its unified ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the final downstream architecture.\n\n**Model Architecture** BERT\u2019s model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al. (2017) and released in the `tensor2tensor` library.\u00b9 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \u201cThe Annotated Transformer.\u201d\u00b2\n\nIn this work, we denote the number of layers (i.e., Transformer blocks) as $L$, the hidden size as $H$, and the number of self-attention heads as $A$.\u00b3\nWe primarily report results on two model sizes:\n**BERT$_{BASE}$** (L=12, H=768, A=12, Total Parameters=110M) and **BERT$_{LARGE}$** (L=24, H=1024, A=16, Total Parameters=340M).\n\nBERT$_{BASE}$ was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left.$^4$\n\n1https://github.com/tensorflow/tensor2tensor  \n2http://nlp.seas.harvard.edu/2018/04/03/attention.html  \n3In all cases we set the feed-forward/filter size to be 4H, i.e., 3072 for the $H=768$ and 4096 for the $H=1024$.  \n4We note that in the literature the bidirectional Trans-\n\n**Input/Output Representations**  To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., ( Question, Answer )) in one token sequence. Throughout this work, a \u201csentence\u201d can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. A \u201csequence\u201d refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary. The first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as $E$, the final hidden vector of the special [CLS] token as $C \\in \\mathbb{R}^H$, and the final hidden vector for the $i^{th}$ input token as $T_i \\in \\mathbb{R}^H$.\n\nFor a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualization of this construction can be seen in Figure 2."
        },
        {
            "text": "mal difference between the pre-trained architec-\nture and the final downstream architecture.",
            "page": 3,
            "x": 304,
            "y": 337,
            "width": 224,
            "height": 29,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-bert",
            "chunk_id": "d456a3ca-5f0f-4c54-a5dd-a249777abe7d",
            "group_text": "3   BERT\n\nWe introduce BERT and its detailed implementation in this section. There are two steps in our framework: _pre-training_ and _fine-tuning_. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine-tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters. The question-answering example in Figure 1 will serve as a running example for this section.\n\nA distinctive feature of BERT is its unified ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the final downstream architecture.\n\n**Model Architecture** BERT\u2019s model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al. (2017) and released in the `tensor2tensor` library.\u00b9 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \u201cThe Annotated Transformer.\u201d\u00b2\n\nIn this work, we denote the number of layers (i.e., Transformer blocks) as $L$, the hidden size as $H$, and the number of self-attention heads as $A$.\u00b3\nWe primarily report results on two model sizes:\n**BERT$_{BASE}$** (L=12, H=768, A=12, Total Parameters=110M) and **BERT$_{LARGE}$** (L=24, H=1024, A=16, Total Parameters=340M).\n\nBERT$_{BASE}$ was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left.$^4$\n\n1https://github.com/tensorflow/tensor2tensor  \n2http://nlp.seas.harvard.edu/2018/04/03/attention.html  \n3In all cases we set the feed-forward/filter size to be 4H, i.e., 3072 for the $H=768$ and 4096 for the $H=1024$.  \n4We note that in the literature the bidirectional Trans-\n\n**Input/Output Representations**  To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., ( Question, Answer )) in one token sequence. Throughout this work, a \u201csentence\u201d can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. A \u201csequence\u201d refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary. The first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as $E$, the final hidden vector of the special [CLS] token as $C \\in \\mathbb{R}^H$, and the final hidden vector for the $i^{th}$ input token as $T_i \\in \\mathbb{R}^H$.\n\nFor a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualization of this construction can be seen in Figure 2."
        },
        {
            "text": "**Model Architecture** BERT\u2019s model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al. (2017) and released in the `tensor2tensor` library.\u00b9 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \u201cThe Annotated Transformer.\u201d\u00b2",
            "page": 3,
            "x": 304,
            "y": 374,
            "width": 225,
            "height": 150,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-bert",
            "chunk_id": "3e8c21f7-7109-428f-8f91-b1240a5d9917",
            "group_text": "3   BERT\n\nWe introduce BERT and its detailed implementation in this section. There are two steps in our framework: _pre-training_ and _fine-tuning_. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine-tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters. The question-answering example in Figure 1 will serve as a running example for this section.\n\nA distinctive feature of BERT is its unified ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the final downstream architecture.\n\n**Model Architecture** BERT\u2019s model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al. (2017) and released in the `tensor2tensor` library.\u00b9 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \u201cThe Annotated Transformer.\u201d\u00b2\n\nIn this work, we denote the number of layers (i.e., Transformer blocks) as $L$, the hidden size as $H$, and the number of self-attention heads as $A$.\u00b3\nWe primarily report results on two model sizes:\n**BERT$_{BASE}$** (L=12, H=768, A=12, Total Parameters=110M) and **BERT$_{LARGE}$** (L=24, H=1024, A=16, Total Parameters=340M).\n\nBERT$_{BASE}$ was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left.$^4$\n\n1https://github.com/tensorflow/tensor2tensor  \n2http://nlp.seas.harvard.edu/2018/04/03/attention.html  \n3In all cases we set the feed-forward/filter size to be 4H, i.e., 3072 for the $H=768$ and 4096 for the $H=1024$.  \n4We note that in the literature the bidirectional Trans-\n\n**Input/Output Representations**  To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., ( Question, Answer )) in one token sequence. Throughout this work, a \u201csentence\u201d can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. A \u201csequence\u201d refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary. The first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as $E$, the final hidden vector of the special [CLS] token as $C \\in \\mathbb{R}^H$, and the final hidden vector for the $i^{th}$ input token as $T_i \\in \\mathbb{R}^H$.\n\nFor a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualization of this construction can be seen in Figure 2."
        },
        {
            "text": "In this work, we denote the number of layers (i.e., Transformer blocks) as $L$, the hidden size as $H$, and the number of self-attention heads as $A$.\u00b3\nWe primarily report results on two model sizes:\n**BERT$_{BASE}$** (L=12, H=768, A=12, Total Parameters=110M) and **BERT$_{LARGE}$** (L=24, H=1024, A=16, Total Parameters=340M).",
            "page": 3,
            "x": 304,
            "y": 525,
            "width": 226,
            "height": 94,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-bert",
            "chunk_id": "033fc582-5b4d-43b6-b0dc-cfdb32daf84b",
            "group_text": "3   BERT\n\nWe introduce BERT and its detailed implementation in this section. There are two steps in our framework: _pre-training_ and _fine-tuning_. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine-tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters. The question-answering example in Figure 1 will serve as a running example for this section.\n\nA distinctive feature of BERT is its unified ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the final downstream architecture.\n\n**Model Architecture** BERT\u2019s model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al. (2017) and released in the `tensor2tensor` library.\u00b9 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \u201cThe Annotated Transformer.\u201d\u00b2\n\nIn this work, we denote the number of layers (i.e., Transformer blocks) as $L$, the hidden size as $H$, and the number of self-attention heads as $A$.\u00b3\nWe primarily report results on two model sizes:\n**BERT$_{BASE}$** (L=12, H=768, A=12, Total Parameters=110M) and **BERT$_{LARGE}$** (L=24, H=1024, A=16, Total Parameters=340M).\n\nBERT$_{BASE}$ was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left.$^4$\n\n1https://github.com/tensorflow/tensor2tensor  \n2http://nlp.seas.harvard.edu/2018/04/03/attention.html  \n3In all cases we set the feed-forward/filter size to be 4H, i.e., 3072 for the $H=768$ and 4096 for the $H=1024$.  \n4We note that in the literature the bidirectional Trans-\n\n**Input/Output Representations**  To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., ( Question, Answer )) in one token sequence. Throughout this work, a \u201csentence\u201d can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. A \u201csequence\u201d refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary. The first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as $E$, the final hidden vector of the special [CLS] token as $C \\in \\mathbb{R}^H$, and the final hidden vector for the $i^{th}$ input token as $T_i \\in \\mathbb{R}^H$.\n\nFor a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualization of this construction can be seen in Figure 2."
        },
        {
            "text": "BERT$_{BASE}$ was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left.$^4$",
            "page": 3,
            "x": 304,
            "y": 621,
            "width": 225,
            "height": 82,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-bert",
            "chunk_id": "a3375e00-6486-439e-8d2b-6e8bee8fc45a",
            "group_text": "3   BERT\n\nWe introduce BERT and its detailed implementation in this section. There are two steps in our framework: _pre-training_ and _fine-tuning_. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine-tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters. The question-answering example in Figure 1 will serve as a running example for this section.\n\nA distinctive feature of BERT is its unified ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the final downstream architecture.\n\n**Model Architecture** BERT\u2019s model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al. (2017) and released in the `tensor2tensor` library.\u00b9 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \u201cThe Annotated Transformer.\u201d\u00b2\n\nIn this work, we denote the number of layers (i.e., Transformer blocks) as $L$, the hidden size as $H$, and the number of self-attention heads as $A$.\u00b3\nWe primarily report results on two model sizes:\n**BERT$_{BASE}$** (L=12, H=768, A=12, Total Parameters=110M) and **BERT$_{LARGE}$** (L=24, H=1024, A=16, Total Parameters=340M).\n\nBERT$_{BASE}$ was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left.$^4$\n\n1https://github.com/tensorflow/tensor2tensor  \n2http://nlp.seas.harvard.edu/2018/04/03/attention.html  \n3In all cases we set the feed-forward/filter size to be 4H, i.e., 3072 for the $H=768$ and 4096 for the $H=1024$.  \n4We note that in the literature the bidirectional Trans-\n\n**Input/Output Representations**  To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., ( Question, Answer )) in one token sequence. Throughout this work, a \u201csentence\u201d can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. A \u201csequence\u201d refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary. The first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as $E$, the final hidden vector of the special [CLS] token as $C \\in \\mathbb{R}^H$, and the final hidden vector for the $i^{th}$ input token as $T_i \\in \\mathbb{R}^H$.\n\nFor a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualization of this construction can be seen in Figure 2."
        },
        {
            "text": "1https://github.com/tensorflow/tensor2tensor  \n2http://nlp.seas.harvard.edu/2018/04/03/attention.html  \n3In all cases we set the feed-forward/filter size to be 4H, i.e., 3072 for the $H=768$ and 4096 for the $H=1024$.  \n4We note that in the literature the bidirectional Trans-",
            "page": 3,
            "x": 304,
            "y": 710,
            "width": 224,
            "height": 56,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-bert",
            "chunk_id": "46e5e866-f135-4e58-a113-a9c444e50b9d",
            "group_text": "3   BERT\n\nWe introduce BERT and its detailed implementation in this section. There are two steps in our framework: _pre-training_ and _fine-tuning_. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine-tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters. The question-answering example in Figure 1 will serve as a running example for this section.\n\nA distinctive feature of BERT is its unified ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the final downstream architecture.\n\n**Model Architecture** BERT\u2019s model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al. (2017) and released in the `tensor2tensor` library.\u00b9 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \u201cThe Annotated Transformer.\u201d\u00b2\n\nIn this work, we denote the number of layers (i.e., Transformer blocks) as $L$, the hidden size as $H$, and the number of self-attention heads as $A$.\u00b3\nWe primarily report results on two model sizes:\n**BERT$_{BASE}$** (L=12, H=768, A=12, Total Parameters=110M) and **BERT$_{LARGE}$** (L=24, H=1024, A=16, Total Parameters=340M).\n\nBERT$_{BASE}$ was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left.$^4$\n\n1https://github.com/tensorflow/tensor2tensor  \n2http://nlp.seas.harvard.edu/2018/04/03/attention.html  \n3In all cases we set the feed-forward/filter size to be 4H, i.e., 3072 for the $H=768$ and 4096 for the $H=1024$.  \n4We note that in the literature the bidirectional Trans-\n\n**Input/Output Representations**  To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., ( Question, Answer )) in one token sequence. Throughout this work, a \u201csentence\u201d can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. A \u201csequence\u201d refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary. The first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as $E$, the final hidden vector of the special [CLS] token as $C \\in \\mathbb{R}^H$, and the final hidden vector for the $i^{th}$ input token as $T_i \\in \\mathbb{R}^H$.\n\nFor a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualization of this construction can be seen in Figure 2."
        },
        {
            "text": "**Input/Output Representations**  To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., ( Question, Answer )) in one token sequence. Throughout this work, a \u201csentence\u201d can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. A \u201csequence\u201d refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together.",
            "page": 4,
            "x": 68,
            "y": 62,
            "width": 226,
            "height": 137,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-bert",
            "chunk_id": "5647ffc1-5f36-48ab-82d5-70e6aeb1aaa9",
            "group_text": "3   BERT\n\nWe introduce BERT and its detailed implementation in this section. There are two steps in our framework: _pre-training_ and _fine-tuning_. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine-tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters. The question-answering example in Figure 1 will serve as a running example for this section.\n\nA distinctive feature of BERT is its unified ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the final downstream architecture.\n\n**Model Architecture** BERT\u2019s model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al. (2017) and released in the `tensor2tensor` library.\u00b9 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \u201cThe Annotated Transformer.\u201d\u00b2\n\nIn this work, we denote the number of layers (i.e., Transformer blocks) as $L$, the hidden size as $H$, and the number of self-attention heads as $A$.\u00b3\nWe primarily report results on two model sizes:\n**BERT$_{BASE}$** (L=12, H=768, A=12, Total Parameters=110M) and **BERT$_{LARGE}$** (L=24, H=1024, A=16, Total Parameters=340M).\n\nBERT$_{BASE}$ was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left.$^4$\n\n1https://github.com/tensorflow/tensor2tensor  \n2http://nlp.seas.harvard.edu/2018/04/03/attention.html  \n3In all cases we set the feed-forward/filter size to be 4H, i.e., 3072 for the $H=768$ and 4096 for the $H=1024$.  \n4We note that in the literature the bidirectional Trans-\n\n**Input/Output Representations**  To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., ( Question, Answer )) in one token sequence. Throughout this work, a \u201csentence\u201d can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. A \u201csequence\u201d refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary. The first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as $E$, the final hidden vector of the special [CLS] token as $C \\in \\mathbb{R}^H$, and the final hidden vector for the $i^{th}$ input token as $T_i \\in \\mathbb{R}^H$.\n\nFor a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualization of this construction can be seen in Figure 2."
        },
        {
            "text": "We use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary. The first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as $E$, the final hidden vector of the special [CLS] token as $C \\in \\mathbb{R}^H$, and the final hidden vector for the $i^{th}$ input token as $T_i \\in \\mathbb{R}^H$.",
            "page": 4,
            "x": 68,
            "y": 200,
            "width": 226,
            "height": 216,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-bert",
            "chunk_id": "caa25d06-697a-49db-b5a7-5e3dbfcb5831",
            "group_text": "3   BERT\n\nWe introduce BERT and its detailed implementation in this section. There are two steps in our framework: _pre-training_ and _fine-tuning_. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine-tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters. The question-answering example in Figure 1 will serve as a running example for this section.\n\nA distinctive feature of BERT is its unified ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the final downstream architecture.\n\n**Model Architecture** BERT\u2019s model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al. (2017) and released in the `tensor2tensor` library.\u00b9 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \u201cThe Annotated Transformer.\u201d\u00b2\n\nIn this work, we denote the number of layers (i.e., Transformer blocks) as $L$, the hidden size as $H$, and the number of self-attention heads as $A$.\u00b3\nWe primarily report results on two model sizes:\n**BERT$_{BASE}$** (L=12, H=768, A=12, Total Parameters=110M) and **BERT$_{LARGE}$** (L=24, H=1024, A=16, Total Parameters=340M).\n\nBERT$_{BASE}$ was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left.$^4$\n\n1https://github.com/tensorflow/tensor2tensor  \n2http://nlp.seas.harvard.edu/2018/04/03/attention.html  \n3In all cases we set the feed-forward/filter size to be 4H, i.e., 3072 for the $H=768$ and 4096 for the $H=1024$.  \n4We note that in the literature the bidirectional Trans-\n\n**Input/Output Representations**  To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., ( Question, Answer )) in one token sequence. Throughout this work, a \u201csentence\u201d can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. A \u201csequence\u201d refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary. The first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as $E$, the final hidden vector of the special [CLS] token as $C \\in \\mathbb{R}^H$, and the final hidden vector for the $i^{th}$ input token as $T_i \\in \\mathbb{R}^H$.\n\nFor a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualization of this construction can be seen in Figure 2."
        },
        {
            "text": "For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualization of this construction can be seen in Figure 2.",
            "page": 4,
            "x": 68,
            "y": 417,
            "width": 225,
            "height": 56,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-bert",
            "chunk_id": "121fd9c5-0f25-4ae7-9969-7f65c39a61cb",
            "group_text": "3   BERT\n\nWe introduce BERT and its detailed implementation in this section. There are two steps in our framework: _pre-training_ and _fine-tuning_. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine-tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters. The question-answering example in Figure 1 will serve as a running example for this section.\n\nA distinctive feature of BERT is its unified ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the final downstream architecture.\n\n**Model Architecture** BERT\u2019s model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al. (2017) and released in the `tensor2tensor` library.\u00b9 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \u201cThe Annotated Transformer.\u201d\u00b2\n\nIn this work, we denote the number of layers (i.e., Transformer blocks) as $L$, the hidden size as $H$, and the number of self-attention heads as $A$.\u00b3\nWe primarily report results on two model sizes:\n**BERT$_{BASE}$** (L=12, H=768, A=12, Total Parameters=110M) and **BERT$_{LARGE}$** (L=24, H=1024, A=16, Total Parameters=340M).\n\nBERT$_{BASE}$ was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left.$^4$\n\n1https://github.com/tensorflow/tensor2tensor  \n2http://nlp.seas.harvard.edu/2018/04/03/attention.html  \n3In all cases we set the feed-forward/filter size to be 4H, i.e., 3072 for the $H=768$ and 4096 for the $H=1024$.  \n4We note that in the literature the bidirectional Trans-\n\n**Input/Output Representations**  To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., ( Question, Answer )) in one token sequence. Throughout this work, a \u201csentence\u201d can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. A \u201csequence\u201d refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary. The first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as $E$, the final hidden vector of the special [CLS] token as $C \\in \\mathbb{R}^H$, and the final hidden vector for the $i^{th}$ input token as $T_i \\in \\mathbb{R}^H$.\n\nFor a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualization of this construction can be seen in Figure 2."
        },
        {
            "text": "### 3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al. (2018), we do not use traditional left-to-right or right-to-left language models to pre-train BERT. Instead, we pre-train BERT using two unsupervised tasks, described in this section. This step is presented in the left part of Figure 1.",
            "page": 4,
            "x": 68,
            "y": 480,
            "width": 225,
            "height": 103,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "8-pre",
            "chunk_id": "44aad649-d317-4fa9-b2f5-689aed91f590",
            "group_text": "### 3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al. (2018), we do not use traditional left-to-right or right-to-left language models to pre-train BERT. Instead, we pre-train BERT using two unsupervised tasks, described in this section. This step is presented in the left part of Figure 1.\n\nTask #1: Masked LM  Intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-to-right and a right-to-left model.  Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly \u201csee itself\u201d, and the model could trivially predict the target word in a multi-layered context.\n\nformer is often referred to as a \u201cTransformer encoder\u201d while the left-context-only version is referred to as a \u201cTransformer decoder\u201d since it can be used for text generation.\n\nIn order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a \u201cmasked LM\u201d (MLM), although it is often referred to as a *Cloze* task in the literature (Taylor, 1953). In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece tokens in each sequence at random. In contrast to denoising auto-encoders (Vincent et al., 2008), we only predict the masked words rather than reconstructing the entire input.\n\nAlthough this allows us to obtain a bidirec-\ntional pre-trained model, a downside is that we\nare creating a mismatch between pre-training and\nfine-tuning, since the [MASK] token does not ap-\npear during fine-tuning. To mitigate this, we do\nnot always replace \u201cmasked\u201d words with the ac-\ntual [MASK] token. The training data generator\nchooses 15% of the token positions at random for\nprediction. If the i-th token is chosen, we replace\nthe i-th token with (1) the [MASK] token 80% of\nthe time (2) a random token 10% of the time (3)\nthe unchanged i-th token 10% of the time. Then,\n$T_i$ will be used to predict the original token with\ncross entropy loss. We compare variations of this\nprocedure in Appendix C.2.\n\nTask #2: Next Sentence Prediction (NSP)\nMany important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two sentences, which is not directly captured by language modeling. In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pre-training example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext). As we show in Figure 1, C is used for next sentence prediction (NSP).\u2075 Despite its simplicity, we demonstrate in Section 5.1 that pre-training towards this task is very beneficial to both QA and NLI.\u2076\n\n\u2075The final model achieves 97%-98% accuracy on NSP.\n\u2076The vector $C$ is not a meaningful sentence representation without fine-tuning, since it was trained with NSP.\n\nThe NSP task is closely related to representation-\nlearning objectives used in Jernite et al. (2017) and\nLogeswaran and Lee (2018). However, in prior\nwork, only sentence embeddings are transferred to\ndown-stream tasks, where BERT transfers all pa-\nrameters to initialize end-task model parameters.\n\n**Pre-training data** The pre-training procedure largely follows the existing literature on language model pre-training. For the pre-training corpus we use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words). For Wikipedia we extract only the text passages and ignore lists, tables, and headers. It is critical to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark (Chelba et al., 2013) in order to extract long contiguous sequences."
        },
        {
            "text": "Task #1: Masked LM  Intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-to-right and a right-to-left model.  Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly \u201csee itself\u201d, and the model could trivially predict the target word in a multi-layered context.",
            "page": 4,
            "x": 68,
            "y": 590,
            "width": 225,
            "height": 138,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "8-pre",
            "chunk_id": "0d8a59ad-6eff-4967-8605-e30276411230",
            "group_text": "### 3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al. (2018), we do not use traditional left-to-right or right-to-left language models to pre-train BERT. Instead, we pre-train BERT using two unsupervised tasks, described in this section. This step is presented in the left part of Figure 1.\n\nTask #1: Masked LM  Intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-to-right and a right-to-left model.  Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly \u201csee itself\u201d, and the model could trivially predict the target word in a multi-layered context.\n\nformer is often referred to as a \u201cTransformer encoder\u201d while the left-context-only version is referred to as a \u201cTransformer decoder\u201d since it can be used for text generation.\n\nIn order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a \u201cmasked LM\u201d (MLM), although it is often referred to as a *Cloze* task in the literature (Taylor, 1953). In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece tokens in each sequence at random. In contrast to denoising auto-encoders (Vincent et al., 2008), we only predict the masked words rather than reconstructing the entire input.\n\nAlthough this allows us to obtain a bidirec-\ntional pre-trained model, a downside is that we\nare creating a mismatch between pre-training and\nfine-tuning, since the [MASK] token does not ap-\npear during fine-tuning. To mitigate this, we do\nnot always replace \u201cmasked\u201d words with the ac-\ntual [MASK] token. The training data generator\nchooses 15% of the token positions at random for\nprediction. If the i-th token is chosen, we replace\nthe i-th token with (1) the [MASK] token 80% of\nthe time (2) a random token 10% of the time (3)\nthe unchanged i-th token 10% of the time. Then,\n$T_i$ will be used to predict the original token with\ncross entropy loss. We compare variations of this\nprocedure in Appendix C.2.\n\nTask #2: Next Sentence Prediction (NSP)\nMany important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two sentences, which is not directly captured by language modeling. In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pre-training example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext). As we show in Figure 1, C is used for next sentence prediction (NSP).\u2075 Despite its simplicity, we demonstrate in Section 5.1 that pre-training towards this task is very beneficial to both QA and NLI.\u2076\n\n\u2075The final model achieves 97%-98% accuracy on NSP.\n\u2076The vector $C$ is not a meaningful sentence representation without fine-tuning, since it was trained with NSP.\n\nThe NSP task is closely related to representation-\nlearning objectives used in Jernite et al. (2017) and\nLogeswaran and Lee (2018). However, in prior\nwork, only sentence embeddings are transferred to\ndown-stream tasks, where BERT transfers all pa-\nrameters to initialize end-task model parameters.\n\n**Pre-training data** The pre-training procedure largely follows the existing literature on language model pre-training. For the pre-training corpus we use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words). For Wikipedia we extract only the text passages and ignore lists, tables, and headers. It is critical to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark (Chelba et al., 2013) in order to extract long contiguous sequences."
        },
        {
            "text": "former is often referred to as a \u201cTransformer encoder\u201d while the left-context-only version is referred to as a \u201cTransformer decoder\u201d since it can be used for text generation.",
            "page": 4,
            "x": 68,
            "y": 734,
            "width": 225,
            "height": 33,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "8-pre",
            "chunk_id": "67cc4b4f-821e-4925-bd05-3db770274003",
            "group_text": "### 3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al. (2018), we do not use traditional left-to-right or right-to-left language models to pre-train BERT. Instead, we pre-train BERT using two unsupervised tasks, described in this section. This step is presented in the left part of Figure 1.\n\nTask #1: Masked LM  Intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-to-right and a right-to-left model.  Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly \u201csee itself\u201d, and the model could trivially predict the target word in a multi-layered context.\n\nformer is often referred to as a \u201cTransformer encoder\u201d while the left-context-only version is referred to as a \u201cTransformer decoder\u201d since it can be used for text generation.\n\nIn order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a \u201cmasked LM\u201d (MLM), although it is often referred to as a *Cloze* task in the literature (Taylor, 1953). In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece tokens in each sequence at random. In contrast to denoising auto-encoders (Vincent et al., 2008), we only predict the masked words rather than reconstructing the entire input.\n\nAlthough this allows us to obtain a bidirec-\ntional pre-trained model, a downside is that we\nare creating a mismatch between pre-training and\nfine-tuning, since the [MASK] token does not ap-\npear during fine-tuning. To mitigate this, we do\nnot always replace \u201cmasked\u201d words with the ac-\ntual [MASK] token. The training data generator\nchooses 15% of the token positions at random for\nprediction. If the i-th token is chosen, we replace\nthe i-th token with (1) the [MASK] token 80% of\nthe time (2) a random token 10% of the time (3)\nthe unchanged i-th token 10% of the time. Then,\n$T_i$ will be used to predict the original token with\ncross entropy loss. We compare variations of this\nprocedure in Appendix C.2.\n\nTask #2: Next Sentence Prediction (NSP)\nMany important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two sentences, which is not directly captured by language modeling. In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pre-training example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext). As we show in Figure 1, C is used for next sentence prediction (NSP).\u2075 Despite its simplicity, we demonstrate in Section 5.1 that pre-training towards this task is very beneficial to both QA and NLI.\u2076\n\n\u2075The final model achieves 97%-98% accuracy on NSP.\n\u2076The vector $C$ is not a meaningful sentence representation without fine-tuning, since it was trained with NSP.\n\nThe NSP task is closely related to representation-\nlearning objectives used in Jernite et al. (2017) and\nLogeswaran and Lee (2018). However, in prior\nwork, only sentence embeddings are transferred to\ndown-stream tasks, where BERT transfers all pa-\nrameters to initialize end-task model parameters.\n\n**Pre-training data** The pre-training procedure largely follows the existing literature on language model pre-training. For the pre-training corpus we use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words). For Wikipedia we extract only the text passages and ignore lists, tables, and headers. It is critical to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark (Chelba et al., 2013) in order to extract long contiguous sequences."
        },
        {
            "text": "In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a \u201cmasked LM\u201d (MLM), although it is often referred to as a *Cloze* task in the literature (Taylor, 1953). In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece tokens in each sequence at random. In contrast to denoising auto-encoders (Vincent et al., 2008), we only predict the masked words rather than reconstructing the entire input.",
            "page": 4,
            "x": 304,
            "y": 63,
            "width": 225,
            "height": 190,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "8-pre",
            "chunk_id": "d894250f-4909-4972-851c-40d00a9a955e",
            "group_text": "### 3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al. (2018), we do not use traditional left-to-right or right-to-left language models to pre-train BERT. Instead, we pre-train BERT using two unsupervised tasks, described in this section. This step is presented in the left part of Figure 1.\n\nTask #1: Masked LM  Intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-to-right and a right-to-left model.  Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly \u201csee itself\u201d, and the model could trivially predict the target word in a multi-layered context.\n\nformer is often referred to as a \u201cTransformer encoder\u201d while the left-context-only version is referred to as a \u201cTransformer decoder\u201d since it can be used for text generation.\n\nIn order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a \u201cmasked LM\u201d (MLM), although it is often referred to as a *Cloze* task in the literature (Taylor, 1953). In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece tokens in each sequence at random. In contrast to denoising auto-encoders (Vincent et al., 2008), we only predict the masked words rather than reconstructing the entire input.\n\nAlthough this allows us to obtain a bidirec-\ntional pre-trained model, a downside is that we\nare creating a mismatch between pre-training and\nfine-tuning, since the [MASK] token does not ap-\npear during fine-tuning. To mitigate this, we do\nnot always replace \u201cmasked\u201d words with the ac-\ntual [MASK] token. The training data generator\nchooses 15% of the token positions at random for\nprediction. If the i-th token is chosen, we replace\nthe i-th token with (1) the [MASK] token 80% of\nthe time (2) a random token 10% of the time (3)\nthe unchanged i-th token 10% of the time. Then,\n$T_i$ will be used to predict the original token with\ncross entropy loss. We compare variations of this\nprocedure in Appendix C.2.\n\nTask #2: Next Sentence Prediction (NSP)\nMany important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two sentences, which is not directly captured by language modeling. In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pre-training example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext). As we show in Figure 1, C is used for next sentence prediction (NSP).\u2075 Despite its simplicity, we demonstrate in Section 5.1 that pre-training towards this task is very beneficial to both QA and NLI.\u2076\n\n\u2075The final model achieves 97%-98% accuracy on NSP.\n\u2076The vector $C$ is not a meaningful sentence representation without fine-tuning, since it was trained with NSP.\n\nThe NSP task is closely related to representation-\nlearning objectives used in Jernite et al. (2017) and\nLogeswaran and Lee (2018). However, in prior\nwork, only sentence embeddings are transferred to\ndown-stream tasks, where BERT transfers all pa-\nrameters to initialize end-task model parameters.\n\n**Pre-training data** The pre-training procedure largely follows the existing literature on language model pre-training. For the pre-training corpus we use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words). For Wikipedia we extract only the text passages and ignore lists, tables, and headers. It is critical to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark (Chelba et al., 2013) in order to extract long contiguous sequences."
        },
        {
            "text": "Although this allows us to obtain a bidirec-\ntional pre-trained model, a downside is that we\nare creating a mismatch between pre-training and\nfine-tuning, since the [MASK] token does not ap-\npear during fine-tuning. To mitigate this, we do\nnot always replace \u201cmasked\u201d words with the ac-\ntual [MASK] token. The training data generator\nchooses 15% of the token positions at random for\nprediction. If the i-th token is chosen, we replace\nthe i-th token with (1) the [MASK] token 80% of\nthe time (2) a random token 10% of the time (3)\nthe unchanged i-th token 10% of the time. Then,\n$T_i$ will be used to predict the original token with\ncross entropy loss. We compare variations of this\nprocedure in Appendix C.2.",
            "page": 4,
            "x": 304,
            "y": 253,
            "width": 225,
            "height": 204,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "8-pre",
            "chunk_id": "2ab9b730-479e-4a41-a0c8-4a648dde7687",
            "group_text": "### 3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al. (2018), we do not use traditional left-to-right or right-to-left language models to pre-train BERT. Instead, we pre-train BERT using two unsupervised tasks, described in this section. This step is presented in the left part of Figure 1.\n\nTask #1: Masked LM  Intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-to-right and a right-to-left model.  Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly \u201csee itself\u201d, and the model could trivially predict the target word in a multi-layered context.\n\nformer is often referred to as a \u201cTransformer encoder\u201d while the left-context-only version is referred to as a \u201cTransformer decoder\u201d since it can be used for text generation.\n\nIn order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a \u201cmasked LM\u201d (MLM), although it is often referred to as a *Cloze* task in the literature (Taylor, 1953). In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece tokens in each sequence at random. In contrast to denoising auto-encoders (Vincent et al., 2008), we only predict the masked words rather than reconstructing the entire input.\n\nAlthough this allows us to obtain a bidirec-\ntional pre-trained model, a downside is that we\nare creating a mismatch between pre-training and\nfine-tuning, since the [MASK] token does not ap-\npear during fine-tuning. To mitigate this, we do\nnot always replace \u201cmasked\u201d words with the ac-\ntual [MASK] token. The training data generator\nchooses 15% of the token positions at random for\nprediction. If the i-th token is chosen, we replace\nthe i-th token with (1) the [MASK] token 80% of\nthe time (2) a random token 10% of the time (3)\nthe unchanged i-th token 10% of the time. Then,\n$T_i$ will be used to predict the original token with\ncross entropy loss. We compare variations of this\nprocedure in Appendix C.2.\n\nTask #2: Next Sentence Prediction (NSP)\nMany important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two sentences, which is not directly captured by language modeling. In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pre-training example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext). As we show in Figure 1, C is used for next sentence prediction (NSP).\u2075 Despite its simplicity, we demonstrate in Section 5.1 that pre-training towards this task is very beneficial to both QA and NLI.\u2076\n\n\u2075The final model achieves 97%-98% accuracy on NSP.\n\u2076The vector $C$ is not a meaningful sentence representation without fine-tuning, since it was trained with NSP.\n\nThe NSP task is closely related to representation-\nlearning objectives used in Jernite et al. (2017) and\nLogeswaran and Lee (2018). However, in prior\nwork, only sentence embeddings are transferred to\ndown-stream tasks, where BERT transfers all pa-\nrameters to initialize end-task model parameters.\n\n**Pre-training data** The pre-training procedure largely follows the existing literature on language model pre-training. For the pre-training corpus we use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words). For Wikipedia we extract only the text passages and ignore lists, tables, and headers. It is critical to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark (Chelba et al., 2013) in order to extract long contiguous sequences."
        },
        {
            "text": "Task #2: Next Sentence Prediction (NSP)\nMany important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two sentences, which is not directly captured by language modeling. In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pre-training example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext). As we show in Figure 1, C is used for next sentence prediction (NSP).\u2075 Despite its simplicity, we demonstrate in Section 5.1 that pre-training towards this task is very beneficial to both QA and NLI.\u2076",
            "page": 4,
            "x": 303,
            "y": 468,
            "width": 225,
            "height": 260,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "8-pre",
            "chunk_id": "801c6a96-0417-43e8-8fa4-d49559076a98",
            "group_text": "### 3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al. (2018), we do not use traditional left-to-right or right-to-left language models to pre-train BERT. Instead, we pre-train BERT using two unsupervised tasks, described in this section. This step is presented in the left part of Figure 1.\n\nTask #1: Masked LM  Intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-to-right and a right-to-left model.  Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly \u201csee itself\u201d, and the model could trivially predict the target word in a multi-layered context.\n\nformer is often referred to as a \u201cTransformer encoder\u201d while the left-context-only version is referred to as a \u201cTransformer decoder\u201d since it can be used for text generation.\n\nIn order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a \u201cmasked LM\u201d (MLM), although it is often referred to as a *Cloze* task in the literature (Taylor, 1953). In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece tokens in each sequence at random. In contrast to denoising auto-encoders (Vincent et al., 2008), we only predict the masked words rather than reconstructing the entire input.\n\nAlthough this allows us to obtain a bidirec-\ntional pre-trained model, a downside is that we\nare creating a mismatch between pre-training and\nfine-tuning, since the [MASK] token does not ap-\npear during fine-tuning. To mitigate this, we do\nnot always replace \u201cmasked\u201d words with the ac-\ntual [MASK] token. The training data generator\nchooses 15% of the token positions at random for\nprediction. If the i-th token is chosen, we replace\nthe i-th token with (1) the [MASK] token 80% of\nthe time (2) a random token 10% of the time (3)\nthe unchanged i-th token 10% of the time. Then,\n$T_i$ will be used to predict the original token with\ncross entropy loss. We compare variations of this\nprocedure in Appendix C.2.\n\nTask #2: Next Sentence Prediction (NSP)\nMany important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two sentences, which is not directly captured by language modeling. In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pre-training example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext). As we show in Figure 1, C is used for next sentence prediction (NSP).\u2075 Despite its simplicity, we demonstrate in Section 5.1 that pre-training towards this task is very beneficial to both QA and NLI.\u2076\n\n\u2075The final model achieves 97%-98% accuracy on NSP.\n\u2076The vector $C$ is not a meaningful sentence representation without fine-tuning, since it was trained with NSP.\n\nThe NSP task is closely related to representation-\nlearning objectives used in Jernite et al. (2017) and\nLogeswaran and Lee (2018). However, in prior\nwork, only sentence embeddings are transferred to\ndown-stream tasks, where BERT transfers all pa-\nrameters to initialize end-task model parameters.\n\n**Pre-training data** The pre-training procedure largely follows the existing literature on language model pre-training. For the pre-training corpus we use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words). For Wikipedia we extract only the text passages and ignore lists, tables, and headers. It is critical to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark (Chelba et al., 2013) in order to extract long contiguous sequences."
        },
        {
            "text": "\u2075The final model achieves 97%-98% accuracy on NSP.\n\u2076The vector $C$ is not a meaningful sentence representation without fine-tuning, since it was trained with NSP.",
            "page": 4,
            "x": 304,
            "y": 733,
            "width": 223,
            "height": 33,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "8-pre",
            "chunk_id": "fbd3fb79-a171-4cef-abb2-ad8e2d1ed83e",
            "group_text": "### 3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al. (2018), we do not use traditional left-to-right or right-to-left language models to pre-train BERT. Instead, we pre-train BERT using two unsupervised tasks, described in this section. This step is presented in the left part of Figure 1.\n\nTask #1: Masked LM  Intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-to-right and a right-to-left model.  Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly \u201csee itself\u201d, and the model could trivially predict the target word in a multi-layered context.\n\nformer is often referred to as a \u201cTransformer encoder\u201d while the left-context-only version is referred to as a \u201cTransformer decoder\u201d since it can be used for text generation.\n\nIn order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a \u201cmasked LM\u201d (MLM), although it is often referred to as a *Cloze* task in the literature (Taylor, 1953). In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece tokens in each sequence at random. In contrast to denoising auto-encoders (Vincent et al., 2008), we only predict the masked words rather than reconstructing the entire input.\n\nAlthough this allows us to obtain a bidirec-\ntional pre-trained model, a downside is that we\nare creating a mismatch between pre-training and\nfine-tuning, since the [MASK] token does not ap-\npear during fine-tuning. To mitigate this, we do\nnot always replace \u201cmasked\u201d words with the ac-\ntual [MASK] token. The training data generator\nchooses 15% of the token positions at random for\nprediction. If the i-th token is chosen, we replace\nthe i-th token with (1) the [MASK] token 80% of\nthe time (2) a random token 10% of the time (3)\nthe unchanged i-th token 10% of the time. Then,\n$T_i$ will be used to predict the original token with\ncross entropy loss. We compare variations of this\nprocedure in Appendix C.2.\n\nTask #2: Next Sentence Prediction (NSP)\nMany important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two sentences, which is not directly captured by language modeling. In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pre-training example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext). As we show in Figure 1, C is used for next sentence prediction (NSP).\u2075 Despite its simplicity, we demonstrate in Section 5.1 that pre-training towards this task is very beneficial to both QA and NLI.\u2076\n\n\u2075The final model achieves 97%-98% accuracy on NSP.\n\u2076The vector $C$ is not a meaningful sentence representation without fine-tuning, since it was trained with NSP.\n\nThe NSP task is closely related to representation-\nlearning objectives used in Jernite et al. (2017) and\nLogeswaran and Lee (2018). However, in prior\nwork, only sentence embeddings are transferred to\ndown-stream tasks, where BERT transfers all pa-\nrameters to initialize end-task model parameters.\n\n**Pre-training data** The pre-training procedure largely follows the existing literature on language model pre-training. For the pre-training corpus we use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words). For Wikipedia we extract only the text passages and ignore lists, tables, and headers. It is critical to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark (Chelba et al., 2013) in order to extract long contiguous sequences."
        },
        {
            "text": "The NSP task is closely related to representation-\nlearning objectives used in Jernite et al. (2017) and\nLogeswaran and Lee (2018). However, in prior\nwork, only sentence embeddings are transferred to\ndown-stream tasks, where BERT transfers all pa-\nrameters to initialize end-task model parameters.",
            "page": 5,
            "x": 68,
            "y": 226,
            "width": 225,
            "height": 84,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "8-pre",
            "chunk_id": "9075d4ef-0a17-4cfe-b286-3c18666779b9",
            "group_text": "### 3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al. (2018), we do not use traditional left-to-right or right-to-left language models to pre-train BERT. Instead, we pre-train BERT using two unsupervised tasks, described in this section. This step is presented in the left part of Figure 1.\n\nTask #1: Masked LM  Intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-to-right and a right-to-left model.  Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly \u201csee itself\u201d, and the model could trivially predict the target word in a multi-layered context.\n\nformer is often referred to as a \u201cTransformer encoder\u201d while the left-context-only version is referred to as a \u201cTransformer decoder\u201d since it can be used for text generation.\n\nIn order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a \u201cmasked LM\u201d (MLM), although it is often referred to as a *Cloze* task in the literature (Taylor, 1953). In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece tokens in each sequence at random. In contrast to denoising auto-encoders (Vincent et al., 2008), we only predict the masked words rather than reconstructing the entire input.\n\nAlthough this allows us to obtain a bidirec-\ntional pre-trained model, a downside is that we\nare creating a mismatch between pre-training and\nfine-tuning, since the [MASK] token does not ap-\npear during fine-tuning. To mitigate this, we do\nnot always replace \u201cmasked\u201d words with the ac-\ntual [MASK] token. The training data generator\nchooses 15% of the token positions at random for\nprediction. If the i-th token is chosen, we replace\nthe i-th token with (1) the [MASK] token 80% of\nthe time (2) a random token 10% of the time (3)\nthe unchanged i-th token 10% of the time. Then,\n$T_i$ will be used to predict the original token with\ncross entropy loss. We compare variations of this\nprocedure in Appendix C.2.\n\nTask #2: Next Sentence Prediction (NSP)\nMany important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two sentences, which is not directly captured by language modeling. In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pre-training example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext). As we show in Figure 1, C is used for next sentence prediction (NSP).\u2075 Despite its simplicity, we demonstrate in Section 5.1 that pre-training towards this task is very beneficial to both QA and NLI.\u2076\n\n\u2075The final model achieves 97%-98% accuracy on NSP.\n\u2076The vector $C$ is not a meaningful sentence representation without fine-tuning, since it was trained with NSP.\n\nThe NSP task is closely related to representation-\nlearning objectives used in Jernite et al. (2017) and\nLogeswaran and Lee (2018). However, in prior\nwork, only sentence embeddings are transferred to\ndown-stream tasks, where BERT transfers all pa-\nrameters to initialize end-task model parameters.\n\n**Pre-training data** The pre-training procedure largely follows the existing literature on language model pre-training. For the pre-training corpus we use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words). For Wikipedia we extract only the text passages and ignore lists, tables, and headers. It is critical to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark (Chelba et al., 2013) in order to extract long contiguous sequences."
        },
        {
            "text": "**Pre-training data** The pre-training procedure largely follows the existing literature on language model pre-training. For the pre-training corpus we use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words). For Wikipedia we extract only the text passages and ignore lists, tables, and headers. It is critical to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark (Chelba et al., 2013) in order to extract long contiguous sequences.",
            "page": 5,
            "x": 68,
            "y": 317,
            "width": 226,
            "height": 151,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "8-pre",
            "chunk_id": "1854ac3b-7eb6-4539-83c0-a93251f688a1",
            "group_text": "### 3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al. (2018), we do not use traditional left-to-right or right-to-left language models to pre-train BERT. Instead, we pre-train BERT using two unsupervised tasks, described in this section. This step is presented in the left part of Figure 1.\n\nTask #1: Masked LM  Intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-to-right and a right-to-left model.  Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly \u201csee itself\u201d, and the model could trivially predict the target word in a multi-layered context.\n\nformer is often referred to as a \u201cTransformer encoder\u201d while the left-context-only version is referred to as a \u201cTransformer decoder\u201d since it can be used for text generation.\n\nIn order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a \u201cmasked LM\u201d (MLM), although it is often referred to as a *Cloze* task in the literature (Taylor, 1953). In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece tokens in each sequence at random. In contrast to denoising auto-encoders (Vincent et al., 2008), we only predict the masked words rather than reconstructing the entire input.\n\nAlthough this allows us to obtain a bidirec-\ntional pre-trained model, a downside is that we\nare creating a mismatch between pre-training and\nfine-tuning, since the [MASK] token does not ap-\npear during fine-tuning. To mitigate this, we do\nnot always replace \u201cmasked\u201d words with the ac-\ntual [MASK] token. The training data generator\nchooses 15% of the token positions at random for\nprediction. If the i-th token is chosen, we replace\nthe i-th token with (1) the [MASK] token 80% of\nthe time (2) a random token 10% of the time (3)\nthe unchanged i-th token 10% of the time. Then,\n$T_i$ will be used to predict the original token with\ncross entropy loss. We compare variations of this\nprocedure in Appendix C.2.\n\nTask #2: Next Sentence Prediction (NSP)\nMany important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two sentences, which is not directly captured by language modeling. In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pre-training example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext). As we show in Figure 1, C is used for next sentence prediction (NSP).\u2075 Despite its simplicity, we demonstrate in Section 5.1 that pre-training towards this task is very beneficial to both QA and NLI.\u2076\n\n\u2075The final model achieves 97%-98% accuracy on NSP.\n\u2076The vector $C$ is not a meaningful sentence representation without fine-tuning, since it was trained with NSP.\n\nThe NSP task is closely related to representation-\nlearning objectives used in Jernite et al. (2017) and\nLogeswaran and Lee (2018). However, in prior\nwork, only sentence embeddings are transferred to\ndown-stream tasks, where BERT transfers all pa-\nrameters to initialize end-task model parameters.\n\n**Pre-training data** The pre-training procedure largely follows the existing literature on language model pre-training. For the pre-training corpus we use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words). For Wikipedia we extract only the text passages and ignore lists, tables, and headers. It is critical to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark (Chelba et al., 2013) in order to extract long contiguous sequences."
        },
        {
            "text": "### 3.2  Fine-tuning BERT\n\nFine-tuning is straightforward since the self-attention mechanism in the Transformer allows BERT to model many downstream tasks\u2014whether they involve single text or text pairs\u2014by swapping out the appropriate inputs and outputs. For applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention, such as Parikh et al. (2016); Seo et al. (2017). BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes *bidirectional* cross attention between two sentences.",
            "page": 5,
            "x": 67,
            "y": 475,
            "width": 226,
            "height": 195,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-fine",
            "chunk_id": "37483307-5dd2-4265-a751-9e9dba3128fd",
            "group_text": "### 3.2  Fine-tuning BERT\n\nFine-tuning is straightforward since the self-attention mechanism in the Transformer allows BERT to model many downstream tasks\u2014whether they involve single text or text pairs\u2014by swapping out the appropriate inputs and outputs. For applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention, such as Parikh et al. (2016); Seo et al. (2017). BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes *bidirectional* cross attention between two sentences.\n\nFor each task, we simply plug in the task-specific inputs and outputs into BERT and fine-tune all the parameters end-to-end. At the input, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphrasing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and\n\n(4) a degenerate text-\u2205 pair in text classification\nor sequence tagging. At the output, the token rep-\nresentations are fed into an output layer for token-\nlevel tasks, such as sequence tagging or question\nanswering, and the [CLS] representation is fed\ninto an output layer for classification, such as en-\ntailment or sentiment analysis.\n\nCompared to pre-training, fine-tuning is relatively inexpensive. All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model.\u2077 We describe the task-specific details in the corresponding subsections of Section 4. More details can be found in Appendix A.5."
        },
        {
            "text": "For each task, we simply plug in the task-specific inputs and outputs into BERT and fine-tune all the parameters end-to-end. At the input, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphrasing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and",
            "page": 5,
            "x": 67,
            "y": 672,
            "width": 226,
            "height": 96,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-fine",
            "chunk_id": "d8cfb831-84b1-4bfd-963e-272503ebc7e7",
            "group_text": "### 3.2  Fine-tuning BERT\n\nFine-tuning is straightforward since the self-attention mechanism in the Transformer allows BERT to model many downstream tasks\u2014whether they involve single text or text pairs\u2014by swapping out the appropriate inputs and outputs. For applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention, such as Parikh et al. (2016); Seo et al. (2017). BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes *bidirectional* cross attention between two sentences.\n\nFor each task, we simply plug in the task-specific inputs and outputs into BERT and fine-tune all the parameters end-to-end. At the input, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphrasing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and\n\n(4) a degenerate text-\u2205 pair in text classification\nor sequence tagging. At the output, the token rep-\nresentations are fed into an output layer for token-\nlevel tasks, such as sequence tagging or question\nanswering, and the [CLS] representation is fed\ninto an output layer for classification, such as en-\ntailment or sentiment analysis.\n\nCompared to pre-training, fine-tuning is relatively inexpensive. All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model.\u2077 We describe the task-specific details in the corresponding subsections of Section 4. More details can be found in Appendix A.5."
        },
        {
            "text": "(4) a degenerate text-\u2205 pair in text classification\nor sequence tagging. At the output, the token rep-\nresentations are fed into an output layer for token-\nlevel tasks, such as sequence tagging or question\nanswering, and the [CLS] representation is fed\ninto an output layer for classification, such as en-\ntailment or sentiment analysis.",
            "page": 5,
            "x": 303,
            "y": 227,
            "width": 225,
            "height": 96,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-fine",
            "chunk_id": "5426d5bd-befc-4f40-b974-4d010748b2eb",
            "group_text": "### 3.2  Fine-tuning BERT\n\nFine-tuning is straightforward since the self-attention mechanism in the Transformer allows BERT to model many downstream tasks\u2014whether they involve single text or text pairs\u2014by swapping out the appropriate inputs and outputs. For applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention, such as Parikh et al. (2016); Seo et al. (2017). BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes *bidirectional* cross attention between two sentences.\n\nFor each task, we simply plug in the task-specific inputs and outputs into BERT and fine-tune all the parameters end-to-end. At the input, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphrasing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and\n\n(4) a degenerate text-\u2205 pair in text classification\nor sequence tagging. At the output, the token rep-\nresentations are fed into an output layer for token-\nlevel tasks, such as sequence tagging or question\nanswering, and the [CLS] representation is fed\ninto an output layer for classification, such as en-\ntailment or sentiment analysis.\n\nCompared to pre-training, fine-tuning is relatively inexpensive. All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model.\u2077 We describe the task-specific details in the corresponding subsections of Section 4. More details can be found in Appendix A.5."
        },
        {
            "text": "Compared to pre-training, fine-tuning is relatively inexpensive. All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model.\u2077 We describe the task-specific details in the corresponding subsections of Section 4. More details can be found in Appendix A.5.",
            "page": 5,
            "x": 303,
            "y": 324,
            "width": 226,
            "height": 108,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-fine",
            "chunk_id": "7b726029-0fd5-48f1-b040-449bf2368df5",
            "group_text": "### 3.2  Fine-tuning BERT\n\nFine-tuning is straightforward since the self-attention mechanism in the Transformer allows BERT to model many downstream tasks\u2014whether they involve single text or text pairs\u2014by swapping out the appropriate inputs and outputs. For applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention, such as Parikh et al. (2016); Seo et al. (2017). BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes *bidirectional* cross attention between two sentences.\n\nFor each task, we simply plug in the task-specific inputs and outputs into BERT and fine-tune all the parameters end-to-end. At the input, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphrasing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and\n\n(4) a degenerate text-\u2205 pair in text classification\nor sequence tagging. At the output, the token rep-\nresentations are fed into an output layer for token-\nlevel tasks, such as sequence tagging or question\nanswering, and the [CLS] representation is fed\ninto an output layer for classification, such as en-\ntailment or sentiment analysis.\n\nCompared to pre-training, fine-tuning is relatively inexpensive. All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model.\u2077 We describe the task-specific details in the corresponding subsections of Section 4. More details can be found in Appendix A.5."
        },
        {
            "text": "4  Experiments\n\nIn this section, we present BERT fine-tuning results on 11 NLP tasks.",
            "page": 5,
            "x": 303,
            "y": 439,
            "width": 224,
            "height": 50,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "10-experiments",
            "chunk_id": "576529a7-212c-44c9-90b2-0f969ed70625",
            "group_text": "4  Experiments\n\nIn this section, we present BERT fine-tuning results on 11 NLP tasks."
        },
        {
            "text": "## 4.1 GLUE\n\nThe General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018a) is a collection of diverse natural language understanding tasks. Detailed descriptions of GLUE datasets are included in Appendix B.1.",
            "page": 5,
            "x": 303,
            "y": 495,
            "width": 225,
            "height": 86,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-glue",
            "chunk_id": "8a613080-b1d6-4899-95cb-f7e5b46efd5f",
            "group_text": "## 4.1 GLUE\n\nThe General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018a) is a collection of diverse natural language understanding tasks. Detailed descriptions of GLUE datasets are included in Appendix B.1.\n\nTo fine-tune on GLUE, we represent the input sequence (for single sentence or sentence pairs) as described in Section 3, and use the final hidden vector $C \\in \\mathbb{R}^H$ corresponding to the first input token ([CLS]) as the aggregate representation. The only new parameters introduced during fine-tuning are classification layer weights $W \\in \\mathbb{R}^{K \\times H}$, where $K$ is the number of labels. We compute a standard classification loss with $C$ and $W$, i.e., $\\log(\\text{softmax}(CW^T))$.\n\n\u2077For example, the BERT SQuAD model can be trained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of 91.0%.  \n\u2078See (10) in https://gluebenchmark.com/faq.\n\nTable 1: GLUE Test results, scored by the evaluation server (https://gluebenchmark.com/leaderboard). The number below each task denotes the number of training examples. The \u201cAverage\u201d column is slightly different than the official GLUE score, since we exclude the problematic WNLI set.\u2078 BERT and OpenAI GPT are single-model, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.\n\nWe use a batch size of 32 and fine-tune for 3 epochs over the data for all GLUE tasks. For each task, we selected the best fine-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set. Additionally, for BERT\\textsubscript{LARGE} we found that fine-tuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the Dev set. With random restarts, we use the same pre-trained checkpoint but perform different fine-tuning data shuffling and classifier layer initialization.\\textsuperscript{9}\n\nResults are presented in Table 1. Both BERT$_{\\text{BASE}}$ and BERT$_{\\text{LARGE}}$ outperform all systems on all tasks by a substantial margin, obtaining 4.5% and 7.0% respective average accuracy improvement over the prior state of the art. Note that BERT$_{\\text{BASE}}$ and OpenAI GPT are nearly identical in terms of model architecture apart from the attention masking. For the largest and most widely reported GLUE task, MNLI, BERT obtains a 4.6% absolute accuracy improvement. On the official GLUE leaderboard$^{10}$, BERT$_{\\text{LARGE}}$ obtains a score of 80.5, compared to OpenAI GPT, which obtains 72.8 as of the date of writing.\n\nWe find that BERT\\textsubscript{LARGE} significantly outperforms BERT\\textsubscript{BASE} across all tasks, especially those with very little training data. The effect of model size is explored more thoroughly in Section 5.2."
        },
        {
            "text": "To fine-tune on GLUE, we represent the input sequence (for single sentence or sentence pairs) as described in Section 3, and use the final hidden vector $C \\in \\mathbb{R}^H$ corresponding to the first input token ([CLS]) as the aggregate representation. The only new parameters introduced during fine-tuning are classification layer weights $W \\in \\mathbb{R}^{K \\times H}$, where $K$ is the number of labels. We compute a standard classification loss with $C$ and $W$, i.e., $\\log(\\text{softmax}(CW^T))$.",
            "page": 5,
            "x": 304,
            "y": 582,
            "width": 225,
            "height": 137,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-glue",
            "chunk_id": "c8307d00-2f62-4f45-9af1-2cda31b4ecf0",
            "group_text": "## 4.1 GLUE\n\nThe General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018a) is a collection of diverse natural language understanding tasks. Detailed descriptions of GLUE datasets are included in Appendix B.1.\n\nTo fine-tune on GLUE, we represent the input sequence (for single sentence or sentence pairs) as described in Section 3, and use the final hidden vector $C \\in \\mathbb{R}^H$ corresponding to the first input token ([CLS]) as the aggregate representation. The only new parameters introduced during fine-tuning are classification layer weights $W \\in \\mathbb{R}^{K \\times H}$, where $K$ is the number of labels. We compute a standard classification loss with $C$ and $W$, i.e., $\\log(\\text{softmax}(CW^T))$.\n\n\u2077For example, the BERT SQuAD model can be trained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of 91.0%.  \n\u2078See (10) in https://gluebenchmark.com/faq.\n\nTable 1: GLUE Test results, scored by the evaluation server (https://gluebenchmark.com/leaderboard). The number below each task denotes the number of training examples. The \u201cAverage\u201d column is slightly different than the official GLUE score, since we exclude the problematic WNLI set.\u2078 BERT and OpenAI GPT are single-model, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.\n\nWe use a batch size of 32 and fine-tune for 3 epochs over the data for all GLUE tasks. For each task, we selected the best fine-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set. Additionally, for BERT\\textsubscript{LARGE} we found that fine-tuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the Dev set. With random restarts, we use the same pre-trained checkpoint but perform different fine-tuning data shuffling and classifier layer initialization.\\textsuperscript{9}\n\nResults are presented in Table 1. Both BERT$_{\\text{BASE}}$ and BERT$_{\\text{LARGE}}$ outperform all systems on all tasks by a substantial margin, obtaining 4.5% and 7.0% respective average accuracy improvement over the prior state of the art. Note that BERT$_{\\text{BASE}}$ and OpenAI GPT are nearly identical in terms of model architecture apart from the attention masking. For the largest and most widely reported GLUE task, MNLI, BERT obtains a 4.6% absolute accuracy improvement. On the official GLUE leaderboard$^{10}$, BERT$_{\\text{LARGE}}$ obtains a score of 80.5, compared to OpenAI GPT, which obtains 72.8 as of the date of writing.\n\nWe find that BERT\\textsubscript{LARGE} significantly outperforms BERT\\textsubscript{BASE} across all tasks, especially those with very little training data. The effect of model size is explored more thoroughly in Section 5.2."
        },
        {
            "text": "\u2077For example, the BERT SQuAD model can be trained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of 91.0%.  \n\u2078See (10) in https://gluebenchmark.com/faq.",
            "page": 5,
            "x": 304,
            "y": 722,
            "width": 224,
            "height": 44,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-glue",
            "chunk_id": "0fa10524-238f-4b13-a223-3d8fb5e1af85",
            "group_text": "## 4.1 GLUE\n\nThe General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018a) is a collection of diverse natural language understanding tasks. Detailed descriptions of GLUE datasets are included in Appendix B.1.\n\nTo fine-tune on GLUE, we represent the input sequence (for single sentence or sentence pairs) as described in Section 3, and use the final hidden vector $C \\in \\mathbb{R}^H$ corresponding to the first input token ([CLS]) as the aggregate representation. The only new parameters introduced during fine-tuning are classification layer weights $W \\in \\mathbb{R}^{K \\times H}$, where $K$ is the number of labels. We compute a standard classification loss with $C$ and $W$, i.e., $\\log(\\text{softmax}(CW^T))$.\n\n\u2077For example, the BERT SQuAD model can be trained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of 91.0%.  \n\u2078See (10) in https://gluebenchmark.com/faq.\n\nTable 1: GLUE Test results, scored by the evaluation server (https://gluebenchmark.com/leaderboard). The number below each task denotes the number of training examples. The \u201cAverage\u201d column is slightly different than the official GLUE score, since we exclude the problematic WNLI set.\u2078 BERT and OpenAI GPT are single-model, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.\n\nWe use a batch size of 32 and fine-tune for 3 epochs over the data for all GLUE tasks. For each task, we selected the best fine-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set. Additionally, for BERT\\textsubscript{LARGE} we found that fine-tuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the Dev set. With random restarts, we use the same pre-trained checkpoint but perform different fine-tuning data shuffling and classifier layer initialization.\\textsuperscript{9}\n\nResults are presented in Table 1. Both BERT$_{\\text{BASE}}$ and BERT$_{\\text{LARGE}}$ outperform all systems on all tasks by a substantial margin, obtaining 4.5% and 7.0% respective average accuracy improvement over the prior state of the art. Note that BERT$_{\\text{BASE}}$ and OpenAI GPT are nearly identical in terms of model architecture apart from the attention masking. For the largest and most widely reported GLUE task, MNLI, BERT obtains a 4.6% absolute accuracy improvement. On the official GLUE leaderboard$^{10}$, BERT$_{\\text{LARGE}}$ obtains a score of 80.5, compared to OpenAI GPT, which obtains 72.8 as of the date of writing.\n\nWe find that BERT\\textsubscript{LARGE} significantly outperforms BERT\\textsubscript{BASE} across all tasks, especially those with very little training data. The effect of model size is explored more thoroughly in Section 5.2."
        },
        {
            "text": "Table 1: GLUE Test results, scored by the evaluation server (https://gluebenchmark.com/leaderboard). The number below each task denotes the number of training examples. The \u201cAverage\u201d column is slightly different than the official GLUE score, since we exclude the problematic WNLI set.\u2078 BERT and OpenAI GPT are single-model, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.",
            "page": 6,
            "x": 68,
            "y": 161,
            "width": 461,
            "height": 64,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-glue",
            "chunk_id": "8da0e50a-c4ec-42ca-b79a-05d1cb918561",
            "group_text": "## 4.1 GLUE\n\nThe General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018a) is a collection of diverse natural language understanding tasks. Detailed descriptions of GLUE datasets are included in Appendix B.1.\n\nTo fine-tune on GLUE, we represent the input sequence (for single sentence or sentence pairs) as described in Section 3, and use the final hidden vector $C \\in \\mathbb{R}^H$ corresponding to the first input token ([CLS]) as the aggregate representation. The only new parameters introduced during fine-tuning are classification layer weights $W \\in \\mathbb{R}^{K \\times H}$, where $K$ is the number of labels. We compute a standard classification loss with $C$ and $W$, i.e., $\\log(\\text{softmax}(CW^T))$.\n\n\u2077For example, the BERT SQuAD model can be trained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of 91.0%.  \n\u2078See (10) in https://gluebenchmark.com/faq.\n\nTable 1: GLUE Test results, scored by the evaluation server (https://gluebenchmark.com/leaderboard). The number below each task denotes the number of training examples. The \u201cAverage\u201d column is slightly different than the official GLUE score, since we exclude the problematic WNLI set.\u2078 BERT and OpenAI GPT are single-model, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.\n\nWe use a batch size of 32 and fine-tune for 3 epochs over the data for all GLUE tasks. For each task, we selected the best fine-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set. Additionally, for BERT\\textsubscript{LARGE} we found that fine-tuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the Dev set. With random restarts, we use the same pre-trained checkpoint but perform different fine-tuning data shuffling and classifier layer initialization.\\textsuperscript{9}\n\nResults are presented in Table 1. Both BERT$_{\\text{BASE}}$ and BERT$_{\\text{LARGE}}$ outperform all systems on all tasks by a substantial margin, obtaining 4.5% and 7.0% respective average accuracy improvement over the prior state of the art. Note that BERT$_{\\text{BASE}}$ and OpenAI GPT are nearly identical in terms of model architecture apart from the attention masking. For the largest and most widely reported GLUE task, MNLI, BERT obtains a 4.6% absolute accuracy improvement. On the official GLUE leaderboard$^{10}$, BERT$_{\\text{LARGE}}$ obtains a score of 80.5, compared to OpenAI GPT, which obtains 72.8 as of the date of writing.\n\nWe find that BERT\\textsubscript{LARGE} significantly outperforms BERT\\textsubscript{BASE} across all tasks, especially those with very little training data. The effect of model size is explored more thoroughly in Section 5.2."
        },
        {
            "text": "We use a batch size of 32 and fine-tune for 3 epochs over the data for all GLUE tasks. For each task, we selected the best fine-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set. Additionally, for BERT\\textsubscript{LARGE} we found that fine-tuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the Dev set. With random restarts, we use the same pre-trained checkpoint but perform different fine-tuning data shuffling and classifier layer initialization.\\textsuperscript{9}",
            "page": 6,
            "x": 68,
            "y": 248,
            "width": 225,
            "height": 149,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-glue",
            "chunk_id": "b0adafc5-3cf9-462c-8e75-edb324023bdc",
            "group_text": "## 4.1 GLUE\n\nThe General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018a) is a collection of diverse natural language understanding tasks. Detailed descriptions of GLUE datasets are included in Appendix B.1.\n\nTo fine-tune on GLUE, we represent the input sequence (for single sentence or sentence pairs) as described in Section 3, and use the final hidden vector $C \\in \\mathbb{R}^H$ corresponding to the first input token ([CLS]) as the aggregate representation. The only new parameters introduced during fine-tuning are classification layer weights $W \\in \\mathbb{R}^{K \\times H}$, where $K$ is the number of labels. We compute a standard classification loss with $C$ and $W$, i.e., $\\log(\\text{softmax}(CW^T))$.\n\n\u2077For example, the BERT SQuAD model can be trained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of 91.0%.  \n\u2078See (10) in https://gluebenchmark.com/faq.\n\nTable 1: GLUE Test results, scored by the evaluation server (https://gluebenchmark.com/leaderboard). The number below each task denotes the number of training examples. The \u201cAverage\u201d column is slightly different than the official GLUE score, since we exclude the problematic WNLI set.\u2078 BERT and OpenAI GPT are single-model, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.\n\nWe use a batch size of 32 and fine-tune for 3 epochs over the data for all GLUE tasks. For each task, we selected the best fine-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set. Additionally, for BERT\\textsubscript{LARGE} we found that fine-tuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the Dev set. With random restarts, we use the same pre-trained checkpoint but perform different fine-tuning data shuffling and classifier layer initialization.\\textsuperscript{9}\n\nResults are presented in Table 1. Both BERT$_{\\text{BASE}}$ and BERT$_{\\text{LARGE}}$ outperform all systems on all tasks by a substantial margin, obtaining 4.5% and 7.0% respective average accuracy improvement over the prior state of the art. Note that BERT$_{\\text{BASE}}$ and OpenAI GPT are nearly identical in terms of model architecture apart from the attention masking. For the largest and most widely reported GLUE task, MNLI, BERT obtains a 4.6% absolute accuracy improvement. On the official GLUE leaderboard$^{10}$, BERT$_{\\text{LARGE}}$ obtains a score of 80.5, compared to OpenAI GPT, which obtains 72.8 as of the date of writing.\n\nWe find that BERT\\textsubscript{LARGE} significantly outperforms BERT\\textsubscript{BASE} across all tasks, especially those with very little training data. The effect of model size is explored more thoroughly in Section 5.2."
        },
        {
            "text": "Results are presented in Table 1. Both BERT$_{\\text{BASE}}$ and BERT$_{\\text{LARGE}}$ outperform all systems on all tasks by a substantial margin, obtaining 4.5% and 7.0% respective average accuracy improvement over the prior state of the art. Note that BERT$_{\\text{BASE}}$ and OpenAI GPT are nearly identical in terms of model architecture apart from the attention masking. For the largest and most widely reported GLUE task, MNLI, BERT obtains a 4.6% absolute accuracy improvement. On the official GLUE leaderboard$^{10}$, BERT$_{\\text{LARGE}}$ obtains a score of 80.5, compared to OpenAI GPT, which obtains 72.8 as of the date of writing.",
            "page": 6,
            "x": 68,
            "y": 399,
            "width": 225,
            "height": 176,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-glue",
            "chunk_id": "f207b6ad-51fa-4b01-8992-f5bafadd8ec5",
            "group_text": "## 4.1 GLUE\n\nThe General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018a) is a collection of diverse natural language understanding tasks. Detailed descriptions of GLUE datasets are included in Appendix B.1.\n\nTo fine-tune on GLUE, we represent the input sequence (for single sentence or sentence pairs) as described in Section 3, and use the final hidden vector $C \\in \\mathbb{R}^H$ corresponding to the first input token ([CLS]) as the aggregate representation. The only new parameters introduced during fine-tuning are classification layer weights $W \\in \\mathbb{R}^{K \\times H}$, where $K$ is the number of labels. We compute a standard classification loss with $C$ and $W$, i.e., $\\log(\\text{softmax}(CW^T))$.\n\n\u2077For example, the BERT SQuAD model can be trained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of 91.0%.  \n\u2078See (10) in https://gluebenchmark.com/faq.\n\nTable 1: GLUE Test results, scored by the evaluation server (https://gluebenchmark.com/leaderboard). The number below each task denotes the number of training examples. The \u201cAverage\u201d column is slightly different than the official GLUE score, since we exclude the problematic WNLI set.\u2078 BERT and OpenAI GPT are single-model, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.\n\nWe use a batch size of 32 and fine-tune for 3 epochs over the data for all GLUE tasks. For each task, we selected the best fine-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set. Additionally, for BERT\\textsubscript{LARGE} we found that fine-tuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the Dev set. With random restarts, we use the same pre-trained checkpoint but perform different fine-tuning data shuffling and classifier layer initialization.\\textsuperscript{9}\n\nResults are presented in Table 1. Both BERT$_{\\text{BASE}}$ and BERT$_{\\text{LARGE}}$ outperform all systems on all tasks by a substantial margin, obtaining 4.5% and 7.0% respective average accuracy improvement over the prior state of the art. Note that BERT$_{\\text{BASE}}$ and OpenAI GPT are nearly identical in terms of model architecture apart from the attention masking. For the largest and most widely reported GLUE task, MNLI, BERT obtains a 4.6% absolute accuracy improvement. On the official GLUE leaderboard$^{10}$, BERT$_{\\text{LARGE}}$ obtains a score of 80.5, compared to OpenAI GPT, which obtains 72.8 as of the date of writing.\n\nWe find that BERT\\textsubscript{LARGE} significantly outperforms BERT\\textsubscript{BASE} across all tasks, especially those with very little training data. The effect of model size is explored more thoroughly in Section 5.2."
        },
        {
            "text": "We find that BERT\\textsubscript{LARGE} significantly outperforms BERT\\textsubscript{BASE} across all tasks, especially those with very little training data. The effect of model size is explored more thoroughly in Section 5.2.",
            "page": 6,
            "x": 69,
            "y": 576,
            "width": 224,
            "height": 55,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-glue",
            "chunk_id": "2bc140c2-654f-48b9-a5a5-023c8722ac76",
            "group_text": "## 4.1 GLUE\n\nThe General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018a) is a collection of diverse natural language understanding tasks. Detailed descriptions of GLUE datasets are included in Appendix B.1.\n\nTo fine-tune on GLUE, we represent the input sequence (for single sentence or sentence pairs) as described in Section 3, and use the final hidden vector $C \\in \\mathbb{R}^H$ corresponding to the first input token ([CLS]) as the aggregate representation. The only new parameters introduced during fine-tuning are classification layer weights $W \\in \\mathbb{R}^{K \\times H}$, where $K$ is the number of labels. We compute a standard classification loss with $C$ and $W$, i.e., $\\log(\\text{softmax}(CW^T))$.\n\n\u2077For example, the BERT SQuAD model can be trained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of 91.0%.  \n\u2078See (10) in https://gluebenchmark.com/faq.\n\nTable 1: GLUE Test results, scored by the evaluation server (https://gluebenchmark.com/leaderboard). The number below each task denotes the number of training examples. The \u201cAverage\u201d column is slightly different than the official GLUE score, since we exclude the problematic WNLI set.\u2078 BERT and OpenAI GPT are single-model, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.\n\nWe use a batch size of 32 and fine-tune for 3 epochs over the data for all GLUE tasks. For each task, we selected the best fine-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set. Additionally, for BERT\\textsubscript{LARGE} we found that fine-tuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the Dev set. With random restarts, we use the same pre-trained checkpoint but perform different fine-tuning data shuffling and classifier layer initialization.\\textsuperscript{9}\n\nResults are presented in Table 1. Both BERT$_{\\text{BASE}}$ and BERT$_{\\text{LARGE}}$ outperform all systems on all tasks by a substantial margin, obtaining 4.5% and 7.0% respective average accuracy improvement over the prior state of the art. Note that BERT$_{\\text{BASE}}$ and OpenAI GPT are nearly identical in terms of model architecture apart from the attention masking. For the largest and most widely reported GLUE task, MNLI, BERT obtains a 4.6% absolute accuracy improvement. On the official GLUE leaderboard$^{10}$, BERT$_{\\text{LARGE}}$ obtains a score of 80.5, compared to OpenAI GPT, which obtains 72.8 as of the date of writing.\n\nWe find that BERT\\textsubscript{LARGE} significantly outperforms BERT\\textsubscript{BASE} across all tasks, especially those with very little training data. The effect of model size is explored more thoroughly in Section 5.2."
        },
        {
            "text": "## 4.2 SQuAD v1.1\n\nThe Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowd-sourced question/answer pairs (Rajpurkar et al., 2016). Given a question and a passage from",
            "page": 6,
            "x": 68,
            "y": 639,
            "width": 225,
            "height": 76,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-squad",
            "chunk_id": "2ace80f7-b0d5-4144-9991-5bcfdfc5a6ee",
            "group_text": "## 4.2 SQuAD v1.1\n\nThe Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowd-sourced question/answer pairs (Rajpurkar et al., 2016). Given a question and a passage from\n\n\u2079The GLUE data set distribution does not include the Test labels, and we only made a single GLUE evaluation server submission for each of BERT\u1d2e\u1d2c\u02e2\u1d31 and BERT\u1d38\u1d2c\u1d3f\u1d33\u1d31.\n\u00b9\u2070https://gluebenchmark.com/leaderboard\n\nWikipedia containing the answer, the task is to predict the answer text span in the passage.\n\nAs shown in Figure 1, in the question answering task, we represent the input question and passage as a single packed sequence, with the question using the A embedding and the passage using the B embedding. We only introduce a start vector $S \\in \\mathbb{R}^H$ and an end vector $E \\in \\mathbb{R}^H$ during fine-tuning. The probability of word $i$ being the start of the answer span is computed as a dot product between $T_i$ and $S$ followed by a softmax over all of the words in the paragraph: $P_i = \\frac{e^{S \\cdot T_i}}{\\sum_j e^{S \\cdot T_j}}$.\n\nThe analogous formula is used for the end of the answer span. The score of a candidate span from position $i$ to position $j$ is defined as $S \\cdot T_i + E \\cdot T_j$, and the maximum scoring span where $j \\geq i$ is used as a prediction. The training objective is the sum of the log-likelihoods of the correct start and end positions. We fine-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32.\n\nTable 2 shows top leaderboard entries as well as results from top published systems (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018). The top results from the SQuAD leaderboard do not have up-to-date public system descriptions available,\u00b9\u00b9 and are allowed to use any public data when training their systems. We therefore use modest data augmentation in our system by first fine-tuning on TriviaQA (Joshi et al., 2017) befor fine-tuning on SQuAD.\n\nOur best performing system outperforms the top\nleaderboard system by +1.5 F1 in ensembling and\n+1.3 F1 as a single system. In fact, our single\nBERT model outperforms the top ensemble sys-\ntem in terms of F1 score. Without TriviaQA fine-\n\n$^{11}$QANet is described in Yu et al. (2018), but the system has improved substantially after publication.\n\nTable 2:  SQuAD 1.1 results.  The BERT ensemble is 7x systems which use different pre-training checkpoints and fine-tuning seeds.\n\nTable 3:  SQuAD 2.0 results.  We exclude entries that use BERT as one of their components.\n\ntuning data, we only lose 0.1-0.4 F1, still outper-\nforming all existing systems by a wide margin.\u00b9\u00b2"
        },
        {
            "text": "\u2079The GLUE data set distribution does not include the Test labels, and we only made a single GLUE evaluation server submission for each of BERT\u1d2e\u1d2c\u02e2\u1d31 and BERT\u1d38\u1d2c\u1d3f\u1d33\u1d31.\n\u00b9\u2070https://gluebenchmark.com/leaderboard",
            "page": 6,
            "x": 68,
            "y": 722,
            "width": 224,
            "height": 43,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-squad",
            "chunk_id": "9eef29a2-06c8-4d83-b443-b4c4cb85eab0",
            "group_text": "## 4.2 SQuAD v1.1\n\nThe Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowd-sourced question/answer pairs (Rajpurkar et al., 2016). Given a question and a passage from\n\n\u2079The GLUE data set distribution does not include the Test labels, and we only made a single GLUE evaluation server submission for each of BERT\u1d2e\u1d2c\u02e2\u1d31 and BERT\u1d38\u1d2c\u1d3f\u1d33\u1d31.\n\u00b9\u2070https://gluebenchmark.com/leaderboard\n\nWikipedia containing the answer, the task is to predict the answer text span in the passage.\n\nAs shown in Figure 1, in the question answering task, we represent the input question and passage as a single packed sequence, with the question using the A embedding and the passage using the B embedding. We only introduce a start vector $S \\in \\mathbb{R}^H$ and an end vector $E \\in \\mathbb{R}^H$ during fine-tuning. The probability of word $i$ being the start of the answer span is computed as a dot product between $T_i$ and $S$ followed by a softmax over all of the words in the paragraph: $P_i = \\frac{e^{S \\cdot T_i}}{\\sum_j e^{S \\cdot T_j}}$.\n\nThe analogous formula is used for the end of the answer span. The score of a candidate span from position $i$ to position $j$ is defined as $S \\cdot T_i + E \\cdot T_j$, and the maximum scoring span where $j \\geq i$ is used as a prediction. The training objective is the sum of the log-likelihoods of the correct start and end positions. We fine-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32.\n\nTable 2 shows top leaderboard entries as well as results from top published systems (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018). The top results from the SQuAD leaderboard do not have up-to-date public system descriptions available,\u00b9\u00b9 and are allowed to use any public data when training their systems. We therefore use modest data augmentation in our system by first fine-tuning on TriviaQA (Joshi et al., 2017) befor fine-tuning on SQuAD.\n\nOur best performing system outperforms the top\nleaderboard system by +1.5 F1 in ensembling and\n+1.3 F1 as a single system. In fact, our single\nBERT model outperforms the top ensemble sys-\ntem in terms of F1 score. Without TriviaQA fine-\n\n$^{11}$QANet is described in Yu et al. (2018), but the system has improved substantially after publication.\n\nTable 2:  SQuAD 1.1 results.  The BERT ensemble is 7x systems which use different pre-training checkpoints and fine-tuning seeds.\n\nTable 3:  SQuAD 2.0 results.  We exclude entries that use BERT as one of their components.\n\ntuning data, we only lose 0.1-0.4 F1, still outper-\nforming all existing systems by a wide margin.\u00b9\u00b2"
        },
        {
            "text": "Wikipedia containing the answer, the task is to predict the answer text span in the passage.",
            "page": 6,
            "x": 304,
            "y": 249,
            "width": 223,
            "height": 27,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-squad",
            "chunk_id": "e5041c6b-8818-4b94-9369-47ceaefc842f",
            "group_text": "## 4.2 SQuAD v1.1\n\nThe Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowd-sourced question/answer pairs (Rajpurkar et al., 2016). Given a question and a passage from\n\n\u2079The GLUE data set distribution does not include the Test labels, and we only made a single GLUE evaluation server submission for each of BERT\u1d2e\u1d2c\u02e2\u1d31 and BERT\u1d38\u1d2c\u1d3f\u1d33\u1d31.\n\u00b9\u2070https://gluebenchmark.com/leaderboard\n\nWikipedia containing the answer, the task is to predict the answer text span in the passage.\n\nAs shown in Figure 1, in the question answering task, we represent the input question and passage as a single packed sequence, with the question using the A embedding and the passage using the B embedding. We only introduce a start vector $S \\in \\mathbb{R}^H$ and an end vector $E \\in \\mathbb{R}^H$ during fine-tuning. The probability of word $i$ being the start of the answer span is computed as a dot product between $T_i$ and $S$ followed by a softmax over all of the words in the paragraph: $P_i = \\frac{e^{S \\cdot T_i}}{\\sum_j e^{S \\cdot T_j}}$.\n\nThe analogous formula is used for the end of the answer span. The score of a candidate span from position $i$ to position $j$ is defined as $S \\cdot T_i + E \\cdot T_j$, and the maximum scoring span where $j \\geq i$ is used as a prediction. The training objective is the sum of the log-likelihoods of the correct start and end positions. We fine-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32.\n\nTable 2 shows top leaderboard entries as well as results from top published systems (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018). The top results from the SQuAD leaderboard do not have up-to-date public system descriptions available,\u00b9\u00b9 and are allowed to use any public data when training their systems. We therefore use modest data augmentation in our system by first fine-tuning on TriviaQA (Joshi et al., 2017) befor fine-tuning on SQuAD.\n\nOur best performing system outperforms the top\nleaderboard system by +1.5 F1 in ensembling and\n+1.3 F1 as a single system. In fact, our single\nBERT model outperforms the top ensemble sys-\ntem in terms of F1 score. Without TriviaQA fine-\n\n$^{11}$QANet is described in Yu et al. (2018), but the system has improved substantially after publication.\n\nTable 2:  SQuAD 1.1 results.  The BERT ensemble is 7x systems which use different pre-training checkpoints and fine-tuning seeds.\n\nTable 3:  SQuAD 2.0 results.  We exclude entries that use BERT as one of their components.\n\ntuning data, we only lose 0.1-0.4 F1, still outper-\nforming all existing systems by a wide margin.\u00b9\u00b2"
        },
        {
            "text": "As shown in Figure 1, in the question answering task, we represent the input question and passage as a single packed sequence, with the question using the A embedding and the passage using the B embedding. We only introduce a start vector $S \\in \\mathbb{R}^H$ and an end vector $E \\in \\mathbb{R}^H$ during fine-tuning. The probability of word $i$ being the start of the answer span is computed as a dot product between $T_i$ and $S$ followed by a softmax over all of the words in the paragraph: $P_i = \\frac{e^{S \\cdot T_i}}{\\sum_j e^{S \\cdot T_j}}$.",
            "page": 6,
            "x": 304,
            "y": 278,
            "width": 224,
            "height": 138,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-squad",
            "chunk_id": "6d9ef8fd-74d6-4701-ae3a-ba9cc2f31bb5",
            "group_text": "## 4.2 SQuAD v1.1\n\nThe Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowd-sourced question/answer pairs (Rajpurkar et al., 2016). Given a question and a passage from\n\n\u2079The GLUE data set distribution does not include the Test labels, and we only made a single GLUE evaluation server submission for each of BERT\u1d2e\u1d2c\u02e2\u1d31 and BERT\u1d38\u1d2c\u1d3f\u1d33\u1d31.\n\u00b9\u2070https://gluebenchmark.com/leaderboard\n\nWikipedia containing the answer, the task is to predict the answer text span in the passage.\n\nAs shown in Figure 1, in the question answering task, we represent the input question and passage as a single packed sequence, with the question using the A embedding and the passage using the B embedding. We only introduce a start vector $S \\in \\mathbb{R}^H$ and an end vector $E \\in \\mathbb{R}^H$ during fine-tuning. The probability of word $i$ being the start of the answer span is computed as a dot product between $T_i$ and $S$ followed by a softmax over all of the words in the paragraph: $P_i = \\frac{e^{S \\cdot T_i}}{\\sum_j e^{S \\cdot T_j}}$.\n\nThe analogous formula is used for the end of the answer span. The score of a candidate span from position $i$ to position $j$ is defined as $S \\cdot T_i + E \\cdot T_j$, and the maximum scoring span where $j \\geq i$ is used as a prediction. The training objective is the sum of the log-likelihoods of the correct start and end positions. We fine-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32.\n\nTable 2 shows top leaderboard entries as well as results from top published systems (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018). The top results from the SQuAD leaderboard do not have up-to-date public system descriptions available,\u00b9\u00b9 and are allowed to use any public data when training their systems. We therefore use modest data augmentation in our system by first fine-tuning on TriviaQA (Joshi et al., 2017) befor fine-tuning on SQuAD.\n\nOur best performing system outperforms the top\nleaderboard system by +1.5 F1 in ensembling and\n+1.3 F1 as a single system. In fact, our single\nBERT model outperforms the top ensemble sys-\ntem in terms of F1 score. Without TriviaQA fine-\n\n$^{11}$QANet is described in Yu et al. (2018), but the system has improved substantially after publication.\n\nTable 2:  SQuAD 1.1 results.  The BERT ensemble is 7x systems which use different pre-training checkpoints and fine-tuning seeds.\n\nTable 3:  SQuAD 2.0 results.  We exclude entries that use BERT as one of their components.\n\ntuning data, we only lose 0.1-0.4 F1, still outper-\nforming all existing systems by a wide margin.\u00b9\u00b2"
        },
        {
            "text": "The analogous formula is used for the end of the answer span. The score of a candidate span from position $i$ to position $j$ is defined as $S \\cdot T_i + E \\cdot T_j$, and the maximum scoring span where $j \\geq i$ is used as a prediction. The training objective is the sum of the log-likelihoods of the correct start and end positions. We fine-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32.",
            "page": 6,
            "x": 304,
            "y": 419,
            "width": 223,
            "height": 108,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-squad",
            "chunk_id": "025bfc8b-c46c-4fa0-bf8e-6a32655b78d7",
            "group_text": "## 4.2 SQuAD v1.1\n\nThe Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowd-sourced question/answer pairs (Rajpurkar et al., 2016). Given a question and a passage from\n\n\u2079The GLUE data set distribution does not include the Test labels, and we only made a single GLUE evaluation server submission for each of BERT\u1d2e\u1d2c\u02e2\u1d31 and BERT\u1d38\u1d2c\u1d3f\u1d33\u1d31.\n\u00b9\u2070https://gluebenchmark.com/leaderboard\n\nWikipedia containing the answer, the task is to predict the answer text span in the passage.\n\nAs shown in Figure 1, in the question answering task, we represent the input question and passage as a single packed sequence, with the question using the A embedding and the passage using the B embedding. We only introduce a start vector $S \\in \\mathbb{R}^H$ and an end vector $E \\in \\mathbb{R}^H$ during fine-tuning. The probability of word $i$ being the start of the answer span is computed as a dot product between $T_i$ and $S$ followed by a softmax over all of the words in the paragraph: $P_i = \\frac{e^{S \\cdot T_i}}{\\sum_j e^{S \\cdot T_j}}$.\n\nThe analogous formula is used for the end of the answer span. The score of a candidate span from position $i$ to position $j$ is defined as $S \\cdot T_i + E \\cdot T_j$, and the maximum scoring span where $j \\geq i$ is used as a prediction. The training objective is the sum of the log-likelihoods of the correct start and end positions. We fine-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32.\n\nTable 2 shows top leaderboard entries as well as results from top published systems (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018). The top results from the SQuAD leaderboard do not have up-to-date public system descriptions available,\u00b9\u00b9 and are allowed to use any public data when training their systems. We therefore use modest data augmentation in our system by first fine-tuning on TriviaQA (Joshi et al., 2017) befor fine-tuning on SQuAD.\n\nOur best performing system outperforms the top\nleaderboard system by +1.5 F1 in ensembling and\n+1.3 F1 as a single system. In fact, our single\nBERT model outperforms the top ensemble sys-\ntem in terms of F1 score. Without TriviaQA fine-\n\n$^{11}$QANet is described in Yu et al. (2018), but the system has improved substantially after publication.\n\nTable 2:  SQuAD 1.1 results.  The BERT ensemble is 7x systems which use different pre-training checkpoints and fine-tuning seeds.\n\nTable 3:  SQuAD 2.0 results.  We exclude entries that use BERT as one of their components.\n\ntuning data, we only lose 0.1-0.4 F1, still outper-\nforming all existing systems by a wide margin.\u00b9\u00b2"
        },
        {
            "text": "Table 2 shows top leaderboard entries as well as results from top published systems (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018). The top results from the SQuAD leaderboard do not have up-to-date public system descriptions available,\u00b9\u00b9 and are allowed to use any public data when training their systems. We therefore use modest data augmentation in our system by first fine-tuning on TriviaQA (Joshi et al., 2017) befor fine-tuning on SQuAD.",
            "page": 6,
            "x": 304,
            "y": 528,
            "width": 224,
            "height": 136,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-squad",
            "chunk_id": "a14cbcc8-0f62-4a72-9afc-c47c0dc9463d",
            "group_text": "## 4.2 SQuAD v1.1\n\nThe Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowd-sourced question/answer pairs (Rajpurkar et al., 2016). Given a question and a passage from\n\n\u2079The GLUE data set distribution does not include the Test labels, and we only made a single GLUE evaluation server submission for each of BERT\u1d2e\u1d2c\u02e2\u1d31 and BERT\u1d38\u1d2c\u1d3f\u1d33\u1d31.\n\u00b9\u2070https://gluebenchmark.com/leaderboard\n\nWikipedia containing the answer, the task is to predict the answer text span in the passage.\n\nAs shown in Figure 1, in the question answering task, we represent the input question and passage as a single packed sequence, with the question using the A embedding and the passage using the B embedding. We only introduce a start vector $S \\in \\mathbb{R}^H$ and an end vector $E \\in \\mathbb{R}^H$ during fine-tuning. The probability of word $i$ being the start of the answer span is computed as a dot product between $T_i$ and $S$ followed by a softmax over all of the words in the paragraph: $P_i = \\frac{e^{S \\cdot T_i}}{\\sum_j e^{S \\cdot T_j}}$.\n\nThe analogous formula is used for the end of the answer span. The score of a candidate span from position $i$ to position $j$ is defined as $S \\cdot T_i + E \\cdot T_j$, and the maximum scoring span where $j \\geq i$ is used as a prediction. The training objective is the sum of the log-likelihoods of the correct start and end positions. We fine-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32.\n\nTable 2 shows top leaderboard entries as well as results from top published systems (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018). The top results from the SQuAD leaderboard do not have up-to-date public system descriptions available,\u00b9\u00b9 and are allowed to use any public data when training their systems. We therefore use modest data augmentation in our system by first fine-tuning on TriviaQA (Joshi et al., 2017) befor fine-tuning on SQuAD.\n\nOur best performing system outperforms the top\nleaderboard system by +1.5 F1 in ensembling and\n+1.3 F1 as a single system. In fact, our single\nBERT model outperforms the top ensemble sys-\ntem in terms of F1 score. Without TriviaQA fine-\n\n$^{11}$QANet is described in Yu et al. (2018), but the system has improved substantially after publication.\n\nTable 2:  SQuAD 1.1 results.  The BERT ensemble is 7x systems which use different pre-training checkpoints and fine-tuning seeds.\n\nTable 3:  SQuAD 2.0 results.  We exclude entries that use BERT as one of their components.\n\ntuning data, we only lose 0.1-0.4 F1, still outper-\nforming all existing systems by a wide margin.\u00b9\u00b2"
        },
        {
            "text": "Our best performing system outperforms the top\nleaderboard system by +1.5 F1 in ensembling and\n+1.3 F1 as a single system. In fact, our single\nBERT model outperforms the top ensemble sys-\ntem in terms of F1 score. Without TriviaQA fine-",
            "page": 6,
            "x": 304,
            "y": 665,
            "width": 223,
            "height": 67,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-squad",
            "chunk_id": "23fbe53d-b63b-4108-b795-a6c0e2d8d90b",
            "group_text": "## 4.2 SQuAD v1.1\n\nThe Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowd-sourced question/answer pairs (Rajpurkar et al., 2016). Given a question and a passage from\n\n\u2079The GLUE data set distribution does not include the Test labels, and we only made a single GLUE evaluation server submission for each of BERT\u1d2e\u1d2c\u02e2\u1d31 and BERT\u1d38\u1d2c\u1d3f\u1d33\u1d31.\n\u00b9\u2070https://gluebenchmark.com/leaderboard\n\nWikipedia containing the answer, the task is to predict the answer text span in the passage.\n\nAs shown in Figure 1, in the question answering task, we represent the input question and passage as a single packed sequence, with the question using the A embedding and the passage using the B embedding. We only introduce a start vector $S \\in \\mathbb{R}^H$ and an end vector $E \\in \\mathbb{R}^H$ during fine-tuning. The probability of word $i$ being the start of the answer span is computed as a dot product between $T_i$ and $S$ followed by a softmax over all of the words in the paragraph: $P_i = \\frac{e^{S \\cdot T_i}}{\\sum_j e^{S \\cdot T_j}}$.\n\nThe analogous formula is used for the end of the answer span. The score of a candidate span from position $i$ to position $j$ is defined as $S \\cdot T_i + E \\cdot T_j$, and the maximum scoring span where $j \\geq i$ is used as a prediction. The training objective is the sum of the log-likelihoods of the correct start and end positions. We fine-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32.\n\nTable 2 shows top leaderboard entries as well as results from top published systems (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018). The top results from the SQuAD leaderboard do not have up-to-date public system descriptions available,\u00b9\u00b9 and are allowed to use any public data when training their systems. We therefore use modest data augmentation in our system by first fine-tuning on TriviaQA (Joshi et al., 2017) befor fine-tuning on SQuAD.\n\nOur best performing system outperforms the top\nleaderboard system by +1.5 F1 in ensembling and\n+1.3 F1 as a single system. In fact, our single\nBERT model outperforms the top ensemble sys-\ntem in terms of F1 score. Without TriviaQA fine-\n\n$^{11}$QANet is described in Yu et al. (2018), but the system has improved substantially after publication.\n\nTable 2:  SQuAD 1.1 results.  The BERT ensemble is 7x systems which use different pre-training checkpoints and fine-tuning seeds.\n\nTable 3:  SQuAD 2.0 results.  We exclude entries that use BERT as one of their components.\n\ntuning data, we only lose 0.1-0.4 F1, still outper-\nforming all existing systems by a wide margin.\u00b9\u00b2"
        },
        {
            "text": "$^{11}$QANet is described in Yu et al. (2018), but the system has improved substantially after publication.",
            "page": 6,
            "x": 304,
            "y": 743,
            "width": 222,
            "height": 23,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-squad",
            "chunk_id": "e802a9b4-f11a-4714-9909-8e41289e0368",
            "group_text": "## 4.2 SQuAD v1.1\n\nThe Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowd-sourced question/answer pairs (Rajpurkar et al., 2016). Given a question and a passage from\n\n\u2079The GLUE data set distribution does not include the Test labels, and we only made a single GLUE evaluation server submission for each of BERT\u1d2e\u1d2c\u02e2\u1d31 and BERT\u1d38\u1d2c\u1d3f\u1d33\u1d31.\n\u00b9\u2070https://gluebenchmark.com/leaderboard\n\nWikipedia containing the answer, the task is to predict the answer text span in the passage.\n\nAs shown in Figure 1, in the question answering task, we represent the input question and passage as a single packed sequence, with the question using the A embedding and the passage using the B embedding. We only introduce a start vector $S \\in \\mathbb{R}^H$ and an end vector $E \\in \\mathbb{R}^H$ during fine-tuning. The probability of word $i$ being the start of the answer span is computed as a dot product between $T_i$ and $S$ followed by a softmax over all of the words in the paragraph: $P_i = \\frac{e^{S \\cdot T_i}}{\\sum_j e^{S \\cdot T_j}}$.\n\nThe analogous formula is used for the end of the answer span. The score of a candidate span from position $i$ to position $j$ is defined as $S \\cdot T_i + E \\cdot T_j$, and the maximum scoring span where $j \\geq i$ is used as a prediction. The training objective is the sum of the log-likelihoods of the correct start and end positions. We fine-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32.\n\nTable 2 shows top leaderboard entries as well as results from top published systems (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018). The top results from the SQuAD leaderboard do not have up-to-date public system descriptions available,\u00b9\u00b9 and are allowed to use any public data when training their systems. We therefore use modest data augmentation in our system by first fine-tuning on TriviaQA (Joshi et al., 2017) befor fine-tuning on SQuAD.\n\nOur best performing system outperforms the top\nleaderboard system by +1.5 F1 in ensembling and\n+1.3 F1 as a single system. In fact, our single\nBERT model outperforms the top ensemble sys-\ntem in terms of F1 score. Without TriviaQA fine-\n\n$^{11}$QANet is described in Yu et al. (2018), but the system has improved substantially after publication.\n\nTable 2:  SQuAD 1.1 results.  The BERT ensemble is 7x systems which use different pre-training checkpoints and fine-tuning seeds.\n\nTable 3:  SQuAD 2.0 results.  We exclude entries that use BERT as one of their components.\n\ntuning data, we only lose 0.1-0.4 F1, still outper-\nforming all existing systems by a wide margin.\u00b9\u00b2"
        },
        {
            "text": "Table 2:  SQuAD 1.1 results.  The BERT ensemble is 7x systems which use different pre-training checkpoints and fine-tuning seeds.",
            "page": 7,
            "x": 68,
            "y": 247,
            "width": 225,
            "height": 40,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-squad",
            "chunk_id": "da1dfe7b-f734-400b-bc35-5ee929826845",
            "group_text": "## 4.2 SQuAD v1.1\n\nThe Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowd-sourced question/answer pairs (Rajpurkar et al., 2016). Given a question and a passage from\n\n\u2079The GLUE data set distribution does not include the Test labels, and we only made a single GLUE evaluation server submission for each of BERT\u1d2e\u1d2c\u02e2\u1d31 and BERT\u1d38\u1d2c\u1d3f\u1d33\u1d31.\n\u00b9\u2070https://gluebenchmark.com/leaderboard\n\nWikipedia containing the answer, the task is to predict the answer text span in the passage.\n\nAs shown in Figure 1, in the question answering task, we represent the input question and passage as a single packed sequence, with the question using the A embedding and the passage using the B embedding. We only introduce a start vector $S \\in \\mathbb{R}^H$ and an end vector $E \\in \\mathbb{R}^H$ during fine-tuning. The probability of word $i$ being the start of the answer span is computed as a dot product between $T_i$ and $S$ followed by a softmax over all of the words in the paragraph: $P_i = \\frac{e^{S \\cdot T_i}}{\\sum_j e^{S \\cdot T_j}}$.\n\nThe analogous formula is used for the end of the answer span. The score of a candidate span from position $i$ to position $j$ is defined as $S \\cdot T_i + E \\cdot T_j$, and the maximum scoring span where $j \\geq i$ is used as a prediction. The training objective is the sum of the log-likelihoods of the correct start and end positions. We fine-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32.\n\nTable 2 shows top leaderboard entries as well as results from top published systems (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018). The top results from the SQuAD leaderboard do not have up-to-date public system descriptions available,\u00b9\u00b9 and are allowed to use any public data when training their systems. We therefore use modest data augmentation in our system by first fine-tuning on TriviaQA (Joshi et al., 2017) befor fine-tuning on SQuAD.\n\nOur best performing system outperforms the top\nleaderboard system by +1.5 F1 in ensembling and\n+1.3 F1 as a single system. In fact, our single\nBERT model outperforms the top ensemble sys-\ntem in terms of F1 score. Without TriviaQA fine-\n\n$^{11}$QANet is described in Yu et al. (2018), but the system has improved substantially after publication.\n\nTable 2:  SQuAD 1.1 results.  The BERT ensemble is 7x systems which use different pre-training checkpoints and fine-tuning seeds.\n\nTable 3:  SQuAD 2.0 results.  We exclude entries that use BERT as one of their components.\n\ntuning data, we only lose 0.1-0.4 F1, still outper-\nforming all existing systems by a wide margin.\u00b9\u00b2"
        },
        {
            "text": "Table 3:  SQuAD 2.0 results.  We exclude entries that use BERT as one of their components.",
            "page": 7,
            "x": 69,
            "y": 444,
            "width": 223,
            "height": 27,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-squad",
            "chunk_id": "69431efe-46eb-4cbd-820e-b0b261d65a16",
            "group_text": "## 4.2 SQuAD v1.1\n\nThe Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowd-sourced question/answer pairs (Rajpurkar et al., 2016). Given a question and a passage from\n\n\u2079The GLUE data set distribution does not include the Test labels, and we only made a single GLUE evaluation server submission for each of BERT\u1d2e\u1d2c\u02e2\u1d31 and BERT\u1d38\u1d2c\u1d3f\u1d33\u1d31.\n\u00b9\u2070https://gluebenchmark.com/leaderboard\n\nWikipedia containing the answer, the task is to predict the answer text span in the passage.\n\nAs shown in Figure 1, in the question answering task, we represent the input question and passage as a single packed sequence, with the question using the A embedding and the passage using the B embedding. We only introduce a start vector $S \\in \\mathbb{R}^H$ and an end vector $E \\in \\mathbb{R}^H$ during fine-tuning. The probability of word $i$ being the start of the answer span is computed as a dot product between $T_i$ and $S$ followed by a softmax over all of the words in the paragraph: $P_i = \\frac{e^{S \\cdot T_i}}{\\sum_j e^{S \\cdot T_j}}$.\n\nThe analogous formula is used for the end of the answer span. The score of a candidate span from position $i$ to position $j$ is defined as $S \\cdot T_i + E \\cdot T_j$, and the maximum scoring span where $j \\geq i$ is used as a prediction. The training objective is the sum of the log-likelihoods of the correct start and end positions. We fine-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32.\n\nTable 2 shows top leaderboard entries as well as results from top published systems (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018). The top results from the SQuAD leaderboard do not have up-to-date public system descriptions available,\u00b9\u00b9 and are allowed to use any public data when training their systems. We therefore use modest data augmentation in our system by first fine-tuning on TriviaQA (Joshi et al., 2017) befor fine-tuning on SQuAD.\n\nOur best performing system outperforms the top\nleaderboard system by +1.5 F1 in ensembling and\n+1.3 F1 as a single system. In fact, our single\nBERT model outperforms the top ensemble sys-\ntem in terms of F1 score. Without TriviaQA fine-\n\n$^{11}$QANet is described in Yu et al. (2018), but the system has improved substantially after publication.\n\nTable 2:  SQuAD 1.1 results.  The BERT ensemble is 7x systems which use different pre-training checkpoints and fine-tuning seeds.\n\nTable 3:  SQuAD 2.0 results.  We exclude entries that use BERT as one of their components.\n\ntuning data, we only lose 0.1-0.4 F1, still outper-\nforming all existing systems by a wide margin.\u00b9\u00b2"
        },
        {
            "text": "tuning data, we only lose 0.1-0.4 F1, still outper-\nforming all existing systems by a wide margin.\u00b9\u00b2",
            "page": 7,
            "x": 69,
            "y": 492,
            "width": 222,
            "height": 30,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-squad",
            "chunk_id": "0a7649c5-5cd8-4d0f-9832-02a1cfbd9dcf",
            "group_text": "## 4.2 SQuAD v1.1\n\nThe Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowd-sourced question/answer pairs (Rajpurkar et al., 2016). Given a question and a passage from\n\n\u2079The GLUE data set distribution does not include the Test labels, and we only made a single GLUE evaluation server submission for each of BERT\u1d2e\u1d2c\u02e2\u1d31 and BERT\u1d38\u1d2c\u1d3f\u1d33\u1d31.\n\u00b9\u2070https://gluebenchmark.com/leaderboard\n\nWikipedia containing the answer, the task is to predict the answer text span in the passage.\n\nAs shown in Figure 1, in the question answering task, we represent the input question and passage as a single packed sequence, with the question using the A embedding and the passage using the B embedding. We only introduce a start vector $S \\in \\mathbb{R}^H$ and an end vector $E \\in \\mathbb{R}^H$ during fine-tuning. The probability of word $i$ being the start of the answer span is computed as a dot product between $T_i$ and $S$ followed by a softmax over all of the words in the paragraph: $P_i = \\frac{e^{S \\cdot T_i}}{\\sum_j e^{S \\cdot T_j}}$.\n\nThe analogous formula is used for the end of the answer span. The score of a candidate span from position $i$ to position $j$ is defined as $S \\cdot T_i + E \\cdot T_j$, and the maximum scoring span where $j \\geq i$ is used as a prediction. The training objective is the sum of the log-likelihoods of the correct start and end positions. We fine-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32.\n\nTable 2 shows top leaderboard entries as well as results from top published systems (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018). The top results from the SQuAD leaderboard do not have up-to-date public system descriptions available,\u00b9\u00b9 and are allowed to use any public data when training their systems. We therefore use modest data augmentation in our system by first fine-tuning on TriviaQA (Joshi et al., 2017) befor fine-tuning on SQuAD.\n\nOur best performing system outperforms the top\nleaderboard system by +1.5 F1 in ensembling and\n+1.3 F1 as a single system. In fact, our single\nBERT model outperforms the top ensemble sys-\ntem in terms of F1 score. Without TriviaQA fine-\n\n$^{11}$QANet is described in Yu et al. (2018), but the system has improved substantially after publication.\n\nTable 2:  SQuAD 1.1 results.  The BERT ensemble is 7x systems which use different pre-training checkpoints and fine-tuning seeds.\n\nTable 3:  SQuAD 2.0 results.  We exclude entries that use BERT as one of their components.\n\ntuning data, we only lose 0.1-0.4 F1, still outper-\nforming all existing systems by a wide margin.\u00b9\u00b2"
        },
        {
            "text": "4.3  SQuAD v2.0\n\nThe SQuAD 2.0 task extends the SQuAD 1.1\nproblem definition by allowing for the possibility\nthat no short answer exists in the provided para-\ngraph, making the problem more realistic.",
            "page": 7,
            "x": 69,
            "y": 529,
            "width": 223,
            "height": 74,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "13-squad",
            "chunk_id": "5b2c30ba-ce31-460f-b34f-96b4abcfcca2",
            "group_text": "4.3  SQuAD v2.0\n\nThe SQuAD 2.0 task extends the SQuAD 1.1\nproblem definition by allowing for the possibility\nthat no short answer exists in the provided para-\ngraph, making the problem more realistic.\n\nWe use a simple approach to extend the SQuAD v1.1 BERT model for this task. We treat questions that do not have an answer as having an answer span with start and end at the [CLS] token. The probability space for the start and end answer span positions is extended to include the position of the [CLS] token. For prediction, we compare the score of the no-answer span: $s_{null} = S \\cdot C + E \\cdot C$ to the score of the best non-null span\n\n\u00b9\u00b2The TriviaQA data we used consists of paragraphs from TriviaQA-Wiki formed of the first 400 tokens in documents, that contain at least one of the provided possible answers.\n\n$\\hat{s}_{i,j} = \\max_{j \\geq i} S \\cdot T_i + E \\cdot T_j$. We predict a non-null answer when $\\hat{s}_{i,j} > s_{\\text{null}} + \\tau$, where the threshold $\\tau$ is selected on the dev set to maximize F1. We did not use TriviaQA data for this model. We fine-tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48.\n\nThe results compared to prior leaderboard entries and top published work (Sun et al., 2018; Wang et al., 2018b) are shown in Table 3, excluding systems that use BERT as one of their components. We observe a +5.1 F1 improvement over the previous best system."
        },
        {
            "text": "We use a simple approach to extend the SQuAD v1.1 BERT model for this task. We treat questions that do not have an answer as having an answer span with start and end at the [CLS] token. The probability space for the start and end answer span positions is extended to include the position of the [CLS] token. For prediction, we compare the score of the no-answer span: $s_{null} = S \\cdot C + E \\cdot C$ to the score of the best non-null span",
            "page": 7,
            "x": 69,
            "y": 603,
            "width": 223,
            "height": 122,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "13-squad",
            "chunk_id": "6b3f7c7d-8695-4e6d-82a8-150e1b55d490",
            "group_text": "4.3  SQuAD v2.0\n\nThe SQuAD 2.0 task extends the SQuAD 1.1\nproblem definition by allowing for the possibility\nthat no short answer exists in the provided para-\ngraph, making the problem more realistic.\n\nWe use a simple approach to extend the SQuAD v1.1 BERT model for this task. We treat questions that do not have an answer as having an answer span with start and end at the [CLS] token. The probability space for the start and end answer span positions is extended to include the position of the [CLS] token. For prediction, we compare the score of the no-answer span: $s_{null} = S \\cdot C + E \\cdot C$ to the score of the best non-null span\n\n\u00b9\u00b2The TriviaQA data we used consists of paragraphs from TriviaQA-Wiki formed of the first 400 tokens in documents, that contain at least one of the provided possible answers.\n\n$\\hat{s}_{i,j} = \\max_{j \\geq i} S \\cdot T_i + E \\cdot T_j$. We predict a non-null answer when $\\hat{s}_{i,j} > s_{\\text{null}} + \\tau$, where the threshold $\\tau$ is selected on the dev set to maximize F1. We did not use TriviaQA data for this model. We fine-tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48.\n\nThe results compared to prior leaderboard entries and top published work (Sun et al., 2018; Wang et al., 2018b) are shown in Table 3, excluding systems that use BERT as one of their components. We observe a +5.1 F1 improvement over the previous best system."
        },
        {
            "text": "\u00b9\u00b2The TriviaQA data we used consists of paragraphs from TriviaQA-Wiki formed of the first 400 tokens in documents, that contain at least one of the provided possible answers.",
            "page": 7,
            "x": 68,
            "y": 733,
            "width": 223,
            "height": 33,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "13-squad",
            "chunk_id": "58a8bd19-76e8-4e06-93fb-cc37b961a990",
            "group_text": "4.3  SQuAD v2.0\n\nThe SQuAD 2.0 task extends the SQuAD 1.1\nproblem definition by allowing for the possibility\nthat no short answer exists in the provided para-\ngraph, making the problem more realistic.\n\nWe use a simple approach to extend the SQuAD v1.1 BERT model for this task. We treat questions that do not have an answer as having an answer span with start and end at the [CLS] token. The probability space for the start and end answer span positions is extended to include the position of the [CLS] token. For prediction, we compare the score of the no-answer span: $s_{null} = S \\cdot C + E \\cdot C$ to the score of the best non-null span\n\n\u00b9\u00b2The TriviaQA data we used consists of paragraphs from TriviaQA-Wiki formed of the first 400 tokens in documents, that contain at least one of the provided possible answers.\n\n$\\hat{s}_{i,j} = \\max_{j \\geq i} S \\cdot T_i + E \\cdot T_j$. We predict a non-null answer when $\\hat{s}_{i,j} > s_{\\text{null}} + \\tau$, where the threshold $\\tau$ is selected on the dev set to maximize F1. We did not use TriviaQA data for this model. We fine-tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48.\n\nThe results compared to prior leaderboard entries and top published work (Sun et al., 2018; Wang et al., 2018b) are shown in Table 3, excluding systems that use BERT as one of their components. We observe a +5.1 F1 improvement over the previous best system."
        },
        {
            "text": "$\\hat{s}_{i,j} = \\max_{j \\geq i} S \\cdot T_i + E \\cdot T_j$. We predict a non-null answer when $\\hat{s}_{i,j} > s_{\\text{null}} + \\tau$, where the threshold $\\tau$ is selected on the dev set to maximize F1. We did not use TriviaQA data for this model. We fine-tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48.",
            "page": 7,
            "x": 304,
            "y": 240,
            "width": 223,
            "height": 80,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "13-squad",
            "chunk_id": "2fefe0e8-3a61-4412-b281-ed763bf37948",
            "group_text": "4.3  SQuAD v2.0\n\nThe SQuAD 2.0 task extends the SQuAD 1.1\nproblem definition by allowing for the possibility\nthat no short answer exists in the provided para-\ngraph, making the problem more realistic.\n\nWe use a simple approach to extend the SQuAD v1.1 BERT model for this task. We treat questions that do not have an answer as having an answer span with start and end at the [CLS] token. The probability space for the start and end answer span positions is extended to include the position of the [CLS] token. For prediction, we compare the score of the no-answer span: $s_{null} = S \\cdot C + E \\cdot C$ to the score of the best non-null span\n\n\u00b9\u00b2The TriviaQA data we used consists of paragraphs from TriviaQA-Wiki formed of the first 400 tokens in documents, that contain at least one of the provided possible answers.\n\n$\\hat{s}_{i,j} = \\max_{j \\geq i} S \\cdot T_i + E \\cdot T_j$. We predict a non-null answer when $\\hat{s}_{i,j} > s_{\\text{null}} + \\tau$, where the threshold $\\tau$ is selected on the dev set to maximize F1. We did not use TriviaQA data for this model. We fine-tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48.\n\nThe results compared to prior leaderboard entries and top published work (Sun et al., 2018; Wang et al., 2018b) are shown in Table 3, excluding systems that use BERT as one of their components. We observe a +5.1 F1 improvement over the previous best system."
        },
        {
            "text": "The results compared to prior leaderboard entries and top published work (Sun et al., 2018; Wang et al., 2018b) are shown in Table 3, excluding systems that use BERT as one of their components. We observe a +5.1 F1 improvement over the previous best system.",
            "page": 7,
            "x": 304,
            "y": 322,
            "width": 224,
            "height": 82,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "13-squad",
            "chunk_id": "5aa1aac5-5699-4aef-a27b-7cc44f4b7767",
            "group_text": "4.3  SQuAD v2.0\n\nThe SQuAD 2.0 task extends the SQuAD 1.1\nproblem definition by allowing for the possibility\nthat no short answer exists in the provided para-\ngraph, making the problem more realistic.\n\nWe use a simple approach to extend the SQuAD v1.1 BERT model for this task. We treat questions that do not have an answer as having an answer span with start and end at the [CLS] token. The probability space for the start and end answer span positions is extended to include the position of the [CLS] token. For prediction, we compare the score of the no-answer span: $s_{null} = S \\cdot C + E \\cdot C$ to the score of the best non-null span\n\n\u00b9\u00b2The TriviaQA data we used consists of paragraphs from TriviaQA-Wiki formed of the first 400 tokens in documents, that contain at least one of the provided possible answers.\n\n$\\hat{s}_{i,j} = \\max_{j \\geq i} S \\cdot T_i + E \\cdot T_j$. We predict a non-null answer when $\\hat{s}_{i,j} > s_{\\text{null}} + \\tau$, where the threshold $\\tau$ is selected on the dev set to maximize F1. We did not use TriviaQA data for this model. We fine-tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48.\n\nThe results compared to prior leaderboard entries and top published work (Sun et al., 2018; Wang et al., 2018b) are shown in Table 3, excluding systems that use BERT as one of their components. We observe a +5.1 F1 improvement over the previous best system."
        },
        {
            "text": "4.4 SWAG\n\nThe Situations With Adversarial Generations (SWAG) dataset contains 113k sentence-pair completion examples that evaluate grounded commonsense inference (Zellers et al., 2018). Given a sentence, the task is to choose the most plausible continuation among four choices.",
            "page": 7,
            "x": 304,
            "y": 413,
            "width": 223,
            "height": 100,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "14-swag",
            "chunk_id": "719d284b-79f6-468f-b03e-71a02519a91f",
            "group_text": "4.4 SWAG\n\nThe Situations With Adversarial Generations (SWAG) dataset contains 113k sentence-pair completion examples that evaluate grounded commonsense inference (Zellers et al., 2018). Given a sentence, the task is to choose the most plausible continuation among four choices.\n\nWhen fine-tuning on the SWAG dataset, we\nconstruct four input sequences, each containing\nthe concatenation of the given sentence (sentence\nA) and a possible continuation (sentence B). The\nonly task-specific parameters introduced is a vec-\ntor whose dot product with the [CLS] token rep-\nresentation $C$ denotes a score for each choice\nwhich is normalized with a softmax layer.\n\nWe fine-tune the model for 3 epochs with a learning rate of 2e-5 and a batch size of 16. Results are presented in Table 4. BERT\\textsubscript{LARGE} outperforms the authors\u2019 baseline ESIM+ELMo system by +27.1% and OpenAI GPT by 8.3%."
        },
        {
            "text": "When fine-tuning on the SWAG dataset, we\nconstruct four input sequences, each containing\nthe concatenation of the given sentence (sentence\nA) and a possible continuation (sentence B). The\nonly task-specific parameters introduced is a vec-\ntor whose dot product with the [CLS] token rep-\nresentation $C$ denotes a score for each choice\nwhich is normalized with a softmax layer.",
            "page": 7,
            "x": 305,
            "y": 515,
            "width": 223,
            "height": 107,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "14-swag",
            "chunk_id": "2f8f1e63-b578-4d0b-a7ec-0a5a740174c0",
            "group_text": "4.4 SWAG\n\nThe Situations With Adversarial Generations (SWAG) dataset contains 113k sentence-pair completion examples that evaluate grounded commonsense inference (Zellers et al., 2018). Given a sentence, the task is to choose the most plausible continuation among four choices.\n\nWhen fine-tuning on the SWAG dataset, we\nconstruct four input sequences, each containing\nthe concatenation of the given sentence (sentence\nA) and a possible continuation (sentence B). The\nonly task-specific parameters introduced is a vec-\ntor whose dot product with the [CLS] token rep-\nresentation $C$ denotes a score for each choice\nwhich is normalized with a softmax layer.\n\nWe fine-tune the model for 3 epochs with a learning rate of 2e-5 and a batch size of 16. Results are presented in Table 4. BERT\\textsubscript{LARGE} outperforms the authors\u2019 baseline ESIM+ELMo system by +27.1% and OpenAI GPT by 8.3%."
        },
        {
            "text": "We fine-tune the model for 3 epochs with a learning rate of 2e-5 and a batch size of 16. Results are presented in Table 4. BERT\\textsubscript{LARGE} outperforms the authors\u2019 baseline ESIM+ELMo system by +27.1% and OpenAI GPT by 8.3%.",
            "page": 7,
            "x": 305,
            "y": 624,
            "width": 223,
            "height": 67,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "14-swag",
            "chunk_id": "3d76095a-445d-4f6c-92ea-8b0391ec1ace",
            "group_text": "4.4 SWAG\n\nThe Situations With Adversarial Generations (SWAG) dataset contains 113k sentence-pair completion examples that evaluate grounded commonsense inference (Zellers et al., 2018). Given a sentence, the task is to choose the most plausible continuation among four choices.\n\nWhen fine-tuning on the SWAG dataset, we\nconstruct four input sequences, each containing\nthe concatenation of the given sentence (sentence\nA) and a possible continuation (sentence B). The\nonly task-specific parameters introduced is a vec-\ntor whose dot product with the [CLS] token rep-\nresentation $C$ denotes a score for each choice\nwhich is normalized with a softmax layer.\n\nWe fine-tune the model for 3 epochs with a learning rate of 2e-5 and a batch size of 16. Results are presented in Table 4. BERT\\textsubscript{LARGE} outperforms the authors\u2019 baseline ESIM+ELMo system by +27.1% and OpenAI GPT by 8.3%."
        },
        {
            "text": "## 5 Ablation Studies\n\nIn this section, we perform ablation experiments over a number of facets of BERT in order to better understand their relative importance. Additional",
            "page": 7,
            "x": 304,
            "y": 702,
            "width": 223,
            "height": 65,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "15-ablation",
            "chunk_id": "5716e522-4f24-4e3c-86ae-10603b1343d5",
            "group_text": "## 5 Ablation Studies\n\nIn this section, we perform ablation experiments over a number of facets of BERT in order to better understand their relative importance. Additional\n\nTable 5: Ablation over the pre-training tasks using the BERT\\textsubscript{BASE} architecture. \u201cNo NSP\u201d is trained without the next sentence prediction task. \u201cLTR & No NSP\u201d is trained as a left-to-right LM without the next sentence prediction, like OpenAI GPT. \u201c+ BiLSTM\u201d adds a randomly initialized BiLSTM on top of the \u201cLTR + No NSP\u201d model during fine-tuning.\n\nablation studies can be found in Appendix C."
        },
        {
            "text": "Table 5: Ablation over the pre-training tasks using the BERT\\textsubscript{BASE} architecture. \u201cNo NSP\u201d is trained without the next sentence prediction task. \u201cLTR & No NSP\u201d is trained as a left-to-right LM without the next sentence prediction, like OpenAI GPT. \u201c+ BiLSTM\u201d adds a randomly initialized BiLSTM on top of the \u201cLTR + No NSP\u201d model during fine-tuning.",
            "page": 8,
            "x": 67,
            "y": 152,
            "width": 227,
            "height": 87,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "15-ablation",
            "chunk_id": "fbb7f97d-d8a6-48c5-acbd-fc26dddadf4c",
            "group_text": "## 5 Ablation Studies\n\nIn this section, we perform ablation experiments over a number of facets of BERT in order to better understand their relative importance. Additional\n\nTable 5: Ablation over the pre-training tasks using the BERT\\textsubscript{BASE} architecture. \u201cNo NSP\u201d is trained without the next sentence prediction task. \u201cLTR & No NSP\u201d is trained as a left-to-right LM without the next sentence prediction, like OpenAI GPT. \u201c+ BiLSTM\u201d adds a randomly initialized BiLSTM on top of the \u201cLTR + No NSP\u201d model during fine-tuning.\n\nablation studies can be found in Appendix C."
        },
        {
            "text": "ablation studies can be found in Appendix C.",
            "page": 8,
            "x": 68,
            "y": 260,
            "width": 204,
            "height": 17,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "15-ablation",
            "chunk_id": "bdf583dc-2d51-4ddc-9d0a-6340d98f1ffb",
            "group_text": "## 5 Ablation Studies\n\nIn this section, we perform ablation experiments over a number of facets of BERT in order to better understand their relative importance. Additional\n\nTable 5: Ablation over the pre-training tasks using the BERT\\textsubscript{BASE} architecture. \u201cNo NSP\u201d is trained without the next sentence prediction task. \u201cLTR & No NSP\u201d is trained as a left-to-right LM without the next sentence prediction, like OpenAI GPT. \u201c+ BiLSTM\u201d adds a randomly initialized BiLSTM on top of the \u201cLTR + No NSP\u201d model during fine-tuning.\n\nablation studies can be found in Appendix C."
        },
        {
            "text": "## 5.1  Effect of Pre-training Tasks\n\nWe demonstrate the importance of the deep bidirectionality of BERT by evaluating two pre-training objectives using exactly the same pre-training data, fine-tuning scheme, and hyperparameters as BERT$_{BASE}$:",
            "page": 8,
            "x": 68,
            "y": 286,
            "width": 226,
            "height": 91,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "16-effect",
            "chunk_id": "b80317a2-0cca-4b38-997a-e8f3833645dd",
            "group_text": "## 5.1  Effect of Pre-training Tasks\n\nWe demonstrate the importance of the deep bidirectionality of BERT by evaluating two pre-training objectives using exactly the same pre-training data, fine-tuning scheme, and hyperparameters as BERT$_{BASE}$:\n\nNo NSP: A bidirectional model which is trained using the \u201cmasked LM\u201d (MLM) but without the \u201cnext sentence prediction\u201d (NSP) task.\n\n**LTR & No NSP**: A left-context-only model which is trained using a standard Left-to-Right (LTR) LM, rather than an MLM. The left-only constraint was also applied at fine-tuning, because removing it introduced a pre-train/fine-tune mismatch that degraded downstream performance. Additionally, this model was pre-trained without the NSP task. This is directly comparable to OpenAI GPT, but using our larger training dataset, our input representation, and our fine-tuning scheme.\n\nWe first examine the impact brought by the NSP task. In Table 5, we show that removing NSP hurts performance significantly on QNLI, MNLI, and SQuAD 1.1. Next, we evaluate the impact of training bidirectional representations by comparing \u201cNo NSP\u201d to \u201cLTR & No NSP\u201d. The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD.\n\nFor SQuAD it is intuitively clear that a LTR\nmodel will perform poorly at token predictions,\nsince the token-level hidden states have no right-\nside context. In order to make a good faith at-\ntempt at strengthening the LTR system, we added\na randomly initialized BiLSTM on top. This does\nsignificantly improve results on SQuAD, but the\n\nresults are still far worse than those of the pre-\ntrained bidirectional models. The BiLSTM hurts\nperformance on the GLUE tasks.\n\nWe recognize that it would also be possible to\ntrain separate LTR and RTL models and represent\neach token as the concatenation of the two mod-\nels, as ELMo does. However: (a) this is twice as\nexpensive as a single bidirectional model; (b) this\nis non-intuitive for tasks like QA, since the RTL\nmodel would not be able to condition the answer\non the question; (c) this it is strictly less powerful\nthan a deep bidirectional model, since it can use\nboth left and right context at every layer."
        },
        {
            "text": "No NSP: A bidirectional model which is trained using the \u201cmasked LM\u201d (MLM) but without the \u201cnext sentence prediction\u201d (NSP) task.",
            "page": 8,
            "x": 68,
            "y": 383,
            "width": 224,
            "height": 41,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "16-effect",
            "chunk_id": "a362e553-dfff-4551-80d4-dd42842d6535",
            "group_text": "## 5.1  Effect of Pre-training Tasks\n\nWe demonstrate the importance of the deep bidirectionality of BERT by evaluating two pre-training objectives using exactly the same pre-training data, fine-tuning scheme, and hyperparameters as BERT$_{BASE}$:\n\nNo NSP: A bidirectional model which is trained using the \u201cmasked LM\u201d (MLM) but without the \u201cnext sentence prediction\u201d (NSP) task.\n\n**LTR & No NSP**: A left-context-only model which is trained using a standard Left-to-Right (LTR) LM, rather than an MLM. The left-only constraint was also applied at fine-tuning, because removing it introduced a pre-train/fine-tune mismatch that degraded downstream performance. Additionally, this model was pre-trained without the NSP task. This is directly comparable to OpenAI GPT, but using our larger training dataset, our input representation, and our fine-tuning scheme.\n\nWe first examine the impact brought by the NSP task. In Table 5, we show that removing NSP hurts performance significantly on QNLI, MNLI, and SQuAD 1.1. Next, we evaluate the impact of training bidirectional representations by comparing \u201cNo NSP\u201d to \u201cLTR & No NSP\u201d. The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD.\n\nFor SQuAD it is intuitively clear that a LTR\nmodel will perform poorly at token predictions,\nsince the token-level hidden states have no right-\nside context. In order to make a good faith at-\ntempt at strengthening the LTR system, we added\na randomly initialized BiLSTM on top. This does\nsignificantly improve results on SQuAD, but the\n\nresults are still far worse than those of the pre-\ntrained bidirectional models. The BiLSTM hurts\nperformance on the GLUE tasks.\n\nWe recognize that it would also be possible to\ntrain separate LTR and RTL models and represent\neach token as the concatenation of the two mod-\nels, as ELMo does. However: (a) this is twice as\nexpensive as a single bidirectional model; (b) this\nis non-intuitive for tasks like QA, since the RTL\nmodel would not be able to condition the answer\non the question; (c) this it is strictly less powerful\nthan a deep bidirectional model, since it can use\nboth left and right context at every layer."
        },
        {
            "text": "**LTR & No NSP**: A left-context-only model which is trained using a standard Left-to-Right (LTR) LM, rather than an MLM. The left-only constraint was also applied at fine-tuning, because removing it introduced a pre-train/fine-tune mismatch that degraded downstream performance. Additionally, this model was pre-trained without the NSP task. This is directly comparable to OpenAI GPT, but using our larger training dataset, our input representation, and our fine-tuning scheme.",
            "page": 8,
            "x": 68,
            "y": 425,
            "width": 225,
            "height": 135,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "16-effect",
            "chunk_id": "abe12da6-3522-4ae5-a5ef-a8f8a95a60e6",
            "group_text": "## 5.1  Effect of Pre-training Tasks\n\nWe demonstrate the importance of the deep bidirectionality of BERT by evaluating two pre-training objectives using exactly the same pre-training data, fine-tuning scheme, and hyperparameters as BERT$_{BASE}$:\n\nNo NSP: A bidirectional model which is trained using the \u201cmasked LM\u201d (MLM) but without the \u201cnext sentence prediction\u201d (NSP) task.\n\n**LTR & No NSP**: A left-context-only model which is trained using a standard Left-to-Right (LTR) LM, rather than an MLM. The left-only constraint was also applied at fine-tuning, because removing it introduced a pre-train/fine-tune mismatch that degraded downstream performance. Additionally, this model was pre-trained without the NSP task. This is directly comparable to OpenAI GPT, but using our larger training dataset, our input representation, and our fine-tuning scheme.\n\nWe first examine the impact brought by the NSP task. In Table 5, we show that removing NSP hurts performance significantly on QNLI, MNLI, and SQuAD 1.1. Next, we evaluate the impact of training bidirectional representations by comparing \u201cNo NSP\u201d to \u201cLTR & No NSP\u201d. The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD.\n\nFor SQuAD it is intuitively clear that a LTR\nmodel will perform poorly at token predictions,\nsince the token-level hidden states have no right-\nside context. In order to make a good faith at-\ntempt at strengthening the LTR system, we added\na randomly initialized BiLSTM on top. This does\nsignificantly improve results on SQuAD, but the\n\nresults are still far worse than those of the pre-\ntrained bidirectional models. The BiLSTM hurts\nperformance on the GLUE tasks.\n\nWe recognize that it would also be possible to\ntrain separate LTR and RTL models and represent\neach token as the concatenation of the two mod-\nels, as ELMo does. However: (a) this is twice as\nexpensive as a single bidirectional model; (b) this\nis non-intuitive for tasks like QA, since the RTL\nmodel would not be able to condition the answer\non the question; (c) this it is strictly less powerful\nthan a deep bidirectional model, since it can use\nboth left and right context at every layer."
        },
        {
            "text": "We first examine the impact brought by the NSP task. In Table 5, we show that removing NSP hurts performance significantly on QNLI, MNLI, and SQuAD 1.1. Next, we evaluate the impact of training bidirectional representations by comparing \u201cNo NSP\u201d to \u201cLTR & No NSP\u201d. The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD.",
            "page": 8,
            "x": 68,
            "y": 561,
            "width": 226,
            "height": 108,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "16-effect",
            "chunk_id": "9a11e058-afd4-4766-83df-c60dc2db20e8",
            "group_text": "## 5.1  Effect of Pre-training Tasks\n\nWe demonstrate the importance of the deep bidirectionality of BERT by evaluating two pre-training objectives using exactly the same pre-training data, fine-tuning scheme, and hyperparameters as BERT$_{BASE}$:\n\nNo NSP: A bidirectional model which is trained using the \u201cmasked LM\u201d (MLM) but without the \u201cnext sentence prediction\u201d (NSP) task.\n\n**LTR & No NSP**: A left-context-only model which is trained using a standard Left-to-Right (LTR) LM, rather than an MLM. The left-only constraint was also applied at fine-tuning, because removing it introduced a pre-train/fine-tune mismatch that degraded downstream performance. Additionally, this model was pre-trained without the NSP task. This is directly comparable to OpenAI GPT, but using our larger training dataset, our input representation, and our fine-tuning scheme.\n\nWe first examine the impact brought by the NSP task. In Table 5, we show that removing NSP hurts performance significantly on QNLI, MNLI, and SQuAD 1.1. Next, we evaluate the impact of training bidirectional representations by comparing \u201cNo NSP\u201d to \u201cLTR & No NSP\u201d. The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD.\n\nFor SQuAD it is intuitively clear that a LTR\nmodel will perform poorly at token predictions,\nsince the token-level hidden states have no right-\nside context. In order to make a good faith at-\ntempt at strengthening the LTR system, we added\na randomly initialized BiLSTM on top. This does\nsignificantly improve results on SQuAD, but the\n\nresults are still far worse than those of the pre-\ntrained bidirectional models. The BiLSTM hurts\nperformance on the GLUE tasks.\n\nWe recognize that it would also be possible to\ntrain separate LTR and RTL models and represent\neach token as the concatenation of the two mod-\nels, as ELMo does. However: (a) this is twice as\nexpensive as a single bidirectional model; (b) this\nis non-intuitive for tasks like QA, since the RTL\nmodel would not be able to condition the answer\non the question; (c) this it is strictly less powerful\nthan a deep bidirectional model, since it can use\nboth left and right context at every layer."
        },
        {
            "text": "For SQuAD it is intuitively clear that a LTR\nmodel will perform poorly at token predictions,\nsince the token-level hidden states have no right-\nside context. In order to make a good faith at-\ntempt at strengthening the LTR system, we added\na randomly initialized BiLSTM on top. This does\nsignificantly improve results on SQuAD, but the",
            "page": 8,
            "x": 68,
            "y": 671,
            "width": 225,
            "height": 97,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "16-effect",
            "chunk_id": "f841f4c2-36ae-411f-8220-8a2a6cf82759",
            "group_text": "## 5.1  Effect of Pre-training Tasks\n\nWe demonstrate the importance of the deep bidirectionality of BERT by evaluating two pre-training objectives using exactly the same pre-training data, fine-tuning scheme, and hyperparameters as BERT$_{BASE}$:\n\nNo NSP: A bidirectional model which is trained using the \u201cmasked LM\u201d (MLM) but without the \u201cnext sentence prediction\u201d (NSP) task.\n\n**LTR & No NSP**: A left-context-only model which is trained using a standard Left-to-Right (LTR) LM, rather than an MLM. The left-only constraint was also applied at fine-tuning, because removing it introduced a pre-train/fine-tune mismatch that degraded downstream performance. Additionally, this model was pre-trained without the NSP task. This is directly comparable to OpenAI GPT, but using our larger training dataset, our input representation, and our fine-tuning scheme.\n\nWe first examine the impact brought by the NSP task. In Table 5, we show that removing NSP hurts performance significantly on QNLI, MNLI, and SQuAD 1.1. Next, we evaluate the impact of training bidirectional representations by comparing \u201cNo NSP\u201d to \u201cLTR & No NSP\u201d. The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD.\n\nFor SQuAD it is intuitively clear that a LTR\nmodel will perform poorly at token predictions,\nsince the token-level hidden states have no right-\nside context. In order to make a good faith at-\ntempt at strengthening the LTR system, we added\na randomly initialized BiLSTM on top. This does\nsignificantly improve results on SQuAD, but the\n\nresults are still far worse than those of the pre-\ntrained bidirectional models. The BiLSTM hurts\nperformance on the GLUE tasks.\n\nWe recognize that it would also be possible to\ntrain separate LTR and RTL models and represent\neach token as the concatenation of the two mod-\nels, as ELMo does. However: (a) this is twice as\nexpensive as a single bidirectional model; (b) this\nis non-intuitive for tasks like QA, since the RTL\nmodel would not be able to condition the answer\non the question; (c) this it is strictly less powerful\nthan a deep bidirectional model, since it can use\nboth left and right context at every layer."
        },
        {
            "text": "results are still far worse than those of the pre-\ntrained bidirectional models. The BiLSTM hurts\nperformance on the GLUE tasks.",
            "page": 8,
            "x": 303,
            "y": 63,
            "width": 225,
            "height": 41,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "16-effect",
            "chunk_id": "aa4cb0b2-5ce5-4e59-91c9-3d76f289240d",
            "group_text": "## 5.1  Effect of Pre-training Tasks\n\nWe demonstrate the importance of the deep bidirectionality of BERT by evaluating two pre-training objectives using exactly the same pre-training data, fine-tuning scheme, and hyperparameters as BERT$_{BASE}$:\n\nNo NSP: A bidirectional model which is trained using the \u201cmasked LM\u201d (MLM) but without the \u201cnext sentence prediction\u201d (NSP) task.\n\n**LTR & No NSP**: A left-context-only model which is trained using a standard Left-to-Right (LTR) LM, rather than an MLM. The left-only constraint was also applied at fine-tuning, because removing it introduced a pre-train/fine-tune mismatch that degraded downstream performance. Additionally, this model was pre-trained without the NSP task. This is directly comparable to OpenAI GPT, but using our larger training dataset, our input representation, and our fine-tuning scheme.\n\nWe first examine the impact brought by the NSP task. In Table 5, we show that removing NSP hurts performance significantly on QNLI, MNLI, and SQuAD 1.1. Next, we evaluate the impact of training bidirectional representations by comparing \u201cNo NSP\u201d to \u201cLTR & No NSP\u201d. The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD.\n\nFor SQuAD it is intuitively clear that a LTR\nmodel will perform poorly at token predictions,\nsince the token-level hidden states have no right-\nside context. In order to make a good faith at-\ntempt at strengthening the LTR system, we added\na randomly initialized BiLSTM on top. This does\nsignificantly improve results on SQuAD, but the\n\nresults are still far worse than those of the pre-\ntrained bidirectional models. The BiLSTM hurts\nperformance on the GLUE tasks.\n\nWe recognize that it would also be possible to\ntrain separate LTR and RTL models and represent\neach token as the concatenation of the two mod-\nels, as ELMo does. However: (a) this is twice as\nexpensive as a single bidirectional model; (b) this\nis non-intuitive for tasks like QA, since the RTL\nmodel would not be able to condition the answer\non the question; (c) this it is strictly less powerful\nthan a deep bidirectional model, since it can use\nboth left and right context at every layer."
        },
        {
            "text": "We recognize that it would also be possible to\ntrain separate LTR and RTL models and represent\neach token as the concatenation of the two mod-\nels, as ELMo does. However: (a) this is twice as\nexpensive as a single bidirectional model; (b) this\nis non-intuitive for tasks like QA, since the RTL\nmodel would not be able to condition the answer\non the question; (c) this it is strictly less powerful\nthan a deep bidirectional model, since it can use\nboth left and right context at every layer.",
            "page": 8,
            "x": 303,
            "y": 106,
            "width": 226,
            "height": 137,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "16-effect",
            "chunk_id": "5d3d87d4-97d7-4858-b8b6-b831e11515d7",
            "group_text": "## 5.1  Effect of Pre-training Tasks\n\nWe demonstrate the importance of the deep bidirectionality of BERT by evaluating two pre-training objectives using exactly the same pre-training data, fine-tuning scheme, and hyperparameters as BERT$_{BASE}$:\n\nNo NSP: A bidirectional model which is trained using the \u201cmasked LM\u201d (MLM) but without the \u201cnext sentence prediction\u201d (NSP) task.\n\n**LTR & No NSP**: A left-context-only model which is trained using a standard Left-to-Right (LTR) LM, rather than an MLM. The left-only constraint was also applied at fine-tuning, because removing it introduced a pre-train/fine-tune mismatch that degraded downstream performance. Additionally, this model was pre-trained without the NSP task. This is directly comparable to OpenAI GPT, but using our larger training dataset, our input representation, and our fine-tuning scheme.\n\nWe first examine the impact brought by the NSP task. In Table 5, we show that removing NSP hurts performance significantly on QNLI, MNLI, and SQuAD 1.1. Next, we evaluate the impact of training bidirectional representations by comparing \u201cNo NSP\u201d to \u201cLTR & No NSP\u201d. The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD.\n\nFor SQuAD it is intuitively clear that a LTR\nmodel will perform poorly at token predictions,\nsince the token-level hidden states have no right-\nside context. In order to make a good faith at-\ntempt at strengthening the LTR system, we added\na randomly initialized BiLSTM on top. This does\nsignificantly improve results on SQuAD, but the\n\nresults are still far worse than those of the pre-\ntrained bidirectional models. The BiLSTM hurts\nperformance on the GLUE tasks.\n\nWe recognize that it would also be possible to\ntrain separate LTR and RTL models and represent\neach token as the concatenation of the two mod-\nels, as ELMo does. However: (a) this is twice as\nexpensive as a single bidirectional model; (b) this\nis non-intuitive for tasks like QA, since the RTL\nmodel would not be able to condition the answer\non the question; (c) this it is strictly less powerful\nthan a deep bidirectional model, since it can use\nboth left and right context at every layer."
        },
        {
            "text": "### 5.2  Effect of Model Size\n\nIn this section, we explore the effect of model size on fine-tuning task accuracy. We trained a number of BERT models with a differing number of layers, hidden units, and attention heads, while otherwise using the same hyperparameters and training procedure as described previously.",
            "page": 8,
            "x": 303,
            "y": 253,
            "width": 225,
            "height": 103,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "17-effect",
            "chunk_id": "5695d519-1946-4dd2-a427-8d62e4c9fda5",
            "group_text": "### 5.2  Effect of Model Size\n\nIn this section, we explore the effect of model size on fine-tuning task accuracy. We trained a number of BERT models with a differing number of layers, hidden units, and attention heads, while otherwise using the same hyperparameters and training procedure as described previously.\n\nResults on selected GLUE tasks are shown in Table 6. In this table, we report the average Dev Set accuracy from 5 random restarts of fine-tuning. We can see that larger models lead to a strict accuracy improvement across all four datasets, even for MRPC which only has 3,600 labeled training examples, and is substantially different from the pre-training tasks. It is also perhaps surprising that we are able to achieve such significant improvements on top of models which are already quite large relative to the existing literature. For example, the largest Transformer explored in Vaswani et al. (2017) is (L=6, H=1024, A=16) with 100M parameters for the encoder, and the largest Transformer we have found in the literature is (L=64, H=512, A=2) with 235M parameters (Al-Rfou et al., 2018). By contrast, BERT_BASE contains 110M parameters and BERT_LARGE contains 340M parameters.\n\nIt has long been known that increasing the model size will lead to continual improvements on large-scale tasks such as machine translation and language modeling, which is demonstrated by the LM perplexity of held-out training data shown in Table 6. However, we believe that this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been sufficiently pre-trained. Peters et al. (2018b) presented\n\nmixed results on the downstream task impact of\nincreasing the pre-trained bi-LM size from two\nto four layers and Melamud et al. (2016) men-\ntioned in passing that increasing hidden dimen-\nsion size from 200 to 600 helped, but increasing\nfurther to 1,000 did not bring further improve-\nments. Both of these prior works used a feature-\nbased approach \u2014 we hypothesize that when the\nmodel is fine-tuned directly on the downstream\ntasks and uses only a very small number of ran-\ndomly initialized additional parameters, the task-\nspecific models can benefit from the larger, more\nexpressive pre-trained representations even when\ndownstream task data is very small."
        },
        {
            "text": "Results on selected GLUE tasks are shown in Table 6. In this table, we report the average Dev Set accuracy from 5 random restarts of fine-tuning. We can see that larger models lead to a strict accuracy improvement across all four datasets, even for MRPC which only has 3,600 labeled training examples, and is substantially different from the pre-training tasks. It is also perhaps surprising that we are able to achieve such significant improvements on top of models which are already quite large relative to the existing literature. For example, the largest Transformer explored in Vaswani et al. (2017) is (L=6, H=1024, A=16) with 100M parameters for the encoder, and the largest Transformer we have found in the literature is (L=64, H=512, A=2) with 235M parameters (Al-Rfou et al., 2018). By contrast, BERT_BASE contains 110M parameters and BERT_LARGE contains 340M parameters.",
            "page": 8,
            "x": 303,
            "y": 358,
            "width": 226,
            "height": 257,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "17-effect",
            "chunk_id": "0650e308-335c-41c8-acb2-78a568e87a61",
            "group_text": "### 5.2  Effect of Model Size\n\nIn this section, we explore the effect of model size on fine-tuning task accuracy. We trained a number of BERT models with a differing number of layers, hidden units, and attention heads, while otherwise using the same hyperparameters and training procedure as described previously.\n\nResults on selected GLUE tasks are shown in Table 6. In this table, we report the average Dev Set accuracy from 5 random restarts of fine-tuning. We can see that larger models lead to a strict accuracy improvement across all four datasets, even for MRPC which only has 3,600 labeled training examples, and is substantially different from the pre-training tasks. It is also perhaps surprising that we are able to achieve such significant improvements on top of models which are already quite large relative to the existing literature. For example, the largest Transformer explored in Vaswani et al. (2017) is (L=6, H=1024, A=16) with 100M parameters for the encoder, and the largest Transformer we have found in the literature is (L=64, H=512, A=2) with 235M parameters (Al-Rfou et al., 2018). By contrast, BERT_BASE contains 110M parameters and BERT_LARGE contains 340M parameters.\n\nIt has long been known that increasing the model size will lead to continual improvements on large-scale tasks such as machine translation and language modeling, which is demonstrated by the LM perplexity of held-out training data shown in Table 6. However, we believe that this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been sufficiently pre-trained. Peters et al. (2018b) presented\n\nmixed results on the downstream task impact of\nincreasing the pre-trained bi-LM size from two\nto four layers and Melamud et al. (2016) men-\ntioned in passing that increasing hidden dimen-\nsion size from 200 to 600 helped, but increasing\nfurther to 1,000 did not bring further improve-\nments. Both of these prior works used a feature-\nbased approach \u2014 we hypothesize that when the\nmodel is fine-tuned directly on the downstream\ntasks and uses only a very small number of ran-\ndomly initialized additional parameters, the task-\nspecific models can benefit from the larger, more\nexpressive pre-trained representations even when\ndownstream task data is very small."
        },
        {
            "text": "It has long been known that increasing the model size will lead to continual improvements on large-scale tasks such as machine translation and language modeling, which is demonstrated by the LM perplexity of held-out training data shown in Table 6. However, we believe that this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been sufficiently pre-trained. Peters et al. (2018b) presented",
            "page": 8,
            "x": 304,
            "y": 616,
            "width": 226,
            "height": 151,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "17-effect",
            "chunk_id": "8d2b162c-f96f-416f-91b5-c1671dd02d6b",
            "group_text": "### 5.2  Effect of Model Size\n\nIn this section, we explore the effect of model size on fine-tuning task accuracy. We trained a number of BERT models with a differing number of layers, hidden units, and attention heads, while otherwise using the same hyperparameters and training procedure as described previously.\n\nResults on selected GLUE tasks are shown in Table 6. In this table, we report the average Dev Set accuracy from 5 random restarts of fine-tuning. We can see that larger models lead to a strict accuracy improvement across all four datasets, even for MRPC which only has 3,600 labeled training examples, and is substantially different from the pre-training tasks. It is also perhaps surprising that we are able to achieve such significant improvements on top of models which are already quite large relative to the existing literature. For example, the largest Transformer explored in Vaswani et al. (2017) is (L=6, H=1024, A=16) with 100M parameters for the encoder, and the largest Transformer we have found in the literature is (L=64, H=512, A=2) with 235M parameters (Al-Rfou et al., 2018). By contrast, BERT_BASE contains 110M parameters and BERT_LARGE contains 340M parameters.\n\nIt has long been known that increasing the model size will lead to continual improvements on large-scale tasks such as machine translation and language modeling, which is demonstrated by the LM perplexity of held-out training data shown in Table 6. However, we believe that this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been sufficiently pre-trained. Peters et al. (2018b) presented\n\nmixed results on the downstream task impact of\nincreasing the pre-trained bi-LM size from two\nto four layers and Melamud et al. (2016) men-\ntioned in passing that increasing hidden dimen-\nsion size from 200 to 600 helped, but increasing\nfurther to 1,000 did not bring further improve-\nments. Both of these prior works used a feature-\nbased approach \u2014 we hypothesize that when the\nmodel is fine-tuned directly on the downstream\ntasks and uses only a very small number of ran-\ndomly initialized additional parameters, the task-\nspecific models can benefit from the larger, more\nexpressive pre-trained representations even when\ndownstream task data is very small."
        },
        {
            "text": "mixed results on the downstream task impact of\nincreasing the pre-trained bi-LM size from two\nto four layers and Melamud et al. (2016) men-\ntioned in passing that increasing hidden dimen-\nsion size from 200 to 600 helped, but increasing\nfurther to 1,000 did not bring further improve-\nments. Both of these prior works used a feature-\nbased approach \u2014 we hypothesize that when the\nmodel is fine-tuned directly on the downstream\ntasks and uses only a very small number of ran-\ndomly initialized additional parameters, the task-\nspecific models can benefit from the larger, more\nexpressive pre-trained representations even when\ndownstream task data is very small.",
            "page": 9,
            "x": 67,
            "y": 61,
            "width": 226,
            "height": 192,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "17-effect",
            "chunk_id": "a3bd6575-efc6-44ff-b936-4dd5db804d30",
            "group_text": "### 5.2  Effect of Model Size\n\nIn this section, we explore the effect of model size on fine-tuning task accuracy. We trained a number of BERT models with a differing number of layers, hidden units, and attention heads, while otherwise using the same hyperparameters and training procedure as described previously.\n\nResults on selected GLUE tasks are shown in Table 6. In this table, we report the average Dev Set accuracy from 5 random restarts of fine-tuning. We can see that larger models lead to a strict accuracy improvement across all four datasets, even for MRPC which only has 3,600 labeled training examples, and is substantially different from the pre-training tasks. It is also perhaps surprising that we are able to achieve such significant improvements on top of models which are already quite large relative to the existing literature. For example, the largest Transformer explored in Vaswani et al. (2017) is (L=6, H=1024, A=16) with 100M parameters for the encoder, and the largest Transformer we have found in the literature is (L=64, H=512, A=2) with 235M parameters (Al-Rfou et al., 2018). By contrast, BERT_BASE contains 110M parameters and BERT_LARGE contains 340M parameters.\n\nIt has long been known that increasing the model size will lead to continual improvements on large-scale tasks such as machine translation and language modeling, which is demonstrated by the LM perplexity of held-out training data shown in Table 6. However, we believe that this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been sufficiently pre-trained. Peters et al. (2018b) presented\n\nmixed results on the downstream task impact of\nincreasing the pre-trained bi-LM size from two\nto four layers and Melamud et al. (2016) men-\ntioned in passing that increasing hidden dimen-\nsion size from 200 to 600 helped, but increasing\nfurther to 1,000 did not bring further improve-\nments. Both of these prior works used a feature-\nbased approach \u2014 we hypothesize that when the\nmodel is fine-tuned directly on the downstream\ntasks and uses only a very small number of ran-\ndomly initialized additional parameters, the task-\nspecific models can benefit from the larger, more\nexpressive pre-trained representations even when\ndownstream task data is very small."
        },
        {
            "text": "5.3  Feature-based Approach with BERT\n\nAll of the BERT results presented so far have used the fine-tuning approach, where a simple classification layer is added to the pre-trained model, and all parameters are jointly fine-tuned on a downstream task. However, the feature-based approach, where fixed features are extracted from the pre-trained model, has certain advantages. First, not all tasks can be easily represented by a Transformer encoder architecture, and therefore require a task-specific model architecture to be added. Second, there are major computational benefits to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation.",
            "page": 9,
            "x": 67,
            "y": 262,
            "width": 226,
            "height": 211,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "18-feature",
            "chunk_id": "d40065dc-32ef-4a6d-b62d-a425ce845788",
            "group_text": "5.3  Feature-based Approach with BERT\n\nAll of the BERT results presented so far have used the fine-tuning approach, where a simple classification layer is added to the pre-trained model, and all parameters are jointly fine-tuned on a downstream task. However, the feature-based approach, where fixed features are extracted from the pre-trained model, has certain advantages. First, not all tasks can be easily represented by a Transformer encoder architecture, and therefore require a task-specific model architecture to be added. Second, there are major computational benefits to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation.\n\nIn this section, we compare the two approaches by applying BERT to the CoNLL-2003 Named Entity Recognition (NER) task (Tjong Kim Sang and De Meulder, 2003). In the input to BERT, we use a case-preserving WordPiece model, and we include the maximal document context provided by the data. Following standard practice, we formulate this as a tagging task but do not use a CRF\n\nTable 7: CoNLL-2003 Named Entity Recognition results. Hyperparameters were selected using the Dev set. The reported Dev and Test scores are averaged over 5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe first sub-token as the input to the token-level\nclassifier over the NER label set.\n\nTo ablate the fine-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers *without* fine-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classification layer.\n\nResults are presented in Table 7. BERT\\textsubscript{LARGE}\nperforms competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model. This demonstrates that BERT is effective for both fine-tuning and feature-based approaches."
        },
        {
            "text": "In this section, we compare the two approaches by applying BERT to the CoNLL-2003 Named Entity Recognition (NER) task (Tjong Kim Sang and De Meulder, 2003). In the input to BERT, we use a case-preserving WordPiece model, and we include the maximal document context provided by the data. Following standard practice, we formulate this as a tagging task but do not use a CRF",
            "page": 9,
            "x": 68,
            "y": 474,
            "width": 226,
            "height": 111,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "18-feature",
            "chunk_id": "b1f4fd20-b059-49c9-8735-d1fa06ec7c9f",
            "group_text": "5.3  Feature-based Approach with BERT\n\nAll of the BERT results presented so far have used the fine-tuning approach, where a simple classification layer is added to the pre-trained model, and all parameters are jointly fine-tuned on a downstream task. However, the feature-based approach, where fixed features are extracted from the pre-trained model, has certain advantages. First, not all tasks can be easily represented by a Transformer encoder architecture, and therefore require a task-specific model architecture to be added. Second, there are major computational benefits to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation.\n\nIn this section, we compare the two approaches by applying BERT to the CoNLL-2003 Named Entity Recognition (NER) task (Tjong Kim Sang and De Meulder, 2003). In the input to BERT, we use a case-preserving WordPiece model, and we include the maximal document context provided by the data. Following standard practice, we formulate this as a tagging task but do not use a CRF\n\nTable 7: CoNLL-2003 Named Entity Recognition results. Hyperparameters were selected using the Dev set. The reported Dev and Test scores are averaged over 5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe first sub-token as the input to the token-level\nclassifier over the NER label set.\n\nTo ablate the fine-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers *without* fine-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classification layer.\n\nResults are presented in Table 7. BERT\\textsubscript{LARGE}\nperforms competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model. This demonstrates that BERT is effective for both fine-tuning and feature-based approaches."
        },
        {
            "text": "Table 7: CoNLL-2003 Named Entity Recognition results. Hyperparameters were selected using the Dev set. The reported Dev and Test scores are averaged over 5 random restarts using those hyperparameters.",
            "page": 9,
            "x": 303,
            "y": 232,
            "width": 226,
            "height": 51,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "18-feature",
            "chunk_id": "850bd027-dad7-4b19-aad8-cc1dfb79db7c",
            "group_text": "5.3  Feature-based Approach with BERT\n\nAll of the BERT results presented so far have used the fine-tuning approach, where a simple classification layer is added to the pre-trained model, and all parameters are jointly fine-tuned on a downstream task. However, the feature-based approach, where fixed features are extracted from the pre-trained model, has certain advantages. First, not all tasks can be easily represented by a Transformer encoder architecture, and therefore require a task-specific model architecture to be added. Second, there are major computational benefits to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation.\n\nIn this section, we compare the two approaches by applying BERT to the CoNLL-2003 Named Entity Recognition (NER) task (Tjong Kim Sang and De Meulder, 2003). In the input to BERT, we use a case-preserving WordPiece model, and we include the maximal document context provided by the data. Following standard practice, we formulate this as a tagging task but do not use a CRF\n\nTable 7: CoNLL-2003 Named Entity Recognition results. Hyperparameters were selected using the Dev set. The reported Dev and Test scores are averaged over 5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe first sub-token as the input to the token-level\nclassifier over the NER label set.\n\nTo ablate the fine-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers *without* fine-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classification layer.\n\nResults are presented in Table 7. BERT\\textsubscript{LARGE}\nperforms competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model. This demonstrates that BERT is effective for both fine-tuning and feature-based approaches."
        },
        {
            "text": "layer in the output. We use the representation of\nthe first sub-token as the input to the token-level\nclassifier over the NER label set.",
            "page": 9,
            "x": 304,
            "y": 311,
            "width": 225,
            "height": 41,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "18-feature",
            "chunk_id": "a276e19e-28d9-4db5-85e3-61f389bb9766",
            "group_text": "5.3  Feature-based Approach with BERT\n\nAll of the BERT results presented so far have used the fine-tuning approach, where a simple classification layer is added to the pre-trained model, and all parameters are jointly fine-tuned on a downstream task. However, the feature-based approach, where fixed features are extracted from the pre-trained model, has certain advantages. First, not all tasks can be easily represented by a Transformer encoder architecture, and therefore require a task-specific model architecture to be added. Second, there are major computational benefits to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation.\n\nIn this section, we compare the two approaches by applying BERT to the CoNLL-2003 Named Entity Recognition (NER) task (Tjong Kim Sang and De Meulder, 2003). In the input to BERT, we use a case-preserving WordPiece model, and we include the maximal document context provided by the data. Following standard practice, we formulate this as a tagging task but do not use a CRF\n\nTable 7: CoNLL-2003 Named Entity Recognition results. Hyperparameters were selected using the Dev set. The reported Dev and Test scores are averaged over 5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe first sub-token as the input to the token-level\nclassifier over the NER label set.\n\nTo ablate the fine-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers *without* fine-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classification layer.\n\nResults are presented in Table 7. BERT\\textsubscript{LARGE}\nperforms competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model. This demonstrates that BERT is effective for both fine-tuning and feature-based approaches."
        },
        {
            "text": "To ablate the fine-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers *without* fine-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classification layer.",
            "page": 9,
            "x": 303,
            "y": 355,
            "width": 225,
            "height": 96,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "18-feature",
            "chunk_id": "46d09998-aff8-406d-8768-86d438edbec5",
            "group_text": "5.3  Feature-based Approach with BERT\n\nAll of the BERT results presented so far have used the fine-tuning approach, where a simple classification layer is added to the pre-trained model, and all parameters are jointly fine-tuned on a downstream task. However, the feature-based approach, where fixed features are extracted from the pre-trained model, has certain advantages. First, not all tasks can be easily represented by a Transformer encoder architecture, and therefore require a task-specific model architecture to be added. Second, there are major computational benefits to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation.\n\nIn this section, we compare the two approaches by applying BERT to the CoNLL-2003 Named Entity Recognition (NER) task (Tjong Kim Sang and De Meulder, 2003). In the input to BERT, we use a case-preserving WordPiece model, and we include the maximal document context provided by the data. Following standard practice, we formulate this as a tagging task but do not use a CRF\n\nTable 7: CoNLL-2003 Named Entity Recognition results. Hyperparameters were selected using the Dev set. The reported Dev and Test scores are averaged over 5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe first sub-token as the input to the token-level\nclassifier over the NER label set.\n\nTo ablate the fine-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers *without* fine-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classification layer.\n\nResults are presented in Table 7. BERT\\textsubscript{LARGE}\nperforms competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model. This demonstrates that BERT is effective for both fine-tuning and feature-based approaches."
        },
        {
            "text": "Results are presented in Table 7. BERT\\textsubscript{LARGE}\nperforms competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model. This demonstrates that BERT is effective for both fine-tuning and feature-based approaches.",
            "page": 9,
            "x": 303,
            "y": 454,
            "width": 225,
            "height": 111,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "18-feature",
            "chunk_id": "18ff97e0-e680-4b5b-baaa-a276265cfa18",
            "group_text": "5.3  Feature-based Approach with BERT\n\nAll of the BERT results presented so far have used the fine-tuning approach, where a simple classification layer is added to the pre-trained model, and all parameters are jointly fine-tuned on a downstream task. However, the feature-based approach, where fixed features are extracted from the pre-trained model, has certain advantages. First, not all tasks can be easily represented by a Transformer encoder architecture, and therefore require a task-specific model architecture to be added. Second, there are major computational benefits to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation.\n\nIn this section, we compare the two approaches by applying BERT to the CoNLL-2003 Named Entity Recognition (NER) task (Tjong Kim Sang and De Meulder, 2003). In the input to BERT, we use a case-preserving WordPiece model, and we include the maximal document context provided by the data. Following standard practice, we formulate this as a tagging task but do not use a CRF\n\nTable 7: CoNLL-2003 Named Entity Recognition results. Hyperparameters were selected using the Dev set. The reported Dev and Test scores are averaged over 5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe first sub-token as the input to the token-level\nclassifier over the NER label set.\n\nTo ablate the fine-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers *without* fine-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classification layer.\n\nResults are presented in Table 7. BERT\\textsubscript{LARGE}\nperforms competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model. This demonstrates that BERT is effective for both fine-tuning and feature-based approaches."
        },
        {
            "text": "## 6 Conclusion\n\nRecent empirical improvements due to transfer learning with language models have demonstrated that rich, unsupervised pre-training is an integral part of many language understanding systems. In particular, these results enable even low-resource tasks to benefit from deep unidirectional architectures. Our major contribution is further generalizing these findings to deep *bidirectional* architectures, allowing the same pre-trained model to successfully tackle a broad set of NLP tasks.",
            "page": 9,
            "x": 302,
            "y": 583,
            "width": 227,
            "height": 171,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "19-conclusion",
            "chunk_id": "ed839c11-4f23-44f4-93ef-d8d1284d36c5",
            "group_text": "## 6 Conclusion\n\nRecent empirical improvements due to transfer learning with language models have demonstrated that rich, unsupervised pre-training is an integral part of many language understanding systems. In particular, these results enable even low-resource tasks to benefit from deep unidirectional architectures. Our major contribution is further generalizing these findings to deep *bidirectional* architectures, allowing the same pre-trained model to successfully tackle a broad set of NLP tasks."
        }
    ]
}