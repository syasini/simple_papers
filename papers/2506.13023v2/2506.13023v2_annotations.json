{
    "annotations": [
        {
            "text": "# A Practical Guide for Evaluating LLMs and LLM-Reliant Systems",
            "page": 1,
            "x": 89,
            "y": 66,
            "width": 419,
            "height": 23,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "0-title",
            "chunk_id": "01c9893b-9a67-47f0-82ce-423fe8efb7c2",
            "group_text": "# A Practical Guide for Evaluating LLMs and LLM-Reliant Systems\n\nEthan M. Rudd  \nGoogle  \nethanrudd@google.com\n\nChristopher Andrews  \nGoogle  \nandrewsch@google.com\n\nPhilip Tully  \nGoogle  \nphtully@google.com"
        },
        {
            "text": "Ethan M. Rudd  \nGoogle  \nethanrudd@google.com\n\nChristopher Andrews  \nGoogle  \nandrewsch@google.com\n\nPhilip Tully  \nGoogle  \nphtully@google.com",
            "page": 1,
            "x": 74,
            "y": 108,
            "width": 444,
            "height": 49,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "0-title",
            "chunk_id": "712b9026-aab9-427d-bf44-ab1ae2473911",
            "group_text": "# A Practical Guide for Evaluating LLMs and LLM-Reliant Systems\n\nEthan M. Rudd  \nGoogle  \nethanrudd@google.com\n\nChristopher Andrews  \nGoogle  \nandrewsch@google.com\n\nPhilip Tully  \nGoogle  \nphtully@google.com"
        },
        {
            "text": "# Abstract\n\nRecent advances in generative AI have led to remarkable interest in using systems that rely on large language models (LLMs) for practical applications. However, meaningful evaluation of these systems in real-world scenarios comes with a distinct set of challenges, which are not well-addressed by synthetic benchmarks and de-facto metrics that are often seen in the literature. We present a practical evaluation framework which outlines how to proactively curate representative datasets, select meaningful evaluation metrics, and employ meaningful evaluation methodologies that integrate well with practical development and deployment of LLM-reliant systems that must adhere to real-world requirements and meet user-facing needs.",
            "page": 1,
            "x": 88,
            "y": 202,
            "width": 185,
            "height": 244,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "1-abstract",
            "chunk_id": "86f889c5-1b20-4699-bb85-9de757e9b5bf",
            "group_text": "# Abstract\n\nRecent advances in generative AI have led to remarkable interest in using systems that rely on large language models (LLMs) for practical applications. However, meaningful evaluation of these systems in real-world scenarios comes with a distinct set of challenges, which are not well-addressed by synthetic benchmarks and de-facto metrics that are often seen in the literature. We present a practical evaluation framework which outlines how to proactively curate representative datasets, select meaningful evaluation metrics, and employ meaningful evaluation methodologies that integrate well with practical development and deployment of LLM-reliant systems that must adhere to real-world requirements and meet user-facing needs."
        },
        {
            "text": "1   Introduction\n\nGenerative AI systems that rely on large language models (LLMs) have seen remarkable adoption, yet their evaluation remains a significant challenge. Users can supply infinitely many prompts, and the systems can in turn output infinitely many responses, often unpredictably. This is made even more challenging for multi-turn agentic workflows where errors can compound with each additional agent execution. Complexity is further exacerbated by sensitivity to minor prompt changes, hallucinations, undesired refusals to respond, and inherent non-determinism stemming from stochastic computations, chained model calls, and grounding data/API inconsistencies. Deploying these systems reliably requires robust methods for measuring response quality against real-world requirements.",
            "page": 1,
            "x": 66,
            "y": 457,
            "width": 229,
            "height": 253,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "8fdcd928-6197-407c-86ef-03e4f8c0cdf0",
            "group_text": "1   Introduction\n\nGenerative AI systems that rely on large language models (LLMs) have seen remarkable adoption, yet their evaluation remains a significant challenge. Users can supply infinitely many prompts, and the systems can in turn output infinitely many responses, often unpredictably. This is made even more challenging for multi-turn agentic workflows where errors can compound with each additional agent execution. Complexity is further exacerbated by sensitivity to minor prompt changes, hallucinations, undesired refusals to respond, and inherent non-determinism stemming from stochastic computations, chained model calls, and grounding data/API inconsistencies. Deploying these systems reliably requires robust methods for measuring response quality against real-world requirements.\n\nWhile the importance of evaluation is clear \u2013 enabling iterative progress, building user trust, ensuring consistency, and improving efficiency \u2013 practitioners often select from a plethora of\n\n\"de-facto\" evaluation techniques without a consistent implementation strategy to address real-world evaluation goals, system requirements, or end-user experience. This paper addresses this gap by proposing a structured, actionable framework for designing and implementing evaluation of LLM-reliant AI systems (cf. Figure 1). The evaluation design process is organized around three fundamental pillars:"
        },
        {
            "text": "While the importance of evaluation is clear \u2013 enabling iterative progress, building user trust, ensuring consistency, and improving efficiency \u2013 practitioners often select from a plethora of",
            "page": 1,
            "x": 66,
            "y": 712,
            "width": 228,
            "height": 56,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "af1ccbc0-40c6-4fcd-9fb1-1b4d7a5c319d",
            "group_text": "1   Introduction\n\nGenerative AI systems that rely on large language models (LLMs) have seen remarkable adoption, yet their evaluation remains a significant challenge. Users can supply infinitely many prompts, and the systems can in turn output infinitely many responses, often unpredictably. This is made even more challenging for multi-turn agentic workflows where errors can compound with each additional agent execution. Complexity is further exacerbated by sensitivity to minor prompt changes, hallucinations, undesired refusals to respond, and inherent non-determinism stemming from stochastic computations, chained model calls, and grounding data/API inconsistencies. Deploying these systems reliably requires robust methods for measuring response quality against real-world requirements.\n\nWhile the importance of evaluation is clear \u2013 enabling iterative progress, building user trust, ensuring consistency, and improving efficiency \u2013 practitioners often select from a plethora of\n\n\"de-facto\" evaluation techniques without a consistent implementation strategy to address real-world evaluation goals, system requirements, or end-user experience. This paper addresses this gap by proposing a structured, actionable framework for designing and implementing evaluation of LLM-reliant AI systems (cf. Figure 1). The evaluation design process is organized around three fundamental pillars:"
        },
        {
            "text": "\"de-facto\" evaluation techniques without a consistent implementation strategy to address real-world evaluation goals, system requirements, or end-user experience. This paper addresses this gap by proposing a structured, actionable framework for designing and implementing evaluation of LLM-reliant AI systems (cf. Figure 1). The evaluation design process is organized around three fundamental pillars:",
            "page": 1,
            "x": 302,
            "y": 202,
            "width": 227,
            "height": 126,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "2-introduction",
            "chunk_id": "401abc40-886f-4205-934e-d6bd864eec55",
            "group_text": "1   Introduction\n\nGenerative AI systems that rely on large language models (LLMs) have seen remarkable adoption, yet their evaluation remains a significant challenge. Users can supply infinitely many prompts, and the systems can in turn output infinitely many responses, often unpredictably. This is made even more challenging for multi-turn agentic workflows where errors can compound with each additional agent execution. Complexity is further exacerbated by sensitivity to minor prompt changes, hallucinations, undesired refusals to respond, and inherent non-determinism stemming from stochastic computations, chained model calls, and grounding data/API inconsistencies. Deploying these systems reliably requires robust methods for measuring response quality against real-world requirements.\n\nWhile the importance of evaluation is clear \u2013 enabling iterative progress, building user trust, ensuring consistency, and improving efficiency \u2013 practitioners often select from a plethora of\n\n\"de-facto\" evaluation techniques without a consistent implementation strategy to address real-world evaluation goals, system requirements, or end-user experience. This paper addresses this gap by proposing a structured, actionable framework for designing and implementing evaluation of LLM-reliant AI systems (cf. Figure 1). The evaluation design process is organized around three fundamental pillars:"
        },
        {
            "text": "1. Datasets: Curating representative and high-quality data tailored to the evaluation goals.\n2. Metrics: Selecting appropriate quantitative and qualitative measures to assess performance against specified objectives.\n3. Methodology: Designing the overall evaluation approach, including specific strategies to handle challenges like non-determinism, prompt sensitivity, and hallucination measurement.",
            "page": 1,
            "x": 311,
            "y": 332,
            "width": 221,
            "height": 154,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "3-datasets",
            "chunk_id": "b96d59fc-fdb2-403b-98f5-6586ea82e1a7",
            "group_text": "1. Datasets: Curating representative and high-quality data tailored to the evaluation goals.\n2. Metrics: Selecting appropriate quantitative and qualitative measures to assess performance against specified objectives.\n3. Methodology: Designing the overall evaluation approach, including specific strategies to handle challenges like non-determinism, prompt sensitivity, and hallucination measurement.\n\nThe aim of this framework is to assist in proactively designing evaluation suites that integrate with the development and deployment lifecycles of generative AI systems. By defining scope and objectives up-front, these evaluation suites can be tailored around operational goals and requirements, and through continuous evaluation and iterative refinement, the evaluation suites can mature with their corresponding generative AI systems.\n\nIn the following sections, we expand on the high-level flowchart presented in Figure 1, outlining key stages and decision points in developing an evaluation plan. This structured framework serves to equip researchers and practitioners with the tools needed to develop robust, reliable, and informative evaluation strategies."
        },
        {
            "text": "The aim of this framework is to assist in proactively designing evaluation suites that integrate with the development and deployment lifecycles of generative AI systems. By defining scope and objectives up-front, these evaluation suites can be tailored around operational goals and requirements, and through continuous evaluation and iterative refinement, the evaluation suites can mature with their corresponding generative AI systems.",
            "page": 1,
            "x": 303,
            "y": 491,
            "width": 226,
            "height": 124,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "3-datasets",
            "chunk_id": "6b9f61c3-2ffa-4e1e-b544-290bc6f8b980",
            "group_text": "1. Datasets: Curating representative and high-quality data tailored to the evaluation goals.\n2. Metrics: Selecting appropriate quantitative and qualitative measures to assess performance against specified objectives.\n3. Methodology: Designing the overall evaluation approach, including specific strategies to handle challenges like non-determinism, prompt sensitivity, and hallucination measurement.\n\nThe aim of this framework is to assist in proactively designing evaluation suites that integrate with the development and deployment lifecycles of generative AI systems. By defining scope and objectives up-front, these evaluation suites can be tailored around operational goals and requirements, and through continuous evaluation and iterative refinement, the evaluation suites can mature with their corresponding generative AI systems.\n\nIn the following sections, we expand on the high-level flowchart presented in Figure 1, outlining key stages and decision points in developing an evaluation plan. This structured framework serves to equip researchers and practitioners with the tools needed to develop robust, reliable, and informative evaluation strategies."
        },
        {
            "text": "In the following sections, we expand on the high-level flowchart presented in Figure 1, outlining key stages and decision points in developing an evaluation plan. This structured framework serves to equip researchers and practitioners with the tools needed to develop robust, reliable, and informative evaluation strategies.",
            "page": 1,
            "x": 303,
            "y": 615,
            "width": 226,
            "height": 95,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "3-datasets",
            "chunk_id": "60f6293f-7934-4584-82ea-779281fdaf35",
            "group_text": "1. Datasets: Curating representative and high-quality data tailored to the evaluation goals.\n2. Metrics: Selecting appropriate quantitative and qualitative measures to assess performance against specified objectives.\n3. Methodology: Designing the overall evaluation approach, including specific strategies to handle challenges like non-determinism, prompt sensitivity, and hallucination measurement.\n\nThe aim of this framework is to assist in proactively designing evaluation suites that integrate with the development and deployment lifecycles of generative AI systems. By defining scope and objectives up-front, these evaluation suites can be tailored around operational goals and requirements, and through continuous evaluation and iterative refinement, the evaluation suites can mature with their corresponding generative AI systems.\n\nIn the following sections, we expand on the high-level flowchart presented in Figure 1, outlining key stages and decision points in developing an evaluation plan. This structured framework serves to equip researchers and practitioners with the tools needed to develop robust, reliable, and informative evaluation strategies."
        },
        {
            "text": "## 2   Evaluation Dataset Formulation\n\nDatasets are composed of prompts and optionally associated ground truth responses, depending on",
            "page": 1,
            "x": 303,
            "y": 716,
            "width": 227,
            "height": 53,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "4-evaluation",
            "chunk_id": "9ecee87a-8d35-4867-a25d-5172214d4626",
            "group_text": "## 2   Evaluation Dataset Formulation\n\nDatasets are composed of prompts and optionally associated ground truth responses, depending on\n\nFigure 1: *Proposed framework for developing an LLM evaluation strategy.* This flowchart guides decisions across three key stages, aligning with the paper\u2019s focus on Datasets, Metrics, and Methodology. These stages are shown in green. Additional inputs and actions to inform and guide system development based on these stages are shown in red. Key factors to consider and questions to ask at each stage of the evaluation design are listed as bullet points.\n\nthe chosen evaluation metrics and methodology.\nTo create a robust and reliable evaluation dataset,\nfive core principles, the **5 D\u2019s**, should be followed.\nIn accordance with translating high-level goals\nthat go into our framework, the dataset must have\na **1. Defined Scope** that aligns with specific tasks\nthe model is meant to perform. The dataset should\nfurther be **2. Demonstrative of Production Us-\nage**, mimicking the inputs and scenarios expected\nfrom actual users. The dataset should be **3. Di-\nverse**, capturing the variety of the problem space\nto avoid a narrow or biased evaluation. To ensure\nthe integrity of the results, the dataset must be **4.\nDecontaminated**, meaning it is distinct from any\ndata used during model training to prevent mis-\nleadingly high performance metrics. Finally, the\ndataset should be **5. Dynamic**, treated as a living\nbody of work that evolves as the real-world appli-\ncation changes."
        },
        {
            "text": "Figure 1: *Proposed framework for developing an LLM evaluation strategy.* This flowchart guides decisions across three key stages, aligning with the paper\u2019s focus on Datasets, Metrics, and Methodology. These stages are shown in green. Additional inputs and actions to inform and guide system development based on these stages are shown in red. Key factors to consider and questions to ask at each stage of the evaluation design are listed as bullet points.",
            "page": 2,
            "x": 68,
            "y": 211,
            "width": 461,
            "height": 69,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "4-evaluation",
            "chunk_id": "af4a760b-3973-4f9d-9326-b1c86625f265",
            "group_text": "## 2   Evaluation Dataset Formulation\n\nDatasets are composed of prompts and optionally associated ground truth responses, depending on\n\nFigure 1: *Proposed framework for developing an LLM evaluation strategy.* This flowchart guides decisions across three key stages, aligning with the paper\u2019s focus on Datasets, Metrics, and Methodology. These stages are shown in green. Additional inputs and actions to inform and guide system development based on these stages are shown in red. Key factors to consider and questions to ask at each stage of the evaluation design are listed as bullet points.\n\nthe chosen evaluation metrics and methodology.\nTo create a robust and reliable evaluation dataset,\nfive core principles, the **5 D\u2019s**, should be followed.\nIn accordance with translating high-level goals\nthat go into our framework, the dataset must have\na **1. Defined Scope** that aligns with specific tasks\nthe model is meant to perform. The dataset should\nfurther be **2. Demonstrative of Production Us-\nage**, mimicking the inputs and scenarios expected\nfrom actual users. The dataset should be **3. Di-\nverse**, capturing the variety of the problem space\nto avoid a narrow or biased evaluation. To ensure\nthe integrity of the results, the dataset must be **4.\nDecontaminated**, meaning it is distinct from any\ndata used during model training to prevent mis-\nleadingly high performance metrics. Finally, the\ndataset should be **5. Dynamic**, treated as a living\nbody of work that evolves as the real-world appli-\ncation changes."
        },
        {
            "text": "the chosen evaluation metrics and methodology.\nTo create a robust and reliable evaluation dataset,\nfive core principles, the **5 D\u2019s**, should be followed.\nIn accordance with translating high-level goals\nthat go into our framework, the dataset must have\na **1. Defined Scope** that aligns with specific tasks\nthe model is meant to perform. The dataset should\nfurther be **2. Demonstrative of Production Us-\nage**, mimicking the inputs and scenarios expected\nfrom actual users. The dataset should be **3. Di-\nverse**, capturing the variety of the problem space\nto avoid a narrow or biased evaluation. To ensure\nthe integrity of the results, the dataset must be **4.\nDecontaminated**, meaning it is distinct from any\ndata used during model training to prevent mis-\nleadingly high performance metrics. Finally, the\ndataset should be **5. Dynamic**, treated as a living\nbody of work that evolves as the real-world appli-\ncation changes.",
            "page": 2,
            "x": 68,
            "y": 291,
            "width": 225,
            "height": 259,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "4-evaluation",
            "chunk_id": "a72ba129-c8f9-42df-8d4e-97673b75e168",
            "group_text": "## 2   Evaluation Dataset Formulation\n\nDatasets are composed of prompts and optionally associated ground truth responses, depending on\n\nFigure 1: *Proposed framework for developing an LLM evaluation strategy.* This flowchart guides decisions across three key stages, aligning with the paper\u2019s focus on Datasets, Metrics, and Methodology. These stages are shown in green. Additional inputs and actions to inform and guide system development based on these stages are shown in red. Key factors to consider and questions to ask at each stage of the evaluation design are listed as bullet points.\n\nthe chosen evaluation metrics and methodology.\nTo create a robust and reliable evaluation dataset,\nfive core principles, the **5 D\u2019s**, should be followed.\nIn accordance with translating high-level goals\nthat go into our framework, the dataset must have\na **1. Defined Scope** that aligns with specific tasks\nthe model is meant to perform. The dataset should\nfurther be **2. Demonstrative of Production Us-\nage**, mimicking the inputs and scenarios expected\nfrom actual users. The dataset should be **3. Di-\nverse**, capturing the variety of the problem space\nto avoid a narrow or biased evaluation. To ensure\nthe integrity of the results, the dataset must be **4.\nDecontaminated**, meaning it is distinct from any\ndata used during model training to prevent mis-\nleadingly high performance metrics. Finally, the\ndataset should be **5. Dynamic**, treated as a living\nbody of work that evolves as the real-world appli-\ncation changes."
        },
        {
            "text": "## 2.1 Amassing Evaluation Datasets\n\nThe process of collecting or creating evaluation datasets that adhere to the 5 D\u2019s can be broken down into three high-level approaches: benchmark analysis, _golden_ human annotated datasets, and synthetically-generated _silver_ datasets.",
            "page": 2,
            "x": 67,
            "y": 562,
            "width": 226,
            "height": 92,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-amassing",
            "chunk_id": "688650b7-b257-496c-9ef0-376e190e1cd4",
            "group_text": "## 2.1 Amassing Evaluation Datasets\n\nThe process of collecting or creating evaluation datasets that adhere to the 5 D\u2019s can be broken down into three high-level approaches: benchmark analysis, _golden_ human annotated datasets, and synthetically-generated _silver_ datasets.\n\n### 2.1.1 Benchmark Analysis\nPublicly available benchmarks (e.g., (Pustejovsky et al., 2023; Hendrycks et al., 2020; Guo et al., 2023; Alam et al., 2024)) offer quick initial insights but often lack use-case specificity, may be contaminated with training data, and require careful auditing for quality and licensing.\n\n2.1.2   Human-Annotated Golden Datasets\nHuman annotation is preferable for tasks requir-\ning specialized knowledge not found in founda-\ntion models, as it can reveal subtleties in what con-\nstitutes a high-quality response (e.g., correctness,\nusefulness, completeness). However, this pro-\ncess is often difficult, costly, and time-consuming,\nrisking low-quality or biased annotations due to\nthe need for deep domain expertise and team co-\nordination. Common collection workflows in-\nclude leveraging in-house Subject Matter Experts\n(SMEs), outsourcing to third parties, employing\nuser data and feedback, or gathering prompts\nthrough User Experience Research (UXR) sur-\nveys. These methods require clear instructions\nto ensure data quality, and practitioners should\nbe aware that user data may have privacy con-\nstraints and that feedback tends to be biased to-\nward negative experiences. To streamline these\nworkflows and improve the quality of the final\n\"golden\" datasets, specialized data labeling and\nannotation platforms (Labelbox, Inc., 2024; Ama-\nzon Web Services, Inc., 2024; Montani and Honni-\nbal, 2018; Nakayama et al., 2018) can help enforce\nconsistency and manage annotator agreement.\n\n### 2.1.3  Synthetically-Generated Silver Datasets\nSynthetic \"silver\" datasets offer a scalable, cost-effective alternative to human annotation that can bypass potential legal and privacy issues. This approach, however, requires careful human oversight to maintain data quality, minimize bias, and ensure the data is decontaminated from the model\u2019s original training set. Common generation techniques are as follows.\n\n**Distilling from Frontier Models:** This involves using powerful foundation models to generate prompt-response pairs. Advanced methods (Wang et al., 2022; Wei et al., 2022; Taori et al., 2023; Chiang et al., 2023; Chase, 2022; Liu, 2023; Es et al., 2023) can be used to guide this process, such as having the model self-critique its responses based on a set of principles (Constitutional AI) (Bai et al., 2022b) or iteratively making prompts more complex or diverse (Evol-Instruct) (Bai et al., 2022a).\n\n**Increasing Diversity:** To ensure variety in the generated data, strategies include prompting the LLM to adopt different personas or using sampling techniques like adjusting the model\u2019s temperature or employing nucleus sampling (top-p) (Holtzman et al., 2019).\n\n**Leveraging Automatic Data Scraping Pipelines:** Leveraging automated data scraping pipelines is a common method for amassing large volumes of domain-specific text from the internet or internal repositories. This process can be implemented using a range of tools, from building custom crawlers with open-source Python frameworks like Scrapy (Scrapy Developers, 2024) and Beautiful Soup (Richardson, 2024), to using browser automation libraries like Selenium (Selenium Project, 2024) for dynamic websites, or employing commercial platforms for managed scraping. However, it is difficult to guarantee that data curated in this fashion is fully decontaminated. As mentioned earlier, it is critical to rigorously filter these web-based datasets. Various filtering methods can be applied, ranging from manual inspection to more scalable approaches. For instance, libraries like Pandas (McKinney et al., 2011) can be used to remove texts with low token counts, while LLMs or other lighter-weight machine learning models (Wolf et al., 2020) can be trained to identify and discard low-quality or irrelevant content.\n\n2.1.4 Curating Evaluation Dataset Metadata  \nBeyond prompts and responses, metadata can be curated for each data point.  \n\u2003\u2003Tags: Descriptive text categories that can be manually or automatically applied to prompt/response pairs to measure and monitor dataset diversity.\n\n**Grounding Information:** Supporting context (e.g., a relevant news article) can be provided to an evaluation, particularly an autorater, to help it ac-\n\ncurately judge factuality on topics that may be too\nrecent or outside its training data. This informa-\ntion should not be sourced from the system being\nevaluated.\n\n**Expected Information:** Specific terms, steps, or fields that the model is required to recall in its response. Unlike tags, this information requires auditing for correctness, as its fidelity can directly impact certain evaluation metrics.\n\n2.1.5   When to Use Which Data Curation Approach\n\nBenchmark, human-annotated, and synthetically-generated datasets should complement one another. Starting with benchmarks or human-annotated data points, evaluators can easily generate more synthetic data points from there to glean directional insights on the task at hand. This bootstrapped dataset can be iteratively improved over time to meet the 5 D\u2019s, effectively turning a silver dataset into a golden one."
        },
        {
            "text": "### 2.1.1 Benchmark Analysis\nPublicly available benchmarks (e.g., (Pustejovsky et al., 2023; Hendrycks et al., 2020; Guo et al., 2023; Alam et al., 2024)) offer quick initial insights but often lack use-case specificity, may be contaminated with training data, and require careful auditing for quality and licensing.",
            "page": 2,
            "x": 68,
            "y": 664,
            "width": 225,
            "height": 104,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-amassing",
            "chunk_id": "881c1637-a0b6-4393-92ef-dd1d6ed8dc09",
            "group_text": "## 2.1 Amassing Evaluation Datasets\n\nThe process of collecting or creating evaluation datasets that adhere to the 5 D\u2019s can be broken down into three high-level approaches: benchmark analysis, _golden_ human annotated datasets, and synthetically-generated _silver_ datasets.\n\n### 2.1.1 Benchmark Analysis\nPublicly available benchmarks (e.g., (Pustejovsky et al., 2023; Hendrycks et al., 2020; Guo et al., 2023; Alam et al., 2024)) offer quick initial insights but often lack use-case specificity, may be contaminated with training data, and require careful auditing for quality and licensing.\n\n2.1.2   Human-Annotated Golden Datasets\nHuman annotation is preferable for tasks requir-\ning specialized knowledge not found in founda-\ntion models, as it can reveal subtleties in what con-\nstitutes a high-quality response (e.g., correctness,\nusefulness, completeness). However, this pro-\ncess is often difficult, costly, and time-consuming,\nrisking low-quality or biased annotations due to\nthe need for deep domain expertise and team co-\nordination. Common collection workflows in-\nclude leveraging in-house Subject Matter Experts\n(SMEs), outsourcing to third parties, employing\nuser data and feedback, or gathering prompts\nthrough User Experience Research (UXR) sur-\nveys. These methods require clear instructions\nto ensure data quality, and practitioners should\nbe aware that user data may have privacy con-\nstraints and that feedback tends to be biased to-\nward negative experiences. To streamline these\nworkflows and improve the quality of the final\n\"golden\" datasets, specialized data labeling and\nannotation platforms (Labelbox, Inc., 2024; Ama-\nzon Web Services, Inc., 2024; Montani and Honni-\nbal, 2018; Nakayama et al., 2018) can help enforce\nconsistency and manage annotator agreement.\n\n### 2.1.3  Synthetically-Generated Silver Datasets\nSynthetic \"silver\" datasets offer a scalable, cost-effective alternative to human annotation that can bypass potential legal and privacy issues. This approach, however, requires careful human oversight to maintain data quality, minimize bias, and ensure the data is decontaminated from the model\u2019s original training set. Common generation techniques are as follows.\n\n**Distilling from Frontier Models:** This involves using powerful foundation models to generate prompt-response pairs. Advanced methods (Wang et al., 2022; Wei et al., 2022; Taori et al., 2023; Chiang et al., 2023; Chase, 2022; Liu, 2023; Es et al., 2023) can be used to guide this process, such as having the model self-critique its responses based on a set of principles (Constitutional AI) (Bai et al., 2022b) or iteratively making prompts more complex or diverse (Evol-Instruct) (Bai et al., 2022a).\n\n**Increasing Diversity:** To ensure variety in the generated data, strategies include prompting the LLM to adopt different personas or using sampling techniques like adjusting the model\u2019s temperature or employing nucleus sampling (top-p) (Holtzman et al., 2019).\n\n**Leveraging Automatic Data Scraping Pipelines:** Leveraging automated data scraping pipelines is a common method for amassing large volumes of domain-specific text from the internet or internal repositories. This process can be implemented using a range of tools, from building custom crawlers with open-source Python frameworks like Scrapy (Scrapy Developers, 2024) and Beautiful Soup (Richardson, 2024), to using browser automation libraries like Selenium (Selenium Project, 2024) for dynamic websites, or employing commercial platforms for managed scraping. However, it is difficult to guarantee that data curated in this fashion is fully decontaminated. As mentioned earlier, it is critical to rigorously filter these web-based datasets. Various filtering methods can be applied, ranging from manual inspection to more scalable approaches. For instance, libraries like Pandas (McKinney et al., 2011) can be used to remove texts with low token counts, while LLMs or other lighter-weight machine learning models (Wolf et al., 2020) can be trained to identify and discard low-quality or irrelevant content.\n\n2.1.4 Curating Evaluation Dataset Metadata  \nBeyond prompts and responses, metadata can be curated for each data point.  \n\u2003\u2003Tags: Descriptive text categories that can be manually or automatically applied to prompt/response pairs to measure and monitor dataset diversity.\n\n**Grounding Information:** Supporting context (e.g., a relevant news article) can be provided to an evaluation, particularly an autorater, to help it ac-\n\ncurately judge factuality on topics that may be too\nrecent or outside its training data. This informa-\ntion should not be sourced from the system being\nevaluated.\n\n**Expected Information:** Specific terms, steps, or fields that the model is required to recall in its response. Unlike tags, this information requires auditing for correctness, as its fidelity can directly impact certain evaluation metrics.\n\n2.1.5   When to Use Which Data Curation Approach\n\nBenchmark, human-annotated, and synthetically-generated datasets should complement one another. Starting with benchmarks or human-annotated data points, evaluators can easily generate more synthetic data points from there to glean directional insights on the task at hand. This bootstrapped dataset can be iteratively improved over time to meet the 5 D\u2019s, effectively turning a silver dataset into a golden one."
        },
        {
            "text": "2.1.2   Human-Annotated Golden Datasets\nHuman annotation is preferable for tasks requir-\ning specialized knowledge not found in founda-\ntion models, as it can reveal subtleties in what con-\nstitutes a high-quality response (e.g., correctness,\nusefulness, completeness). However, this pro-\ncess is often difficult, costly, and time-consuming,\nrisking low-quality or biased annotations due to\nthe need for deep domain expertise and team co-\nordination. Common collection workflows in-\nclude leveraging in-house Subject Matter Experts\n(SMEs), outsourcing to third parties, employing\nuser data and feedback, or gathering prompts\nthrough User Experience Research (UXR) sur-\nveys. These methods require clear instructions\nto ensure data quality, and practitioners should\nbe aware that user data may have privacy con-\nstraints and that feedback tends to be biased to-\nward negative experiences. To streamline these\nworkflows and improve the quality of the final\n\"golden\" datasets, specialized data labeling and\nannotation platforms (Labelbox, Inc., 2024; Ama-\nzon Web Services, Inc., 2024; Montani and Honni-\nbal, 2018; Nakayama et al., 2018) can help enforce\nconsistency and manage annotator agreement.",
            "page": 2,
            "x": 303,
            "y": 290,
            "width": 226,
            "height": 344,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-amassing",
            "chunk_id": "c4bc7dde-4ff0-436b-af76-cd462bc39403",
            "group_text": "## 2.1 Amassing Evaluation Datasets\n\nThe process of collecting or creating evaluation datasets that adhere to the 5 D\u2019s can be broken down into three high-level approaches: benchmark analysis, _golden_ human annotated datasets, and synthetically-generated _silver_ datasets.\n\n### 2.1.1 Benchmark Analysis\nPublicly available benchmarks (e.g., (Pustejovsky et al., 2023; Hendrycks et al., 2020; Guo et al., 2023; Alam et al., 2024)) offer quick initial insights but often lack use-case specificity, may be contaminated with training data, and require careful auditing for quality and licensing.\n\n2.1.2   Human-Annotated Golden Datasets\nHuman annotation is preferable for tasks requir-\ning specialized knowledge not found in founda-\ntion models, as it can reveal subtleties in what con-\nstitutes a high-quality response (e.g., correctness,\nusefulness, completeness). However, this pro-\ncess is often difficult, costly, and time-consuming,\nrisking low-quality or biased annotations due to\nthe need for deep domain expertise and team co-\nordination. Common collection workflows in-\nclude leveraging in-house Subject Matter Experts\n(SMEs), outsourcing to third parties, employing\nuser data and feedback, or gathering prompts\nthrough User Experience Research (UXR) sur-\nveys. These methods require clear instructions\nto ensure data quality, and practitioners should\nbe aware that user data may have privacy con-\nstraints and that feedback tends to be biased to-\nward negative experiences. To streamline these\nworkflows and improve the quality of the final\n\"golden\" datasets, specialized data labeling and\nannotation platforms (Labelbox, Inc., 2024; Ama-\nzon Web Services, Inc., 2024; Montani and Honni-\nbal, 2018; Nakayama et al., 2018) can help enforce\nconsistency and manage annotator agreement.\n\n### 2.1.3  Synthetically-Generated Silver Datasets\nSynthetic \"silver\" datasets offer a scalable, cost-effective alternative to human annotation that can bypass potential legal and privacy issues. This approach, however, requires careful human oversight to maintain data quality, minimize bias, and ensure the data is decontaminated from the model\u2019s original training set. Common generation techniques are as follows.\n\n**Distilling from Frontier Models:** This involves using powerful foundation models to generate prompt-response pairs. Advanced methods (Wang et al., 2022; Wei et al., 2022; Taori et al., 2023; Chiang et al., 2023; Chase, 2022; Liu, 2023; Es et al., 2023) can be used to guide this process, such as having the model self-critique its responses based on a set of principles (Constitutional AI) (Bai et al., 2022b) or iteratively making prompts more complex or diverse (Evol-Instruct) (Bai et al., 2022a).\n\n**Increasing Diversity:** To ensure variety in the generated data, strategies include prompting the LLM to adopt different personas or using sampling techniques like adjusting the model\u2019s temperature or employing nucleus sampling (top-p) (Holtzman et al., 2019).\n\n**Leveraging Automatic Data Scraping Pipelines:** Leveraging automated data scraping pipelines is a common method for amassing large volumes of domain-specific text from the internet or internal repositories. This process can be implemented using a range of tools, from building custom crawlers with open-source Python frameworks like Scrapy (Scrapy Developers, 2024) and Beautiful Soup (Richardson, 2024), to using browser automation libraries like Selenium (Selenium Project, 2024) for dynamic websites, or employing commercial platforms for managed scraping. However, it is difficult to guarantee that data curated in this fashion is fully decontaminated. As mentioned earlier, it is critical to rigorously filter these web-based datasets. Various filtering methods can be applied, ranging from manual inspection to more scalable approaches. For instance, libraries like Pandas (McKinney et al., 2011) can be used to remove texts with low token counts, while LLMs or other lighter-weight machine learning models (Wolf et al., 2020) can be trained to identify and discard low-quality or irrelevant content.\n\n2.1.4 Curating Evaluation Dataset Metadata  \nBeyond prompts and responses, metadata can be curated for each data point.  \n\u2003\u2003Tags: Descriptive text categories that can be manually or automatically applied to prompt/response pairs to measure and monitor dataset diversity.\n\n**Grounding Information:** Supporting context (e.g., a relevant news article) can be provided to an evaluation, particularly an autorater, to help it ac-\n\ncurately judge factuality on topics that may be too\nrecent or outside its training data. This informa-\ntion should not be sourced from the system being\nevaluated.\n\n**Expected Information:** Specific terms, steps, or fields that the model is required to recall in its response. Unlike tags, this information requires auditing for correctness, as its fidelity can directly impact certain evaluation metrics.\n\n2.1.5   When to Use Which Data Curation Approach\n\nBenchmark, human-annotated, and synthetically-generated datasets should complement one another. Starting with benchmarks or human-annotated data points, evaluators can easily generate more synthetic data points from there to glean directional insights on the task at hand. This bootstrapped dataset can be iteratively improved over time to meet the 5 D\u2019s, effectively turning a silver dataset into a golden one."
        },
        {
            "text": "### 2.1.3  Synthetically-Generated Silver Datasets\nSynthetic \"silver\" datasets offer a scalable, cost-effective alternative to human annotation that can bypass potential legal and privacy issues. This approach, however, requires careful human oversight to maintain data quality, minimize bias, and ensure the data is decontaminated from the model\u2019s original training set. Common generation techniques are as follows.",
            "page": 2,
            "x": 303,
            "y": 640,
            "width": 226,
            "height": 126,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-amassing",
            "chunk_id": "3cfa0820-25ac-4c83-9c41-4b49082fe9d6",
            "group_text": "## 2.1 Amassing Evaluation Datasets\n\nThe process of collecting or creating evaluation datasets that adhere to the 5 D\u2019s can be broken down into three high-level approaches: benchmark analysis, _golden_ human annotated datasets, and synthetically-generated _silver_ datasets.\n\n### 2.1.1 Benchmark Analysis\nPublicly available benchmarks (e.g., (Pustejovsky et al., 2023; Hendrycks et al., 2020; Guo et al., 2023; Alam et al., 2024)) offer quick initial insights but often lack use-case specificity, may be contaminated with training data, and require careful auditing for quality and licensing.\n\n2.1.2   Human-Annotated Golden Datasets\nHuman annotation is preferable for tasks requir-\ning specialized knowledge not found in founda-\ntion models, as it can reveal subtleties in what con-\nstitutes a high-quality response (e.g., correctness,\nusefulness, completeness). However, this pro-\ncess is often difficult, costly, and time-consuming,\nrisking low-quality or biased annotations due to\nthe need for deep domain expertise and team co-\nordination. Common collection workflows in-\nclude leveraging in-house Subject Matter Experts\n(SMEs), outsourcing to third parties, employing\nuser data and feedback, or gathering prompts\nthrough User Experience Research (UXR) sur-\nveys. These methods require clear instructions\nto ensure data quality, and practitioners should\nbe aware that user data may have privacy con-\nstraints and that feedback tends to be biased to-\nward negative experiences. To streamline these\nworkflows and improve the quality of the final\n\"golden\" datasets, specialized data labeling and\nannotation platforms (Labelbox, Inc., 2024; Ama-\nzon Web Services, Inc., 2024; Montani and Honni-\nbal, 2018; Nakayama et al., 2018) can help enforce\nconsistency and manage annotator agreement.\n\n### 2.1.3  Synthetically-Generated Silver Datasets\nSynthetic \"silver\" datasets offer a scalable, cost-effective alternative to human annotation that can bypass potential legal and privacy issues. This approach, however, requires careful human oversight to maintain data quality, minimize bias, and ensure the data is decontaminated from the model\u2019s original training set. Common generation techniques are as follows.\n\n**Distilling from Frontier Models:** This involves using powerful foundation models to generate prompt-response pairs. Advanced methods (Wang et al., 2022; Wei et al., 2022; Taori et al., 2023; Chiang et al., 2023; Chase, 2022; Liu, 2023; Es et al., 2023) can be used to guide this process, such as having the model self-critique its responses based on a set of principles (Constitutional AI) (Bai et al., 2022b) or iteratively making prompts more complex or diverse (Evol-Instruct) (Bai et al., 2022a).\n\n**Increasing Diversity:** To ensure variety in the generated data, strategies include prompting the LLM to adopt different personas or using sampling techniques like adjusting the model\u2019s temperature or employing nucleus sampling (top-p) (Holtzman et al., 2019).\n\n**Leveraging Automatic Data Scraping Pipelines:** Leveraging automated data scraping pipelines is a common method for amassing large volumes of domain-specific text from the internet or internal repositories. This process can be implemented using a range of tools, from building custom crawlers with open-source Python frameworks like Scrapy (Scrapy Developers, 2024) and Beautiful Soup (Richardson, 2024), to using browser automation libraries like Selenium (Selenium Project, 2024) for dynamic websites, or employing commercial platforms for managed scraping. However, it is difficult to guarantee that data curated in this fashion is fully decontaminated. As mentioned earlier, it is critical to rigorously filter these web-based datasets. Various filtering methods can be applied, ranging from manual inspection to more scalable approaches. For instance, libraries like Pandas (McKinney et al., 2011) can be used to remove texts with low token counts, while LLMs or other lighter-weight machine learning models (Wolf et al., 2020) can be trained to identify and discard low-quality or irrelevant content.\n\n2.1.4 Curating Evaluation Dataset Metadata  \nBeyond prompts and responses, metadata can be curated for each data point.  \n\u2003\u2003Tags: Descriptive text categories that can be manually or automatically applied to prompt/response pairs to measure and monitor dataset diversity.\n\n**Grounding Information:** Supporting context (e.g., a relevant news article) can be provided to an evaluation, particularly an autorater, to help it ac-\n\ncurately judge factuality on topics that may be too\nrecent or outside its training data. This informa-\ntion should not be sourced from the system being\nevaluated.\n\n**Expected Information:** Specific terms, steps, or fields that the model is required to recall in its response. Unlike tags, this information requires auditing for correctness, as its fidelity can directly impact certain evaluation metrics.\n\n2.1.5   When to Use Which Data Curation Approach\n\nBenchmark, human-annotated, and synthetically-generated datasets should complement one another. Starting with benchmarks or human-annotated data points, evaluators can easily generate more synthetic data points from there to glean directional insights on the task at hand. This bootstrapped dataset can be iteratively improved over time to meet the 5 D\u2019s, effectively turning a silver dataset into a golden one."
        },
        {
            "text": "**Distilling from Frontier Models:** This involves using powerful foundation models to generate prompt-response pairs. Advanced methods (Wang et al., 2022; Wei et al., 2022; Taori et al., 2023; Chiang et al., 2023; Chase, 2022; Liu, 2023; Es et al., 2023) can be used to guide this process, such as having the model self-critique its responses based on a set of principles (Constitutional AI) (Bai et al., 2022b) or iteratively making prompts more complex or diverse (Evol-Instruct) (Bai et al., 2022a).",
            "page": 3,
            "x": 68,
            "y": 61,
            "width": 225,
            "height": 151,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-amassing",
            "chunk_id": "24d07706-fa8b-4ecf-929e-3793ddb7d6a8",
            "group_text": "## 2.1 Amassing Evaluation Datasets\n\nThe process of collecting or creating evaluation datasets that adhere to the 5 D\u2019s can be broken down into three high-level approaches: benchmark analysis, _golden_ human annotated datasets, and synthetically-generated _silver_ datasets.\n\n### 2.1.1 Benchmark Analysis\nPublicly available benchmarks (e.g., (Pustejovsky et al., 2023; Hendrycks et al., 2020; Guo et al., 2023; Alam et al., 2024)) offer quick initial insights but often lack use-case specificity, may be contaminated with training data, and require careful auditing for quality and licensing.\n\n2.1.2   Human-Annotated Golden Datasets\nHuman annotation is preferable for tasks requir-\ning specialized knowledge not found in founda-\ntion models, as it can reveal subtleties in what con-\nstitutes a high-quality response (e.g., correctness,\nusefulness, completeness). However, this pro-\ncess is often difficult, costly, and time-consuming,\nrisking low-quality or biased annotations due to\nthe need for deep domain expertise and team co-\nordination. Common collection workflows in-\nclude leveraging in-house Subject Matter Experts\n(SMEs), outsourcing to third parties, employing\nuser data and feedback, or gathering prompts\nthrough User Experience Research (UXR) sur-\nveys. These methods require clear instructions\nto ensure data quality, and practitioners should\nbe aware that user data may have privacy con-\nstraints and that feedback tends to be biased to-\nward negative experiences. To streamline these\nworkflows and improve the quality of the final\n\"golden\" datasets, specialized data labeling and\nannotation platforms (Labelbox, Inc., 2024; Ama-\nzon Web Services, Inc., 2024; Montani and Honni-\nbal, 2018; Nakayama et al., 2018) can help enforce\nconsistency and manage annotator agreement.\n\n### 2.1.3  Synthetically-Generated Silver Datasets\nSynthetic \"silver\" datasets offer a scalable, cost-effective alternative to human annotation that can bypass potential legal and privacy issues. This approach, however, requires careful human oversight to maintain data quality, minimize bias, and ensure the data is decontaminated from the model\u2019s original training set. Common generation techniques are as follows.\n\n**Distilling from Frontier Models:** This involves using powerful foundation models to generate prompt-response pairs. Advanced methods (Wang et al., 2022; Wei et al., 2022; Taori et al., 2023; Chiang et al., 2023; Chase, 2022; Liu, 2023; Es et al., 2023) can be used to guide this process, such as having the model self-critique its responses based on a set of principles (Constitutional AI) (Bai et al., 2022b) or iteratively making prompts more complex or diverse (Evol-Instruct) (Bai et al., 2022a).\n\n**Increasing Diversity:** To ensure variety in the generated data, strategies include prompting the LLM to adopt different personas or using sampling techniques like adjusting the model\u2019s temperature or employing nucleus sampling (top-p) (Holtzman et al., 2019).\n\n**Leveraging Automatic Data Scraping Pipelines:** Leveraging automated data scraping pipelines is a common method for amassing large volumes of domain-specific text from the internet or internal repositories. This process can be implemented using a range of tools, from building custom crawlers with open-source Python frameworks like Scrapy (Scrapy Developers, 2024) and Beautiful Soup (Richardson, 2024), to using browser automation libraries like Selenium (Selenium Project, 2024) for dynamic websites, or employing commercial platforms for managed scraping. However, it is difficult to guarantee that data curated in this fashion is fully decontaminated. As mentioned earlier, it is critical to rigorously filter these web-based datasets. Various filtering methods can be applied, ranging from manual inspection to more scalable approaches. For instance, libraries like Pandas (McKinney et al., 2011) can be used to remove texts with low token counts, while LLMs or other lighter-weight machine learning models (Wolf et al., 2020) can be trained to identify and discard low-quality or irrelevant content.\n\n2.1.4 Curating Evaluation Dataset Metadata  \nBeyond prompts and responses, metadata can be curated for each data point.  \n\u2003\u2003Tags: Descriptive text categories that can be manually or automatically applied to prompt/response pairs to measure and monitor dataset diversity.\n\n**Grounding Information:** Supporting context (e.g., a relevant news article) can be provided to an evaluation, particularly an autorater, to help it ac-\n\ncurately judge factuality on topics that may be too\nrecent or outside its training data. This informa-\ntion should not be sourced from the system being\nevaluated.\n\n**Expected Information:** Specific terms, steps, or fields that the model is required to recall in its response. Unlike tags, this information requires auditing for correctness, as its fidelity can directly impact certain evaluation metrics.\n\n2.1.5   When to Use Which Data Curation Approach\n\nBenchmark, human-annotated, and synthetically-generated datasets should complement one another. Starting with benchmarks or human-annotated data points, evaluators can easily generate more synthetic data points from there to glean directional insights on the task at hand. This bootstrapped dataset can be iteratively improved over time to meet the 5 D\u2019s, effectively turning a silver dataset into a golden one."
        },
        {
            "text": "**Increasing Diversity:** To ensure variety in the generated data, strategies include prompting the LLM to adopt different personas or using sampling techniques like adjusting the model\u2019s temperature or employing nucleus sampling (top-p) (Holtzman et al., 2019).",
            "page": 3,
            "x": 68,
            "y": 213,
            "width": 224,
            "height": 80,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-amassing",
            "chunk_id": "b83aa613-9a19-47a7-9a81-ae1c5fa085e2",
            "group_text": "## 2.1 Amassing Evaluation Datasets\n\nThe process of collecting or creating evaluation datasets that adhere to the 5 D\u2019s can be broken down into three high-level approaches: benchmark analysis, _golden_ human annotated datasets, and synthetically-generated _silver_ datasets.\n\n### 2.1.1 Benchmark Analysis\nPublicly available benchmarks (e.g., (Pustejovsky et al., 2023; Hendrycks et al., 2020; Guo et al., 2023; Alam et al., 2024)) offer quick initial insights but often lack use-case specificity, may be contaminated with training data, and require careful auditing for quality and licensing.\n\n2.1.2   Human-Annotated Golden Datasets\nHuman annotation is preferable for tasks requir-\ning specialized knowledge not found in founda-\ntion models, as it can reveal subtleties in what con-\nstitutes a high-quality response (e.g., correctness,\nusefulness, completeness). However, this pro-\ncess is often difficult, costly, and time-consuming,\nrisking low-quality or biased annotations due to\nthe need for deep domain expertise and team co-\nordination. Common collection workflows in-\nclude leveraging in-house Subject Matter Experts\n(SMEs), outsourcing to third parties, employing\nuser data and feedback, or gathering prompts\nthrough User Experience Research (UXR) sur-\nveys. These methods require clear instructions\nto ensure data quality, and practitioners should\nbe aware that user data may have privacy con-\nstraints and that feedback tends to be biased to-\nward negative experiences. To streamline these\nworkflows and improve the quality of the final\n\"golden\" datasets, specialized data labeling and\nannotation platforms (Labelbox, Inc., 2024; Ama-\nzon Web Services, Inc., 2024; Montani and Honni-\nbal, 2018; Nakayama et al., 2018) can help enforce\nconsistency and manage annotator agreement.\n\n### 2.1.3  Synthetically-Generated Silver Datasets\nSynthetic \"silver\" datasets offer a scalable, cost-effective alternative to human annotation that can bypass potential legal and privacy issues. This approach, however, requires careful human oversight to maintain data quality, minimize bias, and ensure the data is decontaminated from the model\u2019s original training set. Common generation techniques are as follows.\n\n**Distilling from Frontier Models:** This involves using powerful foundation models to generate prompt-response pairs. Advanced methods (Wang et al., 2022; Wei et al., 2022; Taori et al., 2023; Chiang et al., 2023; Chase, 2022; Liu, 2023; Es et al., 2023) can be used to guide this process, such as having the model self-critique its responses based on a set of principles (Constitutional AI) (Bai et al., 2022b) or iteratively making prompts more complex or diverse (Evol-Instruct) (Bai et al., 2022a).\n\n**Increasing Diversity:** To ensure variety in the generated data, strategies include prompting the LLM to adopt different personas or using sampling techniques like adjusting the model\u2019s temperature or employing nucleus sampling (top-p) (Holtzman et al., 2019).\n\n**Leveraging Automatic Data Scraping Pipelines:** Leveraging automated data scraping pipelines is a common method for amassing large volumes of domain-specific text from the internet or internal repositories. This process can be implemented using a range of tools, from building custom crawlers with open-source Python frameworks like Scrapy (Scrapy Developers, 2024) and Beautiful Soup (Richardson, 2024), to using browser automation libraries like Selenium (Selenium Project, 2024) for dynamic websites, or employing commercial platforms for managed scraping. However, it is difficult to guarantee that data curated in this fashion is fully decontaminated. As mentioned earlier, it is critical to rigorously filter these web-based datasets. Various filtering methods can be applied, ranging from manual inspection to more scalable approaches. For instance, libraries like Pandas (McKinney et al., 2011) can be used to remove texts with low token counts, while LLMs or other lighter-weight machine learning models (Wolf et al., 2020) can be trained to identify and discard low-quality or irrelevant content.\n\n2.1.4 Curating Evaluation Dataset Metadata  \nBeyond prompts and responses, metadata can be curated for each data point.  \n\u2003\u2003Tags: Descriptive text categories that can be manually or automatically applied to prompt/response pairs to measure and monitor dataset diversity.\n\n**Grounding Information:** Supporting context (e.g., a relevant news article) can be provided to an evaluation, particularly an autorater, to help it ac-\n\ncurately judge factuality on topics that may be too\nrecent or outside its training data. This informa-\ntion should not be sourced from the system being\nevaluated.\n\n**Expected Information:** Specific terms, steps, or fields that the model is required to recall in its response. Unlike tags, this information requires auditing for correctness, as its fidelity can directly impact certain evaluation metrics.\n\n2.1.5   When to Use Which Data Curation Approach\n\nBenchmark, human-annotated, and synthetically-generated datasets should complement one another. Starting with benchmarks or human-annotated data points, evaluators can easily generate more synthetic data points from there to glean directional insights on the task at hand. This bootstrapped dataset can be iteratively improved over time to meet the 5 D\u2019s, effectively turning a silver dataset into a golden one."
        },
        {
            "text": "**Leveraging Automatic Data Scraping Pipelines:** Leveraging automated data scraping pipelines is a common method for amassing large volumes of domain-specific text from the internet or internal repositories. This process can be implemented using a range of tools, from building custom crawlers with open-source Python frameworks like Scrapy (Scrapy Developers, 2024) and Beautiful Soup (Richardson, 2024), to using browser automation libraries like Selenium (Selenium Project, 2024) for dynamic websites, or employing commercial platforms for managed scraping. However, it is difficult to guarantee that data curated in this fashion is fully decontaminated. As mentioned earlier, it is critical to rigorously filter these web-based datasets. Various filtering methods can be applied, ranging from manual inspection to more scalable approaches. For instance, libraries like Pandas (McKinney et al., 2011) can be used to remove texts with low token counts, while LLMs or other lighter-weight machine learning models (Wolf et al., 2020) can be trained to identify and discard low-quality or irrelevant content.",
            "page": 3,
            "x": 68,
            "y": 295,
            "width": 226,
            "height": 325,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-amassing",
            "chunk_id": "c17005da-197e-42f1-9613-dfb23954ef62",
            "group_text": "## 2.1 Amassing Evaluation Datasets\n\nThe process of collecting or creating evaluation datasets that adhere to the 5 D\u2019s can be broken down into three high-level approaches: benchmark analysis, _golden_ human annotated datasets, and synthetically-generated _silver_ datasets.\n\n### 2.1.1 Benchmark Analysis\nPublicly available benchmarks (e.g., (Pustejovsky et al., 2023; Hendrycks et al., 2020; Guo et al., 2023; Alam et al., 2024)) offer quick initial insights but often lack use-case specificity, may be contaminated with training data, and require careful auditing for quality and licensing.\n\n2.1.2   Human-Annotated Golden Datasets\nHuman annotation is preferable for tasks requir-\ning specialized knowledge not found in founda-\ntion models, as it can reveal subtleties in what con-\nstitutes a high-quality response (e.g., correctness,\nusefulness, completeness). However, this pro-\ncess is often difficult, costly, and time-consuming,\nrisking low-quality or biased annotations due to\nthe need for deep domain expertise and team co-\nordination. Common collection workflows in-\nclude leveraging in-house Subject Matter Experts\n(SMEs), outsourcing to third parties, employing\nuser data and feedback, or gathering prompts\nthrough User Experience Research (UXR) sur-\nveys. These methods require clear instructions\nto ensure data quality, and practitioners should\nbe aware that user data may have privacy con-\nstraints and that feedback tends to be biased to-\nward negative experiences. To streamline these\nworkflows and improve the quality of the final\n\"golden\" datasets, specialized data labeling and\nannotation platforms (Labelbox, Inc., 2024; Ama-\nzon Web Services, Inc., 2024; Montani and Honni-\nbal, 2018; Nakayama et al., 2018) can help enforce\nconsistency and manage annotator agreement.\n\n### 2.1.3  Synthetically-Generated Silver Datasets\nSynthetic \"silver\" datasets offer a scalable, cost-effective alternative to human annotation that can bypass potential legal and privacy issues. This approach, however, requires careful human oversight to maintain data quality, minimize bias, and ensure the data is decontaminated from the model\u2019s original training set. Common generation techniques are as follows.\n\n**Distilling from Frontier Models:** This involves using powerful foundation models to generate prompt-response pairs. Advanced methods (Wang et al., 2022; Wei et al., 2022; Taori et al., 2023; Chiang et al., 2023; Chase, 2022; Liu, 2023; Es et al., 2023) can be used to guide this process, such as having the model self-critique its responses based on a set of principles (Constitutional AI) (Bai et al., 2022b) or iteratively making prompts more complex or diverse (Evol-Instruct) (Bai et al., 2022a).\n\n**Increasing Diversity:** To ensure variety in the generated data, strategies include prompting the LLM to adopt different personas or using sampling techniques like adjusting the model\u2019s temperature or employing nucleus sampling (top-p) (Holtzman et al., 2019).\n\n**Leveraging Automatic Data Scraping Pipelines:** Leveraging automated data scraping pipelines is a common method for amassing large volumes of domain-specific text from the internet or internal repositories. This process can be implemented using a range of tools, from building custom crawlers with open-source Python frameworks like Scrapy (Scrapy Developers, 2024) and Beautiful Soup (Richardson, 2024), to using browser automation libraries like Selenium (Selenium Project, 2024) for dynamic websites, or employing commercial platforms for managed scraping. However, it is difficult to guarantee that data curated in this fashion is fully decontaminated. As mentioned earlier, it is critical to rigorously filter these web-based datasets. Various filtering methods can be applied, ranging from manual inspection to more scalable approaches. For instance, libraries like Pandas (McKinney et al., 2011) can be used to remove texts with low token counts, while LLMs or other lighter-weight machine learning models (Wolf et al., 2020) can be trained to identify and discard low-quality or irrelevant content.\n\n2.1.4 Curating Evaluation Dataset Metadata  \nBeyond prompts and responses, metadata can be curated for each data point.  \n\u2003\u2003Tags: Descriptive text categories that can be manually or automatically applied to prompt/response pairs to measure and monitor dataset diversity.\n\n**Grounding Information:** Supporting context (e.g., a relevant news article) can be provided to an evaluation, particularly an autorater, to help it ac-\n\ncurately judge factuality on topics that may be too\nrecent or outside its training data. This informa-\ntion should not be sourced from the system being\nevaluated.\n\n**Expected Information:** Specific terms, steps, or fields that the model is required to recall in its response. Unlike tags, this information requires auditing for correctness, as its fidelity can directly impact certain evaluation metrics.\n\n2.1.5   When to Use Which Data Curation Approach\n\nBenchmark, human-annotated, and synthetically-generated datasets should complement one another. Starting with benchmarks or human-annotated data points, evaluators can easily generate more synthetic data points from there to glean directional insights on the task at hand. This bootstrapped dataset can be iteratively improved over time to meet the 5 D\u2019s, effectively turning a silver dataset into a golden one."
        },
        {
            "text": "2.1.4 Curating Evaluation Dataset Metadata  \nBeyond prompts and responses, metadata can be curated for each data point.  \n\u2003\u2003Tags: Descriptive text categories that can be manually or automatically applied to prompt/response pairs to measure and monitor dataset diversity.",
            "page": 3,
            "x": 68,
            "y": 625,
            "width": 225,
            "height": 100,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-amassing",
            "chunk_id": "7e5dac5b-689f-481b-b551-49efb78e46b8",
            "group_text": "## 2.1 Amassing Evaluation Datasets\n\nThe process of collecting or creating evaluation datasets that adhere to the 5 D\u2019s can be broken down into three high-level approaches: benchmark analysis, _golden_ human annotated datasets, and synthetically-generated _silver_ datasets.\n\n### 2.1.1 Benchmark Analysis\nPublicly available benchmarks (e.g., (Pustejovsky et al., 2023; Hendrycks et al., 2020; Guo et al., 2023; Alam et al., 2024)) offer quick initial insights but often lack use-case specificity, may be contaminated with training data, and require careful auditing for quality and licensing.\n\n2.1.2   Human-Annotated Golden Datasets\nHuman annotation is preferable for tasks requir-\ning specialized knowledge not found in founda-\ntion models, as it can reveal subtleties in what con-\nstitutes a high-quality response (e.g., correctness,\nusefulness, completeness). However, this pro-\ncess is often difficult, costly, and time-consuming,\nrisking low-quality or biased annotations due to\nthe need for deep domain expertise and team co-\nordination. Common collection workflows in-\nclude leveraging in-house Subject Matter Experts\n(SMEs), outsourcing to third parties, employing\nuser data and feedback, or gathering prompts\nthrough User Experience Research (UXR) sur-\nveys. These methods require clear instructions\nto ensure data quality, and practitioners should\nbe aware that user data may have privacy con-\nstraints and that feedback tends to be biased to-\nward negative experiences. To streamline these\nworkflows and improve the quality of the final\n\"golden\" datasets, specialized data labeling and\nannotation platforms (Labelbox, Inc., 2024; Ama-\nzon Web Services, Inc., 2024; Montani and Honni-\nbal, 2018; Nakayama et al., 2018) can help enforce\nconsistency and manage annotator agreement.\n\n### 2.1.3  Synthetically-Generated Silver Datasets\nSynthetic \"silver\" datasets offer a scalable, cost-effective alternative to human annotation that can bypass potential legal and privacy issues. This approach, however, requires careful human oversight to maintain data quality, minimize bias, and ensure the data is decontaminated from the model\u2019s original training set. Common generation techniques are as follows.\n\n**Distilling from Frontier Models:** This involves using powerful foundation models to generate prompt-response pairs. Advanced methods (Wang et al., 2022; Wei et al., 2022; Taori et al., 2023; Chiang et al., 2023; Chase, 2022; Liu, 2023; Es et al., 2023) can be used to guide this process, such as having the model self-critique its responses based on a set of principles (Constitutional AI) (Bai et al., 2022b) or iteratively making prompts more complex or diverse (Evol-Instruct) (Bai et al., 2022a).\n\n**Increasing Diversity:** To ensure variety in the generated data, strategies include prompting the LLM to adopt different personas or using sampling techniques like adjusting the model\u2019s temperature or employing nucleus sampling (top-p) (Holtzman et al., 2019).\n\n**Leveraging Automatic Data Scraping Pipelines:** Leveraging automated data scraping pipelines is a common method for amassing large volumes of domain-specific text from the internet or internal repositories. This process can be implemented using a range of tools, from building custom crawlers with open-source Python frameworks like Scrapy (Scrapy Developers, 2024) and Beautiful Soup (Richardson, 2024), to using browser automation libraries like Selenium (Selenium Project, 2024) for dynamic websites, or employing commercial platforms for managed scraping. However, it is difficult to guarantee that data curated in this fashion is fully decontaminated. As mentioned earlier, it is critical to rigorously filter these web-based datasets. Various filtering methods can be applied, ranging from manual inspection to more scalable approaches. For instance, libraries like Pandas (McKinney et al., 2011) can be used to remove texts with low token counts, while LLMs or other lighter-weight machine learning models (Wolf et al., 2020) can be trained to identify and discard low-quality or irrelevant content.\n\n2.1.4 Curating Evaluation Dataset Metadata  \nBeyond prompts and responses, metadata can be curated for each data point.  \n\u2003\u2003Tags: Descriptive text categories that can be manually or automatically applied to prompt/response pairs to measure and monitor dataset diversity.\n\n**Grounding Information:** Supporting context (e.g., a relevant news article) can be provided to an evaluation, particularly an autorater, to help it ac-\n\ncurately judge factuality on topics that may be too\nrecent or outside its training data. This informa-\ntion should not be sourced from the system being\nevaluated.\n\n**Expected Information:** Specific terms, steps, or fields that the model is required to recall in its response. Unlike tags, this information requires auditing for correctness, as its fidelity can directly impact certain evaluation metrics.\n\n2.1.5   When to Use Which Data Curation Approach\n\nBenchmark, human-annotated, and synthetically-generated datasets should complement one another. Starting with benchmarks or human-annotated data points, evaluators can easily generate more synthetic data points from there to glean directional insights on the task at hand. This bootstrapped dataset can be iteratively improved over time to meet the 5 D\u2019s, effectively turning a silver dataset into a golden one."
        },
        {
            "text": "**Grounding Information:** Supporting context (e.g., a relevant news article) can be provided to an evaluation, particularly an autorater, to help it ac-",
            "page": 3,
            "x": 68,
            "y": 726,
            "width": 225,
            "height": 41,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-amassing",
            "chunk_id": "3a00a585-183a-49c1-9066-3bdea8e8f547",
            "group_text": "## 2.1 Amassing Evaluation Datasets\n\nThe process of collecting or creating evaluation datasets that adhere to the 5 D\u2019s can be broken down into three high-level approaches: benchmark analysis, _golden_ human annotated datasets, and synthetically-generated _silver_ datasets.\n\n### 2.1.1 Benchmark Analysis\nPublicly available benchmarks (e.g., (Pustejovsky et al., 2023; Hendrycks et al., 2020; Guo et al., 2023; Alam et al., 2024)) offer quick initial insights but often lack use-case specificity, may be contaminated with training data, and require careful auditing for quality and licensing.\n\n2.1.2   Human-Annotated Golden Datasets\nHuman annotation is preferable for tasks requir-\ning specialized knowledge not found in founda-\ntion models, as it can reveal subtleties in what con-\nstitutes a high-quality response (e.g., correctness,\nusefulness, completeness). However, this pro-\ncess is often difficult, costly, and time-consuming,\nrisking low-quality or biased annotations due to\nthe need for deep domain expertise and team co-\nordination. Common collection workflows in-\nclude leveraging in-house Subject Matter Experts\n(SMEs), outsourcing to third parties, employing\nuser data and feedback, or gathering prompts\nthrough User Experience Research (UXR) sur-\nveys. These methods require clear instructions\nto ensure data quality, and practitioners should\nbe aware that user data may have privacy con-\nstraints and that feedback tends to be biased to-\nward negative experiences. To streamline these\nworkflows and improve the quality of the final\n\"golden\" datasets, specialized data labeling and\nannotation platforms (Labelbox, Inc., 2024; Ama-\nzon Web Services, Inc., 2024; Montani and Honni-\nbal, 2018; Nakayama et al., 2018) can help enforce\nconsistency and manage annotator agreement.\n\n### 2.1.3  Synthetically-Generated Silver Datasets\nSynthetic \"silver\" datasets offer a scalable, cost-effective alternative to human annotation that can bypass potential legal and privacy issues. This approach, however, requires careful human oversight to maintain data quality, minimize bias, and ensure the data is decontaminated from the model\u2019s original training set. Common generation techniques are as follows.\n\n**Distilling from Frontier Models:** This involves using powerful foundation models to generate prompt-response pairs. Advanced methods (Wang et al., 2022; Wei et al., 2022; Taori et al., 2023; Chiang et al., 2023; Chase, 2022; Liu, 2023; Es et al., 2023) can be used to guide this process, such as having the model self-critique its responses based on a set of principles (Constitutional AI) (Bai et al., 2022b) or iteratively making prompts more complex or diverse (Evol-Instruct) (Bai et al., 2022a).\n\n**Increasing Diversity:** To ensure variety in the generated data, strategies include prompting the LLM to adopt different personas or using sampling techniques like adjusting the model\u2019s temperature or employing nucleus sampling (top-p) (Holtzman et al., 2019).\n\n**Leveraging Automatic Data Scraping Pipelines:** Leveraging automated data scraping pipelines is a common method for amassing large volumes of domain-specific text from the internet or internal repositories. This process can be implemented using a range of tools, from building custom crawlers with open-source Python frameworks like Scrapy (Scrapy Developers, 2024) and Beautiful Soup (Richardson, 2024), to using browser automation libraries like Selenium (Selenium Project, 2024) for dynamic websites, or employing commercial platforms for managed scraping. However, it is difficult to guarantee that data curated in this fashion is fully decontaminated. As mentioned earlier, it is critical to rigorously filter these web-based datasets. Various filtering methods can be applied, ranging from manual inspection to more scalable approaches. For instance, libraries like Pandas (McKinney et al., 2011) can be used to remove texts with low token counts, while LLMs or other lighter-weight machine learning models (Wolf et al., 2020) can be trained to identify and discard low-quality or irrelevant content.\n\n2.1.4 Curating Evaluation Dataset Metadata  \nBeyond prompts and responses, metadata can be curated for each data point.  \n\u2003\u2003Tags: Descriptive text categories that can be manually or automatically applied to prompt/response pairs to measure and monitor dataset diversity.\n\n**Grounding Information:** Supporting context (e.g., a relevant news article) can be provided to an evaluation, particularly an autorater, to help it ac-\n\ncurately judge factuality on topics that may be too\nrecent or outside its training data. This informa-\ntion should not be sourced from the system being\nevaluated.\n\n**Expected Information:** Specific terms, steps, or fields that the model is required to recall in its response. Unlike tags, this information requires auditing for correctness, as its fidelity can directly impact certain evaluation metrics.\n\n2.1.5   When to Use Which Data Curation Approach\n\nBenchmark, human-annotated, and synthetically-generated datasets should complement one another. Starting with benchmarks or human-annotated data points, evaluators can easily generate more synthetic data points from there to glean directional insights on the task at hand. This bootstrapped dataset can be iteratively improved over time to meet the 5 D\u2019s, effectively turning a silver dataset into a golden one."
        },
        {
            "text": "curately judge factuality on topics that may be too\nrecent or outside its training data. This informa-\ntion should not be sourced from the system being\nevaluated.",
            "page": 3,
            "x": 304,
            "y": 64,
            "width": 224,
            "height": 53,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-amassing",
            "chunk_id": "b756f717-b5ee-4b5a-851c-5db00f43463b",
            "group_text": "## 2.1 Amassing Evaluation Datasets\n\nThe process of collecting or creating evaluation datasets that adhere to the 5 D\u2019s can be broken down into three high-level approaches: benchmark analysis, _golden_ human annotated datasets, and synthetically-generated _silver_ datasets.\n\n### 2.1.1 Benchmark Analysis\nPublicly available benchmarks (e.g., (Pustejovsky et al., 2023; Hendrycks et al., 2020; Guo et al., 2023; Alam et al., 2024)) offer quick initial insights but often lack use-case specificity, may be contaminated with training data, and require careful auditing for quality and licensing.\n\n2.1.2   Human-Annotated Golden Datasets\nHuman annotation is preferable for tasks requir-\ning specialized knowledge not found in founda-\ntion models, as it can reveal subtleties in what con-\nstitutes a high-quality response (e.g., correctness,\nusefulness, completeness). However, this pro-\ncess is often difficult, costly, and time-consuming,\nrisking low-quality or biased annotations due to\nthe need for deep domain expertise and team co-\nordination. Common collection workflows in-\nclude leveraging in-house Subject Matter Experts\n(SMEs), outsourcing to third parties, employing\nuser data and feedback, or gathering prompts\nthrough User Experience Research (UXR) sur-\nveys. These methods require clear instructions\nto ensure data quality, and practitioners should\nbe aware that user data may have privacy con-\nstraints and that feedback tends to be biased to-\nward negative experiences. To streamline these\nworkflows and improve the quality of the final\n\"golden\" datasets, specialized data labeling and\nannotation platforms (Labelbox, Inc., 2024; Ama-\nzon Web Services, Inc., 2024; Montani and Honni-\nbal, 2018; Nakayama et al., 2018) can help enforce\nconsistency and manage annotator agreement.\n\n### 2.1.3  Synthetically-Generated Silver Datasets\nSynthetic \"silver\" datasets offer a scalable, cost-effective alternative to human annotation that can bypass potential legal and privacy issues. This approach, however, requires careful human oversight to maintain data quality, minimize bias, and ensure the data is decontaminated from the model\u2019s original training set. Common generation techniques are as follows.\n\n**Distilling from Frontier Models:** This involves using powerful foundation models to generate prompt-response pairs. Advanced methods (Wang et al., 2022; Wei et al., 2022; Taori et al., 2023; Chiang et al., 2023; Chase, 2022; Liu, 2023; Es et al., 2023) can be used to guide this process, such as having the model self-critique its responses based on a set of principles (Constitutional AI) (Bai et al., 2022b) or iteratively making prompts more complex or diverse (Evol-Instruct) (Bai et al., 2022a).\n\n**Increasing Diversity:** To ensure variety in the generated data, strategies include prompting the LLM to adopt different personas or using sampling techniques like adjusting the model\u2019s temperature or employing nucleus sampling (top-p) (Holtzman et al., 2019).\n\n**Leveraging Automatic Data Scraping Pipelines:** Leveraging automated data scraping pipelines is a common method for amassing large volumes of domain-specific text from the internet or internal repositories. This process can be implemented using a range of tools, from building custom crawlers with open-source Python frameworks like Scrapy (Scrapy Developers, 2024) and Beautiful Soup (Richardson, 2024), to using browser automation libraries like Selenium (Selenium Project, 2024) for dynamic websites, or employing commercial platforms for managed scraping. However, it is difficult to guarantee that data curated in this fashion is fully decontaminated. As mentioned earlier, it is critical to rigorously filter these web-based datasets. Various filtering methods can be applied, ranging from manual inspection to more scalable approaches. For instance, libraries like Pandas (McKinney et al., 2011) can be used to remove texts with low token counts, while LLMs or other lighter-weight machine learning models (Wolf et al., 2020) can be trained to identify and discard low-quality or irrelevant content.\n\n2.1.4 Curating Evaluation Dataset Metadata  \nBeyond prompts and responses, metadata can be curated for each data point.  \n\u2003\u2003Tags: Descriptive text categories that can be manually or automatically applied to prompt/response pairs to measure and monitor dataset diversity.\n\n**Grounding Information:** Supporting context (e.g., a relevant news article) can be provided to an evaluation, particularly an autorater, to help it ac-\n\ncurately judge factuality on topics that may be too\nrecent or outside its training data. This informa-\ntion should not be sourced from the system being\nevaluated.\n\n**Expected Information:** Specific terms, steps, or fields that the model is required to recall in its response. Unlike tags, this information requires auditing for correctness, as its fidelity can directly impact certain evaluation metrics.\n\n2.1.5   When to Use Which Data Curation Approach\n\nBenchmark, human-annotated, and synthetically-generated datasets should complement one another. Starting with benchmarks or human-annotated data points, evaluators can easily generate more synthetic data points from there to glean directional insights on the task at hand. This bootstrapped dataset can be iteratively improved over time to meet the 5 D\u2019s, effectively turning a silver dataset into a golden one."
        },
        {
            "text": "**Expected Information:** Specific terms, steps, or fields that the model is required to recall in its response. Unlike tags, this information requires auditing for correctness, as its fidelity can directly impact certain evaluation metrics.",
            "page": 3,
            "x": 304,
            "y": 119,
            "width": 224,
            "height": 68,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-amassing",
            "chunk_id": "cb0e673d-b779-4470-a3e4-4e94a461906e",
            "group_text": "## 2.1 Amassing Evaluation Datasets\n\nThe process of collecting or creating evaluation datasets that adhere to the 5 D\u2019s can be broken down into three high-level approaches: benchmark analysis, _golden_ human annotated datasets, and synthetically-generated _silver_ datasets.\n\n### 2.1.1 Benchmark Analysis\nPublicly available benchmarks (e.g., (Pustejovsky et al., 2023; Hendrycks et al., 2020; Guo et al., 2023; Alam et al., 2024)) offer quick initial insights but often lack use-case specificity, may be contaminated with training data, and require careful auditing for quality and licensing.\n\n2.1.2   Human-Annotated Golden Datasets\nHuman annotation is preferable for tasks requir-\ning specialized knowledge not found in founda-\ntion models, as it can reveal subtleties in what con-\nstitutes a high-quality response (e.g., correctness,\nusefulness, completeness). However, this pro-\ncess is often difficult, costly, and time-consuming,\nrisking low-quality or biased annotations due to\nthe need for deep domain expertise and team co-\nordination. Common collection workflows in-\nclude leveraging in-house Subject Matter Experts\n(SMEs), outsourcing to third parties, employing\nuser data and feedback, or gathering prompts\nthrough User Experience Research (UXR) sur-\nveys. These methods require clear instructions\nto ensure data quality, and practitioners should\nbe aware that user data may have privacy con-\nstraints and that feedback tends to be biased to-\nward negative experiences. To streamline these\nworkflows and improve the quality of the final\n\"golden\" datasets, specialized data labeling and\nannotation platforms (Labelbox, Inc., 2024; Ama-\nzon Web Services, Inc., 2024; Montani and Honni-\nbal, 2018; Nakayama et al., 2018) can help enforce\nconsistency and manage annotator agreement.\n\n### 2.1.3  Synthetically-Generated Silver Datasets\nSynthetic \"silver\" datasets offer a scalable, cost-effective alternative to human annotation that can bypass potential legal and privacy issues. This approach, however, requires careful human oversight to maintain data quality, minimize bias, and ensure the data is decontaminated from the model\u2019s original training set. Common generation techniques are as follows.\n\n**Distilling from Frontier Models:** This involves using powerful foundation models to generate prompt-response pairs. Advanced methods (Wang et al., 2022; Wei et al., 2022; Taori et al., 2023; Chiang et al., 2023; Chase, 2022; Liu, 2023; Es et al., 2023) can be used to guide this process, such as having the model self-critique its responses based on a set of principles (Constitutional AI) (Bai et al., 2022b) or iteratively making prompts more complex or diverse (Evol-Instruct) (Bai et al., 2022a).\n\n**Increasing Diversity:** To ensure variety in the generated data, strategies include prompting the LLM to adopt different personas or using sampling techniques like adjusting the model\u2019s temperature or employing nucleus sampling (top-p) (Holtzman et al., 2019).\n\n**Leveraging Automatic Data Scraping Pipelines:** Leveraging automated data scraping pipelines is a common method for amassing large volumes of domain-specific text from the internet or internal repositories. This process can be implemented using a range of tools, from building custom crawlers with open-source Python frameworks like Scrapy (Scrapy Developers, 2024) and Beautiful Soup (Richardson, 2024), to using browser automation libraries like Selenium (Selenium Project, 2024) for dynamic websites, or employing commercial platforms for managed scraping. However, it is difficult to guarantee that data curated in this fashion is fully decontaminated. As mentioned earlier, it is critical to rigorously filter these web-based datasets. Various filtering methods can be applied, ranging from manual inspection to more scalable approaches. For instance, libraries like Pandas (McKinney et al., 2011) can be used to remove texts with low token counts, while LLMs or other lighter-weight machine learning models (Wolf et al., 2020) can be trained to identify and discard low-quality or irrelevant content.\n\n2.1.4 Curating Evaluation Dataset Metadata  \nBeyond prompts and responses, metadata can be curated for each data point.  \n\u2003\u2003Tags: Descriptive text categories that can be manually or automatically applied to prompt/response pairs to measure and monitor dataset diversity.\n\n**Grounding Information:** Supporting context (e.g., a relevant news article) can be provided to an evaluation, particularly an autorater, to help it ac-\n\ncurately judge factuality on topics that may be too\nrecent or outside its training data. This informa-\ntion should not be sourced from the system being\nevaluated.\n\n**Expected Information:** Specific terms, steps, or fields that the model is required to recall in its response. Unlike tags, this information requires auditing for correctness, as its fidelity can directly impact certain evaluation metrics.\n\n2.1.5   When to Use Which Data Curation Approach\n\nBenchmark, human-annotated, and synthetically-generated datasets should complement one another. Starting with benchmarks or human-annotated data points, evaluators can easily generate more synthetic data points from there to glean directional insights on the task at hand. This bootstrapped dataset can be iteratively improved over time to meet the 5 D\u2019s, effectively turning a silver dataset into a golden one."
        },
        {
            "text": "2.1.5   When to Use Which Data Curation Approach\n\nBenchmark, human-annotated, and synthetically-generated datasets should complement one another. Starting with benchmarks or human-annotated data points, evaluators can easily generate more synthetic data points from there to glean directional insights on the task at hand. This bootstrapped dataset can be iteratively improved over time to meet the 5 D\u2019s, effectively turning a silver dataset into a golden one.",
            "page": 3,
            "x": 303,
            "y": 196,
            "width": 226,
            "height": 156,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "5-amassing",
            "chunk_id": "206976ba-79f2-49d6-8db0-0a672a120a26",
            "group_text": "## 2.1 Amassing Evaluation Datasets\n\nThe process of collecting or creating evaluation datasets that adhere to the 5 D\u2019s can be broken down into three high-level approaches: benchmark analysis, _golden_ human annotated datasets, and synthetically-generated _silver_ datasets.\n\n### 2.1.1 Benchmark Analysis\nPublicly available benchmarks (e.g., (Pustejovsky et al., 2023; Hendrycks et al., 2020; Guo et al., 2023; Alam et al., 2024)) offer quick initial insights but often lack use-case specificity, may be contaminated with training data, and require careful auditing for quality and licensing.\n\n2.1.2   Human-Annotated Golden Datasets\nHuman annotation is preferable for tasks requir-\ning specialized knowledge not found in founda-\ntion models, as it can reveal subtleties in what con-\nstitutes a high-quality response (e.g., correctness,\nusefulness, completeness). However, this pro-\ncess is often difficult, costly, and time-consuming,\nrisking low-quality or biased annotations due to\nthe need for deep domain expertise and team co-\nordination. Common collection workflows in-\nclude leveraging in-house Subject Matter Experts\n(SMEs), outsourcing to third parties, employing\nuser data and feedback, or gathering prompts\nthrough User Experience Research (UXR) sur-\nveys. These methods require clear instructions\nto ensure data quality, and practitioners should\nbe aware that user data may have privacy con-\nstraints and that feedback tends to be biased to-\nward negative experiences. To streamline these\nworkflows and improve the quality of the final\n\"golden\" datasets, specialized data labeling and\nannotation platforms (Labelbox, Inc., 2024; Ama-\nzon Web Services, Inc., 2024; Montani and Honni-\nbal, 2018; Nakayama et al., 2018) can help enforce\nconsistency and manage annotator agreement.\n\n### 2.1.3  Synthetically-Generated Silver Datasets\nSynthetic \"silver\" datasets offer a scalable, cost-effective alternative to human annotation that can bypass potential legal and privacy issues. This approach, however, requires careful human oversight to maintain data quality, minimize bias, and ensure the data is decontaminated from the model\u2019s original training set. Common generation techniques are as follows.\n\n**Distilling from Frontier Models:** This involves using powerful foundation models to generate prompt-response pairs. Advanced methods (Wang et al., 2022; Wei et al., 2022; Taori et al., 2023; Chiang et al., 2023; Chase, 2022; Liu, 2023; Es et al., 2023) can be used to guide this process, such as having the model self-critique its responses based on a set of principles (Constitutional AI) (Bai et al., 2022b) or iteratively making prompts more complex or diverse (Evol-Instruct) (Bai et al., 2022a).\n\n**Increasing Diversity:** To ensure variety in the generated data, strategies include prompting the LLM to adopt different personas or using sampling techniques like adjusting the model\u2019s temperature or employing nucleus sampling (top-p) (Holtzman et al., 2019).\n\n**Leveraging Automatic Data Scraping Pipelines:** Leveraging automated data scraping pipelines is a common method for amassing large volumes of domain-specific text from the internet or internal repositories. This process can be implemented using a range of tools, from building custom crawlers with open-source Python frameworks like Scrapy (Scrapy Developers, 2024) and Beautiful Soup (Richardson, 2024), to using browser automation libraries like Selenium (Selenium Project, 2024) for dynamic websites, or employing commercial platforms for managed scraping. However, it is difficult to guarantee that data curated in this fashion is fully decontaminated. As mentioned earlier, it is critical to rigorously filter these web-based datasets. Various filtering methods can be applied, ranging from manual inspection to more scalable approaches. For instance, libraries like Pandas (McKinney et al., 2011) can be used to remove texts with low token counts, while LLMs or other lighter-weight machine learning models (Wolf et al., 2020) can be trained to identify and discard low-quality or irrelevant content.\n\n2.1.4 Curating Evaluation Dataset Metadata  \nBeyond prompts and responses, metadata can be curated for each data point.  \n\u2003\u2003Tags: Descriptive text categories that can be manually or automatically applied to prompt/response pairs to measure and monitor dataset diversity.\n\n**Grounding Information:** Supporting context (e.g., a relevant news article) can be provided to an evaluation, particularly an autorater, to help it ac-\n\ncurately judge factuality on topics that may be too\nrecent or outside its training data. This informa-\ntion should not be sourced from the system being\nevaluated.\n\n**Expected Information:** Specific terms, steps, or fields that the model is required to recall in its response. Unlike tags, this information requires auditing for correctness, as its fidelity can directly impact certain evaluation metrics.\n\n2.1.5   When to Use Which Data Curation Approach\n\nBenchmark, human-annotated, and synthetically-generated datasets should complement one another. Starting with benchmarks or human-annotated data points, evaluators can easily generate more synthetic data points from there to glean directional insights on the task at hand. This bootstrapped dataset can be iteratively improved over time to meet the 5 D\u2019s, effectively turning a silver dataset into a golden one."
        },
        {
            "text": "### 2.2 Adhering to and Quantifying the 5 D\u2019s\n\nBeyond relying on performance metrics, dataset quality can be assessed by quantitatively measuring adherence to the 5 D\u2019s. Consistently high performance may signal a weak dataset, so it is crucial that the dataset evolves with the metrics. Core strategies for measuring and adhering to each principle include the following.",
            "page": 3,
            "x": 303,
            "y": 362,
            "width": 225,
            "height": 116,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "6-adhering",
            "chunk_id": "d39b0c17-64e5-4273-a3fa-538e759f4abb",
            "group_text": "### 2.2 Adhering to and Quantifying the 5 D\u2019s\n\nBeyond relying on performance metrics, dataset quality can be assessed by quantitatively measuring adherence to the 5 D\u2019s. Consistently high performance may signal a weak dataset, so it is crucial that the dataset evolves with the metrics. Core strategies for measuring and adhering to each principle include the following."
        },
        {
            "text": "1. Demonstrative of Production Usage: The evaluation dataset can be aligned with real-world use by classifying prompts into representative categories, incorporating user-reported bugs and correlating offline metrics with in-product user satisfaction surveys. A mismatch between high metric scores and low user satisfaction indicates the dataset is not representative.",
            "page": 3,
            "x": 304,
            "y": 479,
            "width": 225,
            "height": 108,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-demonstrative",
            "chunk_id": "1335e45a-bd61-49a9-80c7-1b64ff051f02",
            "group_text": "1. Demonstrative of Production Usage: The evaluation dataset can be aligned with real-world use by classifying prompts into representative categories, incorporating user-reported bugs and correlating offline metrics with in-product user satisfaction surveys. A mismatch between high metric scores and low user satisfaction indicates the dataset is not representative.\n\n2. **Diverse:** Diversity can be measured by categorizing data along relevant dimensions (e.g., topic, difficulty), grouping similar prompts (e.g., (e.g., using LSH clustering (Indyk and Motwani, 1998) or embeddings (Rudd et al., 2024))), and using n-gram or embedding-based metrics to quantify the similarity between data points.\n\n3. **Defined Scope:** Defined Scope can be achieved by creating tailored datasets for individual system components (e.g., grounding APIs, tool use) in addition to the end-to-end system. This modular approach, analogous to unit testing, helps isolate the source of failures.\n\n4. **Decontaminated:** Dataset contamination is caused by overlapping training\u00b9 and evaluation data, and can lead to inflated performance metrics and poor end-system design, model selection, and functionality. To prevent this, it is best practice to maintain a separate, uncontaminated evaluation dataset, ideally private and collected after the training cutoff. There are several ways to check for contamination, including continuation testing(Carlini et al., 2021), i.e., checking if the model can reproduce evaluation data from partial prompts or generate exact responses and examining response confidence by inspecting log-probabilities or perplexity (cf. Section 3.5). Assuming access to the training data (not always available), direct content comparisons between the evaluation set and the training set can be performed through exact string matching, substring matching, or hash comparisons. Efficient approaches can be employed for large data (Manber and Myers, 1993; Bloom, 1970)."
        },
        {
            "text": "2. **Diverse:** Diversity can be measured by categorizing data along relevant dimensions (e.g., topic, difficulty), grouping similar prompts (e.g., (e.g., using LSH clustering (Indyk and Motwani, 1998) or embeddings (Rudd et al., 2024))), and using n-gram or embedding-based metrics to quantify the similarity between data points.",
            "page": 3,
            "x": 304,
            "y": 589,
            "width": 225,
            "height": 95,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-demonstrative",
            "chunk_id": "0cc1c012-449c-424c-ab7c-6d803447ca27",
            "group_text": "1. Demonstrative of Production Usage: The evaluation dataset can be aligned with real-world use by classifying prompts into representative categories, incorporating user-reported bugs and correlating offline metrics with in-product user satisfaction surveys. A mismatch between high metric scores and low user satisfaction indicates the dataset is not representative.\n\n2. **Diverse:** Diversity can be measured by categorizing data along relevant dimensions (e.g., topic, difficulty), grouping similar prompts (e.g., (e.g., using LSH clustering (Indyk and Motwani, 1998) or embeddings (Rudd et al., 2024))), and using n-gram or embedding-based metrics to quantify the similarity between data points.\n\n3. **Defined Scope:** Defined Scope can be achieved by creating tailored datasets for individual system components (e.g., grounding APIs, tool use) in addition to the end-to-end system. This modular approach, analogous to unit testing, helps isolate the source of failures.\n\n4. **Decontaminated:** Dataset contamination is caused by overlapping training\u00b9 and evaluation data, and can lead to inflated performance metrics and poor end-system design, model selection, and functionality. To prevent this, it is best practice to maintain a separate, uncontaminated evaluation dataset, ideally private and collected after the training cutoff. There are several ways to check for contamination, including continuation testing(Carlini et al., 2021), i.e., checking if the model can reproduce evaluation data from partial prompts or generate exact responses and examining response confidence by inspecting log-probabilities or perplexity (cf. Section 3.5). Assuming access to the training data (not always available), direct content comparisons between the evaluation set and the training set can be performed through exact string matching, substring matching, or hash comparisons. Efficient approaches can be employed for large data (Manber and Myers, 1993; Bloom, 1970)."
        },
        {
            "text": "3. **Defined Scope:** Defined Scope can be achieved by creating tailored datasets for individual system components (e.g., grounding APIs, tool use) in addition to the end-to-end system. This modular approach, analogous to unit testing, helps isolate the source of failures.",
            "page": 3,
            "x": 304,
            "y": 685,
            "width": 224,
            "height": 81,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-demonstrative",
            "chunk_id": "6f265089-cd4e-47f4-8fcc-2bc50d3d5be5",
            "group_text": "1. Demonstrative of Production Usage: The evaluation dataset can be aligned with real-world use by classifying prompts into representative categories, incorporating user-reported bugs and correlating offline metrics with in-product user satisfaction surveys. A mismatch between high metric scores and low user satisfaction indicates the dataset is not representative.\n\n2. **Diverse:** Diversity can be measured by categorizing data along relevant dimensions (e.g., topic, difficulty), grouping similar prompts (e.g., (e.g., using LSH clustering (Indyk and Motwani, 1998) or embeddings (Rudd et al., 2024))), and using n-gram or embedding-based metrics to quantify the similarity between data points.\n\n3. **Defined Scope:** Defined Scope can be achieved by creating tailored datasets for individual system components (e.g., grounding APIs, tool use) in addition to the end-to-end system. This modular approach, analogous to unit testing, helps isolate the source of failures.\n\n4. **Decontaminated:** Dataset contamination is caused by overlapping training\u00b9 and evaluation data, and can lead to inflated performance metrics and poor end-system design, model selection, and functionality. To prevent this, it is best practice to maintain a separate, uncontaminated evaluation dataset, ideally private and collected after the training cutoff. There are several ways to check for contamination, including continuation testing(Carlini et al., 2021), i.e., checking if the model can reproduce evaluation data from partial prompts or generate exact responses and examining response confidence by inspecting log-probabilities or perplexity (cf. Section 3.5). Assuming access to the training data (not always available), direct content comparisons between the evaluation set and the training set can be performed through exact string matching, substring matching, or hash comparisons. Efficient approaches can be employed for large data (Manber and Myers, 1993; Bloom, 1970)."
        },
        {
            "text": "4. **Decontaminated:** Dataset contamination is caused by overlapping training\u00b9 and evaluation data, and can lead to inflated performance metrics and poor end-system design, model selection, and functionality. To prevent this, it is best practice to maintain a separate, uncontaminated evaluation dataset, ideally private and collected after the training cutoff. There are several ways to check for contamination, including continuation testing(Carlini et al., 2021), i.e., checking if the model can reproduce evaluation data from partial prompts or generate exact responses and examining response confidence by inspecting log-probabilities or perplexity (cf. Section 3.5). Assuming access to the training data (not always available), direct content comparisons between the evaluation set and the training set can be performed through exact string matching, substring matching, or hash comparisons. Efficient approaches can be employed for large data (Manber and Myers, 1993; Bloom, 1970).",
            "page": 4,
            "x": 67,
            "y": 62,
            "width": 226,
            "height": 286,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "7-demonstrative",
            "chunk_id": "2d10d422-30b1-4107-ac01-f62962917669",
            "group_text": "1. Demonstrative of Production Usage: The evaluation dataset can be aligned with real-world use by classifying prompts into representative categories, incorporating user-reported bugs and correlating offline metrics with in-product user satisfaction surveys. A mismatch between high metric scores and low user satisfaction indicates the dataset is not representative.\n\n2. **Diverse:** Diversity can be measured by categorizing data along relevant dimensions (e.g., topic, difficulty), grouping similar prompts (e.g., (e.g., using LSH clustering (Indyk and Motwani, 1998) or embeddings (Rudd et al., 2024))), and using n-gram or embedding-based metrics to quantify the similarity between data points.\n\n3. **Defined Scope:** Defined Scope can be achieved by creating tailored datasets for individual system components (e.g., grounding APIs, tool use) in addition to the end-to-end system. This modular approach, analogous to unit testing, helps isolate the source of failures.\n\n4. **Decontaminated:** Dataset contamination is caused by overlapping training\u00b9 and evaluation data, and can lead to inflated performance metrics and poor end-system design, model selection, and functionality. To prevent this, it is best practice to maintain a separate, uncontaminated evaluation dataset, ideally private and collected after the training cutoff. There are several ways to check for contamination, including continuation testing(Carlini et al., 2021), i.e., checking if the model can reproduce evaluation data from partial prompts or generate exact responses and examining response confidence by inspecting log-probabilities or perplexity (cf. Section 3.5). Assuming access to the training data (not always available), direct content comparisons between the evaluation set and the training set can be performed through exact string matching, substring matching, or hash comparisons. Efficient approaches can be employed for large data (Manber and Myers, 1993; Bloom, 1970)."
        },
        {
            "text": "5. Dynamic: The dataset should be kept current. Implementing a process to regularly audit, update, version control, and add new data points while removing irrelevant or outdated ones, can help ensure that the evaluation remains relevant over time.",
            "page": 4,
            "x": 68,
            "y": 349,
            "width": 226,
            "height": 82,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "8-dynamic",
            "chunk_id": "63c1f0c8-ac2b-4946-b468-08833edb3795",
            "group_text": "5. Dynamic: The dataset should be kept current. Implementing a process to regularly audit, update, version control, and add new data points while removing irrelevant or outdated ones, can help ensure that the evaluation remains relevant over time."
        },
        {
            "text": "2.3   Required Evaluation Set Sample Sizes\n\nDatasets must be large enough to measure representative metrics, while not imposing significant computational or quota costs given the number of evaluations needed. We can derive a rough estimate of minimal evaluation set size $n$ required to estimate the performance of the model and adequately represent outcomes when deployed via standard statistical sample size calculation (Singh and Masuku, 2014).",
            "page": 4,
            "x": 68,
            "y": 439,
            "width": 226,
            "height": 143,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-required",
            "chunk_id": "f6d2d9dc-28bc-47ea-bf2d-ef3c85109a55",
            "group_text": "2.3   Required Evaluation Set Sample Sizes\n\nDatasets must be large enough to measure representative metrics, while not imposing significant computational or quota costs given the number of evaluations needed. We can derive a rough estimate of minimal evaluation set size $n$ required to estimate the performance of the model and adequately represent outcomes when deployed via standard statistical sample size calculation (Singh and Masuku, 2014).\n\n$n = z^2 \\times \\hat{m}(1-\\hat{m})/\\epsilon^2.$\n\nHere, $z$ is the $z$-score for a chosen confidence level, $\\hat{m}$ is the expected metric score (e.g., 0.8 for 80% accuracy), and $\\epsilon$ is the desired margin of error. For example, to achieve 95% confidence ($z$=1.96) with a 5% margin of error for a metric expected to be 80%, approximately 246 samples\n\n\u00b9LLM training can consist of multiple stages including foundation pre-training, continued pre-training, supervised fine-tuning, preference alignment, and others - by *training datasets*, we refer to the data used within any and all of these stages.\n\nwould be required. Notably, the required sample\nsize increases dramatically as the desired margin\nof error decreases."
        },
        {
            "text": "$n = z^2 \\times \\hat{m}(1-\\hat{m})/\\epsilon^2.$",
            "page": 4,
            "x": 122,
            "y": 592,
            "width": 171,
            "height": 22,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-required",
            "chunk_id": "81deffa3-aefc-4c39-93cc-31900b4a6a12",
            "group_text": "2.3   Required Evaluation Set Sample Sizes\n\nDatasets must be large enough to measure representative metrics, while not imposing significant computational or quota costs given the number of evaluations needed. We can derive a rough estimate of minimal evaluation set size $n$ required to estimate the performance of the model and adequately represent outcomes when deployed via standard statistical sample size calculation (Singh and Masuku, 2014).\n\n$n = z^2 \\times \\hat{m}(1-\\hat{m})/\\epsilon^2.$\n\nHere, $z$ is the $z$-score for a chosen confidence level, $\\hat{m}$ is the expected metric score (e.g., 0.8 for 80% accuracy), and $\\epsilon$ is the desired margin of error. For example, to achieve 95% confidence ($z$=1.96) with a 5% margin of error for a metric expected to be 80%, approximately 246 samples\n\n\u00b9LLM training can consist of multiple stages including foundation pre-training, continued pre-training, supervised fine-tuning, preference alignment, and others - by *training datasets*, we refer to the data used within any and all of these stages.\n\nwould be required. Notably, the required sample\nsize increases dramatically as the desired margin\nof error decreases."
        },
        {
            "text": "Here, $z$ is the $z$-score for a chosen confidence level, $\\hat{m}$ is the expected metric score (e.g., 0.8 for 80% accuracy), and $\\epsilon$ is the desired margin of error. For example, to achieve 95% confidence ($z$=1.96) with a 5% margin of error for a metric expected to be 80%, approximately 246 samples",
            "page": 4,
            "x": 68,
            "y": 619,
            "width": 225,
            "height": 84,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-required",
            "chunk_id": "ac9db68c-788f-41f0-8ef9-dbcecf820233",
            "group_text": "2.3   Required Evaluation Set Sample Sizes\n\nDatasets must be large enough to measure representative metrics, while not imposing significant computational or quota costs given the number of evaluations needed. We can derive a rough estimate of minimal evaluation set size $n$ required to estimate the performance of the model and adequately represent outcomes when deployed via standard statistical sample size calculation (Singh and Masuku, 2014).\n\n$n = z^2 \\times \\hat{m}(1-\\hat{m})/\\epsilon^2.$\n\nHere, $z$ is the $z$-score for a chosen confidence level, $\\hat{m}$ is the expected metric score (e.g., 0.8 for 80% accuracy), and $\\epsilon$ is the desired margin of error. For example, to achieve 95% confidence ($z$=1.96) with a 5% margin of error for a metric expected to be 80%, approximately 246 samples\n\n\u00b9LLM training can consist of multiple stages including foundation pre-training, continued pre-training, supervised fine-tuning, preference alignment, and others - by *training datasets*, we refer to the data used within any and all of these stages.\n\nwould be required. Notably, the required sample\nsize increases dramatically as the desired margin\nof error decreases."
        },
        {
            "text": "\u00b9LLM training can consist of multiple stages including foundation pre-training, continued pre-training, supervised fine-tuning, preference alignment, and others - by *training datasets*, we refer to the data used within any and all of these stages.",
            "page": 4,
            "x": 68,
            "y": 709,
            "width": 225,
            "height": 58,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-required",
            "chunk_id": "3dcf5a38-020b-4da7-a329-446758943f8f",
            "group_text": "2.3   Required Evaluation Set Sample Sizes\n\nDatasets must be large enough to measure representative metrics, while not imposing significant computational or quota costs given the number of evaluations needed. We can derive a rough estimate of minimal evaluation set size $n$ required to estimate the performance of the model and adequately represent outcomes when deployed via standard statistical sample size calculation (Singh and Masuku, 2014).\n\n$n = z^2 \\times \\hat{m}(1-\\hat{m})/\\epsilon^2.$\n\nHere, $z$ is the $z$-score for a chosen confidence level, $\\hat{m}$ is the expected metric score (e.g., 0.8 for 80% accuracy), and $\\epsilon$ is the desired margin of error. For example, to achieve 95% confidence ($z$=1.96) with a 5% margin of error for a metric expected to be 80%, approximately 246 samples\n\n\u00b9LLM training can consist of multiple stages including foundation pre-training, continued pre-training, supervised fine-tuning, preference alignment, and others - by *training datasets*, we refer to the data used within any and all of these stages.\n\nwould be required. Notably, the required sample\nsize increases dramatically as the desired margin\nof error decreases."
        },
        {
            "text": "would be required. Notably, the required sample\nsize increases dramatically as the desired margin\nof error decreases.",
            "page": 4,
            "x": 303,
            "y": 62,
            "width": 225,
            "height": 42,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "9-required",
            "chunk_id": "5e6613b7-593c-4c1c-8b5e-8b4232484bbd",
            "group_text": "2.3   Required Evaluation Set Sample Sizes\n\nDatasets must be large enough to measure representative metrics, while not imposing significant computational or quota costs given the number of evaluations needed. We can derive a rough estimate of minimal evaluation set size $n$ required to estimate the performance of the model and adequately represent outcomes when deployed via standard statistical sample size calculation (Singh and Masuku, 2014).\n\n$n = z^2 \\times \\hat{m}(1-\\hat{m})/\\epsilon^2.$\n\nHere, $z$ is the $z$-score for a chosen confidence level, $\\hat{m}$ is the expected metric score (e.g., 0.8 for 80% accuracy), and $\\epsilon$ is the desired margin of error. For example, to achieve 95% confidence ($z$=1.96) with a 5% margin of error for a metric expected to be 80%, approximately 246 samples\n\n\u00b9LLM training can consist of multiple stages including foundation pre-training, continued pre-training, supervised fine-tuning, preference alignment, and others - by *training datasets*, we refer to the data used within any and all of these stages.\n\nwould be required. Notably, the required sample\nsize increases dramatically as the desired margin\nof error decreases."
        },
        {
            "text": "2.4   Wrap-Up: Connecting Datasets to the Evaluation Framework\n\nThe formulation of evaluation datasets serves as the first foundational pillar in the proposed evaluation framework. The approaches detailed in this section \u2013\u2013 from using public benchmarks to creating human-annotated or synthetically-generated datasets \u2013\u2013 are all tools to achieve the core principle of a dataset that is demonstrative, diverse, decontaminated, dynamic, and defined in scope. A well-curated dataset, tailored to the specific goals that go into the system (Defined Scope and Objectives in Figure 1), is a prerequisite for the subsequent selection of meaningful metrics, forming the empirical bedrock upon which the entire evaluation rests.",
            "page": 4,
            "x": 302,
            "y": 112,
            "width": 227,
            "height": 225,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "10-wrap",
            "chunk_id": "040f6a47-6ece-477a-a315-97d985556d15",
            "group_text": "2.4   Wrap-Up: Connecting Datasets to the Evaluation Framework\n\nThe formulation of evaluation datasets serves as the first foundational pillar in the proposed evaluation framework. The approaches detailed in this section \u2013\u2013 from using public benchmarks to creating human-annotated or synthetically-generated datasets \u2013\u2013 are all tools to achieve the core principle of a dataset that is demonstrative, diverse, decontaminated, dynamic, and defined in scope. A well-curated dataset, tailored to the specific goals that go into the system (Defined Scope and Objectives in Figure 1), is a prerequisite for the subsequent selection of meaningful metrics, forming the empirical bedrock upon which the entire evaluation rests."
        },
        {
            "text": "### 3 Metrics Selection\n\nEvaluation metrics, like datasets, should align with the evaluation\u2019s scope and objectives. Moreover, datasets must contain proper data to support metric calculation, e.g., some metrics require ground truth responses while others do not. Different metrics are better suited for assessing specific qualities, such as factuality, fluency, and summarization/translation quality. Since there is no single \"silver bullet\" metric, multiple metrics should be tracked to form a holistic understanding of system performance. The choice of metrics also depends on the dataset, as some require ground-truth references while others do not.",
            "page": 4,
            "x": 303,
            "y": 346,
            "width": 227,
            "height": 203,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "11-metrics",
            "chunk_id": "2399b6f7-7d2b-40f9-9e01-96fa9e665c71",
            "group_text": "### 3 Metrics Selection\n\nEvaluation metrics, like datasets, should align with the evaluation\u2019s scope and objectives. Moreover, datasets must contain proper data to support metric calculation, e.g., some metrics require ground truth responses while others do not. Different metrics are better suited for assessing specific qualities, such as factuality, fluency, and summarization/translation quality. Since there is no single \"silver bullet\" metric, multiple metrics should be tracked to form a holistic understanding of system performance. The choice of metrics also depends on the dataset, as some require ground-truth references while others do not."
        },
        {
            "text": "3.1   Term Overlap Metrics\n\nTerm overlap metrics, have become de-facto standards throughout the literature. These compare generated text to golden reference samples. They are simple, efficient, and language-agnostic, but have limitations. First, they often miss deeper semantic meaning and context and can yield substantially different scores when encountering paraphrases with identical semantics. Second, they are bad at addressing fluency, factuality, and structure. Finally they are sensitive to length and repetition. Thus, we recommend that metrics be aggregated over multiple diverse, well-written ground truth examples capturing the same semantics for each LLM response.",
            "page": 4,
            "x": 303,
            "y": 556,
            "width": 226,
            "height": 211,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-term",
            "chunk_id": "aecbc184-2054-46a6-9ab0-45b0dc7ae1db",
            "group_text": "3.1   Term Overlap Metrics\n\nTerm overlap metrics, have become de-facto standards throughout the literature. These compare generated text to golden reference samples. They are simple, efficient, and language-agnostic, but have limitations. First, they often miss deeper semantic meaning and context and can yield substantially different scores when encountering paraphrases with identical semantics. Second, they are bad at addressing fluency, factuality, and structure. Finally they are sensitive to length and repetition. Thus, we recommend that metrics be aggregated over multiple diverse, well-written ground truth examples capturing the same semantics for each LLM response.\n\n### 3.1.1   ROUGE Metrics for Summarization\n\nRecall-Oriented Understudy for Gisting Evaluation (ROUGE) ([Lin, 2004; Lin and Hovy, 2003](Lin, 2004; Lin and Hovy, 2003)) is a set of metrics which assesses the quality of automatically generated summaries by comparing overlapping units of text, like words, phrases, *n*-grams (ROUGE-*n*), longest common subsequence (ROUGE-L), and skip-bigrams (ROUGE-S) between a generated summary and a human-written reference summary. When computed based on recall, ROUGE scores emphasize capturing relevant information from the reference, but they might not penalize a summary for being too long. Precision based ROUGE, on the other hand, favors concise summaries, even if they miss some relevant details. ROUGE can also be computed based on $F_{\\beta}$-Measure, weighting respective contributions between precision and recall according\n\nto $\\beta$. ROUGE scores range from 0 to 1.$^2$\n    ROUGE scores aim to evaluate summarization quality, e.g., when an LLM is tasked with summarizing reports, logs, or briefs into shorter/different versions and there are golden samples of specific length and style with specific meaningful terms. ROUGE scores are also useful when answering questions where the golden response contains multiple key terms and phrases that are considered mandatory for a \"good\" response.\n\n**Limitations:** ROUGE scores often capture only surface-level word/phrase similarity, but do not capture nuanced differences in meaning or context. This precludes accurate evaluation for factuality, and high scores can be achieved based on syntactic term overlap rather than underly-\n\n\u00b2In practice, across a large dataset it is unlikely to have scores of 0 or 1 for either ROUGE or BLEU and often indicates an implementation error.\n\ning semantics, limiting utility in evaluating Q/A\nand generation tasks that require semantic accu-\nracy. Consider Candidate A from Table 1. Be-\ncause it shares most of its n-grams (like \"Apollo\n11 mission\", \"primary purpose\", \"astronauts Neil\nArmstrong and Buzz Aldrin\") with the Golden\nResponse, it would achieve a deceptively high\nROUGE score despite not capturing factual accu-\nracy. Conversely, ROUGE is sensitive to para-\nphrasing: Candidate B is factually correct, but\nwould receive a lower ROUGE score than Can-\ndidate A because it uses synonyms (\"main goal\"\nvs. \"primary purpose\", \"moon rocks\" vs. \"lunar\nsamples\"). This demonstrates ROUGE\u2019s limita-\ntions in handling valid paraphrasing, often reward-\ning syntax over substance, and highlights the need\nfor multiple golden reference examples. In con-\ntexts where responses involve generation of code\nand scripts, ROUGE scores may also err on the\nside of syntax over substance.\n\n3.1.2   BLEU for Translation  \nBilingual Evaluation Understudy (BLEU) (Papineni et al., 2002) is a set of metrics which measure translation quality, comparing a generated translation to a golden reference translation, measuring correspondence between $n$-grams, while penalizing brevity so that shorter translations are not over-rewarded. The brevity penalty is calculated based on the length of the candidate translation and the length of the reference translation. BLEU combines precision scores over several $n$-gram sizes with weights for each $n$-gram size. The larger $n$, the better ordering is taken into account within the BLEU score. Typically several values of $n$ are used. Like ROUGE, BLEU scores range from 0 to 1, with 1 indicating perfect match between translation and reference and 0 indicating no match.\n\nBLEU scores were designed for evaluating translations, and something akin to BLEU may serve as a first order metric for generating code or domain-specific queries from natural language, but BLEU has limitations for these tasks: it can assess syntax and structure but cannot assess underlying functionality, logical correctness, or semantic meaning of generated code, nor does it deal well with comments/documentation. BLEU can also be used for evaluating generations that are short, where the emphasis is on precision. ROUGE should be chosen over BLEU for summarizing sequences, as BLEU may penalize valid rephrasing; ROUGE can offer insight into struc-\n\nture/content and in a recall regime does not penal-\nize valid rephrasing.\n\nMany of the aforementioned limitations of ROUGE extend to BLEU, as both of these are term-overlap metrics. However, with all term-overlap metrics, there is an inherent tradeoff between precision-based metrics and recall-based metrics. Consider Candidate C from Table 1. A recall-oriented metric like ROUGE-L might score this reasonably well for capturing the main event, but a precision-focused metric like BLEU would penalize it for its brevity.\n\nThere are several spinoffs of BLEU. Sacre-BLEU (Post, 2018) aims to standardize tokenization for BLEU. NIST (Doddington, 2002) modifies the brevity penalty by reducing the influence of low frequency $n$-grams on the final score, and weights $n$-grams according to their information value so that the evaluation is not skewed by over emphasis of common $n$-grams. METEOR (Banerjee and Lavie, 2005) uses WordNet to accommodate variations in word forms and synonyms, utilizes both precision and recall, and introduces a fragmentation penalty to preserve word order.\n\n3.1.3   Selection of Relevant Keywords\nOne way to address the aforementioned \"syntax\nover substance\" errors (cf. Sec. 3.1.1) is to com-\npute term-overlap metrics between the candidate\nsummary and selected terms, rather than com-\nputing them over entire candidate responses and\ngolden reference responses. While keyword selec-\ntion may improve evaluation it can also be brit-\ntle; e.g., repeating a question may result in good\nscores, even if the substance of the question is not\naddressed. It is therefore important to be thought-\nful about when to use keyword selection, one's\nchoice of keywords, and what other metrics to\ncombine it with."
        },
        {
            "text": "### 3.1.1   ROUGE Metrics for Summarization\n\nRecall-Oriented Understudy for Gisting Evaluation (ROUGE) ([Lin, 2004; Lin and Hovy, 2003](Lin, 2004; Lin and Hovy, 2003)) is a set of metrics which assesses the quality of automatically generated summaries by comparing overlapping units of text, like words, phrases, *n*-grams (ROUGE-*n*), longest common subsequence (ROUGE-L), and skip-bigrams (ROUGE-S) between a generated summary and a human-written reference summary. When computed based on recall, ROUGE scores emphasize capturing relevant information from the reference, but they might not penalize a summary for being too long. Precision based ROUGE, on the other hand, favors concise summaries, even if they miss some relevant details. ROUGE can also be computed based on $F_{\\beta}$-Measure, weighting respective contributions between precision and recall according",
            "page": 5,
            "x": 67,
            "y": 497,
            "width": 226,
            "height": 271,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-term",
            "chunk_id": "c01e5a45-b1a9-4a7a-8dff-f968d39ff352",
            "group_text": "3.1   Term Overlap Metrics\n\nTerm overlap metrics, have become de-facto standards throughout the literature. These compare generated text to golden reference samples. They are simple, efficient, and language-agnostic, but have limitations. First, they often miss deeper semantic meaning and context and can yield substantially different scores when encountering paraphrases with identical semantics. Second, they are bad at addressing fluency, factuality, and structure. Finally they are sensitive to length and repetition. Thus, we recommend that metrics be aggregated over multiple diverse, well-written ground truth examples capturing the same semantics for each LLM response.\n\n### 3.1.1   ROUGE Metrics for Summarization\n\nRecall-Oriented Understudy for Gisting Evaluation (ROUGE) ([Lin, 2004; Lin and Hovy, 2003](Lin, 2004; Lin and Hovy, 2003)) is a set of metrics which assesses the quality of automatically generated summaries by comparing overlapping units of text, like words, phrases, *n*-grams (ROUGE-*n*), longest common subsequence (ROUGE-L), and skip-bigrams (ROUGE-S) between a generated summary and a human-written reference summary. When computed based on recall, ROUGE scores emphasize capturing relevant information from the reference, but they might not penalize a summary for being too long. Precision based ROUGE, on the other hand, favors concise summaries, even if they miss some relevant details. ROUGE can also be computed based on $F_{\\beta}$-Measure, weighting respective contributions between precision and recall according\n\nto $\\beta$. ROUGE scores range from 0 to 1.$^2$\n    ROUGE scores aim to evaluate summarization quality, e.g., when an LLM is tasked with summarizing reports, logs, or briefs into shorter/different versions and there are golden samples of specific length and style with specific meaningful terms. ROUGE scores are also useful when answering questions where the golden response contains multiple key terms and phrases that are considered mandatory for a \"good\" response.\n\n**Limitations:** ROUGE scores often capture only surface-level word/phrase similarity, but do not capture nuanced differences in meaning or context. This precludes accurate evaluation for factuality, and high scores can be achieved based on syntactic term overlap rather than underly-\n\n\u00b2In practice, across a large dataset it is unlikely to have scores of 0 or 1 for either ROUGE or BLEU and often indicates an implementation error.\n\ning semantics, limiting utility in evaluating Q/A\nand generation tasks that require semantic accu-\nracy. Consider Candidate A from Table 1. Be-\ncause it shares most of its n-grams (like \"Apollo\n11 mission\", \"primary purpose\", \"astronauts Neil\nArmstrong and Buzz Aldrin\") with the Golden\nResponse, it would achieve a deceptively high\nROUGE score despite not capturing factual accu-\nracy. Conversely, ROUGE is sensitive to para-\nphrasing: Candidate B is factually correct, but\nwould receive a lower ROUGE score than Can-\ndidate A because it uses synonyms (\"main goal\"\nvs. \"primary purpose\", \"moon rocks\" vs. \"lunar\nsamples\"). This demonstrates ROUGE\u2019s limita-\ntions in handling valid paraphrasing, often reward-\ning syntax over substance, and highlights the need\nfor multiple golden reference examples. In con-\ntexts where responses involve generation of code\nand scripts, ROUGE scores may also err on the\nside of syntax over substance.\n\n3.1.2   BLEU for Translation  \nBilingual Evaluation Understudy (BLEU) (Papineni et al., 2002) is a set of metrics which measure translation quality, comparing a generated translation to a golden reference translation, measuring correspondence between $n$-grams, while penalizing brevity so that shorter translations are not over-rewarded. The brevity penalty is calculated based on the length of the candidate translation and the length of the reference translation. BLEU combines precision scores over several $n$-gram sizes with weights for each $n$-gram size. The larger $n$, the better ordering is taken into account within the BLEU score. Typically several values of $n$ are used. Like ROUGE, BLEU scores range from 0 to 1, with 1 indicating perfect match between translation and reference and 0 indicating no match.\n\nBLEU scores were designed for evaluating translations, and something akin to BLEU may serve as a first order metric for generating code or domain-specific queries from natural language, but BLEU has limitations for these tasks: it can assess syntax and structure but cannot assess underlying functionality, logical correctness, or semantic meaning of generated code, nor does it deal well with comments/documentation. BLEU can also be used for evaluating generations that are short, where the emphasis is on precision. ROUGE should be chosen over BLEU for summarizing sequences, as BLEU may penalize valid rephrasing; ROUGE can offer insight into struc-\n\nture/content and in a recall regime does not penal-\nize valid rephrasing.\n\nMany of the aforementioned limitations of ROUGE extend to BLEU, as both of these are term-overlap metrics. However, with all term-overlap metrics, there is an inherent tradeoff between precision-based metrics and recall-based metrics. Consider Candidate C from Table 1. A recall-oriented metric like ROUGE-L might score this reasonably well for capturing the main event, but a precision-focused metric like BLEU would penalize it for its brevity.\n\nThere are several spinoffs of BLEU. Sacre-BLEU (Post, 2018) aims to standardize tokenization for BLEU. NIST (Doddington, 2002) modifies the brevity penalty by reducing the influence of low frequency $n$-grams on the final score, and weights $n$-grams according to their information value so that the evaluation is not skewed by over emphasis of common $n$-grams. METEOR (Banerjee and Lavie, 2005) uses WordNet to accommodate variations in word forms and synonyms, utilizes both precision and recall, and introduces a fragmentation penalty to preserve word order.\n\n3.1.3   Selection of Relevant Keywords\nOne way to address the aforementioned \"syntax\nover substance\" errors (cf. Sec. 3.1.1) is to com-\npute term-overlap metrics between the candidate\nsummary and selected terms, rather than com-\nputing them over entire candidate responses and\ngolden reference responses. While keyword selec-\ntion may improve evaluation it can also be brit-\ntle; e.g., repeating a question may result in good\nscores, even if the substance of the question is not\naddressed. It is therefore important to be thought-\nful about when to use keyword selection, one's\nchoice of keywords, and what other metrics to\ncombine it with."
        },
        {
            "text": "to $\\beta$. ROUGE scores range from 0 to 1.$^2$\n    ROUGE scores aim to evaluate summarization quality, e.g., when an LLM is tasked with summarizing reports, logs, or briefs into shorter/different versions and there are golden samples of specific length and style with specific meaningful terms. ROUGE scores are also useful when answering questions where the golden response contains multiple key terms and phrases that are considered mandatory for a \"good\" response.",
            "page": 5,
            "x": 303,
            "y": 499,
            "width": 225,
            "height": 138,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-term",
            "chunk_id": "cca8adcb-2481-4b51-929f-07e10e8ace2f",
            "group_text": "3.1   Term Overlap Metrics\n\nTerm overlap metrics, have become de-facto standards throughout the literature. These compare generated text to golden reference samples. They are simple, efficient, and language-agnostic, but have limitations. First, they often miss deeper semantic meaning and context and can yield substantially different scores when encountering paraphrases with identical semantics. Second, they are bad at addressing fluency, factuality, and structure. Finally they are sensitive to length and repetition. Thus, we recommend that metrics be aggregated over multiple diverse, well-written ground truth examples capturing the same semantics for each LLM response.\n\n### 3.1.1   ROUGE Metrics for Summarization\n\nRecall-Oriented Understudy for Gisting Evaluation (ROUGE) ([Lin, 2004; Lin and Hovy, 2003](Lin, 2004; Lin and Hovy, 2003)) is a set of metrics which assesses the quality of automatically generated summaries by comparing overlapping units of text, like words, phrases, *n*-grams (ROUGE-*n*), longest common subsequence (ROUGE-L), and skip-bigrams (ROUGE-S) between a generated summary and a human-written reference summary. When computed based on recall, ROUGE scores emphasize capturing relevant information from the reference, but they might not penalize a summary for being too long. Precision based ROUGE, on the other hand, favors concise summaries, even if they miss some relevant details. ROUGE can also be computed based on $F_{\\beta}$-Measure, weighting respective contributions between precision and recall according\n\nto $\\beta$. ROUGE scores range from 0 to 1.$^2$\n    ROUGE scores aim to evaluate summarization quality, e.g., when an LLM is tasked with summarizing reports, logs, or briefs into shorter/different versions and there are golden samples of specific length and style with specific meaningful terms. ROUGE scores are also useful when answering questions where the golden response contains multiple key terms and phrases that are considered mandatory for a \"good\" response.\n\n**Limitations:** ROUGE scores often capture only surface-level word/phrase similarity, but do not capture nuanced differences in meaning or context. This precludes accurate evaluation for factuality, and high scores can be achieved based on syntactic term overlap rather than underly-\n\n\u00b2In practice, across a large dataset it is unlikely to have scores of 0 or 1 for either ROUGE or BLEU and often indicates an implementation error.\n\ning semantics, limiting utility in evaluating Q/A\nand generation tasks that require semantic accu-\nracy. Consider Candidate A from Table 1. Be-\ncause it shares most of its n-grams (like \"Apollo\n11 mission\", \"primary purpose\", \"astronauts Neil\nArmstrong and Buzz Aldrin\") with the Golden\nResponse, it would achieve a deceptively high\nROUGE score despite not capturing factual accu-\nracy. Conversely, ROUGE is sensitive to para-\nphrasing: Candidate B is factually correct, but\nwould receive a lower ROUGE score than Can-\ndidate A because it uses synonyms (\"main goal\"\nvs. \"primary purpose\", \"moon rocks\" vs. \"lunar\nsamples\"). This demonstrates ROUGE\u2019s limita-\ntions in handling valid paraphrasing, often reward-\ning syntax over substance, and highlights the need\nfor multiple golden reference examples. In con-\ntexts where responses involve generation of code\nand scripts, ROUGE scores may also err on the\nside of syntax over substance.\n\n3.1.2   BLEU for Translation  \nBilingual Evaluation Understudy (BLEU) (Papineni et al., 2002) is a set of metrics which measure translation quality, comparing a generated translation to a golden reference translation, measuring correspondence between $n$-grams, while penalizing brevity so that shorter translations are not over-rewarded. The brevity penalty is calculated based on the length of the candidate translation and the length of the reference translation. BLEU combines precision scores over several $n$-gram sizes with weights for each $n$-gram size. The larger $n$, the better ordering is taken into account within the BLEU score. Typically several values of $n$ are used. Like ROUGE, BLEU scores range from 0 to 1, with 1 indicating perfect match between translation and reference and 0 indicating no match.\n\nBLEU scores were designed for evaluating translations, and something akin to BLEU may serve as a first order metric for generating code or domain-specific queries from natural language, but BLEU has limitations for these tasks: it can assess syntax and structure but cannot assess underlying functionality, logical correctness, or semantic meaning of generated code, nor does it deal well with comments/documentation. BLEU can also be used for evaluating generations that are short, where the emphasis is on precision. ROUGE should be chosen over BLEU for summarizing sequences, as BLEU may penalize valid rephrasing; ROUGE can offer insight into struc-\n\nture/content and in a recall regime does not penal-\nize valid rephrasing.\n\nMany of the aforementioned limitations of ROUGE extend to BLEU, as both of these are term-overlap metrics. However, with all term-overlap metrics, there is an inherent tradeoff between precision-based metrics and recall-based metrics. Consider Candidate C from Table 1. A recall-oriented metric like ROUGE-L might score this reasonably well for capturing the main event, but a precision-focused metric like BLEU would penalize it for its brevity.\n\nThere are several spinoffs of BLEU. Sacre-BLEU (Post, 2018) aims to standardize tokenization for BLEU. NIST (Doddington, 2002) modifies the brevity penalty by reducing the influence of low frequency $n$-grams on the final score, and weights $n$-grams according to their information value so that the evaluation is not skewed by over emphasis of common $n$-grams. METEOR (Banerjee and Lavie, 2005) uses WordNet to accommodate variations in word forms and synonyms, utilizes both precision and recall, and introduces a fragmentation penalty to preserve word order.\n\n3.1.3   Selection of Relevant Keywords\nOne way to address the aforementioned \"syntax\nover substance\" errors (cf. Sec. 3.1.1) is to com-\npute term-overlap metrics between the candidate\nsummary and selected terms, rather than com-\nputing them over entire candidate responses and\ngolden reference responses. While keyword selec-\ntion may improve evaluation it can also be brit-\ntle; e.g., repeating a question may result in good\nscores, even if the substance of the question is not\naddressed. It is therefore important to be thought-\nful about when to use keyword selection, one's\nchoice of keywords, and what other metrics to\ncombine it with."
        },
        {
            "text": "**Limitations:** ROUGE scores often capture only surface-level word/phrase similarity, but do not capture nuanced differences in meaning or context. This precludes accurate evaluation for factuality, and high scores can be achieved based on syntactic term overlap rather than underly-",
            "page": 5,
            "x": 304,
            "y": 638,
            "width": 224,
            "height": 85,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-term",
            "chunk_id": "1c24dc1c-8ea4-405a-8b3f-36d3c609f42e",
            "group_text": "3.1   Term Overlap Metrics\n\nTerm overlap metrics, have become de-facto standards throughout the literature. These compare generated text to golden reference samples. They are simple, efficient, and language-agnostic, but have limitations. First, they often miss deeper semantic meaning and context and can yield substantially different scores when encountering paraphrases with identical semantics. Second, they are bad at addressing fluency, factuality, and structure. Finally they are sensitive to length and repetition. Thus, we recommend that metrics be aggregated over multiple diverse, well-written ground truth examples capturing the same semantics for each LLM response.\n\n### 3.1.1   ROUGE Metrics for Summarization\n\nRecall-Oriented Understudy for Gisting Evaluation (ROUGE) ([Lin, 2004; Lin and Hovy, 2003](Lin, 2004; Lin and Hovy, 2003)) is a set of metrics which assesses the quality of automatically generated summaries by comparing overlapping units of text, like words, phrases, *n*-grams (ROUGE-*n*), longest common subsequence (ROUGE-L), and skip-bigrams (ROUGE-S) between a generated summary and a human-written reference summary. When computed based on recall, ROUGE scores emphasize capturing relevant information from the reference, but they might not penalize a summary for being too long. Precision based ROUGE, on the other hand, favors concise summaries, even if they miss some relevant details. ROUGE can also be computed based on $F_{\\beta}$-Measure, weighting respective contributions between precision and recall according\n\nto $\\beta$. ROUGE scores range from 0 to 1.$^2$\n    ROUGE scores aim to evaluate summarization quality, e.g., when an LLM is tasked with summarizing reports, logs, or briefs into shorter/different versions and there are golden samples of specific length and style with specific meaningful terms. ROUGE scores are also useful when answering questions where the golden response contains multiple key terms and phrases that are considered mandatory for a \"good\" response.\n\n**Limitations:** ROUGE scores often capture only surface-level word/phrase similarity, but do not capture nuanced differences in meaning or context. This precludes accurate evaluation for factuality, and high scores can be achieved based on syntactic term overlap rather than underly-\n\n\u00b2In practice, across a large dataset it is unlikely to have scores of 0 or 1 for either ROUGE or BLEU and often indicates an implementation error.\n\ning semantics, limiting utility in evaluating Q/A\nand generation tasks that require semantic accu-\nracy. Consider Candidate A from Table 1. Be-\ncause it shares most of its n-grams (like \"Apollo\n11 mission\", \"primary purpose\", \"astronauts Neil\nArmstrong and Buzz Aldrin\") with the Golden\nResponse, it would achieve a deceptively high\nROUGE score despite not capturing factual accu-\nracy. Conversely, ROUGE is sensitive to para-\nphrasing: Candidate B is factually correct, but\nwould receive a lower ROUGE score than Can-\ndidate A because it uses synonyms (\"main goal\"\nvs. \"primary purpose\", \"moon rocks\" vs. \"lunar\nsamples\"). This demonstrates ROUGE\u2019s limita-\ntions in handling valid paraphrasing, often reward-\ning syntax over substance, and highlights the need\nfor multiple golden reference examples. In con-\ntexts where responses involve generation of code\nand scripts, ROUGE scores may also err on the\nside of syntax over substance.\n\n3.1.2   BLEU for Translation  \nBilingual Evaluation Understudy (BLEU) (Papineni et al., 2002) is a set of metrics which measure translation quality, comparing a generated translation to a golden reference translation, measuring correspondence between $n$-grams, while penalizing brevity so that shorter translations are not over-rewarded. The brevity penalty is calculated based on the length of the candidate translation and the length of the reference translation. BLEU combines precision scores over several $n$-gram sizes with weights for each $n$-gram size. The larger $n$, the better ordering is taken into account within the BLEU score. Typically several values of $n$ are used. Like ROUGE, BLEU scores range from 0 to 1, with 1 indicating perfect match between translation and reference and 0 indicating no match.\n\nBLEU scores were designed for evaluating translations, and something akin to BLEU may serve as a first order metric for generating code or domain-specific queries from natural language, but BLEU has limitations for these tasks: it can assess syntax and structure but cannot assess underlying functionality, logical correctness, or semantic meaning of generated code, nor does it deal well with comments/documentation. BLEU can also be used for evaluating generations that are short, where the emphasis is on precision. ROUGE should be chosen over BLEU for summarizing sequences, as BLEU may penalize valid rephrasing; ROUGE can offer insight into struc-\n\nture/content and in a recall regime does not penal-\nize valid rephrasing.\n\nMany of the aforementioned limitations of ROUGE extend to BLEU, as both of these are term-overlap metrics. However, with all term-overlap metrics, there is an inherent tradeoff between precision-based metrics and recall-based metrics. Consider Candidate C from Table 1. A recall-oriented metric like ROUGE-L might score this reasonably well for capturing the main event, but a precision-focused metric like BLEU would penalize it for its brevity.\n\nThere are several spinoffs of BLEU. Sacre-BLEU (Post, 2018) aims to standardize tokenization for BLEU. NIST (Doddington, 2002) modifies the brevity penalty by reducing the influence of low frequency $n$-grams on the final score, and weights $n$-grams according to their information value so that the evaluation is not skewed by over emphasis of common $n$-grams. METEOR (Banerjee and Lavie, 2005) uses WordNet to accommodate variations in word forms and synonyms, utilizes both precision and recall, and introduces a fragmentation penalty to preserve word order.\n\n3.1.3   Selection of Relevant Keywords\nOne way to address the aforementioned \"syntax\nover substance\" errors (cf. Sec. 3.1.1) is to com-\npute term-overlap metrics between the candidate\nsummary and selected terms, rather than com-\nputing them over entire candidate responses and\ngolden reference responses. While keyword selec-\ntion may improve evaluation it can also be brit-\ntle; e.g., repeating a question may result in good\nscores, even if the substance of the question is not\naddressed. It is therefore important to be thought-\nful about when to use keyword selection, one's\nchoice of keywords, and what other metrics to\ncombine it with."
        },
        {
            "text": "\u00b2In practice, across a large dataset it is unlikely to have scores of 0 or 1 for either ROUGE or BLEU and often indicates an implementation error.",
            "page": 5,
            "x": 304,
            "y": 731,
            "width": 224,
            "height": 35,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-term",
            "chunk_id": "43f23793-8c7c-4184-8d63-2fd918063607",
            "group_text": "3.1   Term Overlap Metrics\n\nTerm overlap metrics, have become de-facto standards throughout the literature. These compare generated text to golden reference samples. They are simple, efficient, and language-agnostic, but have limitations. First, they often miss deeper semantic meaning and context and can yield substantially different scores when encountering paraphrases with identical semantics. Second, they are bad at addressing fluency, factuality, and structure. Finally they are sensitive to length and repetition. Thus, we recommend that metrics be aggregated over multiple diverse, well-written ground truth examples capturing the same semantics for each LLM response.\n\n### 3.1.1   ROUGE Metrics for Summarization\n\nRecall-Oriented Understudy for Gisting Evaluation (ROUGE) ([Lin, 2004; Lin and Hovy, 2003](Lin, 2004; Lin and Hovy, 2003)) is a set of metrics which assesses the quality of automatically generated summaries by comparing overlapping units of text, like words, phrases, *n*-grams (ROUGE-*n*), longest common subsequence (ROUGE-L), and skip-bigrams (ROUGE-S) between a generated summary and a human-written reference summary. When computed based on recall, ROUGE scores emphasize capturing relevant information from the reference, but they might not penalize a summary for being too long. Precision based ROUGE, on the other hand, favors concise summaries, even if they miss some relevant details. ROUGE can also be computed based on $F_{\\beta}$-Measure, weighting respective contributions between precision and recall according\n\nto $\\beta$. ROUGE scores range from 0 to 1.$^2$\n    ROUGE scores aim to evaluate summarization quality, e.g., when an LLM is tasked with summarizing reports, logs, or briefs into shorter/different versions and there are golden samples of specific length and style with specific meaningful terms. ROUGE scores are also useful when answering questions where the golden response contains multiple key terms and phrases that are considered mandatory for a \"good\" response.\n\n**Limitations:** ROUGE scores often capture only surface-level word/phrase similarity, but do not capture nuanced differences in meaning or context. This precludes accurate evaluation for factuality, and high scores can be achieved based on syntactic term overlap rather than underly-\n\n\u00b2In practice, across a large dataset it is unlikely to have scores of 0 or 1 for either ROUGE or BLEU and often indicates an implementation error.\n\ning semantics, limiting utility in evaluating Q/A\nand generation tasks that require semantic accu-\nracy. Consider Candidate A from Table 1. Be-\ncause it shares most of its n-grams (like \"Apollo\n11 mission\", \"primary purpose\", \"astronauts Neil\nArmstrong and Buzz Aldrin\") with the Golden\nResponse, it would achieve a deceptively high\nROUGE score despite not capturing factual accu-\nracy. Conversely, ROUGE is sensitive to para-\nphrasing: Candidate B is factually correct, but\nwould receive a lower ROUGE score than Can-\ndidate A because it uses synonyms (\"main goal\"\nvs. \"primary purpose\", \"moon rocks\" vs. \"lunar\nsamples\"). This demonstrates ROUGE\u2019s limita-\ntions in handling valid paraphrasing, often reward-\ning syntax over substance, and highlights the need\nfor multiple golden reference examples. In con-\ntexts where responses involve generation of code\nand scripts, ROUGE scores may also err on the\nside of syntax over substance.\n\n3.1.2   BLEU for Translation  \nBilingual Evaluation Understudy (BLEU) (Papineni et al., 2002) is a set of metrics which measure translation quality, comparing a generated translation to a golden reference translation, measuring correspondence between $n$-grams, while penalizing brevity so that shorter translations are not over-rewarded. The brevity penalty is calculated based on the length of the candidate translation and the length of the reference translation. BLEU combines precision scores over several $n$-gram sizes with weights for each $n$-gram size. The larger $n$, the better ordering is taken into account within the BLEU score. Typically several values of $n$ are used. Like ROUGE, BLEU scores range from 0 to 1, with 1 indicating perfect match between translation and reference and 0 indicating no match.\n\nBLEU scores were designed for evaluating translations, and something akin to BLEU may serve as a first order metric for generating code or domain-specific queries from natural language, but BLEU has limitations for these tasks: it can assess syntax and structure but cannot assess underlying functionality, logical correctness, or semantic meaning of generated code, nor does it deal well with comments/documentation. BLEU can also be used for evaluating generations that are short, where the emphasis is on precision. ROUGE should be chosen over BLEU for summarizing sequences, as BLEU may penalize valid rephrasing; ROUGE can offer insight into struc-\n\nture/content and in a recall regime does not penal-\nize valid rephrasing.\n\nMany of the aforementioned limitations of ROUGE extend to BLEU, as both of these are term-overlap metrics. However, with all term-overlap metrics, there is an inherent tradeoff between precision-based metrics and recall-based metrics. Consider Candidate C from Table 1. A recall-oriented metric like ROUGE-L might score this reasonably well for capturing the main event, but a precision-focused metric like BLEU would penalize it for its brevity.\n\nThere are several spinoffs of BLEU. Sacre-BLEU (Post, 2018) aims to standardize tokenization for BLEU. NIST (Doddington, 2002) modifies the brevity penalty by reducing the influence of low frequency $n$-grams on the final score, and weights $n$-grams according to their information value so that the evaluation is not skewed by over emphasis of common $n$-grams. METEOR (Banerjee and Lavie, 2005) uses WordNet to accommodate variations in word forms and synonyms, utilizes both precision and recall, and introduces a fragmentation penalty to preserve word order.\n\n3.1.3   Selection of Relevant Keywords\nOne way to address the aforementioned \"syntax\nover substance\" errors (cf. Sec. 3.1.1) is to com-\npute term-overlap metrics between the candidate\nsummary and selected terms, rather than com-\nputing them over entire candidate responses and\ngolden reference responses. While keyword selec-\ntion may improve evaluation it can also be brit-\ntle; e.g., repeating a question may result in good\nscores, even if the substance of the question is not\naddressed. It is therefore important to be thought-\nful about when to use keyword selection, one's\nchoice of keywords, and what other metrics to\ncombine it with."
        },
        {
            "text": "ing semantics, limiting utility in evaluating Q/A\nand generation tasks that require semantic accu-\nracy. Consider Candidate A from Table 1. Be-\ncause it shares most of its n-grams (like \"Apollo\n11 mission\", \"primary purpose\", \"astronauts Neil\nArmstrong and Buzz Aldrin\") with the Golden\nResponse, it would achieve a deceptively high\nROUGE score despite not capturing factual accu-\nracy. Conversely, ROUGE is sensitive to para-\nphrasing: Candidate B is factually correct, but\nwould receive a lower ROUGE score than Can-\ndidate A because it uses synonyms (\"main goal\"\nvs. \"primary purpose\", \"moon rocks\" vs. \"lunar\nsamples\"). This demonstrates ROUGE\u2019s limita-\ntions in handling valid paraphrasing, often reward-\ning syntax over substance, and highlights the need\nfor multiple golden reference examples. In con-\ntexts where responses involve generation of code\nand scripts, ROUGE scores may also err on the\nside of syntax over substance.",
            "page": 6,
            "x": 67,
            "y": 61,
            "width": 227,
            "height": 274,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-term",
            "chunk_id": "4e5ff35a-caa3-4266-9353-2c304b25f4b5",
            "group_text": "3.1   Term Overlap Metrics\n\nTerm overlap metrics, have become de-facto standards throughout the literature. These compare generated text to golden reference samples. They are simple, efficient, and language-agnostic, but have limitations. First, they often miss deeper semantic meaning and context and can yield substantially different scores when encountering paraphrases with identical semantics. Second, they are bad at addressing fluency, factuality, and structure. Finally they are sensitive to length and repetition. Thus, we recommend that metrics be aggregated over multiple diverse, well-written ground truth examples capturing the same semantics for each LLM response.\n\n### 3.1.1   ROUGE Metrics for Summarization\n\nRecall-Oriented Understudy for Gisting Evaluation (ROUGE) ([Lin, 2004; Lin and Hovy, 2003](Lin, 2004; Lin and Hovy, 2003)) is a set of metrics which assesses the quality of automatically generated summaries by comparing overlapping units of text, like words, phrases, *n*-grams (ROUGE-*n*), longest common subsequence (ROUGE-L), and skip-bigrams (ROUGE-S) between a generated summary and a human-written reference summary. When computed based on recall, ROUGE scores emphasize capturing relevant information from the reference, but they might not penalize a summary for being too long. Precision based ROUGE, on the other hand, favors concise summaries, even if they miss some relevant details. ROUGE can also be computed based on $F_{\\beta}$-Measure, weighting respective contributions between precision and recall according\n\nto $\\beta$. ROUGE scores range from 0 to 1.$^2$\n    ROUGE scores aim to evaluate summarization quality, e.g., when an LLM is tasked with summarizing reports, logs, or briefs into shorter/different versions and there are golden samples of specific length and style with specific meaningful terms. ROUGE scores are also useful when answering questions where the golden response contains multiple key terms and phrases that are considered mandatory for a \"good\" response.\n\n**Limitations:** ROUGE scores often capture only surface-level word/phrase similarity, but do not capture nuanced differences in meaning or context. This precludes accurate evaluation for factuality, and high scores can be achieved based on syntactic term overlap rather than underly-\n\n\u00b2In practice, across a large dataset it is unlikely to have scores of 0 or 1 for either ROUGE or BLEU and often indicates an implementation error.\n\ning semantics, limiting utility in evaluating Q/A\nand generation tasks that require semantic accu-\nracy. Consider Candidate A from Table 1. Be-\ncause it shares most of its n-grams (like \"Apollo\n11 mission\", \"primary purpose\", \"astronauts Neil\nArmstrong and Buzz Aldrin\") with the Golden\nResponse, it would achieve a deceptively high\nROUGE score despite not capturing factual accu-\nracy. Conversely, ROUGE is sensitive to para-\nphrasing: Candidate B is factually correct, but\nwould receive a lower ROUGE score than Can-\ndidate A because it uses synonyms (\"main goal\"\nvs. \"primary purpose\", \"moon rocks\" vs. \"lunar\nsamples\"). This demonstrates ROUGE\u2019s limita-\ntions in handling valid paraphrasing, often reward-\ning syntax over substance, and highlights the need\nfor multiple golden reference examples. In con-\ntexts where responses involve generation of code\nand scripts, ROUGE scores may also err on the\nside of syntax over substance.\n\n3.1.2   BLEU for Translation  \nBilingual Evaluation Understudy (BLEU) (Papineni et al., 2002) is a set of metrics which measure translation quality, comparing a generated translation to a golden reference translation, measuring correspondence between $n$-grams, while penalizing brevity so that shorter translations are not over-rewarded. The brevity penalty is calculated based on the length of the candidate translation and the length of the reference translation. BLEU combines precision scores over several $n$-gram sizes with weights for each $n$-gram size. The larger $n$, the better ordering is taken into account within the BLEU score. Typically several values of $n$ are used. Like ROUGE, BLEU scores range from 0 to 1, with 1 indicating perfect match between translation and reference and 0 indicating no match.\n\nBLEU scores were designed for evaluating translations, and something akin to BLEU may serve as a first order metric for generating code or domain-specific queries from natural language, but BLEU has limitations for these tasks: it can assess syntax and structure but cannot assess underlying functionality, logical correctness, or semantic meaning of generated code, nor does it deal well with comments/documentation. BLEU can also be used for evaluating generations that are short, where the emphasis is on precision. ROUGE should be chosen over BLEU for summarizing sequences, as BLEU may penalize valid rephrasing; ROUGE can offer insight into struc-\n\nture/content and in a recall regime does not penal-\nize valid rephrasing.\n\nMany of the aforementioned limitations of ROUGE extend to BLEU, as both of these are term-overlap metrics. However, with all term-overlap metrics, there is an inherent tradeoff between precision-based metrics and recall-based metrics. Consider Candidate C from Table 1. A recall-oriented metric like ROUGE-L might score this reasonably well for capturing the main event, but a precision-focused metric like BLEU would penalize it for its brevity.\n\nThere are several spinoffs of BLEU. Sacre-BLEU (Post, 2018) aims to standardize tokenization for BLEU. NIST (Doddington, 2002) modifies the brevity penalty by reducing the influence of low frequency $n$-grams on the final score, and weights $n$-grams according to their information value so that the evaluation is not skewed by over emphasis of common $n$-grams. METEOR (Banerjee and Lavie, 2005) uses WordNet to accommodate variations in word forms and synonyms, utilizes both precision and recall, and introduces a fragmentation penalty to preserve word order.\n\n3.1.3   Selection of Relevant Keywords\nOne way to address the aforementioned \"syntax\nover substance\" errors (cf. Sec. 3.1.1) is to com-\npute term-overlap metrics between the candidate\nsummary and selected terms, rather than com-\nputing them over entire candidate responses and\ngolden reference responses. While keyword selec-\ntion may improve evaluation it can also be brit-\ntle; e.g., repeating a question may result in good\nscores, even if the substance of the question is not\naddressed. It is therefore important to be thought-\nful about when to use keyword selection, one's\nchoice of keywords, and what other metrics to\ncombine it with."
        },
        {
            "text": "3.1.2   BLEU for Translation  \nBilingual Evaluation Understudy (BLEU) (Papineni et al., 2002) is a set of metrics which measure translation quality, comparing a generated translation to a golden reference translation, measuring correspondence between $n$-grams, while penalizing brevity so that shorter translations are not over-rewarded. The brevity penalty is calculated based on the length of the candidate translation and the length of the reference translation. BLEU combines precision scores over several $n$-gram sizes with weights for each $n$-gram size. The larger $n$, the better ordering is taken into account within the BLEU score. Typically several values of $n$ are used. Like ROUGE, BLEU scores range from 0 to 1, with 1 indicating perfect match between translation and reference and 0 indicating no match.",
            "page": 6,
            "x": 68,
            "y": 341,
            "width": 226,
            "height": 235,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-term",
            "chunk_id": "4205b33d-c41b-481f-8b92-40332a8f3fb8",
            "group_text": "3.1   Term Overlap Metrics\n\nTerm overlap metrics, have become de-facto standards throughout the literature. These compare generated text to golden reference samples. They are simple, efficient, and language-agnostic, but have limitations. First, they often miss deeper semantic meaning and context and can yield substantially different scores when encountering paraphrases with identical semantics. Second, they are bad at addressing fluency, factuality, and structure. Finally they are sensitive to length and repetition. Thus, we recommend that metrics be aggregated over multiple diverse, well-written ground truth examples capturing the same semantics for each LLM response.\n\n### 3.1.1   ROUGE Metrics for Summarization\n\nRecall-Oriented Understudy for Gisting Evaluation (ROUGE) ([Lin, 2004; Lin and Hovy, 2003](Lin, 2004; Lin and Hovy, 2003)) is a set of metrics which assesses the quality of automatically generated summaries by comparing overlapping units of text, like words, phrases, *n*-grams (ROUGE-*n*), longest common subsequence (ROUGE-L), and skip-bigrams (ROUGE-S) between a generated summary and a human-written reference summary. When computed based on recall, ROUGE scores emphasize capturing relevant information from the reference, but they might not penalize a summary for being too long. Precision based ROUGE, on the other hand, favors concise summaries, even if they miss some relevant details. ROUGE can also be computed based on $F_{\\beta}$-Measure, weighting respective contributions between precision and recall according\n\nto $\\beta$. ROUGE scores range from 0 to 1.$^2$\n    ROUGE scores aim to evaluate summarization quality, e.g., when an LLM is tasked with summarizing reports, logs, or briefs into shorter/different versions and there are golden samples of specific length and style with specific meaningful terms. ROUGE scores are also useful when answering questions where the golden response contains multiple key terms and phrases that are considered mandatory for a \"good\" response.\n\n**Limitations:** ROUGE scores often capture only surface-level word/phrase similarity, but do not capture nuanced differences in meaning or context. This precludes accurate evaluation for factuality, and high scores can be achieved based on syntactic term overlap rather than underly-\n\n\u00b2In practice, across a large dataset it is unlikely to have scores of 0 or 1 for either ROUGE or BLEU and often indicates an implementation error.\n\ning semantics, limiting utility in evaluating Q/A\nand generation tasks that require semantic accu-\nracy. Consider Candidate A from Table 1. Be-\ncause it shares most of its n-grams (like \"Apollo\n11 mission\", \"primary purpose\", \"astronauts Neil\nArmstrong and Buzz Aldrin\") with the Golden\nResponse, it would achieve a deceptively high\nROUGE score despite not capturing factual accu-\nracy. Conversely, ROUGE is sensitive to para-\nphrasing: Candidate B is factually correct, but\nwould receive a lower ROUGE score than Can-\ndidate A because it uses synonyms (\"main goal\"\nvs. \"primary purpose\", \"moon rocks\" vs. \"lunar\nsamples\"). This demonstrates ROUGE\u2019s limita-\ntions in handling valid paraphrasing, often reward-\ning syntax over substance, and highlights the need\nfor multiple golden reference examples. In con-\ntexts where responses involve generation of code\nand scripts, ROUGE scores may also err on the\nside of syntax over substance.\n\n3.1.2   BLEU for Translation  \nBilingual Evaluation Understudy (BLEU) (Papineni et al., 2002) is a set of metrics which measure translation quality, comparing a generated translation to a golden reference translation, measuring correspondence between $n$-grams, while penalizing brevity so that shorter translations are not over-rewarded. The brevity penalty is calculated based on the length of the candidate translation and the length of the reference translation. BLEU combines precision scores over several $n$-gram sizes with weights for each $n$-gram size. The larger $n$, the better ordering is taken into account within the BLEU score. Typically several values of $n$ are used. Like ROUGE, BLEU scores range from 0 to 1, with 1 indicating perfect match between translation and reference and 0 indicating no match.\n\nBLEU scores were designed for evaluating translations, and something akin to BLEU may serve as a first order metric for generating code or domain-specific queries from natural language, but BLEU has limitations for these tasks: it can assess syntax and structure but cannot assess underlying functionality, logical correctness, or semantic meaning of generated code, nor does it deal well with comments/documentation. BLEU can also be used for evaluating generations that are short, where the emphasis is on precision. ROUGE should be chosen over BLEU for summarizing sequences, as BLEU may penalize valid rephrasing; ROUGE can offer insight into struc-\n\nture/content and in a recall regime does not penal-\nize valid rephrasing.\n\nMany of the aforementioned limitations of ROUGE extend to BLEU, as both of these are term-overlap metrics. However, with all term-overlap metrics, there is an inherent tradeoff between precision-based metrics and recall-based metrics. Consider Candidate C from Table 1. A recall-oriented metric like ROUGE-L might score this reasonably well for capturing the main event, but a precision-focused metric like BLEU would penalize it for its brevity.\n\nThere are several spinoffs of BLEU. Sacre-BLEU (Post, 2018) aims to standardize tokenization for BLEU. NIST (Doddington, 2002) modifies the brevity penalty by reducing the influence of low frequency $n$-grams on the final score, and weights $n$-grams according to their information value so that the evaluation is not skewed by over emphasis of common $n$-grams. METEOR (Banerjee and Lavie, 2005) uses WordNet to accommodate variations in word forms and synonyms, utilizes both precision and recall, and introduces a fragmentation penalty to preserve word order.\n\n3.1.3   Selection of Relevant Keywords\nOne way to address the aforementioned \"syntax\nover substance\" errors (cf. Sec. 3.1.1) is to com-\npute term-overlap metrics between the candidate\nsummary and selected terms, rather than com-\nputing them over entire candidate responses and\ngolden reference responses. While keyword selec-\ntion may improve evaluation it can also be brit-\ntle; e.g., repeating a question may result in good\nscores, even if the substance of the question is not\naddressed. It is therefore important to be thought-\nful about when to use keyword selection, one's\nchoice of keywords, and what other metrics to\ncombine it with."
        },
        {
            "text": "BLEU scores were designed for evaluating translations, and something akin to BLEU may serve as a first order metric for generating code or domain-specific queries from natural language, but BLEU has limitations for these tasks: it can assess syntax and structure but cannot assess underlying functionality, logical correctness, or semantic meaning of generated code, nor does it deal well with comments/documentation. BLEU can also be used for evaluating generations that are short, where the emphasis is on precision. ROUGE should be chosen over BLEU for summarizing sequences, as BLEU may penalize valid rephrasing; ROUGE can offer insight into struc-",
            "page": 6,
            "x": 68,
            "y": 577,
            "width": 226,
            "height": 191,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-term",
            "chunk_id": "55543245-e058-409b-8d24-ac725fb8c64b",
            "group_text": "3.1   Term Overlap Metrics\n\nTerm overlap metrics, have become de-facto standards throughout the literature. These compare generated text to golden reference samples. They are simple, efficient, and language-agnostic, but have limitations. First, they often miss deeper semantic meaning and context and can yield substantially different scores when encountering paraphrases with identical semantics. Second, they are bad at addressing fluency, factuality, and structure. Finally they are sensitive to length and repetition. Thus, we recommend that metrics be aggregated over multiple diverse, well-written ground truth examples capturing the same semantics for each LLM response.\n\n### 3.1.1   ROUGE Metrics for Summarization\n\nRecall-Oriented Understudy for Gisting Evaluation (ROUGE) ([Lin, 2004; Lin and Hovy, 2003](Lin, 2004; Lin and Hovy, 2003)) is a set of metrics which assesses the quality of automatically generated summaries by comparing overlapping units of text, like words, phrases, *n*-grams (ROUGE-*n*), longest common subsequence (ROUGE-L), and skip-bigrams (ROUGE-S) between a generated summary and a human-written reference summary. When computed based on recall, ROUGE scores emphasize capturing relevant information from the reference, but they might not penalize a summary for being too long. Precision based ROUGE, on the other hand, favors concise summaries, even if they miss some relevant details. ROUGE can also be computed based on $F_{\\beta}$-Measure, weighting respective contributions between precision and recall according\n\nto $\\beta$. ROUGE scores range from 0 to 1.$^2$\n    ROUGE scores aim to evaluate summarization quality, e.g., when an LLM is tasked with summarizing reports, logs, or briefs into shorter/different versions and there are golden samples of specific length and style with specific meaningful terms. ROUGE scores are also useful when answering questions where the golden response contains multiple key terms and phrases that are considered mandatory for a \"good\" response.\n\n**Limitations:** ROUGE scores often capture only surface-level word/phrase similarity, but do not capture nuanced differences in meaning or context. This precludes accurate evaluation for factuality, and high scores can be achieved based on syntactic term overlap rather than underly-\n\n\u00b2In practice, across a large dataset it is unlikely to have scores of 0 or 1 for either ROUGE or BLEU and often indicates an implementation error.\n\ning semantics, limiting utility in evaluating Q/A\nand generation tasks that require semantic accu-\nracy. Consider Candidate A from Table 1. Be-\ncause it shares most of its n-grams (like \"Apollo\n11 mission\", \"primary purpose\", \"astronauts Neil\nArmstrong and Buzz Aldrin\") with the Golden\nResponse, it would achieve a deceptively high\nROUGE score despite not capturing factual accu-\nracy. Conversely, ROUGE is sensitive to para-\nphrasing: Candidate B is factually correct, but\nwould receive a lower ROUGE score than Can-\ndidate A because it uses synonyms (\"main goal\"\nvs. \"primary purpose\", \"moon rocks\" vs. \"lunar\nsamples\"). This demonstrates ROUGE\u2019s limita-\ntions in handling valid paraphrasing, often reward-\ning syntax over substance, and highlights the need\nfor multiple golden reference examples. In con-\ntexts where responses involve generation of code\nand scripts, ROUGE scores may also err on the\nside of syntax over substance.\n\n3.1.2   BLEU for Translation  \nBilingual Evaluation Understudy (BLEU) (Papineni et al., 2002) is a set of metrics which measure translation quality, comparing a generated translation to a golden reference translation, measuring correspondence between $n$-grams, while penalizing brevity so that shorter translations are not over-rewarded. The brevity penalty is calculated based on the length of the candidate translation and the length of the reference translation. BLEU combines precision scores over several $n$-gram sizes with weights for each $n$-gram size. The larger $n$, the better ordering is taken into account within the BLEU score. Typically several values of $n$ are used. Like ROUGE, BLEU scores range from 0 to 1, with 1 indicating perfect match between translation and reference and 0 indicating no match.\n\nBLEU scores were designed for evaluating translations, and something akin to BLEU may serve as a first order metric for generating code or domain-specific queries from natural language, but BLEU has limitations for these tasks: it can assess syntax and structure but cannot assess underlying functionality, logical correctness, or semantic meaning of generated code, nor does it deal well with comments/documentation. BLEU can also be used for evaluating generations that are short, where the emphasis is on precision. ROUGE should be chosen over BLEU for summarizing sequences, as BLEU may penalize valid rephrasing; ROUGE can offer insight into struc-\n\nture/content and in a recall regime does not penal-\nize valid rephrasing.\n\nMany of the aforementioned limitations of ROUGE extend to BLEU, as both of these are term-overlap metrics. However, with all term-overlap metrics, there is an inherent tradeoff between precision-based metrics and recall-based metrics. Consider Candidate C from Table 1. A recall-oriented metric like ROUGE-L might score this reasonably well for capturing the main event, but a precision-focused metric like BLEU would penalize it for its brevity.\n\nThere are several spinoffs of BLEU. Sacre-BLEU (Post, 2018) aims to standardize tokenization for BLEU. NIST (Doddington, 2002) modifies the brevity penalty by reducing the influence of low frequency $n$-grams on the final score, and weights $n$-grams according to their information value so that the evaluation is not skewed by over emphasis of common $n$-grams. METEOR (Banerjee and Lavie, 2005) uses WordNet to accommodate variations in word forms and synonyms, utilizes both precision and recall, and introduces a fragmentation penalty to preserve word order.\n\n3.1.3   Selection of Relevant Keywords\nOne way to address the aforementioned \"syntax\nover substance\" errors (cf. Sec. 3.1.1) is to com-\npute term-overlap metrics between the candidate\nsummary and selected terms, rather than com-\nputing them over entire candidate responses and\ngolden reference responses. While keyword selec-\ntion may improve evaluation it can also be brit-\ntle; e.g., repeating a question may result in good\nscores, even if the substance of the question is not\naddressed. It is therefore important to be thought-\nful about when to use keyword selection, one's\nchoice of keywords, and what other metrics to\ncombine it with."
        },
        {
            "text": "ture/content and in a recall regime does not penal-\nize valid rephrasing.",
            "page": 6,
            "x": 303,
            "y": 63,
            "width": 223,
            "height": 27,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-term",
            "chunk_id": "cb421be6-6186-43ee-a1a6-a4597a3cf2e9",
            "group_text": "3.1   Term Overlap Metrics\n\nTerm overlap metrics, have become de-facto standards throughout the literature. These compare generated text to golden reference samples. They are simple, efficient, and language-agnostic, but have limitations. First, they often miss deeper semantic meaning and context and can yield substantially different scores when encountering paraphrases with identical semantics. Second, they are bad at addressing fluency, factuality, and structure. Finally they are sensitive to length and repetition. Thus, we recommend that metrics be aggregated over multiple diverse, well-written ground truth examples capturing the same semantics for each LLM response.\n\n### 3.1.1   ROUGE Metrics for Summarization\n\nRecall-Oriented Understudy for Gisting Evaluation (ROUGE) ([Lin, 2004; Lin and Hovy, 2003](Lin, 2004; Lin and Hovy, 2003)) is a set of metrics which assesses the quality of automatically generated summaries by comparing overlapping units of text, like words, phrases, *n*-grams (ROUGE-*n*), longest common subsequence (ROUGE-L), and skip-bigrams (ROUGE-S) between a generated summary and a human-written reference summary. When computed based on recall, ROUGE scores emphasize capturing relevant information from the reference, but they might not penalize a summary for being too long. Precision based ROUGE, on the other hand, favors concise summaries, even if they miss some relevant details. ROUGE can also be computed based on $F_{\\beta}$-Measure, weighting respective contributions between precision and recall according\n\nto $\\beta$. ROUGE scores range from 0 to 1.$^2$\n    ROUGE scores aim to evaluate summarization quality, e.g., when an LLM is tasked with summarizing reports, logs, or briefs into shorter/different versions and there are golden samples of specific length and style with specific meaningful terms. ROUGE scores are also useful when answering questions where the golden response contains multiple key terms and phrases that are considered mandatory for a \"good\" response.\n\n**Limitations:** ROUGE scores often capture only surface-level word/phrase similarity, but do not capture nuanced differences in meaning or context. This precludes accurate evaluation for factuality, and high scores can be achieved based on syntactic term overlap rather than underly-\n\n\u00b2In practice, across a large dataset it is unlikely to have scores of 0 or 1 for either ROUGE or BLEU and often indicates an implementation error.\n\ning semantics, limiting utility in evaluating Q/A\nand generation tasks that require semantic accu-\nracy. Consider Candidate A from Table 1. Be-\ncause it shares most of its n-grams (like \"Apollo\n11 mission\", \"primary purpose\", \"astronauts Neil\nArmstrong and Buzz Aldrin\") with the Golden\nResponse, it would achieve a deceptively high\nROUGE score despite not capturing factual accu-\nracy. Conversely, ROUGE is sensitive to para-\nphrasing: Candidate B is factually correct, but\nwould receive a lower ROUGE score than Can-\ndidate A because it uses synonyms (\"main goal\"\nvs. \"primary purpose\", \"moon rocks\" vs. \"lunar\nsamples\"). This demonstrates ROUGE\u2019s limita-\ntions in handling valid paraphrasing, often reward-\ning syntax over substance, and highlights the need\nfor multiple golden reference examples. In con-\ntexts where responses involve generation of code\nand scripts, ROUGE scores may also err on the\nside of syntax over substance.\n\n3.1.2   BLEU for Translation  \nBilingual Evaluation Understudy (BLEU) (Papineni et al., 2002) is a set of metrics which measure translation quality, comparing a generated translation to a golden reference translation, measuring correspondence between $n$-grams, while penalizing brevity so that shorter translations are not over-rewarded. The brevity penalty is calculated based on the length of the candidate translation and the length of the reference translation. BLEU combines precision scores over several $n$-gram sizes with weights for each $n$-gram size. The larger $n$, the better ordering is taken into account within the BLEU score. Typically several values of $n$ are used. Like ROUGE, BLEU scores range from 0 to 1, with 1 indicating perfect match between translation and reference and 0 indicating no match.\n\nBLEU scores were designed for evaluating translations, and something akin to BLEU may serve as a first order metric for generating code or domain-specific queries from natural language, but BLEU has limitations for these tasks: it can assess syntax and structure but cannot assess underlying functionality, logical correctness, or semantic meaning of generated code, nor does it deal well with comments/documentation. BLEU can also be used for evaluating generations that are short, where the emphasis is on precision. ROUGE should be chosen over BLEU for summarizing sequences, as BLEU may penalize valid rephrasing; ROUGE can offer insight into struc-\n\nture/content and in a recall regime does not penal-\nize valid rephrasing.\n\nMany of the aforementioned limitations of ROUGE extend to BLEU, as both of these are term-overlap metrics. However, with all term-overlap metrics, there is an inherent tradeoff between precision-based metrics and recall-based metrics. Consider Candidate C from Table 1. A recall-oriented metric like ROUGE-L might score this reasonably well for capturing the main event, but a precision-focused metric like BLEU would penalize it for its brevity.\n\nThere are several spinoffs of BLEU. Sacre-BLEU (Post, 2018) aims to standardize tokenization for BLEU. NIST (Doddington, 2002) modifies the brevity penalty by reducing the influence of low frequency $n$-grams on the final score, and weights $n$-grams according to their information value so that the evaluation is not skewed by over emphasis of common $n$-grams. METEOR (Banerjee and Lavie, 2005) uses WordNet to accommodate variations in word forms and synonyms, utilizes both precision and recall, and introduces a fragmentation penalty to preserve word order.\n\n3.1.3   Selection of Relevant Keywords\nOne way to address the aforementioned \"syntax\nover substance\" errors (cf. Sec. 3.1.1) is to com-\npute term-overlap metrics between the candidate\nsummary and selected terms, rather than com-\nputing them over entire candidate responses and\ngolden reference responses. While keyword selec-\ntion may improve evaluation it can also be brit-\ntle; e.g., repeating a question may result in good\nscores, even if the substance of the question is not\naddressed. It is therefore important to be thought-\nful about when to use keyword selection, one's\nchoice of keywords, and what other metrics to\ncombine it with."
        },
        {
            "text": "Many of the aforementioned limitations of ROUGE extend to BLEU, as both of these are term-overlap metrics. However, with all term-overlap metrics, there is an inherent tradeoff between precision-based metrics and recall-based metrics. Consider Candidate C from Table 1. A recall-oriented metric like ROUGE-L might score this reasonably well for capturing the main event, but a precision-focused metric like BLEU would penalize it for its brevity.",
            "page": 6,
            "x": 303,
            "y": 91,
            "width": 225,
            "height": 135,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-term",
            "chunk_id": "53010092-45c5-42a1-bcf4-a60bdf429b22",
            "group_text": "3.1   Term Overlap Metrics\n\nTerm overlap metrics, have become de-facto standards throughout the literature. These compare generated text to golden reference samples. They are simple, efficient, and language-agnostic, but have limitations. First, they often miss deeper semantic meaning and context and can yield substantially different scores when encountering paraphrases with identical semantics. Second, they are bad at addressing fluency, factuality, and structure. Finally they are sensitive to length and repetition. Thus, we recommend that metrics be aggregated over multiple diverse, well-written ground truth examples capturing the same semantics for each LLM response.\n\n### 3.1.1   ROUGE Metrics for Summarization\n\nRecall-Oriented Understudy for Gisting Evaluation (ROUGE) ([Lin, 2004; Lin and Hovy, 2003](Lin, 2004; Lin and Hovy, 2003)) is a set of metrics which assesses the quality of automatically generated summaries by comparing overlapping units of text, like words, phrases, *n*-grams (ROUGE-*n*), longest common subsequence (ROUGE-L), and skip-bigrams (ROUGE-S) between a generated summary and a human-written reference summary. When computed based on recall, ROUGE scores emphasize capturing relevant information from the reference, but they might not penalize a summary for being too long. Precision based ROUGE, on the other hand, favors concise summaries, even if they miss some relevant details. ROUGE can also be computed based on $F_{\\beta}$-Measure, weighting respective contributions between precision and recall according\n\nto $\\beta$. ROUGE scores range from 0 to 1.$^2$\n    ROUGE scores aim to evaluate summarization quality, e.g., when an LLM is tasked with summarizing reports, logs, or briefs into shorter/different versions and there are golden samples of specific length and style with specific meaningful terms. ROUGE scores are also useful when answering questions where the golden response contains multiple key terms and phrases that are considered mandatory for a \"good\" response.\n\n**Limitations:** ROUGE scores often capture only surface-level word/phrase similarity, but do not capture nuanced differences in meaning or context. This precludes accurate evaluation for factuality, and high scores can be achieved based on syntactic term overlap rather than underly-\n\n\u00b2In practice, across a large dataset it is unlikely to have scores of 0 or 1 for either ROUGE or BLEU and often indicates an implementation error.\n\ning semantics, limiting utility in evaluating Q/A\nand generation tasks that require semantic accu-\nracy. Consider Candidate A from Table 1. Be-\ncause it shares most of its n-grams (like \"Apollo\n11 mission\", \"primary purpose\", \"astronauts Neil\nArmstrong and Buzz Aldrin\") with the Golden\nResponse, it would achieve a deceptively high\nROUGE score despite not capturing factual accu-\nracy. Conversely, ROUGE is sensitive to para-\nphrasing: Candidate B is factually correct, but\nwould receive a lower ROUGE score than Can-\ndidate A because it uses synonyms (\"main goal\"\nvs. \"primary purpose\", \"moon rocks\" vs. \"lunar\nsamples\"). This demonstrates ROUGE\u2019s limita-\ntions in handling valid paraphrasing, often reward-\ning syntax over substance, and highlights the need\nfor multiple golden reference examples. In con-\ntexts where responses involve generation of code\nand scripts, ROUGE scores may also err on the\nside of syntax over substance.\n\n3.1.2   BLEU for Translation  \nBilingual Evaluation Understudy (BLEU) (Papineni et al., 2002) is a set of metrics which measure translation quality, comparing a generated translation to a golden reference translation, measuring correspondence between $n$-grams, while penalizing brevity so that shorter translations are not over-rewarded. The brevity penalty is calculated based on the length of the candidate translation and the length of the reference translation. BLEU combines precision scores over several $n$-gram sizes with weights for each $n$-gram size. The larger $n$, the better ordering is taken into account within the BLEU score. Typically several values of $n$ are used. Like ROUGE, BLEU scores range from 0 to 1, with 1 indicating perfect match between translation and reference and 0 indicating no match.\n\nBLEU scores were designed for evaluating translations, and something akin to BLEU may serve as a first order metric for generating code or domain-specific queries from natural language, but BLEU has limitations for these tasks: it can assess syntax and structure but cannot assess underlying functionality, logical correctness, or semantic meaning of generated code, nor does it deal well with comments/documentation. BLEU can also be used for evaluating generations that are short, where the emphasis is on precision. ROUGE should be chosen over BLEU for summarizing sequences, as BLEU may penalize valid rephrasing; ROUGE can offer insight into struc-\n\nture/content and in a recall regime does not penal-\nize valid rephrasing.\n\nMany of the aforementioned limitations of ROUGE extend to BLEU, as both of these are term-overlap metrics. However, with all term-overlap metrics, there is an inherent tradeoff between precision-based metrics and recall-based metrics. Consider Candidate C from Table 1. A recall-oriented metric like ROUGE-L might score this reasonably well for capturing the main event, but a precision-focused metric like BLEU would penalize it for its brevity.\n\nThere are several spinoffs of BLEU. Sacre-BLEU (Post, 2018) aims to standardize tokenization for BLEU. NIST (Doddington, 2002) modifies the brevity penalty by reducing the influence of low frequency $n$-grams on the final score, and weights $n$-grams according to their information value so that the evaluation is not skewed by over emphasis of common $n$-grams. METEOR (Banerjee and Lavie, 2005) uses WordNet to accommodate variations in word forms and synonyms, utilizes both precision and recall, and introduces a fragmentation penalty to preserve word order.\n\n3.1.3   Selection of Relevant Keywords\nOne way to address the aforementioned \"syntax\nover substance\" errors (cf. Sec. 3.1.1) is to com-\npute term-overlap metrics between the candidate\nsummary and selected terms, rather than com-\nputing them over entire candidate responses and\ngolden reference responses. While keyword selec-\ntion may improve evaluation it can also be brit-\ntle; e.g., repeating a question may result in good\nscores, even if the substance of the question is not\naddressed. It is therefore important to be thought-\nful about when to use keyword selection, one's\nchoice of keywords, and what other metrics to\ncombine it with."
        },
        {
            "text": "There are several spinoffs of BLEU. Sacre-BLEU (Post, 2018) aims to standardize tokenization for BLEU. NIST (Doddington, 2002) modifies the brevity penalty by reducing the influence of low frequency $n$-grams on the final score, and weights $n$-grams according to their information value so that the evaluation is not skewed by over emphasis of common $n$-grams. METEOR (Banerjee and Lavie, 2005) uses WordNet to accommodate variations in word forms and synonyms, utilizes both precision and recall, and introduces a fragmentation penalty to preserve word order.",
            "page": 6,
            "x": 304,
            "y": 227,
            "width": 225,
            "height": 163,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-term",
            "chunk_id": "b765ca70-cc41-4bab-8e95-039ca0764aa6",
            "group_text": "3.1   Term Overlap Metrics\n\nTerm overlap metrics, have become de-facto standards throughout the literature. These compare generated text to golden reference samples. They are simple, efficient, and language-agnostic, but have limitations. First, they often miss deeper semantic meaning and context and can yield substantially different scores when encountering paraphrases with identical semantics. Second, they are bad at addressing fluency, factuality, and structure. Finally they are sensitive to length and repetition. Thus, we recommend that metrics be aggregated over multiple diverse, well-written ground truth examples capturing the same semantics for each LLM response.\n\n### 3.1.1   ROUGE Metrics for Summarization\n\nRecall-Oriented Understudy for Gisting Evaluation (ROUGE) ([Lin, 2004; Lin and Hovy, 2003](Lin, 2004; Lin and Hovy, 2003)) is a set of metrics which assesses the quality of automatically generated summaries by comparing overlapping units of text, like words, phrases, *n*-grams (ROUGE-*n*), longest common subsequence (ROUGE-L), and skip-bigrams (ROUGE-S) between a generated summary and a human-written reference summary. When computed based on recall, ROUGE scores emphasize capturing relevant information from the reference, but they might not penalize a summary for being too long. Precision based ROUGE, on the other hand, favors concise summaries, even if they miss some relevant details. ROUGE can also be computed based on $F_{\\beta}$-Measure, weighting respective contributions between precision and recall according\n\nto $\\beta$. ROUGE scores range from 0 to 1.$^2$\n    ROUGE scores aim to evaluate summarization quality, e.g., when an LLM is tasked with summarizing reports, logs, or briefs into shorter/different versions and there are golden samples of specific length and style with specific meaningful terms. ROUGE scores are also useful when answering questions where the golden response contains multiple key terms and phrases that are considered mandatory for a \"good\" response.\n\n**Limitations:** ROUGE scores often capture only surface-level word/phrase similarity, but do not capture nuanced differences in meaning or context. This precludes accurate evaluation for factuality, and high scores can be achieved based on syntactic term overlap rather than underly-\n\n\u00b2In practice, across a large dataset it is unlikely to have scores of 0 or 1 for either ROUGE or BLEU and often indicates an implementation error.\n\ning semantics, limiting utility in evaluating Q/A\nand generation tasks that require semantic accu-\nracy. Consider Candidate A from Table 1. Be-\ncause it shares most of its n-grams (like \"Apollo\n11 mission\", \"primary purpose\", \"astronauts Neil\nArmstrong and Buzz Aldrin\") with the Golden\nResponse, it would achieve a deceptively high\nROUGE score despite not capturing factual accu-\nracy. Conversely, ROUGE is sensitive to para-\nphrasing: Candidate B is factually correct, but\nwould receive a lower ROUGE score than Can-\ndidate A because it uses synonyms (\"main goal\"\nvs. \"primary purpose\", \"moon rocks\" vs. \"lunar\nsamples\"). This demonstrates ROUGE\u2019s limita-\ntions in handling valid paraphrasing, often reward-\ning syntax over substance, and highlights the need\nfor multiple golden reference examples. In con-\ntexts where responses involve generation of code\nand scripts, ROUGE scores may also err on the\nside of syntax over substance.\n\n3.1.2   BLEU for Translation  \nBilingual Evaluation Understudy (BLEU) (Papineni et al., 2002) is a set of metrics which measure translation quality, comparing a generated translation to a golden reference translation, measuring correspondence between $n$-grams, while penalizing brevity so that shorter translations are not over-rewarded. The brevity penalty is calculated based on the length of the candidate translation and the length of the reference translation. BLEU combines precision scores over several $n$-gram sizes with weights for each $n$-gram size. The larger $n$, the better ordering is taken into account within the BLEU score. Typically several values of $n$ are used. Like ROUGE, BLEU scores range from 0 to 1, with 1 indicating perfect match between translation and reference and 0 indicating no match.\n\nBLEU scores were designed for evaluating translations, and something akin to BLEU may serve as a first order metric for generating code or domain-specific queries from natural language, but BLEU has limitations for these tasks: it can assess syntax and structure but cannot assess underlying functionality, logical correctness, or semantic meaning of generated code, nor does it deal well with comments/documentation. BLEU can also be used for evaluating generations that are short, where the emphasis is on precision. ROUGE should be chosen over BLEU for summarizing sequences, as BLEU may penalize valid rephrasing; ROUGE can offer insight into struc-\n\nture/content and in a recall regime does not penal-\nize valid rephrasing.\n\nMany of the aforementioned limitations of ROUGE extend to BLEU, as both of these are term-overlap metrics. However, with all term-overlap metrics, there is an inherent tradeoff between precision-based metrics and recall-based metrics. Consider Candidate C from Table 1. A recall-oriented metric like ROUGE-L might score this reasonably well for capturing the main event, but a precision-focused metric like BLEU would penalize it for its brevity.\n\nThere are several spinoffs of BLEU. Sacre-BLEU (Post, 2018) aims to standardize tokenization for BLEU. NIST (Doddington, 2002) modifies the brevity penalty by reducing the influence of low frequency $n$-grams on the final score, and weights $n$-grams according to their information value so that the evaluation is not skewed by over emphasis of common $n$-grams. METEOR (Banerjee and Lavie, 2005) uses WordNet to accommodate variations in word forms and synonyms, utilizes both precision and recall, and introduces a fragmentation penalty to preserve word order.\n\n3.1.3   Selection of Relevant Keywords\nOne way to address the aforementioned \"syntax\nover substance\" errors (cf. Sec. 3.1.1) is to com-\npute term-overlap metrics between the candidate\nsummary and selected terms, rather than com-\nputing them over entire candidate responses and\ngolden reference responses. While keyword selec-\ntion may improve evaluation it can also be brit-\ntle; e.g., repeating a question may result in good\nscores, even if the substance of the question is not\naddressed. It is therefore important to be thought-\nful about when to use keyword selection, one's\nchoice of keywords, and what other metrics to\ncombine it with."
        },
        {
            "text": "3.1.3   Selection of Relevant Keywords\nOne way to address the aforementioned \"syntax\nover substance\" errors (cf. Sec. 3.1.1) is to com-\npute term-overlap metrics between the candidate\nsummary and selected terms, rather than com-\nputing them over entire candidate responses and\ngolden reference responses. While keyword selec-\ntion may improve evaluation it can also be brit-\ntle; e.g., repeating a question may result in good\nscores, even if the substance of the question is not\naddressed. It is therefore important to be thought-\nful about when to use keyword selection, one's\nchoice of keywords, and what other metrics to\ncombine it with.",
            "page": 6,
            "x": 304,
            "y": 396,
            "width": 226,
            "height": 194,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "12-term",
            "chunk_id": "af37e575-0313-4f7f-bec6-3c5aac973e7f",
            "group_text": "3.1   Term Overlap Metrics\n\nTerm overlap metrics, have become de-facto standards throughout the literature. These compare generated text to golden reference samples. They are simple, efficient, and language-agnostic, but have limitations. First, they often miss deeper semantic meaning and context and can yield substantially different scores when encountering paraphrases with identical semantics. Second, they are bad at addressing fluency, factuality, and structure. Finally they are sensitive to length and repetition. Thus, we recommend that metrics be aggregated over multiple diverse, well-written ground truth examples capturing the same semantics for each LLM response.\n\n### 3.1.1   ROUGE Metrics for Summarization\n\nRecall-Oriented Understudy for Gisting Evaluation (ROUGE) ([Lin, 2004; Lin and Hovy, 2003](Lin, 2004; Lin and Hovy, 2003)) is a set of metrics which assesses the quality of automatically generated summaries by comparing overlapping units of text, like words, phrases, *n*-grams (ROUGE-*n*), longest common subsequence (ROUGE-L), and skip-bigrams (ROUGE-S) between a generated summary and a human-written reference summary. When computed based on recall, ROUGE scores emphasize capturing relevant information from the reference, but they might not penalize a summary for being too long. Precision based ROUGE, on the other hand, favors concise summaries, even if they miss some relevant details. ROUGE can also be computed based on $F_{\\beta}$-Measure, weighting respective contributions between precision and recall according\n\nto $\\beta$. ROUGE scores range from 0 to 1.$^2$\n    ROUGE scores aim to evaluate summarization quality, e.g., when an LLM is tasked with summarizing reports, logs, or briefs into shorter/different versions and there are golden samples of specific length and style with specific meaningful terms. ROUGE scores are also useful when answering questions where the golden response contains multiple key terms and phrases that are considered mandatory for a \"good\" response.\n\n**Limitations:** ROUGE scores often capture only surface-level word/phrase similarity, but do not capture nuanced differences in meaning or context. This precludes accurate evaluation for factuality, and high scores can be achieved based on syntactic term overlap rather than underly-\n\n\u00b2In practice, across a large dataset it is unlikely to have scores of 0 or 1 for either ROUGE or BLEU and often indicates an implementation error.\n\ning semantics, limiting utility in evaluating Q/A\nand generation tasks that require semantic accu-\nracy. Consider Candidate A from Table 1. Be-\ncause it shares most of its n-grams (like \"Apollo\n11 mission\", \"primary purpose\", \"astronauts Neil\nArmstrong and Buzz Aldrin\") with the Golden\nResponse, it would achieve a deceptively high\nROUGE score despite not capturing factual accu-\nracy. Conversely, ROUGE is sensitive to para-\nphrasing: Candidate B is factually correct, but\nwould receive a lower ROUGE score than Can-\ndidate A because it uses synonyms (\"main goal\"\nvs. \"primary purpose\", \"moon rocks\" vs. \"lunar\nsamples\"). This demonstrates ROUGE\u2019s limita-\ntions in handling valid paraphrasing, often reward-\ning syntax over substance, and highlights the need\nfor multiple golden reference examples. In con-\ntexts where responses involve generation of code\nand scripts, ROUGE scores may also err on the\nside of syntax over substance.\n\n3.1.2   BLEU for Translation  \nBilingual Evaluation Understudy (BLEU) (Papineni et al., 2002) is a set of metrics which measure translation quality, comparing a generated translation to a golden reference translation, measuring correspondence between $n$-grams, while penalizing brevity so that shorter translations are not over-rewarded. The brevity penalty is calculated based on the length of the candidate translation and the length of the reference translation. BLEU combines precision scores over several $n$-gram sizes with weights for each $n$-gram size. The larger $n$, the better ordering is taken into account within the BLEU score. Typically several values of $n$ are used. Like ROUGE, BLEU scores range from 0 to 1, with 1 indicating perfect match between translation and reference and 0 indicating no match.\n\nBLEU scores were designed for evaluating translations, and something akin to BLEU may serve as a first order metric for generating code or domain-specific queries from natural language, but BLEU has limitations for these tasks: it can assess syntax and structure but cannot assess underlying functionality, logical correctness, or semantic meaning of generated code, nor does it deal well with comments/documentation. BLEU can also be used for evaluating generations that are short, where the emphasis is on precision. ROUGE should be chosen over BLEU for summarizing sequences, as BLEU may penalize valid rephrasing; ROUGE can offer insight into struc-\n\nture/content and in a recall regime does not penal-\nize valid rephrasing.\n\nMany of the aforementioned limitations of ROUGE extend to BLEU, as both of these are term-overlap metrics. However, with all term-overlap metrics, there is an inherent tradeoff between precision-based metrics and recall-based metrics. Consider Candidate C from Table 1. A recall-oriented metric like ROUGE-L might score this reasonably well for capturing the main event, but a precision-focused metric like BLEU would penalize it for its brevity.\n\nThere are several spinoffs of BLEU. Sacre-BLEU (Post, 2018) aims to standardize tokenization for BLEU. NIST (Doddington, 2002) modifies the brevity penalty by reducing the influence of low frequency $n$-grams on the final score, and weights $n$-grams according to their information value so that the evaluation is not skewed by over emphasis of common $n$-grams. METEOR (Banerjee and Lavie, 2005) uses WordNet to accommodate variations in word forms and synonyms, utilizes both precision and recall, and introduces a fragmentation penalty to preserve word order.\n\n3.1.3   Selection of Relevant Keywords\nOne way to address the aforementioned \"syntax\nover substance\" errors (cf. Sec. 3.1.1) is to com-\npute term-overlap metrics between the candidate\nsummary and selected terms, rather than com-\nputing them over entire candidate responses and\ngolden reference responses. While keyword selec-\ntion may improve evaluation it can also be brit-\ntle; e.g., repeating a question may result in good\nscores, even if the substance of the question is not\naddressed. It is therefore important to be thought-\nful about when to use keyword selection, one's\nchoice of keywords, and what other metrics to\ncombine it with."
        },
        {
            "text": "3.2  Semantic Similarity Metrics\n\nSemantic similarity metrics compare generated and reference responses by converting them into numerical vectors (embeddings) and calculating their similarity (e.g., using cosine similarity). Unlike heuristic approaches like TF-IDF (Salton et al., 1975) and BM-25(Robertson et al., 1995), embeddings are generated via machine learnt language models and are designed to capture syntax, semantics, and structure. Embedding models can range from lighter-weight, e.g., Word2Vec (Mikolov, 2013) to heavier weight distillations of",
            "page": 6,
            "x": 303,
            "y": 598,
            "width": 226,
            "height": 170,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "13-semantic",
            "chunk_id": "ed76d4bd-e340-46e1-8daa-8cef63e52446",
            "group_text": "3.2  Semantic Similarity Metrics\n\nSemantic similarity metrics compare generated and reference responses by converting them into numerical vectors (embeddings) and calculating their similarity (e.g., using cosine similarity). Unlike heuristic approaches like TF-IDF (Salton et al., 1975) and BM-25(Robertson et al., 1995), embeddings are generated via machine learnt language models and are designed to capture syntax, semantics, and structure. Embedding models can range from lighter-weight, e.g., Word2Vec (Mikolov, 2013) to heavier weight distillations of\n\nLLMs, e.g., (Lee et al., 2024).\n\nEmbeddings are straightforward to compute and provide quantifiable measures of semantic alignment, assuming that the embedding model captures underlying semantics of the data. A \"good\" embedding-based semantic similarity metric would correctly identify that Candidate B from Table 1 is much closer in meaning to the Golden Response than Candidate A is. The vector embeddings resulting from \"Moon\" and \"lunar\" would be close, while the embeddings resulting from \"Mars\" and \"Martian\" would be distant, overcoming the simple term-overlap problem. For proprietary data formats, it may be necessary to train a custom embedding model. Moreover, embedding-based similarity does not directly address fluency or factual accuracy of the candidate response."
        },
        {
            "text": "LLMs, e.g., (Lee et al., 2024).\n\nEmbeddings are straightforward to compute and provide quantifiable measures of semantic alignment, assuming that the embedding model captures underlying semantics of the data. A \"good\" embedding-based semantic similarity metric would correctly identify that Candidate B from Table 1 is much closer in meaning to the Golden Response than Candidate A is. The vector embeddings resulting from \"Moon\" and \"lunar\" would be close, while the embeddings resulting from \"Mars\" and \"Martian\" would be distant, overcoming the simple term-overlap problem. For proprietary data formats, it may be necessary to train a custom embedding model. Moreover, embedding-based similarity does not directly address fluency or factual accuracy of the candidate response.",
            "page": 7,
            "x": 67,
            "y": 61,
            "width": 227,
            "height": 233,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "13-semantic",
            "chunk_id": "51918a7f-02d2-4342-812b-5a745f78ff3d",
            "group_text": "3.2  Semantic Similarity Metrics\n\nSemantic similarity metrics compare generated and reference responses by converting them into numerical vectors (embeddings) and calculating their similarity (e.g., using cosine similarity). Unlike heuristic approaches like TF-IDF (Salton et al., 1975) and BM-25(Robertson et al., 1995), embeddings are generated via machine learnt language models and are designed to capture syntax, semantics, and structure. Embedding models can range from lighter-weight, e.g., Word2Vec (Mikolov, 2013) to heavier weight distillations of\n\nLLMs, e.g., (Lee et al., 2024).\n\nEmbeddings are straightforward to compute and provide quantifiable measures of semantic alignment, assuming that the embedding model captures underlying semantics of the data. A \"good\" embedding-based semantic similarity metric would correctly identify that Candidate B from Table 1 is much closer in meaning to the Golden Response than Candidate A is. The vector embeddings resulting from \"Moon\" and \"lunar\" would be close, while the embeddings resulting from \"Mars\" and \"Martian\" would be distant, overcoming the simple term-overlap problem. For proprietary data formats, it may be necessary to train a custom embedding model. Moreover, embedding-based similarity does not directly address fluency or factual accuracy of the candidate response."
        },
        {
            "text": "3.3  NLI/Entailment Metrics  \nNatural Language Inference (NLI a.k.a. entailment) metrics use ML models to determine the relationship between two statements: a premise (A) and a hypothesis (B). They assess if B logically follows from A (entailment), if A and B are unrelated (neutral), or if B contradicts A (contradiction). This is valuable for evaluating factuality by checking if a statement aligns with known facts.",
            "page": 7,
            "x": 67,
            "y": 300,
            "width": 227,
            "height": 128,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "14-nli",
            "chunk_id": "d074a4a3-9ea9-4137-ba03-d5e62cf12c06",
            "group_text": "3.3  NLI/Entailment Metrics  \nNatural Language Inference (NLI a.k.a. entailment) metrics use ML models to determine the relationship between two statements: a premise (A) and a hypothesis (B). They assess if B logically follows from A (entailment), if A and B are unrelated (neutral), or if B contradicts A (contradiction). This is valuable for evaluating factuality by checking if a statement aligns with known facts.\n\nConsidering the examples in Table 1, the Golden Response would be the premise. Candidate A would result in a \"contradiction\" score, as landing on Mars contradicts landing on the moon. Candidate B would result in an \"entailment\" score, as its claims logically follow from the premise. Candidate D would result in a \"neutral\" score, as its claims about airline safety are unrelated to the premise. This demonstrates how NLI is a powerful tool for evaluating factuality.\n\nTo implement NLI, a language model is finetuned on an entailment dataset to generate an entailment score. The model can also incorporate additional context to guide the evaluation towards specific aspects like factual consistency or fluency.\n\nAs with all model-derived metrics, the quality\nof an entailment metric depends on the quality of\nthe model used to derive the metric \u2013 e.g., the\nmodel must appropriately capture the syntax and\nsemantics of the premise and the hypothesis."
        },
        {
            "text": "Considering the examples in Table 1, the Golden Response would be the premise. Candidate A would result in a \"contradiction\" score, as landing on Mars contradicts landing on the moon. Candidate B would result in an \"entailment\" score, as its claims logically follow from the premise. Candidate D would result in a \"neutral\" score, as its claims about airline safety are unrelated to the premise. This demonstrates how NLI is a powerful tool for evaluating factuality.",
            "page": 7,
            "x": 67,
            "y": 429,
            "width": 227,
            "height": 134,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "14-nli",
            "chunk_id": "55dad2fb-00d5-49a1-a2c8-394c9e6d5cf2",
            "group_text": "3.3  NLI/Entailment Metrics  \nNatural Language Inference (NLI a.k.a. entailment) metrics use ML models to determine the relationship between two statements: a premise (A) and a hypothesis (B). They assess if B logically follows from A (entailment), if A and B are unrelated (neutral), or if B contradicts A (contradiction). This is valuable for evaluating factuality by checking if a statement aligns with known facts.\n\nConsidering the examples in Table 1, the Golden Response would be the premise. Candidate A would result in a \"contradiction\" score, as landing on Mars contradicts landing on the moon. Candidate B would result in an \"entailment\" score, as its claims logically follow from the premise. Candidate D would result in a \"neutral\" score, as its claims about airline safety are unrelated to the premise. This demonstrates how NLI is a powerful tool for evaluating factuality.\n\nTo implement NLI, a language model is finetuned on an entailment dataset to generate an entailment score. The model can also incorporate additional context to guide the evaluation towards specific aspects like factual consistency or fluency.\n\nAs with all model-derived metrics, the quality\nof an entailment metric depends on the quality of\nthe model used to derive the metric \u2013 e.g., the\nmodel must appropriately capture the syntax and\nsemantics of the premise and the hypothesis."
        },
        {
            "text": "To implement NLI, a language model is finetuned on an entailment dataset to generate an entailment score. The model can also incorporate additional context to guide the evaluation towards specific aspects like factual consistency or fluency.",
            "page": 7,
            "x": 68,
            "y": 564,
            "width": 226,
            "height": 68,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "14-nli",
            "chunk_id": "1fe4ec54-e31e-4e83-ae8b-9d3d41657047",
            "group_text": "3.3  NLI/Entailment Metrics  \nNatural Language Inference (NLI a.k.a. entailment) metrics use ML models to determine the relationship between two statements: a premise (A) and a hypothesis (B). They assess if B logically follows from A (entailment), if A and B are unrelated (neutral), or if B contradicts A (contradiction). This is valuable for evaluating factuality by checking if a statement aligns with known facts.\n\nConsidering the examples in Table 1, the Golden Response would be the premise. Candidate A would result in a \"contradiction\" score, as landing on Mars contradicts landing on the moon. Candidate B would result in an \"entailment\" score, as its claims logically follow from the premise. Candidate D would result in a \"neutral\" score, as its claims about airline safety are unrelated to the premise. This demonstrates how NLI is a powerful tool for evaluating factuality.\n\nTo implement NLI, a language model is finetuned on an entailment dataset to generate an entailment score. The model can also incorporate additional context to guide the evaluation towards specific aspects like factual consistency or fluency.\n\nAs with all model-derived metrics, the quality\nof an entailment metric depends on the quality of\nthe model used to derive the metric \u2013 e.g., the\nmodel must appropriately capture the syntax and\nsemantics of the premise and the hypothesis."
        },
        {
            "text": "As with all model-derived metrics, the quality\nof an entailment metric depends on the quality of\nthe model used to derive the metric \u2013 e.g., the\nmodel must appropriately capture the syntax and\nsemantics of the premise and the hypothesis.",
            "page": 7,
            "x": 68,
            "y": 632,
            "width": 226,
            "height": 69,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "14-nli",
            "chunk_id": "809b25ca-5960-4520-8e60-e017ddaf7fee",
            "group_text": "3.3  NLI/Entailment Metrics  \nNatural Language Inference (NLI a.k.a. entailment) metrics use ML models to determine the relationship between two statements: a premise (A) and a hypothesis (B). They assess if B logically follows from A (entailment), if A and B are unrelated (neutral), or if B contradicts A (contradiction). This is valuable for evaluating factuality by checking if a statement aligns with known facts.\n\nConsidering the examples in Table 1, the Golden Response would be the premise. Candidate A would result in a \"contradiction\" score, as landing on Mars contradicts landing on the moon. Candidate B would result in an \"entailment\" score, as its claims logically follow from the premise. Candidate D would result in a \"neutral\" score, as its claims about airline safety are unrelated to the premise. This demonstrates how NLI is a powerful tool for evaluating factuality.\n\nTo implement NLI, a language model is finetuned on an entailment dataset to generate an entailment score. The model can also incorporate additional context to guide the evaluation towards specific aspects like factual consistency or fluency.\n\nAs with all model-derived metrics, the quality\nof an entailment metric depends on the quality of\nthe model used to derive the metric \u2013 e.g., the\nmodel must appropriately capture the syntax and\nsemantics of the premise and the hypothesis."
        },
        {
            "text": "3.4   LLM Autorater Metrics\nLLMs can serve as scalable automated raters (\"autoraters\") to assess response quality on various aspects like fluency, factuality, or safety. They can",
            "page": 7,
            "x": 67,
            "y": 706,
            "width": 227,
            "height": 63,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "15-llm",
            "chunk_id": "06691965-b166-4fbe-b0ca-2c597fcddbb9",
            "group_text": "3.4   LLM Autorater Metrics\nLLMs can serve as scalable automated raters (\"autoraters\") to assess response quality on various aspects like fluency, factuality, or safety. They can\n\nassign individual scores (point-wise) or compare\nmultiple responses directly (side-by-side, or SxS).\nAutoraters can can handle nuance beyond simple\nmetrics, for example, consider the examples in Ta-\nble 1: if prompted to evaluate \"factual accuracy\", a\nproperly functioning autorater would rank the re-\nsponses: Golden > B > C > A > D. If prompted\nto evaluate only \"fluency,\" it might rate all of them\nhighly whereas if prompted to evaluate accuracy\nand completeness comparing B and C, the au-\ntorater would choose B.\n\nThe simplest scenario is where an LLM takes a reference sample with responses A and B as context and is prompted to output a binary score 0/1 depending on whether it prefers response A or response B. More elaborate evaluation often involves analyzing SxS comparisons like an A/B test. Statistical hypothesis tests (cf. Table 2) can determine if one response is preferred over another with statistical significance (i.e., when the resulting *p*-value is below a chosen significance level, \u03b1). However, a low *p*-value does not indicate the magnitude of the difference, so practical significance should also be considered and can be treated more formally by quantifying effect size.\n\nDespite the utility of autoraters, an autorater\u2019s effectiveness depends on its own quality and any grounding context provided (cf. Sec. 2.1.4). Key limitations include:\nResource intensiveness: LLM evaluation requires significant computation.\nLack of standardization: Comparing results across different datasets and models is challenging.\nBiases: LLM raters can exhibit biases towards verbosity, position, self-enhancement, lists, and specific styles.\nDistorted aggregation: Simple win/lose ratings can obscure nuanced differences between responses."
        },
        {
            "text": "assign individual scores (point-wise) or compare\nmultiple responses directly (side-by-side, or SxS).\nAutoraters can can handle nuance beyond simple\nmetrics, for example, consider the examples in Ta-\nble 1: if prompted to evaluate \"factual accuracy\", a\nproperly functioning autorater would rank the re-\nsponses: Golden > B > C > A > D. If prompted\nto evaluate only \"fluency,\" it might rate all of them\nhighly whereas if prompted to evaluate accuracy\nand completeness comparing B and C, the au-\ntorater would choose B.",
            "page": 7,
            "x": 303,
            "y": 63,
            "width": 226,
            "height": 149,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "15-llm",
            "chunk_id": "b0b78350-d246-4f41-98d5-2557744d85ee",
            "group_text": "3.4   LLM Autorater Metrics\nLLMs can serve as scalable automated raters (\"autoraters\") to assess response quality on various aspects like fluency, factuality, or safety. They can\n\nassign individual scores (point-wise) or compare\nmultiple responses directly (side-by-side, or SxS).\nAutoraters can can handle nuance beyond simple\nmetrics, for example, consider the examples in Ta-\nble 1: if prompted to evaluate \"factual accuracy\", a\nproperly functioning autorater would rank the re-\nsponses: Golden > B > C > A > D. If prompted\nto evaluate only \"fluency,\" it might rate all of them\nhighly whereas if prompted to evaluate accuracy\nand completeness comparing B and C, the au-\ntorater would choose B.\n\nThe simplest scenario is where an LLM takes a reference sample with responses A and B as context and is prompted to output a binary score 0/1 depending on whether it prefers response A or response B. More elaborate evaluation often involves analyzing SxS comparisons like an A/B test. Statistical hypothesis tests (cf. Table 2) can determine if one response is preferred over another with statistical significance (i.e., when the resulting *p*-value is below a chosen significance level, \u03b1). However, a low *p*-value does not indicate the magnitude of the difference, so practical significance should also be considered and can be treated more formally by quantifying effect size.\n\nDespite the utility of autoraters, an autorater\u2019s effectiveness depends on its own quality and any grounding context provided (cf. Sec. 2.1.4). Key limitations include:\nResource intensiveness: LLM evaluation requires significant computation.\nLack of standardization: Comparing results across different datasets and models is challenging.\nBiases: LLM raters can exhibit biases towards verbosity, position, self-enhancement, lists, and specific styles.\nDistorted aggregation: Simple win/lose ratings can obscure nuanced differences between responses."
        },
        {
            "text": "The simplest scenario is where an LLM takes a reference sample with responses A and B as context and is prompted to output a binary score 0/1 depending on whether it prefers response A or response B. More elaborate evaluation often involves analyzing SxS comparisons like an A/B test. Statistical hypothesis tests (cf. Table 2) can determine if one response is preferred over another with statistical significance (i.e., when the resulting *p*-value is below a chosen significance level, \u03b1). However, a low *p*-value does not indicate the magnitude of the difference, so practical significance should also be considered and can be treated more formally by quantifying effect size.",
            "page": 7,
            "x": 303,
            "y": 213,
            "width": 226,
            "height": 191,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "15-llm",
            "chunk_id": "48f6ddb5-6567-48ee-ba33-a459bec85375",
            "group_text": "3.4   LLM Autorater Metrics\nLLMs can serve as scalable automated raters (\"autoraters\") to assess response quality on various aspects like fluency, factuality, or safety. They can\n\nassign individual scores (point-wise) or compare\nmultiple responses directly (side-by-side, or SxS).\nAutoraters can can handle nuance beyond simple\nmetrics, for example, consider the examples in Ta-\nble 1: if prompted to evaluate \"factual accuracy\", a\nproperly functioning autorater would rank the re-\nsponses: Golden > B > C > A > D. If prompted\nto evaluate only \"fluency,\" it might rate all of them\nhighly whereas if prompted to evaluate accuracy\nand completeness comparing B and C, the au-\ntorater would choose B.\n\nThe simplest scenario is where an LLM takes a reference sample with responses A and B as context and is prompted to output a binary score 0/1 depending on whether it prefers response A or response B. More elaborate evaluation often involves analyzing SxS comparisons like an A/B test. Statistical hypothesis tests (cf. Table 2) can determine if one response is preferred over another with statistical significance (i.e., when the resulting *p*-value is below a chosen significance level, \u03b1). However, a low *p*-value does not indicate the magnitude of the difference, so practical significance should also be considered and can be treated more formally by quantifying effect size.\n\nDespite the utility of autoraters, an autorater\u2019s effectiveness depends on its own quality and any grounding context provided (cf. Sec. 2.1.4). Key limitations include:\nResource intensiveness: LLM evaluation requires significant computation.\nLack of standardization: Comparing results across different datasets and models is challenging.\nBiases: LLM raters can exhibit biases towards verbosity, position, self-enhancement, lists, and specific styles.\nDistorted aggregation: Simple win/lose ratings can obscure nuanced differences between responses."
        },
        {
            "text": "Despite the utility of autoraters, an autorater\u2019s effectiveness depends on its own quality and any grounding context provided (cf. Sec. 2.1.4). Key limitations include:\nResource intensiveness: LLM evaluation requires significant computation.\nLack of standardization: Comparing results across different datasets and models is challenging.\nBiases: LLM raters can exhibit biases towards verbosity, position, self-enhancement, lists, and specific styles.\nDistorted aggregation: Simple win/lose ratings can obscure nuanced differences between responses.",
            "page": 7,
            "x": 303,
            "y": 405,
            "width": 228,
            "height": 207,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "15-llm",
            "chunk_id": "52a0eee1-e087-43b8-8499-218ab3ad8075",
            "group_text": "3.4   LLM Autorater Metrics\nLLMs can serve as scalable automated raters (\"autoraters\") to assess response quality on various aspects like fluency, factuality, or safety. They can\n\nassign individual scores (point-wise) or compare\nmultiple responses directly (side-by-side, or SxS).\nAutoraters can can handle nuance beyond simple\nmetrics, for example, consider the examples in Ta-\nble 1: if prompted to evaluate \"factual accuracy\", a\nproperly functioning autorater would rank the re-\nsponses: Golden > B > C > A > D. If prompted\nto evaluate only \"fluency,\" it might rate all of them\nhighly whereas if prompted to evaluate accuracy\nand completeness comparing B and C, the au-\ntorater would choose B.\n\nThe simplest scenario is where an LLM takes a reference sample with responses A and B as context and is prompted to output a binary score 0/1 depending on whether it prefers response A or response B. More elaborate evaluation often involves analyzing SxS comparisons like an A/B test. Statistical hypothesis tests (cf. Table 2) can determine if one response is preferred over another with statistical significance (i.e., when the resulting *p*-value is below a chosen significance level, \u03b1). However, a low *p*-value does not indicate the magnitude of the difference, so practical significance should also be considered and can be treated more formally by quantifying effect size.\n\nDespite the utility of autoraters, an autorater\u2019s effectiveness depends on its own quality and any grounding context provided (cf. Sec. 2.1.4). Key limitations include:\nResource intensiveness: LLM evaluation requires significant computation.\nLack of standardization: Comparing results across different datasets and models is challenging.\nBiases: LLM raters can exhibit biases towards verbosity, position, self-enhancement, lists, and specific styles.\nDistorted aggregation: Simple win/lose ratings can obscure nuanced differences between responses."
        },
        {
            "text": "3.5  Perplexity\n\nPerplexity (PPL) (Jelinek et al., 1977) aims to estimate the probability of a response occurring, normalizing based on response length. PPL is a transformation on this such that lower PPL corresponds to higher probability. This simple calculation ostensibly measures the predictive power of a language model without requiring ground truth, but this is a very loose predictor of practical performance at best. Since it uses probabili-",
            "page": 7,
            "x": 302,
            "y": 621,
            "width": 228,
            "height": 148,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "16-perplexity",
            "chunk_id": "87a01fe0-75c4-4fa9-9c5c-40090dca8204",
            "group_text": "3.5  Perplexity\n\nPerplexity (PPL) (Jelinek et al., 1977) aims to estimate the probability of a response occurring, normalizing based on response length. PPL is a transformation on this such that lower PPL corresponds to higher probability. This simple calculation ostensibly measures the predictive power of a language model without requiring ground truth, but this is a very loose predictor of practical performance at best. Since it uses probabili-\n\nties derived from the underlying model (not actual\nprobabilities), it is susceptible to favoring over-\nfit models. Considering Candidate D from Table\n1, the response is grammatically correct and uses\nplausible-sounding language. Therefore, the LLM\nthat generated it might assign it a high probability\n(and thus a low perplexity score). This demon-\nstrates clearly that perplexity is not a reliable mea-\nsure of task performance or factual accuracy, as a\nmodel can be confident in a completely nonsensi-\ncal answer."
        },
        {
            "text": "ties derived from the underlying model (not actual\nprobabilities), it is susceptible to favoring over-\nfit models. Considering Candidate D from Table\n1, the response is grammatically correct and uses\nplausible-sounding language. Therefore, the LLM\nthat generated it might assign it a high probability\n(and thus a low perplexity score). This demon-\nstrates clearly that perplexity is not a reliable mea-\nsure of task performance or factual accuracy, as a\nmodel can be confident in a completely nonsensi-\ncal answer.",
            "page": 8,
            "x": 68,
            "y": 246,
            "width": 226,
            "height": 151,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "16-perplexity",
            "chunk_id": "acfbdb82-5880-408a-9b33-9654f20bbd02",
            "group_text": "3.5  Perplexity\n\nPerplexity (PPL) (Jelinek et al., 1977) aims to estimate the probability of a response occurring, normalizing based on response length. PPL is a transformation on this such that lower PPL corresponds to higher probability. This simple calculation ostensibly measures the predictive power of a language model without requiring ground truth, but this is a very loose predictor of practical performance at best. Since it uses probabili-\n\nties derived from the underlying model (not actual\nprobabilities), it is susceptible to favoring over-\nfit models. Considering Candidate D from Table\n1, the response is grammatically correct and uses\nplausible-sounding language. Therefore, the LLM\nthat generated it might assign it a high probability\n(and thus a low perplexity score). This demon-\nstrates clearly that perplexity is not a reliable mea-\nsure of task performance or factual accuracy, as a\nmodel can be confident in a completely nonsensi-\ncal answer."
        },
        {
            "text": "3.6   Wrap-Up: Connecting Metrics to the Evaluation Framework\n\nMetrics selection is the second foundational pillar in our evaluation framework, building upon the previously established objectives and datasets. As this section details, no single metric is a \"silver bullet\"; instead, a holistic understanding of system performance is formed by computing and tracking multiple different metrics. The key is to create a balanced scorecard depending on the application, complementing traditional term-overlap metrics like ROUGE with semantic similarity, NLI and autorater-based evaluations to capture a more complete picture of quality. Once this suite of metrics is defined, it sets the stage for developing a robust methodology for executing the evaluation and generating these scores.",
            "page": 8,
            "x": 67,
            "y": 403,
            "width": 227,
            "height": 238,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "17-wrap",
            "chunk_id": "944703fa-93b9-481a-96e3-6d275c9d0664",
            "group_text": "3.6   Wrap-Up: Connecting Metrics to the Evaluation Framework\n\nMetrics selection is the second foundational pillar in our evaluation framework, building upon the previously established objectives and datasets. As this section details, no single metric is a \"silver bullet\"; instead, a holistic understanding of system performance is formed by computing and tracking multiple different metrics. The key is to create a balanced scorecard depending on the application, complementing traditional term-overlap metrics like ROUGE with semantic similarity, NLI and autorater-based evaluations to capture a more complete picture of quality. Once this suite of metrics is defined, it sets the stage for developing a robust methodology for executing the evaluation and generating these scores."
        },
        {
            "text": "4  Robust Evaluation Methodology & Execution for Improving LLM-Reliant Systems\n\nIn this section, we discuss how to incorporate specific methodologies into the design and implementation of evaluation suites to address the real-world challenges inherent in LLM-reliant systems. While prior sections focused on creating",
            "page": 8,
            "x": 66,
            "y": 647,
            "width": 228,
            "height": 121,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "18-robust",
            "chunk_id": "e29ea23c-ade1-42dc-9036-595f2e58d0db",
            "group_text": "4  Robust Evaluation Methodology & Execution for Improving LLM-Reliant Systems\n\nIn this section, we discuss how to incorporate specific methodologies into the design and implementation of evaluation suites to address the real-world challenges inherent in LLM-reliant systems. While prior sections focused on creating\n\ndatasets and metrics, this section details the ex-\necution framework required to handle confound-\ning factors like inherent non-determinism from\nstochastic processes, system sensitivity to minor\ninput changes, and the difficulty of disentangling\nthe effects of grounding data from the model\u2019s\nprior knowledge. Furthermore, we present strate-\ngies for measuring complex failure modes such as\nhallucinations and undesirable non-responses. By\ndirectly addressing these issues, practitioners can\nleverage evaluation outputs as actionable guidance\nthroughout the development and deployment life-\ncycles of generative AI systems."
        },
        {
            "text": "datasets and metrics, this section details the ex-\necution framework required to handle confound-\ning factors like inherent non-determinism from\nstochastic processes, system sensitivity to minor\ninput changes, and the difficulty of disentangling\nthe effects of grounding data from the model\u2019s\nprior knowledge. Furthermore, we present strate-\ngies for measuring complex failure modes such as\nhallucinations and undesirable non-responses. By\ndirectly addressing these issues, practitioners can\nleverage evaluation outputs as actionable guidance\nthroughout the development and deployment life-\ncycles of generative AI systems.",
            "page": 8,
            "x": 303,
            "y": 247,
            "width": 226,
            "height": 178,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "18-robust",
            "chunk_id": "7c767b33-c094-4130-bfd4-f5383ace405a",
            "group_text": "4  Robust Evaluation Methodology & Execution for Improving LLM-Reliant Systems\n\nIn this section, we discuss how to incorporate specific methodologies into the design and implementation of evaluation suites to address the real-world challenges inherent in LLM-reliant systems. While prior sections focused on creating\n\ndatasets and metrics, this section details the ex-\necution framework required to handle confound-\ning factors like inherent non-determinism from\nstochastic processes, system sensitivity to minor\ninput changes, and the difficulty of disentangling\nthe effects of grounding data from the model\u2019s\nprior knowledge. Furthermore, we present strate-\ngies for measuring complex failure modes such as\nhallucinations and undesirable non-responses. By\ndirectly addressing these issues, practitioners can\nleverage evaluation outputs as actionable guidance\nthroughout the development and deployment life-\ncycles of generative AI systems."
        },
        {
            "text": "4.1  Handling Non-Determinism and Sensitivity\n\nSources of non-determinism in LLMs stem from stochastic computations, chained model calls, and grounding data/API inconsistencies. Understanding how the pattern of non-determinism occurs is important for making accurate, conservative estimates of system performance given the potential noise it can produce. Note that non-determinism may occur irregularly or over long time windows. For example, estimating system noise only in the evenings when traffic is low may miss the non-determinism that occurs when a single request is broken into multiple sub-tasks that are processed independently under higher load.\"",
            "page": 8,
            "x": 303,
            "y": 433,
            "width": 227,
            "height": 211,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "19-handling",
            "chunk_id": "2225c6b0-bd1b-45e5-9ec9-174de6ae1f22",
            "group_text": "4.1  Handling Non-Determinism and Sensitivity\n\nSources of non-determinism in LLMs stem from stochastic computations, chained model calls, and grounding data/API inconsistencies. Understanding how the pattern of non-determinism occurs is important for making accurate, conservative estimates of system performance given the potential noise it can produce. Note that non-determinism may occur irregularly or over long time windows. For example, estimating system noise only in the evenings when traffic is low may miss the non-determinism that occurs when a single request is broken into multiple sub-tasks that are processed independently under higher load.\"\n\nSelf-Consistency (Wang et al., 2022) is a repeated evaluation strategy for LLMs that can address non-determinism. It involves sampling multiple responses from similar prompts and selecting the most frequent. A common way to generate different responses is by setting the model\u2019s *temperature* parameter to a value greater than zero to enable diverse sampling for the same prompt. Self-Consistency reduces non-determinism by provid-\n\ning the response the system is most likely to generate for a given prompt, and may improve response quality as well. However, it increases the number of LLM calls ($n \\cdot$ # original calls), which may improve response quality but requires more resources and may adversely impact cost/latency.\n\nAnother way to address non-determinism is to\nestimate its effect directly, and adjust metrics es-\ntimates. For example, one could run the evalua-\ntion $n$-times on fixed inputs, average the results,\nand estimate the variability induced by the system\nnoise. By employing a sufficiently large choice of\n$n$, e.g., 10 (or larger), one can then establish an\nongoing baseline of system performance with re-\nported variations, then compare performance im-\npacting changes to this baseline to see if the met-\nrics fall outside the error bounds produced by the\ninherent system noise.\n\nLLM-based systems are sensitive to their inputs such that a small change in the wording, spelling, or even spacing of a prompt can lead to dramatic changes in the response. Thus, understanding how sensitive they are to small variations in the prompt is an important exercise, since these variations are likely to be presented by users in the wild.\n\nSensitivity can be assessed by injecting various types of noise into the prompt and measuring how the responses change. This practice is often part of a broader strategy for behavioral testing (Ribeiro et al., 2020) or robustness evaluation (Wang et al., 2021) and uses a range of perturbations to test model stability. To make this actionable, a variety of noise injection techniques can be employed, ranging from semantically neutral changes to more destructive ones that alter the underlying meaning. For this type of sensitivity evaluation, example ways to introduce prompt noise include case-swapping, random whitespace insertion, random character swapping, and using an LLM to rewrite the prompts. Once these variations are created, one can evaluate the impact of this sensitivity on:\n\n\u2022 Overall metrics: Identify metrics most affected by noise.\n\u2022 Eval set subgroups: Analyze sensitivity across different task types, topics, etc.\n\u2022 Individual data points: Identify data points with high noise susceptibility. These can be especially informative for discovering underlying issues with the prompts, or in the case of a larger composed LLM system, the up-\n\nstream components (e.g., RAG difficulties,\nsummarization failures, etc)."
        },
        {
            "text": "Self-Consistency (Wang et al., 2022) is a repeated evaluation strategy for LLMs that can address non-determinism. It involves sampling multiple responses from similar prompts and selecting the most frequent. A common way to generate different responses is by setting the model\u2019s *temperature* parameter to a value greater than zero to enable diverse sampling for the same prompt. Self-Consistency reduces non-determinism by provid-",
            "page": 8,
            "x": 303,
            "y": 644,
            "width": 227,
            "height": 123,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "19-handling",
            "chunk_id": "8a2e4561-0282-4deb-a68c-250cf41694a9",
            "group_text": "4.1  Handling Non-Determinism and Sensitivity\n\nSources of non-determinism in LLMs stem from stochastic computations, chained model calls, and grounding data/API inconsistencies. Understanding how the pattern of non-determinism occurs is important for making accurate, conservative estimates of system performance given the potential noise it can produce. Note that non-determinism may occur irregularly or over long time windows. For example, estimating system noise only in the evenings when traffic is low may miss the non-determinism that occurs when a single request is broken into multiple sub-tasks that are processed independently under higher load.\"\n\nSelf-Consistency (Wang et al., 2022) is a repeated evaluation strategy for LLMs that can address non-determinism. It involves sampling multiple responses from similar prompts and selecting the most frequent. A common way to generate different responses is by setting the model\u2019s *temperature* parameter to a value greater than zero to enable diverse sampling for the same prompt. Self-Consistency reduces non-determinism by provid-\n\ning the response the system is most likely to generate for a given prompt, and may improve response quality as well. However, it increases the number of LLM calls ($n \\cdot$ # original calls), which may improve response quality but requires more resources and may adversely impact cost/latency.\n\nAnother way to address non-determinism is to\nestimate its effect directly, and adjust metrics es-\ntimates. For example, one could run the evalua-\ntion $n$-times on fixed inputs, average the results,\nand estimate the variability induced by the system\nnoise. By employing a sufficiently large choice of\n$n$, e.g., 10 (or larger), one can then establish an\nongoing baseline of system performance with re-\nported variations, then compare performance im-\npacting changes to this baseline to see if the met-\nrics fall outside the error bounds produced by the\ninherent system noise.\n\nLLM-based systems are sensitive to their inputs such that a small change in the wording, spelling, or even spacing of a prompt can lead to dramatic changes in the response. Thus, understanding how sensitive they are to small variations in the prompt is an important exercise, since these variations are likely to be presented by users in the wild.\n\nSensitivity can be assessed by injecting various types of noise into the prompt and measuring how the responses change. This practice is often part of a broader strategy for behavioral testing (Ribeiro et al., 2020) or robustness evaluation (Wang et al., 2021) and uses a range of perturbations to test model stability. To make this actionable, a variety of noise injection techniques can be employed, ranging from semantically neutral changes to more destructive ones that alter the underlying meaning. For this type of sensitivity evaluation, example ways to introduce prompt noise include case-swapping, random whitespace insertion, random character swapping, and using an LLM to rewrite the prompts. Once these variations are created, one can evaluate the impact of this sensitivity on:\n\n\u2022 Overall metrics: Identify metrics most affected by noise.\n\u2022 Eval set subgroups: Analyze sensitivity across different task types, topics, etc.\n\u2022 Individual data points: Identify data points with high noise susceptibility. These can be especially informative for discovering underlying issues with the prompts, or in the case of a larger composed LLM system, the up-\n\nstream components (e.g., RAG difficulties,\nsummarization failures, etc)."
        },
        {
            "text": "ing the response the system is most likely to generate for a given prompt, and may improve response quality as well. However, it increases the number of LLM calls ($n \\cdot$ # original calls), which may improve response quality but requires more resources and may adversely impact cost/latency.",
            "page": 9,
            "x": 68,
            "y": 62,
            "width": 226,
            "height": 83,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "19-handling",
            "chunk_id": "aa125d3d-6e9f-40cf-a93d-9909e500f9d5",
            "group_text": "4.1  Handling Non-Determinism and Sensitivity\n\nSources of non-determinism in LLMs stem from stochastic computations, chained model calls, and grounding data/API inconsistencies. Understanding how the pattern of non-determinism occurs is important for making accurate, conservative estimates of system performance given the potential noise it can produce. Note that non-determinism may occur irregularly or over long time windows. For example, estimating system noise only in the evenings when traffic is low may miss the non-determinism that occurs when a single request is broken into multiple sub-tasks that are processed independently under higher load.\"\n\nSelf-Consistency (Wang et al., 2022) is a repeated evaluation strategy for LLMs that can address non-determinism. It involves sampling multiple responses from similar prompts and selecting the most frequent. A common way to generate different responses is by setting the model\u2019s *temperature* parameter to a value greater than zero to enable diverse sampling for the same prompt. Self-Consistency reduces non-determinism by provid-\n\ning the response the system is most likely to generate for a given prompt, and may improve response quality as well. However, it increases the number of LLM calls ($n \\cdot$ # original calls), which may improve response quality but requires more resources and may adversely impact cost/latency.\n\nAnother way to address non-determinism is to\nestimate its effect directly, and adjust metrics es-\ntimates. For example, one could run the evalua-\ntion $n$-times on fixed inputs, average the results,\nand estimate the variability induced by the system\nnoise. By employing a sufficiently large choice of\n$n$, e.g., 10 (or larger), one can then establish an\nongoing baseline of system performance with re-\nported variations, then compare performance im-\npacting changes to this baseline to see if the met-\nrics fall outside the error bounds produced by the\ninherent system noise.\n\nLLM-based systems are sensitive to their inputs such that a small change in the wording, spelling, or even spacing of a prompt can lead to dramatic changes in the response. Thus, understanding how sensitive they are to small variations in the prompt is an important exercise, since these variations are likely to be presented by users in the wild.\n\nSensitivity can be assessed by injecting various types of noise into the prompt and measuring how the responses change. This practice is often part of a broader strategy for behavioral testing (Ribeiro et al., 2020) or robustness evaluation (Wang et al., 2021) and uses a range of perturbations to test model stability. To make this actionable, a variety of noise injection techniques can be employed, ranging from semantically neutral changes to more destructive ones that alter the underlying meaning. For this type of sensitivity evaluation, example ways to introduce prompt noise include case-swapping, random whitespace insertion, random character swapping, and using an LLM to rewrite the prompts. Once these variations are created, one can evaluate the impact of this sensitivity on:\n\n\u2022 Overall metrics: Identify metrics most affected by noise.\n\u2022 Eval set subgroups: Analyze sensitivity across different task types, topics, etc.\n\u2022 Individual data points: Identify data points with high noise susceptibility. These can be especially informative for discovering underlying issues with the prompts, or in the case of a larger composed LLM system, the up-\n\nstream components (e.g., RAG difficulties,\nsummarization failures, etc)."
        },
        {
            "text": "Another way to address non-determinism is to\nestimate its effect directly, and adjust metrics es-\ntimates. For example, one could run the evalua-\ntion $n$-times on fixed inputs, average the results,\nand estimate the variability induced by the system\nnoise. By employing a sufficiently large choice of\n$n$, e.g., 10 (or larger), one can then establish an\nongoing baseline of system performance with re-\nported variations, then compare performance im-\npacting changes to this baseline to see if the met-\nrics fall outside the error bounds produced by the\ninherent system noise.",
            "page": 9,
            "x": 68,
            "y": 148,
            "width": 226,
            "height": 162,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "19-handling",
            "chunk_id": "69e76bce-8e52-4b40-9d44-c1b802db195b",
            "group_text": "4.1  Handling Non-Determinism and Sensitivity\n\nSources of non-determinism in LLMs stem from stochastic computations, chained model calls, and grounding data/API inconsistencies. Understanding how the pattern of non-determinism occurs is important for making accurate, conservative estimates of system performance given the potential noise it can produce. Note that non-determinism may occur irregularly or over long time windows. For example, estimating system noise only in the evenings when traffic is low may miss the non-determinism that occurs when a single request is broken into multiple sub-tasks that are processed independently under higher load.\"\n\nSelf-Consistency (Wang et al., 2022) is a repeated evaluation strategy for LLMs that can address non-determinism. It involves sampling multiple responses from similar prompts and selecting the most frequent. A common way to generate different responses is by setting the model\u2019s *temperature* parameter to a value greater than zero to enable diverse sampling for the same prompt. Self-Consistency reduces non-determinism by provid-\n\ning the response the system is most likely to generate for a given prompt, and may improve response quality as well. However, it increases the number of LLM calls ($n \\cdot$ # original calls), which may improve response quality but requires more resources and may adversely impact cost/latency.\n\nAnother way to address non-determinism is to\nestimate its effect directly, and adjust metrics es-\ntimates. For example, one could run the evalua-\ntion $n$-times on fixed inputs, average the results,\nand estimate the variability induced by the system\nnoise. By employing a sufficiently large choice of\n$n$, e.g., 10 (or larger), one can then establish an\nongoing baseline of system performance with re-\nported variations, then compare performance im-\npacting changes to this baseline to see if the met-\nrics fall outside the error bounds produced by the\ninherent system noise.\n\nLLM-based systems are sensitive to their inputs such that a small change in the wording, spelling, or even spacing of a prompt can lead to dramatic changes in the response. Thus, understanding how sensitive they are to small variations in the prompt is an important exercise, since these variations are likely to be presented by users in the wild.\n\nSensitivity can be assessed by injecting various types of noise into the prompt and measuring how the responses change. This practice is often part of a broader strategy for behavioral testing (Ribeiro et al., 2020) or robustness evaluation (Wang et al., 2021) and uses a range of perturbations to test model stability. To make this actionable, a variety of noise injection techniques can be employed, ranging from semantically neutral changes to more destructive ones that alter the underlying meaning. For this type of sensitivity evaluation, example ways to introduce prompt noise include case-swapping, random whitespace insertion, random character swapping, and using an LLM to rewrite the prompts. Once these variations are created, one can evaluate the impact of this sensitivity on:\n\n\u2022 Overall metrics: Identify metrics most affected by noise.\n\u2022 Eval set subgroups: Analyze sensitivity across different task types, topics, etc.\n\u2022 Individual data points: Identify data points with high noise susceptibility. These can be especially informative for discovering underlying issues with the prompts, or in the case of a larger composed LLM system, the up-\n\nstream components (e.g., RAG difficulties,\nsummarization failures, etc)."
        },
        {
            "text": "LLM-based systems are sensitive to their inputs such that a small change in the wording, spelling, or even spacing of a prompt can lead to dramatic changes in the response. Thus, understanding how sensitive they are to small variations in the prompt is an important exercise, since these variations are likely to be presented by users in the wild.",
            "page": 9,
            "x": 68,
            "y": 312,
            "width": 226,
            "height": 96,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "19-handling",
            "chunk_id": "5716c089-9e60-4126-9322-686b4e7eff89",
            "group_text": "4.1  Handling Non-Determinism and Sensitivity\n\nSources of non-determinism in LLMs stem from stochastic computations, chained model calls, and grounding data/API inconsistencies. Understanding how the pattern of non-determinism occurs is important for making accurate, conservative estimates of system performance given the potential noise it can produce. Note that non-determinism may occur irregularly or over long time windows. For example, estimating system noise only in the evenings when traffic is low may miss the non-determinism that occurs when a single request is broken into multiple sub-tasks that are processed independently under higher load.\"\n\nSelf-Consistency (Wang et al., 2022) is a repeated evaluation strategy for LLMs that can address non-determinism. It involves sampling multiple responses from similar prompts and selecting the most frequent. A common way to generate different responses is by setting the model\u2019s *temperature* parameter to a value greater than zero to enable diverse sampling for the same prompt. Self-Consistency reduces non-determinism by provid-\n\ning the response the system is most likely to generate for a given prompt, and may improve response quality as well. However, it increases the number of LLM calls ($n \\cdot$ # original calls), which may improve response quality but requires more resources and may adversely impact cost/latency.\n\nAnother way to address non-determinism is to\nestimate its effect directly, and adjust metrics es-\ntimates. For example, one could run the evalua-\ntion $n$-times on fixed inputs, average the results,\nand estimate the variability induced by the system\nnoise. By employing a sufficiently large choice of\n$n$, e.g., 10 (or larger), one can then establish an\nongoing baseline of system performance with re-\nported variations, then compare performance im-\npacting changes to this baseline to see if the met-\nrics fall outside the error bounds produced by the\ninherent system noise.\n\nLLM-based systems are sensitive to their inputs such that a small change in the wording, spelling, or even spacing of a prompt can lead to dramatic changes in the response. Thus, understanding how sensitive they are to small variations in the prompt is an important exercise, since these variations are likely to be presented by users in the wild.\n\nSensitivity can be assessed by injecting various types of noise into the prompt and measuring how the responses change. This practice is often part of a broader strategy for behavioral testing (Ribeiro et al., 2020) or robustness evaluation (Wang et al., 2021) and uses a range of perturbations to test model stability. To make this actionable, a variety of noise injection techniques can be employed, ranging from semantically neutral changes to more destructive ones that alter the underlying meaning. For this type of sensitivity evaluation, example ways to introduce prompt noise include case-swapping, random whitespace insertion, random character swapping, and using an LLM to rewrite the prompts. Once these variations are created, one can evaluate the impact of this sensitivity on:\n\n\u2022 Overall metrics: Identify metrics most affected by noise.\n\u2022 Eval set subgroups: Analyze sensitivity across different task types, topics, etc.\n\u2022 Individual data points: Identify data points with high noise susceptibility. These can be especially informative for discovering underlying issues with the prompts, or in the case of a larger composed LLM system, the up-\n\nstream components (e.g., RAG difficulties,\nsummarization failures, etc)."
        },
        {
            "text": "Sensitivity can be assessed by injecting various types of noise into the prompt and measuring how the responses change. This practice is often part of a broader strategy for behavioral testing (Ribeiro et al., 2020) or robustness evaluation (Wang et al., 2021) and uses a range of perturbations to test model stability. To make this actionable, a variety of noise injection techniques can be employed, ranging from semantically neutral changes to more destructive ones that alter the underlying meaning. For this type of sensitivity evaluation, example ways to introduce prompt noise include case-swapping, random whitespace insertion, random character swapping, and using an LLM to rewrite the prompts. Once these variations are created, one can evaluate the impact of this sensitivity on:\n\n\u2022 Overall metrics: Identify metrics most affected by noise.\n\u2022 Eval set subgroups: Analyze sensitivity across different task types, topics, etc.\n\u2022 Individual data points: Identify data points with high noise susceptibility. These can be especially informative for discovering underlying issues with the prompts, or in the case of a larger composed LLM system, the up-",
            "page": 9,
            "x": 68,
            "y": 411,
            "width": 226,
            "height": 355,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "19-handling",
            "chunk_id": "df4fa8fb-cda6-4d83-8ad7-f9b95e029102",
            "group_text": "4.1  Handling Non-Determinism and Sensitivity\n\nSources of non-determinism in LLMs stem from stochastic computations, chained model calls, and grounding data/API inconsistencies. Understanding how the pattern of non-determinism occurs is important for making accurate, conservative estimates of system performance given the potential noise it can produce. Note that non-determinism may occur irregularly or over long time windows. For example, estimating system noise only in the evenings when traffic is low may miss the non-determinism that occurs when a single request is broken into multiple sub-tasks that are processed independently under higher load.\"\n\nSelf-Consistency (Wang et al., 2022) is a repeated evaluation strategy for LLMs that can address non-determinism. It involves sampling multiple responses from similar prompts and selecting the most frequent. A common way to generate different responses is by setting the model\u2019s *temperature* parameter to a value greater than zero to enable diverse sampling for the same prompt. Self-Consistency reduces non-determinism by provid-\n\ning the response the system is most likely to generate for a given prompt, and may improve response quality as well. However, it increases the number of LLM calls ($n \\cdot$ # original calls), which may improve response quality but requires more resources and may adversely impact cost/latency.\n\nAnother way to address non-determinism is to\nestimate its effect directly, and adjust metrics es-\ntimates. For example, one could run the evalua-\ntion $n$-times on fixed inputs, average the results,\nand estimate the variability induced by the system\nnoise. By employing a sufficiently large choice of\n$n$, e.g., 10 (or larger), one can then establish an\nongoing baseline of system performance with re-\nported variations, then compare performance im-\npacting changes to this baseline to see if the met-\nrics fall outside the error bounds produced by the\ninherent system noise.\n\nLLM-based systems are sensitive to their inputs such that a small change in the wording, spelling, or even spacing of a prompt can lead to dramatic changes in the response. Thus, understanding how sensitive they are to small variations in the prompt is an important exercise, since these variations are likely to be presented by users in the wild.\n\nSensitivity can be assessed by injecting various types of noise into the prompt and measuring how the responses change. This practice is often part of a broader strategy for behavioral testing (Ribeiro et al., 2020) or robustness evaluation (Wang et al., 2021) and uses a range of perturbations to test model stability. To make this actionable, a variety of noise injection techniques can be employed, ranging from semantically neutral changes to more destructive ones that alter the underlying meaning. For this type of sensitivity evaluation, example ways to introduce prompt noise include case-swapping, random whitespace insertion, random character swapping, and using an LLM to rewrite the prompts. Once these variations are created, one can evaluate the impact of this sensitivity on:\n\n\u2022 Overall metrics: Identify metrics most affected by noise.\n\u2022 Eval set subgroups: Analyze sensitivity across different task types, topics, etc.\n\u2022 Individual data points: Identify data points with high noise susceptibility. These can be especially informative for discovering underlying issues with the prompts, or in the case of a larger composed LLM system, the up-\n\nstream components (e.g., RAG difficulties,\nsummarization failures, etc)."
        },
        {
            "text": "stream components (e.g., RAG difficulties,\nsummarization failures, etc).",
            "page": 9,
            "x": 325,
            "y": 63,
            "width": 203,
            "height": 28,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "19-handling",
            "chunk_id": "d1384a1c-c311-4814-989e-5b2fe071c132",
            "group_text": "4.1  Handling Non-Determinism and Sensitivity\n\nSources of non-determinism in LLMs stem from stochastic computations, chained model calls, and grounding data/API inconsistencies. Understanding how the pattern of non-determinism occurs is important for making accurate, conservative estimates of system performance given the potential noise it can produce. Note that non-determinism may occur irregularly or over long time windows. For example, estimating system noise only in the evenings when traffic is low may miss the non-determinism that occurs when a single request is broken into multiple sub-tasks that are processed independently under higher load.\"\n\nSelf-Consistency (Wang et al., 2022) is a repeated evaluation strategy for LLMs that can address non-determinism. It involves sampling multiple responses from similar prompts and selecting the most frequent. A common way to generate different responses is by setting the model\u2019s *temperature* parameter to a value greater than zero to enable diverse sampling for the same prompt. Self-Consistency reduces non-determinism by provid-\n\ning the response the system is most likely to generate for a given prompt, and may improve response quality as well. However, it increases the number of LLM calls ($n \\cdot$ # original calls), which may improve response quality but requires more resources and may adversely impact cost/latency.\n\nAnother way to address non-determinism is to\nestimate its effect directly, and adjust metrics es-\ntimates. For example, one could run the evalua-\ntion $n$-times on fixed inputs, average the results,\nand estimate the variability induced by the system\nnoise. By employing a sufficiently large choice of\n$n$, e.g., 10 (or larger), one can then establish an\nongoing baseline of system performance with re-\nported variations, then compare performance im-\npacting changes to this baseline to see if the met-\nrics fall outside the error bounds produced by the\ninherent system noise.\n\nLLM-based systems are sensitive to their inputs such that a small change in the wording, spelling, or even spacing of a prompt can lead to dramatic changes in the response. Thus, understanding how sensitive they are to small variations in the prompt is an important exercise, since these variations are likely to be presented by users in the wild.\n\nSensitivity can be assessed by injecting various types of noise into the prompt and measuring how the responses change. This practice is often part of a broader strategy for behavioral testing (Ribeiro et al., 2020) or robustness evaluation (Wang et al., 2021) and uses a range of perturbations to test model stability. To make this actionable, a variety of noise injection techniques can be employed, ranging from semantically neutral changes to more destructive ones that alter the underlying meaning. For this type of sensitivity evaluation, example ways to introduce prompt noise include case-swapping, random whitespace insertion, random character swapping, and using an LLM to rewrite the prompts. Once these variations are created, one can evaluate the impact of this sensitivity on:\n\n\u2022 Overall metrics: Identify metrics most affected by noise.\n\u2022 Eval set subgroups: Analyze sensitivity across different task types, topics, etc.\n\u2022 Individual data points: Identify data points with high noise susceptibility. These can be especially informative for discovering underlying issues with the prompts, or in the case of a larger composed LLM system, the up-\n\nstream components (e.g., RAG difficulties,\nsummarization failures, etc)."
        },
        {
            "text": "### 4.2 Comprehensive Evaluation of LLM System Components\n\nSeveral frameworks (e.g., OneTwo (Bousquet et al., 2024), LangChain (Chase, 2022)) exist for composing multiple LLM calls into sequences, with information contained in the output of one LLM call being used as input to another LLM call. Often, grounding data is retrieved and included as part of this process. Performance of these chained/composed systems is a function of the quality of their intermediate components (LLM calls, RAG/Gounding API calls, etc.). Thus, measuring quality of all intermediate outputs is important, as early-stage quality issues can induce downstream effects.",
            "page": 9,
            "x": 303,
            "y": 102,
            "width": 227,
            "height": 212,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "20-comprehensive",
            "chunk_id": "2788ad37-b7ec-4edc-9e9d-7ddf704c4d7d",
            "group_text": "### 4.2 Comprehensive Evaluation of LLM System Components\n\nSeveral frameworks (e.g., OneTwo (Bousquet et al., 2024), LangChain (Chase, 2022)) exist for composing multiple LLM calls into sequences, with information contained in the output of one LLM call being used as input to another LLM call. Often, grounding data is retrieved and included as part of this process. Performance of these chained/composed systems is a function of the quality of their intermediate components (LLM calls, RAG/Gounding API calls, etc.). Thus, measuring quality of all intermediate outputs is important, as early-stage quality issues can induce downstream effects.\n\nLLMs possess extensive prior knowledge from pre-training, biasing them to produce certain responses for a given input. In topic areas where they have substantial exposure, they may be able to produce high quality responses without grounding. However this reliance on prior knowledge can present difficulties when the LLM is faced with topic areas or questions it is less familiar with, leading to hallucinations. Ultimately, the purpose of grounding is to guide the model toward correct responses on topics that it knows less about.\n\nWhen grounding data is used, it is important to identify whether the correctness of the response comes from the provided context or a reliance on the model\u2019s prior knowledge. To do so, one must evaluate the relevance and completeness of the grounding data relative to the generated response. This can be operationalized through several methods: using human annotators with a scoring rubric, employing a separate autoterat LLM to score the context\u2019s utility, or calculating automated metrics like the semantic similarity between the prompt and the grounding documents. If the grounding data is found to be incomplete or irrelevant through these checks, the LLM may be relying too heavily on its prior knowledge. Overreliance can mask grounding issues, especially for specialized tasks where up-to-date grounding is essential. For further discussion of this tension, see (Gupta et al., 2024).\n\nWhere feasible, one can evaluate the end-to-end system performance both with and without certain grounding components. For example, given a\n\ngrounding extension (Lewis et al., 2020), we could\ncompute the same evaluation metrics both with\nand without grounding data \u2013 in the latter case pro-\nviding no grounding context, then evaluate perfor-\nmance using end-to-end metrics. This approach\nis relatively simple and directly addresses whether\nadding or removing a component contributes to or\ndiminishes the overall score. A disadvantage of\nablation evaluations is that they do a bad job mea-\nsuring individual component quality, when com-\nponent interactions propagate downstream.\n\nTo evaluate grounding modules directly, more traditional retrieval metrics can be used, including Precision ($P$), Recall ($R$), and $F_\\beta$ at $k$, Mean Reciprocal Rank (MRR), Normalized Discounted Cumulative Gain (NDCG), and Mean Average $P/R/F_\\beta$ over multiple retrieval depths."
        },
        {
            "text": "LLMs possess extensive prior knowledge from pre-training, biasing them to produce certain responses for a given input. In topic areas where they have substantial exposure, they may be able to produce high quality responses without grounding. However this reliance on prior knowledge can present difficulties when the LLM is faced with topic areas or questions it is less familiar with, leading to hallucinations. Ultimately, the purpose of grounding is to guide the model toward correct responses on topics that it knows less about.",
            "page": 9,
            "x": 303,
            "y": 316,
            "width": 226,
            "height": 149,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "20-comprehensive",
            "chunk_id": "1ce46ddb-07d3-4ba6-8982-845312c2b921",
            "group_text": "### 4.2 Comprehensive Evaluation of LLM System Components\n\nSeveral frameworks (e.g., OneTwo (Bousquet et al., 2024), LangChain (Chase, 2022)) exist for composing multiple LLM calls into sequences, with information contained in the output of one LLM call being used as input to another LLM call. Often, grounding data is retrieved and included as part of this process. Performance of these chained/composed systems is a function of the quality of their intermediate components (LLM calls, RAG/Gounding API calls, etc.). Thus, measuring quality of all intermediate outputs is important, as early-stage quality issues can induce downstream effects.\n\nLLMs possess extensive prior knowledge from pre-training, biasing them to produce certain responses for a given input. In topic areas where they have substantial exposure, they may be able to produce high quality responses without grounding. However this reliance on prior knowledge can present difficulties when the LLM is faced with topic areas or questions it is less familiar with, leading to hallucinations. Ultimately, the purpose of grounding is to guide the model toward correct responses on topics that it knows less about.\n\nWhen grounding data is used, it is important to identify whether the correctness of the response comes from the provided context or a reliance on the model\u2019s prior knowledge. To do so, one must evaluate the relevance and completeness of the grounding data relative to the generated response. This can be operationalized through several methods: using human annotators with a scoring rubric, employing a separate autoterat LLM to score the context\u2019s utility, or calculating automated metrics like the semantic similarity between the prompt and the grounding documents. If the grounding data is found to be incomplete or irrelevant through these checks, the LLM may be relying too heavily on its prior knowledge. Overreliance can mask grounding issues, especially for specialized tasks where up-to-date grounding is essential. For further discussion of this tension, see (Gupta et al., 2024).\n\nWhere feasible, one can evaluate the end-to-end system performance both with and without certain grounding components. For example, given a\n\ngrounding extension (Lewis et al., 2020), we could\ncompute the same evaluation metrics both with\nand without grounding data \u2013 in the latter case pro-\nviding no grounding context, then evaluate perfor-\nmance using end-to-end metrics. This approach\nis relatively simple and directly addresses whether\nadding or removing a component contributes to or\ndiminishes the overall score. A disadvantage of\nablation evaluations is that they do a bad job mea-\nsuring individual component quality, when com-\nponent interactions propagate downstream.\n\nTo evaluate grounding modules directly, more traditional retrieval metrics can be used, including Precision ($P$), Recall ($R$), and $F_\\beta$ at $k$, Mean Reciprocal Rank (MRR), Normalized Discounted Cumulative Gain (NDCG), and Mean Average $P/R/F_\\beta$ over multiple retrieval depths."
        },
        {
            "text": "When grounding data is used, it is important to identify whether the correctness of the response comes from the provided context or a reliance on the model\u2019s prior knowledge. To do so, one must evaluate the relevance and completeness of the grounding data relative to the generated response. This can be operationalized through several methods: using human annotators with a scoring rubric, employing a separate autoterat LLM to score the context\u2019s utility, or calculating automated metrics like the semantic similarity between the prompt and the grounding documents. If the grounding data is found to be incomplete or irrelevant through these checks, the LLM may be relying too heavily on its prior knowledge. Overreliance can mask grounding issues, especially for specialized tasks where up-to-date grounding is essential. For further discussion of this tension, see (Gupta et al., 2024).",
            "page": 9,
            "x": 303,
            "y": 466,
            "width": 226,
            "height": 257,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "20-comprehensive",
            "chunk_id": "3d08b8c3-b834-4cb4-bc43-a05c690259c8",
            "group_text": "### 4.2 Comprehensive Evaluation of LLM System Components\n\nSeveral frameworks (e.g., OneTwo (Bousquet et al., 2024), LangChain (Chase, 2022)) exist for composing multiple LLM calls into sequences, with information contained in the output of one LLM call being used as input to another LLM call. Often, grounding data is retrieved and included as part of this process. Performance of these chained/composed systems is a function of the quality of their intermediate components (LLM calls, RAG/Gounding API calls, etc.). Thus, measuring quality of all intermediate outputs is important, as early-stage quality issues can induce downstream effects.\n\nLLMs possess extensive prior knowledge from pre-training, biasing them to produce certain responses for a given input. In topic areas where they have substantial exposure, they may be able to produce high quality responses without grounding. However this reliance on prior knowledge can present difficulties when the LLM is faced with topic areas or questions it is less familiar with, leading to hallucinations. Ultimately, the purpose of grounding is to guide the model toward correct responses on topics that it knows less about.\n\nWhen grounding data is used, it is important to identify whether the correctness of the response comes from the provided context or a reliance on the model\u2019s prior knowledge. To do so, one must evaluate the relevance and completeness of the grounding data relative to the generated response. This can be operationalized through several methods: using human annotators with a scoring rubric, employing a separate autoterat LLM to score the context\u2019s utility, or calculating automated metrics like the semantic similarity between the prompt and the grounding documents. If the grounding data is found to be incomplete or irrelevant through these checks, the LLM may be relying too heavily on its prior knowledge. Overreliance can mask grounding issues, especially for specialized tasks where up-to-date grounding is essential. For further discussion of this tension, see (Gupta et al., 2024).\n\nWhere feasible, one can evaluate the end-to-end system performance both with and without certain grounding components. For example, given a\n\ngrounding extension (Lewis et al., 2020), we could\ncompute the same evaluation metrics both with\nand without grounding data \u2013 in the latter case pro-\nviding no grounding context, then evaluate perfor-\nmance using end-to-end metrics. This approach\nis relatively simple and directly addresses whether\nadding or removing a component contributes to or\ndiminishes the overall score. A disadvantage of\nablation evaluations is that they do a bad job mea-\nsuring individual component quality, when com-\nponent interactions propagate downstream.\n\nTo evaluate grounding modules directly, more traditional retrieval metrics can be used, including Precision ($P$), Recall ($R$), and $F_\\beta$ at $k$, Mean Reciprocal Rank (MRR), Normalized Discounted Cumulative Gain (NDCG), and Mean Average $P/R/F_\\beta$ over multiple retrieval depths."
        },
        {
            "text": "Where feasible, one can evaluate the end-to-end system performance both with and without certain grounding components. For example, given a",
            "page": 9,
            "x": 304,
            "y": 725,
            "width": 226,
            "height": 42,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "20-comprehensive",
            "chunk_id": "d8eb1b9c-72c8-4d92-bd75-55ff0feed7ba",
            "group_text": "### 4.2 Comprehensive Evaluation of LLM System Components\n\nSeveral frameworks (e.g., OneTwo (Bousquet et al., 2024), LangChain (Chase, 2022)) exist for composing multiple LLM calls into sequences, with information contained in the output of one LLM call being used as input to another LLM call. Often, grounding data is retrieved and included as part of this process. Performance of these chained/composed systems is a function of the quality of their intermediate components (LLM calls, RAG/Gounding API calls, etc.). Thus, measuring quality of all intermediate outputs is important, as early-stage quality issues can induce downstream effects.\n\nLLMs possess extensive prior knowledge from pre-training, biasing them to produce certain responses for a given input. In topic areas where they have substantial exposure, they may be able to produce high quality responses without grounding. However this reliance on prior knowledge can present difficulties when the LLM is faced with topic areas or questions it is less familiar with, leading to hallucinations. Ultimately, the purpose of grounding is to guide the model toward correct responses on topics that it knows less about.\n\nWhen grounding data is used, it is important to identify whether the correctness of the response comes from the provided context or a reliance on the model\u2019s prior knowledge. To do so, one must evaluate the relevance and completeness of the grounding data relative to the generated response. This can be operationalized through several methods: using human annotators with a scoring rubric, employing a separate autoterat LLM to score the context\u2019s utility, or calculating automated metrics like the semantic similarity between the prompt and the grounding documents. If the grounding data is found to be incomplete or irrelevant through these checks, the LLM may be relying too heavily on its prior knowledge. Overreliance can mask grounding issues, especially for specialized tasks where up-to-date grounding is essential. For further discussion of this tension, see (Gupta et al., 2024).\n\nWhere feasible, one can evaluate the end-to-end system performance both with and without certain grounding components. For example, given a\n\ngrounding extension (Lewis et al., 2020), we could\ncompute the same evaluation metrics both with\nand without grounding data \u2013 in the latter case pro-\nviding no grounding context, then evaluate perfor-\nmance using end-to-end metrics. This approach\nis relatively simple and directly addresses whether\nadding or removing a component contributes to or\ndiminishes the overall score. A disadvantage of\nablation evaluations is that they do a bad job mea-\nsuring individual component quality, when com-\nponent interactions propagate downstream.\n\nTo evaluate grounding modules directly, more traditional retrieval metrics can be used, including Precision ($P$), Recall ($R$), and $F_\\beta$ at $k$, Mean Reciprocal Rank (MRR), Normalized Discounted Cumulative Gain (NDCG), and Mean Average $P/R/F_\\beta$ over multiple retrieval depths."
        },
        {
            "text": "grounding extension (Lewis et al., 2020), we could\ncompute the same evaluation metrics both with\nand without grounding data \u2013 in the latter case pro-\nviding no grounding context, then evaluate perfor-\nmance using end-to-end metrics. This approach\nis relatively simple and directly addresses whether\nadding or removing a component contributes to or\ndiminishes the overall score. A disadvantage of\nablation evaluations is that they do a bad job mea-\nsuring individual component quality, when com-\nponent interactions propagate downstream.",
            "page": 10,
            "x": 67,
            "y": 62,
            "width": 226,
            "height": 151,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "20-comprehensive",
            "chunk_id": "5ca674d1-ef9e-4713-92a2-ac5b5d716f0c",
            "group_text": "### 4.2 Comprehensive Evaluation of LLM System Components\n\nSeveral frameworks (e.g., OneTwo (Bousquet et al., 2024), LangChain (Chase, 2022)) exist for composing multiple LLM calls into sequences, with information contained in the output of one LLM call being used as input to another LLM call. Often, grounding data is retrieved and included as part of this process. Performance of these chained/composed systems is a function of the quality of their intermediate components (LLM calls, RAG/Gounding API calls, etc.). Thus, measuring quality of all intermediate outputs is important, as early-stage quality issues can induce downstream effects.\n\nLLMs possess extensive prior knowledge from pre-training, biasing them to produce certain responses for a given input. In topic areas where they have substantial exposure, they may be able to produce high quality responses without grounding. However this reliance on prior knowledge can present difficulties when the LLM is faced with topic areas or questions it is less familiar with, leading to hallucinations. Ultimately, the purpose of grounding is to guide the model toward correct responses on topics that it knows less about.\n\nWhen grounding data is used, it is important to identify whether the correctness of the response comes from the provided context or a reliance on the model\u2019s prior knowledge. To do so, one must evaluate the relevance and completeness of the grounding data relative to the generated response. This can be operationalized through several methods: using human annotators with a scoring rubric, employing a separate autoterat LLM to score the context\u2019s utility, or calculating automated metrics like the semantic similarity between the prompt and the grounding documents. If the grounding data is found to be incomplete or irrelevant through these checks, the LLM may be relying too heavily on its prior knowledge. Overreliance can mask grounding issues, especially for specialized tasks where up-to-date grounding is essential. For further discussion of this tension, see (Gupta et al., 2024).\n\nWhere feasible, one can evaluate the end-to-end system performance both with and without certain grounding components. For example, given a\n\ngrounding extension (Lewis et al., 2020), we could\ncompute the same evaluation metrics both with\nand without grounding data \u2013 in the latter case pro-\nviding no grounding context, then evaluate perfor-\nmance using end-to-end metrics. This approach\nis relatively simple and directly addresses whether\nadding or removing a component contributes to or\ndiminishes the overall score. A disadvantage of\nablation evaluations is that they do a bad job mea-\nsuring individual component quality, when com-\nponent interactions propagate downstream.\n\nTo evaluate grounding modules directly, more traditional retrieval metrics can be used, including Precision ($P$), Recall ($R$), and $F_\\beta$ at $k$, Mean Reciprocal Rank (MRR), Normalized Discounted Cumulative Gain (NDCG), and Mean Average $P/R/F_\\beta$ over multiple retrieval depths."
        },
        {
            "text": "To evaluate grounding modules directly, more traditional retrieval metrics can be used, including Precision ($P$), Recall ($R$), and $F_\\beta$ at $k$, Mean Reciprocal Rank (MRR), Normalized Discounted Cumulative Gain (NDCG), and Mean Average $P/R/F_\\beta$ over multiple retrieval depths.",
            "page": 10,
            "x": 68,
            "y": 214,
            "width": 226,
            "height": 83,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "20-comprehensive",
            "chunk_id": "4c517a7a-e6e2-4dd8-aec7-4a9cb60f7910",
            "group_text": "### 4.2 Comprehensive Evaluation of LLM System Components\n\nSeveral frameworks (e.g., OneTwo (Bousquet et al., 2024), LangChain (Chase, 2022)) exist for composing multiple LLM calls into sequences, with information contained in the output of one LLM call being used as input to another LLM call. Often, grounding data is retrieved and included as part of this process. Performance of these chained/composed systems is a function of the quality of their intermediate components (LLM calls, RAG/Gounding API calls, etc.). Thus, measuring quality of all intermediate outputs is important, as early-stage quality issues can induce downstream effects.\n\nLLMs possess extensive prior knowledge from pre-training, biasing them to produce certain responses for a given input. In topic areas where they have substantial exposure, they may be able to produce high quality responses without grounding. However this reliance on prior knowledge can present difficulties when the LLM is faced with topic areas or questions it is less familiar with, leading to hallucinations. Ultimately, the purpose of grounding is to guide the model toward correct responses on topics that it knows less about.\n\nWhen grounding data is used, it is important to identify whether the correctness of the response comes from the provided context or a reliance on the model\u2019s prior knowledge. To do so, one must evaluate the relevance and completeness of the grounding data relative to the generated response. This can be operationalized through several methods: using human annotators with a scoring rubric, employing a separate autoterat LLM to score the context\u2019s utility, or calculating automated metrics like the semantic similarity between the prompt and the grounding documents. If the grounding data is found to be incomplete or irrelevant through these checks, the LLM may be relying too heavily on its prior knowledge. Overreliance can mask grounding issues, especially for specialized tasks where up-to-date grounding is essential. For further discussion of this tension, see (Gupta et al., 2024).\n\nWhere feasible, one can evaluate the end-to-end system performance both with and without certain grounding components. For example, given a\n\ngrounding extension (Lewis et al., 2020), we could\ncompute the same evaluation metrics both with\nand without grounding data \u2013 in the latter case pro-\nviding no grounding context, then evaluate perfor-\nmance using end-to-end metrics. This approach\nis relatively simple and directly addresses whether\nadding or removing a component contributes to or\ndiminishes the overall score. A disadvantage of\nablation evaluations is that they do a bad job mea-\nsuring individual component quality, when com-\nponent interactions propagate downstream.\n\nTo evaluate grounding modules directly, more traditional retrieval metrics can be used, including Precision ($P$), Recall ($R$), and $F_\\beta$ at $k$, Mean Reciprocal Rank (MRR), Normalized Discounted Cumulative Gain (NDCG), and Mean Average $P/R/F_\\beta$ over multiple retrieval depths."
        },
        {
            "text": "4.3   Hallucination & Unhelpful Responses\n\nWhen factuality of LLM responses is critical (e.g., for Q/A tasks), directly estimating hallucination rates may be feasible by probing the LLM with fictitious entities. This can be accomplished by generating a series of questions with fictitious entities and observing if the LLM provides information about them or responds with \"I don\u2019t know\" or similar. These \"I don\u2019t know\" type responses can be identified heuristically or with a classifier for greater accuracy. For example, \"Tell me about CVE-2037-1234567\" should result in an \"I don\u2019t know\" or \"does not exist\" type of response (CVE-2037-1234567 does not exist because at the time of writing this 2037 is in the future).",
            "page": 10,
            "x": 67,
            "y": 308,
            "width": 226,
            "height": 213,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "21-hallucination",
            "chunk_id": "45a338bd-be35-45a2-b253-a1f604528e3e",
            "group_text": "4.3   Hallucination & Unhelpful Responses\n\nWhen factuality of LLM responses is critical (e.g., for Q/A tasks), directly estimating hallucination rates may be feasible by probing the LLM with fictitious entities. This can be accomplished by generating a series of questions with fictitious entities and observing if the LLM provides information about them or responds with \"I don\u2019t know\" or similar. These \"I don\u2019t know\" type responses can be identified heuristically or with a classifier for greater accuracy. For example, \"Tell me about CVE-2037-1234567\" should result in an \"I don\u2019t know\" or \"does not exist\" type of response (CVE-2037-1234567 does not exist because at the time of writing this 2037 is in the future).\n\nMeasuring an LLM\u2019s non-response rate is crucial, especially when answers exist and good grounding data is provided. An undesirable non-response occurs when there is sufficient grounding data and/or prior knowledge for the LLM to respond, but it fails to do so opting instead to reply with an \"I don\u2019t know\" type of response. This phenomenon can occur when LLM hallucination is an issue and prompt instructions are provided to reduce hallucination. There is an inherent tension between controlling hallucination via prompt instructions, and eliciting undesirable non-response. Overly cautious prompts aimed at reducing hallucinations can inadvertently increase non-response rates. Instructions like \"If you don\u2019t know the answer, simply say \u2018I don\u2019t know\u2019\" may cause a non-response even when sufficient grounding data is provided.\n\nTo measure undesirable non-response in the Q/A setting, one can generate a dataset of prompts with questions about known (i.e. non-fictitious) entities and evaluate the frequency of \"I don\u2019t know\" or \"does not exist\" types of responses to them, either heuristically or ideally with an \"I don\u2019t know\" classifier. One can then analyze the rate at which the LLM fails to respond to questions that it should, given the grounding and expectations about prior knowledge, be able to answer and revise prompts as necessary."
        },
        {
            "text": "Measuring an LLM\u2019s non-response rate is crucial, especially when answers exist and good grounding data is provided. An undesirable non-response occurs when there is sufficient grounding data and/or prior knowledge for the LLM to respond, but it fails to do so opting instead to reply with an \"I don\u2019t know\" type of response. This phenomenon can occur when LLM hallucination is an issue and prompt instructions are provided to reduce hallucination. There is an inherent tension between controlling hallucination via prompt instructions, and eliciting undesirable non-response. Overly cautious prompts aimed at reducing hallucinations can inadvertently increase non-response rates. Instructions like \"If you don\u2019t know the answer, simply say \u2018I don\u2019t know\u2019\" may cause a non-response even when sufficient grounding data is provided.",
            "page": 10,
            "x": 68,
            "y": 522,
            "width": 226,
            "height": 245,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "21-hallucination",
            "chunk_id": "7ea9b954-a225-46a5-831b-6d631d7a3b26",
            "group_text": "4.3   Hallucination & Unhelpful Responses\n\nWhen factuality of LLM responses is critical (e.g., for Q/A tasks), directly estimating hallucination rates may be feasible by probing the LLM with fictitious entities. This can be accomplished by generating a series of questions with fictitious entities and observing if the LLM provides information about them or responds with \"I don\u2019t know\" or similar. These \"I don\u2019t know\" type responses can be identified heuristically or with a classifier for greater accuracy. For example, \"Tell me about CVE-2037-1234567\" should result in an \"I don\u2019t know\" or \"does not exist\" type of response (CVE-2037-1234567 does not exist because at the time of writing this 2037 is in the future).\n\nMeasuring an LLM\u2019s non-response rate is crucial, especially when answers exist and good grounding data is provided. An undesirable non-response occurs when there is sufficient grounding data and/or prior knowledge for the LLM to respond, but it fails to do so opting instead to reply with an \"I don\u2019t know\" type of response. This phenomenon can occur when LLM hallucination is an issue and prompt instructions are provided to reduce hallucination. There is an inherent tension between controlling hallucination via prompt instructions, and eliciting undesirable non-response. Overly cautious prompts aimed at reducing hallucinations can inadvertently increase non-response rates. Instructions like \"If you don\u2019t know the answer, simply say \u2018I don\u2019t know\u2019\" may cause a non-response even when sufficient grounding data is provided.\n\nTo measure undesirable non-response in the Q/A setting, one can generate a dataset of prompts with questions about known (i.e. non-fictitious) entities and evaluate the frequency of \"I don\u2019t know\" or \"does not exist\" types of responses to them, either heuristically or ideally with an \"I don\u2019t know\" classifier. One can then analyze the rate at which the LLM fails to respond to questions that it should, given the grounding and expectations about prior knowledge, be able to answer and revise prompts as necessary."
        },
        {
            "text": "To measure undesirable non-response in the Q/A setting, one can generate a dataset of prompts with questions about known (i.e. non-fictitious) entities and evaluate the frequency of \"I don\u2019t know\" or \"does not exist\" types of responses to them, either heuristically or ideally with an \"I don\u2019t know\" classifier. One can then analyze the rate at which the LLM fails to respond to questions that it should, given the grounding and expectations about prior knowledge, be able to answer and revise prompts as necessary.",
            "page": 10,
            "x": 303,
            "y": 62,
            "width": 226,
            "height": 152,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "21-hallucination",
            "chunk_id": "636a074b-b2c3-4366-a61c-18e3a8b89b44",
            "group_text": "4.3   Hallucination & Unhelpful Responses\n\nWhen factuality of LLM responses is critical (e.g., for Q/A tasks), directly estimating hallucination rates may be feasible by probing the LLM with fictitious entities. This can be accomplished by generating a series of questions with fictitious entities and observing if the LLM provides information about them or responds with \"I don\u2019t know\" or similar. These \"I don\u2019t know\" type responses can be identified heuristically or with a classifier for greater accuracy. For example, \"Tell me about CVE-2037-1234567\" should result in an \"I don\u2019t know\" or \"does not exist\" type of response (CVE-2037-1234567 does not exist because at the time of writing this 2037 is in the future).\n\nMeasuring an LLM\u2019s non-response rate is crucial, especially when answers exist and good grounding data is provided. An undesirable non-response occurs when there is sufficient grounding data and/or prior knowledge for the LLM to respond, but it fails to do so opting instead to reply with an \"I don\u2019t know\" type of response. This phenomenon can occur when LLM hallucination is an issue and prompt instructions are provided to reduce hallucination. There is an inherent tension between controlling hallucination via prompt instructions, and eliciting undesirable non-response. Overly cautious prompts aimed at reducing hallucinations can inadvertently increase non-response rates. Instructions like \"If you don\u2019t know the answer, simply say \u2018I don\u2019t know\u2019\" may cause a non-response even when sufficient grounding data is provided.\n\nTo measure undesirable non-response in the Q/A setting, one can generate a dataset of prompts with questions about known (i.e. non-fictitious) entities and evaluate the frequency of \"I don\u2019t know\" or \"does not exist\" types of responses to them, either heuristically or ideally with an \"I don\u2019t know\" classifier. One can then analyze the rate at which the LLM fails to respond to questions that it should, given the grounding and expectations about prior knowledge, be able to answer and revise prompts as necessary."
        },
        {
            "text": "4.4  Wrap-Up: Connecting Methodology to the Evaluation Framework\n\nThe strategies for robust evaluation and execution outlined in this section are the third foundational pillar in our evaluation framework, and can be incorporated with the previously discussed datasets and metrics into a functioning evaluation suite. This section has discussed the practical methodologies required to handle the inherent challenges of LLM-reliant systems, such as non-determinism, prompt sensitivity, and the difficulty of measuring complex failures like hallucinations. By directly addressing these confounding factors, this pillar ensures the evaluation\u2019s outputs are reliable and can serve as the actionable guidance needed for Analysis and Iteration (see Figure 1).",
            "page": 10,
            "x": 302,
            "y": 219,
            "width": 227,
            "height": 224,
            "color": "yellowgreen",
            "border": "dashed",
            "background_color": "white",
            "group": "22-wrap",
            "chunk_id": "2ed35465-e99a-4a5c-893b-11ec6ff12c3b",
            "group_text": "4.4  Wrap-Up: Connecting Methodology to the Evaluation Framework\n\nThe strategies for robust evaluation and execution outlined in this section are the third foundational pillar in our evaluation framework, and can be incorporated with the previously discussed datasets and metrics into a functioning evaluation suite. This section has discussed the practical methodologies required to handle the inherent challenges of LLM-reliant systems, such as non-determinism, prompt sensitivity, and the difficulty of measuring complex failures like hallucinations. By directly addressing these confounding factors, this pillar ensures the evaluation\u2019s outputs are reliable and can serve as the actionable guidance needed for Analysis and Iteration (see Figure 1)."
        },
        {
            "text": "5  Putting it All Together\n\nA successful evaluation strategy integrates the core pillars of this paper\u2014Datasets, Metrics, and Methodology\u2014into a single, iterative process. The process begins by defining clear objectives to guide the formulation of representative datasets. These datasets inform the selection of a balanced suite of metrics, and a robust methodology is then applied to execute the evaluation, addressing challenges like non-determinism, prompt sensitivity, and hallucinations. Ultimately, the results should be interpreted not as a final grade, but as actionable guidance for system improvements and decisions. This approach transforms evaluation from a one-off validation exercise into a continuous loop where the evaluation suite matures and evolves in lockstep with the AI system it measures.",
            "page": 10,
            "x": 303,
            "y": 450,
            "width": 227,
            "height": 241,
            "color": "orange",
            "border": "dashed",
            "background_color": "white",
            "group": "23-putting",
            "chunk_id": "d9b8634c-0ede-4b3a-be3b-ca0d76ac7bbc",
            "group_text": "5  Putting it All Together\n\nA successful evaluation strategy integrates the core pillars of this paper\u2014Datasets, Metrics, and Methodology\u2014into a single, iterative process. The process begins by defining clear objectives to guide the formulation of representative datasets. These datasets inform the selection of a balanced suite of metrics, and a robust methodology is then applied to execute the evaluation, addressing challenges like non-determinism, prompt sensitivity, and hallucinations. Ultimately, the results should be interpreted not as a final grade, but as actionable guidance for system improvements and decisions. This approach transforms evaluation from a one-off validation exercise into a continuous loop where the evaluation suite matures and evolves in lockstep with the AI system it measures."
        }
    ]
}